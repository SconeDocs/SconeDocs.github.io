{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to SCONE L1TF (Foreshadow) Mitigation (2018-08-21) Given the recent CPU/SGX vulnerabilities like L1TF, you need to switch off hyperthreading and update your microcode . SCONE supports now R (2018-08-18) We have added experimental support for R . Not that we already had support for PySpark as well as TensorFlow Lite . Improved protection against side channel attacks While SGX is affected by Spectre, we are currently adding protection against Spectre (variants 1 and 2) as well as L1 and L2 side channel attacks to SCONE. Our work is presented at Usenix ATC 2018 . Executive Summary SCONE is a platform to build and run secure applications with the help of Intel SGX (Software Guard eXtensions) 1 . In a nutshell, our objective is to run applications such that data is always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against attackers with root access . Our aim is it to make it as easy as possible to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran. Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in - even into SCONE. SCONE can be used on top of Kubernetes, Ranger and Docker. We provide a tight integration with Docker Swarm. In case you are already using Docker stack files , we provide a simple way to secure your services and applications. We will provide a similar integration with Kubernetes later in 2018. SCONE scales better than competing solutions since it uses an advanced thread management and a very efficient way how to perform system calls . SCONE has an integrated secrets and configuration management - simplifying the distribution of secrets without application changes by performing an attestation of applications. While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux. So, what problems can SCONE help to solve? Secure application configuration SCONE provides applications with secrets in a secure fashion. Why is that a problem? Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways: secure configuration files . SCONE can transparently decrypt encrypted configuration files, i.e., without the need to modify the application. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work. secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. Why would I need this? Consider the MySQL example from above. You can pass user passwords via environment variables like MYSQL_ROOT_PASSWORD and MYSQL_PASSWORD to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database. secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments. Transparent attestation SCONE verifies that the correct code is running before passing any configuration info to the application. To ensure this, SCONE provides a local attestation and configuration service : this service provides only the code with the correct signature ( MrEnclave ) with its secrets: certificates, arguments, environment variables and keys. It also provides the application with a certificate that shows that the application runs inside an enclave. Note that this can be done completely transparent to the application, i.e., no application source code changes are required: the encrypted certificate can be stored in the file system where the application expects its certificates. For debugging and development, you can run code inside of enclaves without attestation. Two applications can ensure that they run inside enclaves via TLS authentication. In this way we can ensure that the client certificate and the server certificate was issued by the SCONE CAS, i.e., both communication partners run inside of enclaves and have the expected MrEnclave . Secure main memory An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to protect the main memory : no access by adversaries - even those who have root access, no access by the operating system - even if compromised, no access by the hypervisor - even if compromised, and no access by the cloud provider, and no access by evil maids - despite having physical access to the host. Integration with secure key store Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one can generate keys and stores in SCONE CAS. SCONE also supports the integration with a keystore like Vault. SCONE can run Vault inside of an enclave to protect Vaults secrets in main memory. Transparent TLS encryption Some popular applications like memcached or Zookeeper 2 do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections: the connections are terminated inside of the enclave. In this way, the plain text is never seen by the operating system or any adversary. Note that one should not use an external process for TLS termination 3 . Transparent file protection SCONE protects the integrity and confidentiality of files via transparent file protection . This protection does not require any source code changes. A file can either be integrity-protected only (i.e., the file is stored in plain text but modifications are detected) or confidentiality- and integrity-protected (i.e., the file is encrypted and modifications are detected). Ease of use We provide slightly extended Docker stack files to start an application consisting of a set of services inside of enclaves. scontain.com , August 2018. Questions or Suggestions? We plan to support alternative trusted execution environments in future releases of SCONE. Zookeeper replicates its state amongst a group of servers. Zookeeper does not support protecting the communication between these servers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected. Memcached could be protected, for example, with the help of a stunnel . The communication between memcached and stunnel is not encrypted and hence, adversaries with root access would see the unencrypted traffic.","title":"Introduction"},{"location":"#welcome-to-scone","text":"L1TF (Foreshadow) Mitigation (2018-08-21) Given the recent CPU/SGX vulnerabilities like L1TF, you need to switch off hyperthreading and update your microcode . SCONE supports now R (2018-08-18) We have added experimental support for R . Not that we already had support for PySpark as well as TensorFlow Lite . Improved protection against side channel attacks While SGX is affected by Spectre, we are currently adding protection against Spectre (variants 1 and 2) as well as L1 and L2 side channel attacks to SCONE. Our work is presented at Usenix ATC 2018 .","title":"Welcome to SCONE"},{"location":"#executive-summary","text":"SCONE is a platform to build and run secure applications with the help of Intel SGX (Software Guard eXtensions) 1 . In a nutshell, our objective is to run applications such that data is always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against attackers with root access . Our aim is it to make it as easy as possible to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran. Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in - even into SCONE. SCONE can be used on top of Kubernetes, Ranger and Docker. We provide a tight integration with Docker Swarm. In case you are already using Docker stack files , we provide a simple way to secure your services and applications. We will provide a similar integration with Kubernetes later in 2018. SCONE scales better than competing solutions since it uses an advanced thread management and a very efficient way how to perform system calls . SCONE has an integrated secrets and configuration management - simplifying the distribution of secrets without application changes by performing an attestation of applications. While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux.","title":"Executive Summary"},{"location":"#so-what-problems-can-scone-help-to-solve","text":"","title":"So, what problems can SCONE help to solve?"},{"location":"#secure-application-configuration","text":"SCONE provides applications with secrets in a secure fashion. Why is that a problem? Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways: secure configuration files . SCONE can transparently decrypt encrypted configuration files, i.e., without the need to modify the application. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work. secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. Why would I need this? Consider the MySQL example from above. You can pass user passwords via environment variables like MYSQL_ROOT_PASSWORD and MYSQL_PASSWORD to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database. secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments.","title":"Secure application configuration"},{"location":"#transparent-attestation","text":"SCONE verifies that the correct code is running before passing any configuration info to the application. To ensure this, SCONE provides a local attestation and configuration service : this service provides only the code with the correct signature ( MrEnclave ) with its secrets: certificates, arguments, environment variables and keys. It also provides the application with a certificate that shows that the application runs inside an enclave. Note that this can be done completely transparent to the application, i.e., no application source code changes are required: the encrypted certificate can be stored in the file system where the application expects its certificates. For debugging and development, you can run code inside of enclaves without attestation. Two applications can ensure that they run inside enclaves via TLS authentication. In this way we can ensure that the client certificate and the server certificate was issued by the SCONE CAS, i.e., both communication partners run inside of enclaves and have the expected MrEnclave .","title":"Transparent attestation"},{"location":"#secure-main-memory","text":"An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to protect the main memory : no access by adversaries - even those who have root access, no access by the operating system - even if compromised, no access by the hypervisor - even if compromised, and no access by the cloud provider, and no access by evil maids - despite having physical access to the host.","title":"Secure main memory"},{"location":"#integration-with-secure-key-store","text":"Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one can generate keys and stores in SCONE CAS. SCONE also supports the integration with a keystore like Vault. SCONE can run Vault inside of an enclave to protect Vaults secrets in main memory.","title":"Integration with secure key store"},{"location":"#transparent-tls-encryption","text":"Some popular applications like memcached or Zookeeper 2 do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections: the connections are terminated inside of the enclave. In this way, the plain text is never seen by the operating system or any adversary. Note that one should not use an external process for TLS termination 3 .","title":"Transparent TLS encryption"},{"location":"#transparent-file-protection","text":"SCONE protects the integrity and confidentiality of files via transparent file protection . This protection does not require any source code changes. A file can either be integrity-protected only (i.e., the file is stored in plain text but modifications are detected) or confidentiality- and integrity-protected (i.e., the file is encrypted and modifications are detected).","title":"Transparent file protection"},{"location":"#ease-of-use","text":"We provide slightly extended Docker stack files to start an application consisting of a set of services inside of enclaves. scontain.com , August 2018. Questions or Suggestions? We plan to support alternative trusted execution environments in future releases of SCONE. Zookeeper replicates its state amongst a group of servers. Zookeeper does not support protecting the communication between these servers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected. Memcached could be protected, for example, with the help of a stunnel . The communication between memcached and stunnel is not encrypted and hence, adversaries with root access would see the unencrypted traffic.","title":"Ease of use"},{"location":"C++/","text":"C++ Program Language Support SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation. Cross-compilation is required to support, in particular, statically linked binaries. This page focuses on the SCONE C++ cross compiler scone g++ (a.k.a. scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++. Image Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Help If you need some help, just execute in the container: $ scone g++ --help Usage: x86_64-linux-musl-g++ [ options ] file... Options: ... Example Let's try to compile a simple program: cat sqrt.cc EOF #include iostream #include cmath using namespace std; int main() { int x = 0; while(x 10) { double y = sqrt((double)x); cout The square root of x is y endl; x++; } return 0; } EOF We compile the program with scone gcc or scone-gcc : scone g++ sqrt.cc -o sqrt Let's execute the binary and switch on debug outputs: SCONE_VERSION = 1 ./sqrt The output will look like: xport SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: ebf98279a2cae1179366f8b5a0fc007decdc5dd3dec2b92ddbf121c2e2bf22f4 The square root of 0 is 0 The square root of 1 is 1 The square root of 2 is 1.41421 The square root of 3 is 1.73205 The square root of 4 is 2 The square root of 5 is 2.23607 The square root of 6 is 2.44949 The square root of 7 is 2.64575 The square root of 8 is 2.82843 The square root of 9 is 3 Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. scontain.com , July 2018. Questions or Suggestions?","title":"C++"},{"location":"C++/#c-program-language-support","text":"SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation. Cross-compilation is required to support, in particular, statically linked binaries. This page focuses on the SCONE C++ cross compiler scone g++ (a.k.a. scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++.","title":"C++ Program Language Support"},{"location":"C++/#image","text":"Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Image"},{"location":"C++/#help","text":"If you need some help, just execute in the container: $ scone g++ --help Usage: x86_64-linux-musl-g++ [ options ] file... Options: ...","title":"Help"},{"location":"C++/#example","text":"Let's try to compile a simple program: cat sqrt.cc EOF #include iostream #include cmath using namespace std; int main() { int x = 0; while(x 10) { double y = sqrt((double)x); cout The square root of x is y endl; x++; } return 0; } EOF We compile the program with scone gcc or scone-gcc : scone g++ sqrt.cc -o sqrt Let's execute the binary and switch on debug outputs: SCONE_VERSION = 1 ./sqrt The output will look like: xport SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: ebf98279a2cae1179366f8b5a0fc007decdc5dd3dec2b92ddbf121c2e2bf22f4 The square root of 0 is 0 The square root of 1 is 1 The square root of 2 is 1.41421 The square root of 3 is 1.73205 The square root of 4 is 2 The square root of 5 is 2.23607 The square root of 6 is 2.44949 The square root of 7 is 2.64575 The square root of 8 is 2.82843 The square root of 9 is 3","title":"Example"},{"location":"C++/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. scontain.com , July 2018. Questions or Suggestions?","title":"Debugging"},{"location":"C/","text":"C Program Language Support SCONE supports native compilation combined with dynamic linking as well as cross-compilation with static as well as dynamic linking. This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc. Image Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Help If you need some help, just execute in the container: $ scone gcc --help Usage: x86_64-linux-musl-gcc [ options ] file... Options: ... Example Let's try to compile a simple program: cat fib.c EOF #include stdio.h #include stdlib.h int main(int argc, char** argv) { int n=0, first = 0, second = 1, next = 0, c; if (argc 1) n=atoi(argv[1]); printf( fib(%d)= 1 ,n); for ( c = 1 ; c n ; c++ ) { next = first + second; first = second; second = next; printf( , %d ,next); } printf( \\n ); } EOF We compile the program with scone gcc or scone-gcc or just gcc (all equivalent): scone gcc fib.c -o fib To compute fib(23), execute: SCONE_VERSION = 1 ./fib 23 The last line of the output should look as follows: fib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657 Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. scontain.com , July 2018. Questions or Suggestions?","title":"C"},{"location":"C/#c-program-language-support","text":"SCONE supports native compilation combined with dynamic linking as well as cross-compilation with static as well as dynamic linking. This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc.","title":"C Program Language Support"},{"location":"C/#image","text":"Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Image"},{"location":"C/#help","text":"If you need some help, just execute in the container: $ scone gcc --help Usage: x86_64-linux-musl-gcc [ options ] file... Options: ...","title":"Help"},{"location":"C/#example","text":"Let's try to compile a simple program: cat fib.c EOF #include stdio.h #include stdlib.h int main(int argc, char** argv) { int n=0, first = 0, second = 1, next = 0, c; if (argc 1) n=atoi(argv[1]); printf( fib(%d)= 1 ,n); for ( c = 1 ; c n ; c++ ) { next = first + second; first = second; second = next; printf( , %d ,next); } printf( \\n ); } EOF We compile the program with scone gcc or scone-gcc or just gcc (all equivalent): scone gcc fib.c -o fib To compute fib(23), execute: SCONE_VERSION = 1 ./fib 23 The last line of the output should look as follows: fib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657","title":"Example"},{"location":"C/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. scontain.com , July 2018. Questions or Suggestions?","title":"Debugging"},{"location":"Fortran/","text":"Fortran SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler scone gfortran (a.k.a. scone-gfortran , a.k.a. gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran. Image Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Help If you need some help, just execute in the container: $ scone gfortran --help Usage: x86_64-linux-musl-gfortran [ options ] file... Options: ... Let's try a simple Fortran program. cat gcd.f EOF * euclid.f (FORTRAN 77) * Find greatest common divisor using the Euclidean algorithm PROGRAM EUCLID PRINT *, A? READ *, NA IF (NA.LE.0) THEN PRINT *, A must be a positive integer. STOP END IF PRINT *, B? READ *, NB IF (NB.LE.0) THEN PRINT *, B must be a positive integer. STOP END IF PRINT *, The GCD of , NA, and , NB, is , NGCD(NA, NB), . STOP END FUNCTION NGCD(NA, NB) IA = NA IB = NB 1 IF (IB.NE.0) THEN ITEMP = IA IA = IB IB = MOD(ITEMP, IB) GOTO 1 END IF NGCD = IA RETURN END EOF We compile the program with scone gfortran (a.k.a. scone-gfortran ): scone gfortran gcd.f -o gcd We can now run this program as follows: SCONE_VERSION = 1 ./gcd EOF 10 15 EOF The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: e9c60984724c6d5ffc8cc8a6ba4377910e63c8534ef24b87d0727e712809ba50 A? B? The GCD of 10 and 15 is 5 . Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. scontain.com , July 2018. Questions or Suggestions?","title":"Fortran"},{"location":"Fortran/#fortran","text":"SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler scone gfortran (a.k.a. scone-gfortran , a.k.a. gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran.","title":"Fortran"},{"location":"Fortran/#image","text":"Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Image"},{"location":"Fortran/#help","text":"If you need some help, just execute in the container: $ scone gfortran --help Usage: x86_64-linux-musl-gfortran [ options ] file... Options: ... Let's try a simple Fortran program. cat gcd.f EOF * euclid.f (FORTRAN 77) * Find greatest common divisor using the Euclidean algorithm PROGRAM EUCLID PRINT *, A? READ *, NA IF (NA.LE.0) THEN PRINT *, A must be a positive integer. STOP END IF PRINT *, B? READ *, NB IF (NB.LE.0) THEN PRINT *, B must be a positive integer. STOP END IF PRINT *, The GCD of , NA, and , NB, is , NGCD(NA, NB), . STOP END FUNCTION NGCD(NA, NB) IA = NA IB = NB 1 IF (IB.NE.0) THEN ITEMP = IA IA = IB IB = MOD(ITEMP, IB) GOTO 1 END IF NGCD = IA RETURN END EOF We compile the program with scone gfortran (a.k.a. scone-gfortran ): scone gfortran gcd.f -o gcd We can now run this program as follows: SCONE_VERSION = 1 ./gcd EOF 10 15 EOF The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: e9c60984724c6d5ffc8cc8a6ba4377910e63c8534ef24b87d0727e712809ba50 A? B? The GCD of 10 and 15 is 5 .","title":"Help"},{"location":"Fortran/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. scontain.com , July 2018. Questions or Suggestions?","title":"Debugging"},{"location":"GO/","text":"GO SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image sconecuratedimages/crosscompilers . Example Start the SCONE crosscompiler container: docker run --device = /dev/isgx -it -p 8080 :8080 sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Lets consider a simple GO program (take from a GO tutorial ): cat web-srv.go EOF package main import ( os fmt net/http ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, Hi there, I love %s!\\n , r.URL.Path[1:]) if r.URL.Path[1:] == EXIT { os.Exit(0) } } func main() { http.HandleFunc( / , handler) http.ListenAndServe( :8080 , nil) } EOF You can cross-compile this program as follows: SCONE_HEAP = 1G scone-gccgo web-srv.go -O3 -o web-srv-go -g You can start the compiled program (and enable some debug messages) as follows: SCONE_VERSION = 1 ./web-srv-go The output should look as follows: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: dea1dadce2884bbfd642c10f436c1d02db7ac0f4e4f3abe5d2fde031056405dd You can now connect to port 8080, for example, with curl : curl localhost:8080/SCONE The output should be as follows: Hi there, I love SCONE! You can terminate the server with curl localhost:8080/EXIT This will output the following text: curl: (52) Empty reply from server Building Dependencies Building larger applications that include external dependencies can be difficult when using scone-gccgo alone. To simplify the building of complex applications, we recommend the use of the go command. First, install go inside a sconecuratedimages/crosscompilers container as follows: $ apk update $ apk add go You can then build your dependencies with the help of go and the SCONE go crosscompiler: $ go build -compiler gccgo -buildmode = exe Note you need to specify gccgo not scone-gccgo : gccgo is an alias of scone-gccgo . For a more detailed example, please read how we compile groupcache . Debugging SCONE supports debugging of programs running inside of an enclave with the help of gdb. Debugging inside of a container Standard containers have not sufficient rights to use the debugger. Hence, you must start a container with SYS_PTRACE capability. For example: docker run --cap-add SYS_PTRACE -it -p 8080 :8080 -v $PWD /EXAMPLE:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers Handling Illegal instructions Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. scone-gdb ./web-srv-go This will produce the following output: GNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git Copyright (C) 2017 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type show copying and show warranty for details. This GDB was configured as x86_64-linux-gnu . Type show configuration for configuration details. For bug reporting instructions, please see: http://www.gnu.org/software/gdb/bugs/ . Find the GDB manual and other documentation resources online at: http://www.gnu.org/software/gdb/documentation/ . For help, type help . Type apropos word to search for commands related to word ... Source directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd Setting environment variable LD_PRELOAD to null value. Reading symbols from ./web-srv-go...done. [SCONE] Initializing... If your program contains some illegal instructions, you need to ask the debugger to forward the signals, that these illegal instructions cause, to the program via handle SIGILL nostop pass : # (gdb) handle SIGILL nostop pass This will produce the following output: Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction (gdb) Since we do not patch the CPUID instructions in this run, run you will see something like this: Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [Thread debugging using libthread_db enabled] Using host libthread_db library /lib/x86_64-linux-gnu/libthread_db.so.1 . [SCONE] Enclave base: 1000000000 [SCONE] Loaded debug symbols [New Thread 0x7f1786d26700 (LWP 105)] [New Thread 0x7f1786525700 (LWP 106)] [New Thread 0x7f1785d24700 (LWP 107)] [New Thread 0x7f1785523700 (LWP 108)] [New Thread 0x7f1787502700 (LWP 109)] [New Thread 0x7f17874fa700 (LWP 110)] [New Thread 0x7f17874f2700 (LWP 111)] [New Thread 0x7f17874ea700 (LWP 112)] Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 8 web-srv-go received signal SIGILL, Illegal instruction. You could interrupt this execution via control c: ^C Thread 1 web-srv-go received signal SIGINT, Interrupt. 0x00007f17870f69dd in pthread_join ( threadid = 139739022911232 , thread_return = 0x7ffe1c807928 ) at pthread_join.c:90 90 pthread_join.c: No such file or directory. ( gdb ) where #0 0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90 #1 0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764 ( gdb ) cont Continuing. Breakpoints scone-gdb support breakpoints. Say, we want to get control in the debugger whenever a request is being processed by the handler. We would set a breakpoint at function main.handler as follows: scone-gdb ./web-srv-go ... [ SCONE ] Initializing... ( gdb ) handle SIGILL nostop pass Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction ( gdb ) break main.handler Function main.handler not defined. Make breakpoint pending on future shared library load? ( y or [ n ]) y Breakpoint 1 ( main.handler ) pending. ( gdb ) run Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] Using host libthread_db library /lib/x86_64-linux-gnu/libthread_db.so.1 . [ SCONE ] Enclave base: 1000000000 [ SCONE ] Loaded debug symbols [ New Thread 0x7fb6cad32700 ( LWP 243 )] [ New Thread 0x7fb6ca531700 ( LWP 244 )] [ New Thread 0x7fb6c9d30700 ( LWP 245 )] [ New Thread 0x7fb6c952f700 ( LWP 246 )] [ New Thread 0x7fb6cb50e700 ( LWP 247 )] [ New Thread 0x7fb6cb506700 ( LWP 248 )] [ New Thread 0x7fb6cb4fe700 ( LWP 249 )] [ New Thread 0x7fb6cb4f6700 ( LWP 250 )] Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Note that at the time when we are setting the breakpoint, the symbols of the code running inside of the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on. We are now sending a request with the help of curl from a different window. This triggers the breakpoint: [ Switching to Thread 0x7fb6cb506700 ( LWP 248 )] Thread 7 web-srv-go hit Breakpoint 1 , main.handler ( w = ..., r = 0x100909e300 ) at web-srv.go:8 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) n 9 fmt.Fprintf ( w, Hi there, I love %s! , r.URL.Path [ 1 : ]) ( gdb ) n 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) c Continuing. scontain.com , July 2018. Questions or Suggestions?","title":"GO"},{"location":"GO/#go","text":"SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image sconecuratedimages/crosscompilers .","title":"GO"},{"location":"GO/#example","text":"Start the SCONE crosscompiler container: docker run --device = /dev/isgx -it -p 8080 :8080 sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Lets consider a simple GO program (take from a GO tutorial ): cat web-srv.go EOF package main import ( os fmt net/http ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, Hi there, I love %s!\\n , r.URL.Path[1:]) if r.URL.Path[1:] == EXIT { os.Exit(0) } } func main() { http.HandleFunc( / , handler) http.ListenAndServe( :8080 , nil) } EOF You can cross-compile this program as follows: SCONE_HEAP = 1G scone-gccgo web-srv.go -O3 -o web-srv-go -g You can start the compiled program (and enable some debug messages) as follows: SCONE_VERSION = 1 ./web-srv-go The output should look as follows: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: dea1dadce2884bbfd642c10f436c1d02db7ac0f4e4f3abe5d2fde031056405dd You can now connect to port 8080, for example, with curl : curl localhost:8080/SCONE The output should be as follows: Hi there, I love SCONE! You can terminate the server with curl localhost:8080/EXIT This will output the following text: curl: (52) Empty reply from server","title":"Example"},{"location":"GO/#building-dependencies","text":"Building larger applications that include external dependencies can be difficult when using scone-gccgo alone. To simplify the building of complex applications, we recommend the use of the go command. First, install go inside a sconecuratedimages/crosscompilers container as follows: $ apk update $ apk add go You can then build your dependencies with the help of go and the SCONE go crosscompiler: $ go build -compiler gccgo -buildmode = exe Note you need to specify gccgo not scone-gccgo : gccgo is an alias of scone-gccgo . For a more detailed example, please read how we compile groupcache .","title":"Building Dependencies"},{"location":"GO/#debugging","text":"SCONE supports debugging of programs running inside of an enclave with the help of gdb.","title":"Debugging"},{"location":"GO/#debugging-inside-of-a-container","text":"Standard containers have not sufficient rights to use the debugger. Hence, you must start a container with SYS_PTRACE capability. For example: docker run --cap-add SYS_PTRACE -it -p 8080 :8080 -v $PWD /EXAMPLE:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers","title":"Debugging inside of a container"},{"location":"GO/#handling-illegal-instructions","text":"Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. scone-gdb ./web-srv-go This will produce the following output: GNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git Copyright (C) 2017 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type show copying and show warranty for details. This GDB was configured as x86_64-linux-gnu . Type show configuration for configuration details. For bug reporting instructions, please see: http://www.gnu.org/software/gdb/bugs/ . Find the GDB manual and other documentation resources online at: http://www.gnu.org/software/gdb/documentation/ . For help, type help . Type apropos word to search for commands related to word ... Source directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd Setting environment variable LD_PRELOAD to null value. Reading symbols from ./web-srv-go...done. [SCONE] Initializing... If your program contains some illegal instructions, you need to ask the debugger to forward the signals, that these illegal instructions cause, to the program via handle SIGILL nostop pass : # (gdb) handle SIGILL nostop pass This will produce the following output: Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction (gdb) Since we do not patch the CPUID instructions in this run, run you will see something like this: Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [Thread debugging using libthread_db enabled] Using host libthread_db library /lib/x86_64-linux-gnu/libthread_db.so.1 . [SCONE] Enclave base: 1000000000 [SCONE] Loaded debug symbols [New Thread 0x7f1786d26700 (LWP 105)] [New Thread 0x7f1786525700 (LWP 106)] [New Thread 0x7f1785d24700 (LWP 107)] [New Thread 0x7f1785523700 (LWP 108)] [New Thread 0x7f1787502700 (LWP 109)] [New Thread 0x7f17874fa700 (LWP 110)] [New Thread 0x7f17874f2700 (LWP 111)] [New Thread 0x7f17874ea700 (LWP 112)] Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 8 web-srv-go received signal SIGILL, Illegal instruction. You could interrupt this execution via control c: ^C Thread 1 web-srv-go received signal SIGINT, Interrupt. 0x00007f17870f69dd in pthread_join ( threadid = 139739022911232 , thread_return = 0x7ffe1c807928 ) at pthread_join.c:90 90 pthread_join.c: No such file or directory. ( gdb ) where #0 0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90 #1 0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764 ( gdb ) cont Continuing.","title":"Handling Illegal instructions"},{"location":"GO/#breakpoints","text":"scone-gdb support breakpoints. Say, we want to get control in the debugger whenever a request is being processed by the handler. We would set a breakpoint at function main.handler as follows: scone-gdb ./web-srv-go ... [ SCONE ] Initializing... ( gdb ) handle SIGILL nostop pass Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction ( gdb ) break main.handler Function main.handler not defined. Make breakpoint pending on future shared library load? ( y or [ n ]) y Breakpoint 1 ( main.handler ) pending. ( gdb ) run Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] Using host libthread_db library /lib/x86_64-linux-gnu/libthread_db.so.1 . [ SCONE ] Enclave base: 1000000000 [ SCONE ] Loaded debug symbols [ New Thread 0x7fb6cad32700 ( LWP 243 )] [ New Thread 0x7fb6ca531700 ( LWP 244 )] [ New Thread 0x7fb6c9d30700 ( LWP 245 )] [ New Thread 0x7fb6c952f700 ( LWP 246 )] [ New Thread 0x7fb6cb50e700 ( LWP 247 )] [ New Thread 0x7fb6cb506700 ( LWP 248 )] [ New Thread 0x7fb6cb4fe700 ( LWP 249 )] [ New Thread 0x7fb6cb4f6700 ( LWP 250 )] Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Thread 6 web-srv-go received signal SIGILL, Illegal instruction. Note that at the time when we are setting the breakpoint, the symbols of the code running inside of the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on. We are now sending a request with the help of curl from a different window. This triggers the breakpoint: [ Switching to Thread 0x7fb6cb506700 ( LWP 248 )] Thread 7 web-srv-go hit Breakpoint 1 , main.handler ( w = ..., r = 0x100909e300 ) at web-srv.go:8 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) n 9 fmt.Fprintf ( w, Hi there, I love %s! , r.URL.Path [ 1 : ]) ( gdb ) n 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) c Continuing. scontain.com , July 2018. Questions or Suggestions?","title":"Breakpoints"},{"location":"Java/","text":"Java SCONE supports Java: we have an image containing OpenJDK8: docker run --device = /dev/isgx -it sconecuratedimages/apps:8-jdk-alpine Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Example Let us start with a simple hello world example: cat HelloWorld.java EOF public class HelloWorld { public static void main(String[] args) { System.out.println( Hello World ); } } EOF We can compile as follows: javac HelloWorld.java and run inside of an enclave as follows: java HelloWorld This might produce some warnings on SGX version 1 CPUs: Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World but correctly printing Hello World . Environment variables SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM, container or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . You can increase the heap to get rid of the above warnings: SCONE_HEAP = 12G java HelloWorld Example: Zookeeper The above image runs Zookeeper without any modification. Our Zookeeper image is available here: sconecuratedimages/apps:zookeeper-alpine . Zookeeper inside of an enclave runs zk-smoketest without any issues. scontain.com , August 2018. Questions or Suggestions?","title":"Java"},{"location":"Java/#java","text":"SCONE supports Java: we have an image containing OpenJDK8: docker run --device = /dev/isgx -it sconecuratedimages/apps:8-jdk-alpine Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Java"},{"location":"Java/#example","text":"Let us start with a simple hello world example: cat HelloWorld.java EOF public class HelloWorld { public static void main(String[] args) { System.out.println( Hello World ); } } EOF We can compile as follows: javac HelloWorld.java and run inside of an enclave as follows: java HelloWorld This might produce some warnings on SGX version 1 CPUs: Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World but correctly printing Hello World .","title":"Example"},{"location":"Java/#environment-variables","text":"SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM, container or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . You can increase the heap to get rid of the above warnings: SCONE_HEAP = 12G java HelloWorld","title":"Environment variables"},{"location":"Java/#example-zookeeper","text":"The above image runs Zookeeper without any modification. Our Zookeeper image is available here: sconecuratedimages/apps:zookeeper-alpine . Zookeeper inside of an enclave runs zk-smoketest without any issues. scontain.com , August 2018. Questions or Suggestions?","title":"Example: Zookeeper"},{"location":"MrEnclave/","text":"Determining MrEnclave An enclave is identified by a hash value which is called MrEnclave . This has is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave . To determine MrEnclave , we provide a simple way to determine MrEnclave on the developer site via environment variable SCONE_HASH=1 . Example: MrEnclave of Python Let us determine MrEnclave of our python interpreter. We start the container and then set environment variable SCONE_HASH=1 to ask SCONE to print MrEnclave and then terminate and SCONE_ALPINE=1 to ensure that the application is indeed started with SCONE. Note When setting SCONE_HASH=1 the program is not executed - only MrEnclave is printed on stdout.** docker run -it sconecuratedimages/apps:python-2.7-alpine3.6 sh $ SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541 Now, let us change the heap size via environment variable SCONE_HEAP by asking for a 2GB heap: $ SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python aa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465 By default, SCONE does not permit to load dynamic libraries after startup. By setting SCONE_ALLOW_DLOPEN=1 , we permit to load dynamic libraries during runtime. This changes MrEnclave : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Using debug output As an alternative to SCONE_HASH=1 is to print MrEnclave via debug messages by setting SCONE_VERSION=1 : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_VERSION = 1 SCONE_ALPINE = 1 /usr/local/bin/python export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( protected ) export SCONE_MPROTECT = no Revision: b6a40e091e2adb253f019401723d2a734e887a74 ( Fri Jan 26 07 :44:44 2018 +0100 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Enclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Python 2 .7.14 ( default, Jan 10 2018 , 05 :35:30 ) [ GCC 6 .4.0 ] on linux2 Type help , copyright , credits or license for more information. scontain.com , January 2018. Questions or Suggestions?","title":"MrEnclave"},{"location":"MrEnclave/#determining-mrenclave","text":"An enclave is identified by a hash value which is called MrEnclave . This has is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave . To determine MrEnclave , we provide a simple way to determine MrEnclave on the developer site via environment variable SCONE_HASH=1 .","title":"Determining MrEnclave"},{"location":"MrEnclave/#example-mrenclave-of-python","text":"Let us determine MrEnclave of our python interpreter. We start the container and then set environment variable SCONE_HASH=1 to ask SCONE to print MrEnclave and then terminate and SCONE_ALPINE=1 to ensure that the application is indeed started with SCONE. Note When setting SCONE_HASH=1 the program is not executed - only MrEnclave is printed on stdout.** docker run -it sconecuratedimages/apps:python-2.7-alpine3.6 sh $ SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541 Now, let us change the heap size via environment variable SCONE_HEAP by asking for a 2GB heap: $ SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python aa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465 By default, SCONE does not permit to load dynamic libraries after startup. By setting SCONE_ALLOW_DLOPEN=1 , we permit to load dynamic libraries during runtime. This changes MrEnclave : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508","title":"Example: MrEnclave of Python"},{"location":"MrEnclave/#using-debug-output","text":"As an alternative to SCONE_HASH=1 is to print MrEnclave via debug messages by setting SCONE_VERSION=1 : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_VERSION = 1 SCONE_ALPINE = 1 /usr/local/bin/python export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( protected ) export SCONE_MPROTECT = no Revision: b6a40e091e2adb253f019401723d2a734e887a74 ( Fri Jan 26 07 :44:44 2018 +0100 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Enclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Python 2 .7.14 ( default, Jan 10 2018 , 05 :35:30 ) [ GCC 6 .4.0 ] on linux2 Type help , copyright , credits or license for more information. scontain.com , January 2018. Questions or Suggestions?","title":"Using debug output"},{"location":"Nodejs/","text":"Node We provide Node 8.9.4 image that runs inside of an enclave: docker pull sconecuratedimages/apps:node-8.9.4-alpine Example Let's look at a little hello world program. First, we need to start a node container: docker run -it --device = /dev/isgx sconecuratedimages/apps:node-8.9.4-alpine sh In case you have no sgx driver installed, you can drop option --device=/dev/isgx for testing. The programs will then run in simulation mode , i.e., the SCONE software runs but in native mode and not inside an enclave. Inside of the container, we first add npm : apk add --no-cache nodejs-npm Ensure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB: export SCONE_HEAP=1G We create a new application myapp : mkdir myapp cd myapp cat package.json EOF { name : myapp , version : 1.0.0 , description : , main : app.js , scripts : { test : echo \\ Error: no test specified\\ exit 1 }, author : , license : ISC } EOF We install express with the help of npm : npm install express --save Let's store the hello world code: cat app.js EOF var express = require( express ); var app = express(); app.get( / , function (req, res) { res.send( Hello World! ); }); app.listen(3000, function () { console.log( Example app listening on port 3000! ); }); EOF We can run this application inside of an enclave with node . We can also enable some debug messages by setting environment variable SCONE_VERSION=1 to print that we run inside of an enclave: SCONE_VERSION = 1 node app.js This results in an output like this: export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 4294967296 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: e349ed6e4821f0cbfe895413c616409848216173 ( Wed Feb 28 19 :28:04 2018 +0100 ) Branch: master Configure options: --enable-shared --enable-debug --prefix = /builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb Example app listening on port 3000 ! You can now try to send a request to myapp from another shell in the container. Assuming that you did not start a new container in meantime, execute in another shell of your host: docker exec -it $( docker ps -l -q ) sh Inside of the container, first install curl and then query myapp : apk add --no-cache curl curl localhost:3000/ This results in an output like this: Hello World!/ # Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. errno ENOSYS SCONE does not yet support the fork system call (- this will happen later this year). If you spawn processes, there will be some error message like: npm ERR! spawn ENOSYS Environment variables SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . Environment variable SCONE_VERSION=1 prints debug messages - to show that the program runs inside of an enclave. SCONE_MODE=hw enforce that program runs in hardware enclave. By default, we set SCONE_MODE=auto which uses hardware enclave if available and software emulation otherwise. Dockerfile The above example, you could more easily put the following text in a Dockerfile: FROM sconecuratedimages/apps:node-8.9.4-alpine ENV SCONE_HEAP = 1G EXPOSE 3000 RUN apk add --no-cache nodejs-npm \\ mkdir myapp \\ cd myapp \\ echo { package.json \\ echo name : myapp , package.json \\ echo version : 1.0.0 , package.json \\ echo description : , package.json \\ echo main : app.js , package.json \\ echo scripts : package.json { \\ echo test : echo \\ Error: no test specified\\ exit 1 package.json \\ echo }, package.json \\ echo author : , package.json \\ echo license : ISC package.json \\ echo } package.json \\ npm install express --save \\ echo var express = require( express ); app.js \\ echo var app = express(); app.js \\ echo app.get( / , function (req, res) { app.js \\ echo res.send( Hello World! ); app.js \\ echo }); app.js \\ echo app.listen(3000, function () { app.js \\ echo console.log( Example app listening on port 3000! ); app.js \\ echo }); app.js CMD SCONE_VERSION = 1 node /myapp/app.js Now create an image myapp as follows: docker build -t myapp . You can run a container of this image as a daemon as follows: docker run -d -p 3000 :3000 myapp You can now query myapp as follows: curl localhost:3000 This results in an output like this: Hello World! scontain.com , March 2018. Questions or Suggestions?","title":"Node"},{"location":"Nodejs/#node","text":"We provide Node 8.9.4 image that runs inside of an enclave: docker pull sconecuratedimages/apps:node-8.9.4-alpine","title":"Node"},{"location":"Nodejs/#example","text":"Let's look at a little hello world program. First, we need to start a node container: docker run -it --device = /dev/isgx sconecuratedimages/apps:node-8.9.4-alpine sh In case you have no sgx driver installed, you can drop option --device=/dev/isgx for testing. The programs will then run in simulation mode , i.e., the SCONE software runs but in native mode and not inside an enclave. Inside of the container, we first add npm : apk add --no-cache nodejs-npm Ensure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB: export SCONE_HEAP=1G We create a new application myapp : mkdir myapp cd myapp cat package.json EOF { name : myapp , version : 1.0.0 , description : , main : app.js , scripts : { test : echo \\ Error: no test specified\\ exit 1 }, author : , license : ISC } EOF We install express with the help of npm : npm install express --save Let's store the hello world code: cat app.js EOF var express = require( express ); var app = express(); app.get( / , function (req, res) { res.send( Hello World! ); }); app.listen(3000, function () { console.log( Example app listening on port 3000! ); }); EOF We can run this application inside of an enclave with node . We can also enable some debug messages by setting environment variable SCONE_VERSION=1 to print that we run inside of an enclave: SCONE_VERSION = 1 node app.js This results in an output like this: export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 4294967296 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: e349ed6e4821f0cbfe895413c616409848216173 ( Wed Feb 28 19 :28:04 2018 +0100 ) Branch: master Configure options: --enable-shared --enable-debug --prefix = /builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb Example app listening on port 3000 ! You can now try to send a request to myapp from another shell in the container. Assuming that you did not start a new container in meantime, execute in another shell of your host: docker exec -it $( docker ps -l -q ) sh Inside of the container, first install curl and then query myapp : apk add --no-cache curl curl localhost:3000/ This results in an output like this: Hello World!/ # Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. errno ENOSYS SCONE does not yet support the fork system call (- this will happen later this year). If you spawn processes, there will be some error message like: npm ERR! spawn ENOSYS","title":"Example"},{"location":"Nodejs/#environment-variables","text":"SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . Environment variable SCONE_VERSION=1 prints debug messages - to show that the program runs inside of an enclave. SCONE_MODE=hw enforce that program runs in hardware enclave. By default, we set SCONE_MODE=auto which uses hardware enclave if available and software emulation otherwise.","title":"Environment variables"},{"location":"Nodejs/#dockerfile","text":"The above example, you could more easily put the following text in a Dockerfile: FROM sconecuratedimages/apps:node-8.9.4-alpine ENV SCONE_HEAP = 1G EXPOSE 3000 RUN apk add --no-cache nodejs-npm \\ mkdir myapp \\ cd myapp \\ echo { package.json \\ echo name : myapp , package.json \\ echo version : 1.0.0 , package.json \\ echo description : , package.json \\ echo main : app.js , package.json \\ echo scripts : package.json { \\ echo test : echo \\ Error: no test specified\\ exit 1 package.json \\ echo }, package.json \\ echo author : , package.json \\ echo license : ISC package.json \\ echo } package.json \\ npm install express --save \\ echo var express = require( express ); app.js \\ echo var app = express(); app.js \\ echo app.get( / , function (req, res) { app.js \\ echo res.send( Hello World! ); app.js \\ echo }); app.js \\ echo app.listen(3000, function () { app.js \\ echo console.log( Example app listening on port 3000! ); app.js \\ echo }); app.js CMD SCONE_VERSION = 1 node /myapp/app.js Now create an image myapp as follows: docker build -t myapp . You can run a container of this image as a daemon as follows: docker run -d -p 3000 :3000 myapp You can now query myapp as follows: curl localhost:3000 This results in an output like this: Hello World! scontain.com , March 2018. Questions or Suggestions?","title":"Dockerfile"},{"location":"Python/","text":"Python SCONE supports running Python programs inside of SGX enclaves. We maintain Docker images 1 for various Python versions. PyPy for SCONE We also support PyPy for SCONE: PyPySCONE 's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is PyPy inside an enclave faster than native Python. Send us email if you want to try this out. Image Currently, we provde a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine. You can pull this image as follows: docker pull sconecuratedimages/apps:python-2-alpine3.6 Python Interpreter To run the Python interpreter inside an enclave in interactive mode, first start the container: docker run --rm -it -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/apps:python-2-alpine3.6 sh The execute python inside of the container: SCONE_HEAP = 256M SCONE_VERSION = 1 python Since we set SCONE_VERSION=1 , we get the following outputs 2 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 268435456 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab ( Thu Aug 16 16 :45:05 2018 +0200 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /mnt/ssd/franz/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: f129bbd19627367c03e2980c0f04a32809a7aae1d795a75220d9054daf537b30 Python 2 .7.13 ( default, Jun 1 2018 , 13 :20:58 ) [ GCC 7 .3.0 ] on linux2 Type help , copyright , credits or license for more information. Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. The meaning of protected versus unprotected library is explained in the faq . Running an application Say, you have a Python application called myapp.py in your current directory. To execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. To run Python inside of an enclave, you can set the environment variable SCONE_MODE=HW and SCONE_ALPINE=1 . To issue some debug messages that show that we are running inside an enclave, set SCONE_VERSION=1 In general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of MRENCLAVE , i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set SCONE_ALLOW_DLOPEN=2 . For operations, the environment variables are set by the CAS and you must set SCONE_ALLOW_DLOPEN either to SCONE_ALLOW_DLOPEN=1 to enable loading of dynamic libraries or must not define SCONE_ALLOW_DLOPEN . Python applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable SCONE_HEAP and the stack size via STACK_SIZE at startup. Python program exits Python does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like, SCONE_HEAP=256M to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.** Note that you can set the environment variable of a process - in our case python - running inside a container with docker option -e : docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp -e SCONE_HEAP = 256M SCONE_MODE = HW -e SCONE_ALLOW_DLOPEN = 2 -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/apps:python-2-alpine3.6 python myapp.py Will produce an output like export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 ... NumPy Let's see how we can install some extra packages that your python program might need. Let us focus on NumPy first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a Dockerfile . First, we start the SCONE Python image: docker run -it --rm sconecuratedimages/apps:python-2-alpine3.6 sh This is a minimal image and you need to add some packages to be able to install packages that compile external code: apk add --no-cache bats libbsd openssl musl-dev build-base We then install numpy inside of the container with the help of pip : pip install numpy == 1 .14.5 This results in an output like Collecting numpy == 1 .14.5 Downloading https://files.pythonhosted.org/packages/d5/6e/f00492653d0fdf6497a181a1c1d46bbea5a2383e7faf4c8ca6d6f3d2581d/numpy-1.14.5.zip ( 4 .9MB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 .9MB 375kB/s Installing collected packages: numpy Running setup.py install for numpy ... done Successfully installed numpy-1.14.5 Ok, let's try to execute some examples with NumPy . Let's run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages: SCONE_HEAP = 256M SCONE_VERSION = 1 python during startup this issues the following messages export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 10000000000 export SCONE_STACK = 0 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes export SCONE_ALLOW_DLOPEN2 = yes Revision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Python 2 .7.14 ( default, Dec 19 2017 , 22 :29:22 ) [ GCC 6 .4.0 ] on linux2 Type help , copyright , credits or license for more information. Now, we can import numpy and execute some commands: import numpy as np a = np.arange ( 15 ) .reshape ( 3 , 5 ) a array ([[ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ]]) a.shape ( 3 , 5 ) a.ndim 2 a.dtype.name int64 a.itemsize 8 type ( a ) type numpy.ndarray Cairo Let's look at another popular library: the cairo graphics library. cairo is written in C and has Python bindings provided by package pycairo . In this case, we need to install the C-library first: In Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows: apk add --no-cache cairo-dev cairo fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ( 1 /55 ) Installing expat-dev ( 2 .2.5-r0 ) ... ( 55 /55 ) Installing cairo-dev ( 1 .14.10-r0 ) Executing busybox-1.27.2-r6.trigger Executing glib-2.54.2-r0.trigger No schema files found: doing nothing. OK: 297 MiB in 112 packages $ Now we can install the Python bindings of cairo with pip : pip install pycairo Collecting pycairo Downloading pycairo-1.15.4.tar.gz ( 178kB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 184kB 1 .7MB/s Building wheels for collected packages: pycairo Running setup.py bdist_wheel for pycairo ... done Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f Successfully built pycairo Installing collected packages: pycairo Successfully installed pycairo-1.15.4 We can now start Python again via SCONE_HEAP = 256M SCONE_VERSION = 1 python before we execute some cairo examples: import cairo import math WIDTH, HEIGHT = 256 , 256 surface = cairo.ImageSurface ( cairo.FORMAT_ARGB32, WIDTH, HEIGHT ) ctx = cairo.Context ( surface ) ctx.scale ( WIDTH, HEIGHT ) # Normalizing the canvas pat = cairo.LinearGradient ( 0 .0, 0 .0, 0 .0, 1 .0 ) pat.add_color_stop_rgba ( 1 , 0 .7, 0 , 0 , 0 .5 ) # First stop, 50% opacity pat.add_color_stop_rgba ( 0 , 0 .9, 0 .7, 0 .2, 1 ) # Last stop, 100% opacity ctx.rectangle ( 0 , 0 , 1 , 1 ) # Rectangle(x0, y0, x1, y1) ctx.set_source ( pat ) ctx.fill () ctx.translate ( 0 .1, 0 .1 ) # Changing the current transformation matrix ctx.move_to ( 0 , 0 ) # Arc(cx, cy, radius, start_angle, stop_angle) ... ctx.arc ( 0 .2, 0 .1, 0 .1, -math.pi/2, 0 ) ctx.line_to ( 0 .5, 0 .1 ) # Line to (x,y) # Curve(x1, y1, x2, y2, x3, y3) ... ctx.curve_to ( 0 .5, 0 .2, 0 .5, 0 .4, 0 .2, 0 .8 ) ctx.close_path () ctx.set_source_rgb ( 0 .3, 0 .2, 0 .5 ) # Solid color ctx.set_line_width ( 0 .02 ) ctx.stroke () surface.write_to_png ( example.png ) # Output to PNG exit () This generates a file example.png in the working directory. Example Let's look at another example: We use pip to install a Python chess library. Then we run Python inside of an enclave and import the chess library. We use the Scholar's mate example from https://pypi.python.org/pypi/python-chess Limitation We do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner. scontain.com , August 2018. Questions or Suggestions? Send email to to get access for evaluation version. The other environment variables are explained below. Also read Section Environment Variables for further details.","title":"Python"},{"location":"Python/#python","text":"SCONE supports running Python programs inside of SGX enclaves. We maintain Docker images 1 for various Python versions. PyPy for SCONE We also support PyPy for SCONE: PyPySCONE 's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is PyPy inside an enclave faster than native Python. Send us email if you want to try this out.","title":"Python"},{"location":"Python/#image","text":"Currently, we provde a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine. You can pull this image as follows: docker pull sconecuratedimages/apps:python-2-alpine3.6","title":"Image"},{"location":"Python/#python-interpreter","text":"To run the Python interpreter inside an enclave in interactive mode, first start the container: docker run --rm -it -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/apps:python-2-alpine3.6 sh The execute python inside of the container: SCONE_HEAP = 256M SCONE_VERSION = 1 python Since we set SCONE_VERSION=1 , we get the following outputs 2 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 268435456 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab ( Thu Aug 16 16 :45:05 2018 +0200 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /mnt/ssd/franz/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: f129bbd19627367c03e2980c0f04a32809a7aae1d795a75220d9054daf537b30 Python 2 .7.13 ( default, Jun 1 2018 , 13 :20:58 ) [ GCC 7 .3.0 ] on linux2 Type help , copyright , credits or license for more information. Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. The meaning of protected versus unprotected library is explained in the faq .","title":"Python Interpreter"},{"location":"Python/#running-an-application","text":"Say, you have a Python application called myapp.py in your current directory. To execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. To run Python inside of an enclave, you can set the environment variable SCONE_MODE=HW and SCONE_ALPINE=1 . To issue some debug messages that show that we are running inside an enclave, set SCONE_VERSION=1 In general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of MRENCLAVE , i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set SCONE_ALLOW_DLOPEN=2 . For operations, the environment variables are set by the CAS and you must set SCONE_ALLOW_DLOPEN either to SCONE_ALLOW_DLOPEN=1 to enable loading of dynamic libraries or must not define SCONE_ALLOW_DLOPEN . Python applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable SCONE_HEAP and the stack size via STACK_SIZE at startup. Python program exits Python does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like, SCONE_HEAP=256M to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.** Note that you can set the environment variable of a process - in our case python - running inside a container with docker option -e : docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp -e SCONE_HEAP = 256M SCONE_MODE = HW -e SCONE_ALLOW_DLOPEN = 2 -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/apps:python-2-alpine3.6 python myapp.py Will produce an output like export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 ...","title":"Running an application"},{"location":"Python/#numpy","text":"Let's see how we can install some extra packages that your python program might need. Let us focus on NumPy first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a Dockerfile . First, we start the SCONE Python image: docker run -it --rm sconecuratedimages/apps:python-2-alpine3.6 sh This is a minimal image and you need to add some packages to be able to install packages that compile external code: apk add --no-cache bats libbsd openssl musl-dev build-base We then install numpy inside of the container with the help of pip : pip install numpy == 1 .14.5 This results in an output like Collecting numpy == 1 .14.5 Downloading https://files.pythonhosted.org/packages/d5/6e/f00492653d0fdf6497a181a1c1d46bbea5a2383e7faf4c8ca6d6f3d2581d/numpy-1.14.5.zip ( 4 .9MB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 .9MB 375kB/s Installing collected packages: numpy Running setup.py install for numpy ... done Successfully installed numpy-1.14.5 Ok, let's try to execute some examples with NumPy . Let's run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages: SCONE_HEAP = 256M SCONE_VERSION = 1 python during startup this issues the following messages export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 10000000000 export SCONE_STACK = 0 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes export SCONE_ALLOW_DLOPEN2 = yes Revision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Python 2 .7.14 ( default, Dec 19 2017 , 22 :29:22 ) [ GCC 6 .4.0 ] on linux2 Type help , copyright , credits or license for more information. Now, we can import numpy and execute some commands: import numpy as np a = np.arange ( 15 ) .reshape ( 3 , 5 ) a array ([[ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ]]) a.shape ( 3 , 5 ) a.ndim 2 a.dtype.name int64 a.itemsize 8 type ( a ) type numpy.ndarray","title":"NumPy"},{"location":"Python/#cairo","text":"Let's look at another popular library: the cairo graphics library. cairo is written in C and has Python bindings provided by package pycairo . In this case, we need to install the C-library first: In Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows: apk add --no-cache cairo-dev cairo fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ( 1 /55 ) Installing expat-dev ( 2 .2.5-r0 ) ... ( 55 /55 ) Installing cairo-dev ( 1 .14.10-r0 ) Executing busybox-1.27.2-r6.trigger Executing glib-2.54.2-r0.trigger No schema files found: doing nothing. OK: 297 MiB in 112 packages $ Now we can install the Python bindings of cairo with pip : pip install pycairo Collecting pycairo Downloading pycairo-1.15.4.tar.gz ( 178kB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 184kB 1 .7MB/s Building wheels for collected packages: pycairo Running setup.py bdist_wheel for pycairo ... done Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f Successfully built pycairo Installing collected packages: pycairo Successfully installed pycairo-1.15.4 We can now start Python again via SCONE_HEAP = 256M SCONE_VERSION = 1 python before we execute some cairo examples: import cairo import math WIDTH, HEIGHT = 256 , 256 surface = cairo.ImageSurface ( cairo.FORMAT_ARGB32, WIDTH, HEIGHT ) ctx = cairo.Context ( surface ) ctx.scale ( WIDTH, HEIGHT ) # Normalizing the canvas pat = cairo.LinearGradient ( 0 .0, 0 .0, 0 .0, 1 .0 ) pat.add_color_stop_rgba ( 1 , 0 .7, 0 , 0 , 0 .5 ) # First stop, 50% opacity pat.add_color_stop_rgba ( 0 , 0 .9, 0 .7, 0 .2, 1 ) # Last stop, 100% opacity ctx.rectangle ( 0 , 0 , 1 , 1 ) # Rectangle(x0, y0, x1, y1) ctx.set_source ( pat ) ctx.fill () ctx.translate ( 0 .1, 0 .1 ) # Changing the current transformation matrix ctx.move_to ( 0 , 0 ) # Arc(cx, cy, radius, start_angle, stop_angle) ... ctx.arc ( 0 .2, 0 .1, 0 .1, -math.pi/2, 0 ) ctx.line_to ( 0 .5, 0 .1 ) # Line to (x,y) # Curve(x1, y1, x2, y2, x3, y3) ... ctx.curve_to ( 0 .5, 0 .2, 0 .5, 0 .4, 0 .2, 0 .8 ) ctx.close_path () ctx.set_source_rgb ( 0 .3, 0 .2, 0 .5 ) # Solid color ctx.set_line_width ( 0 .02 ) ctx.stroke () surface.write_to_png ( example.png ) # Output to PNG exit () This generates a file example.png in the working directory.","title":"Cairo"},{"location":"Python/#example","text":"Let's look at another example: We use pip to install a Python chess library. Then we run Python inside of an enclave and import the chess library. We use the Scholar's mate example from https://pypi.python.org/pypi/python-chess","title":"Example"},{"location":"Python/#limitation","text":"We do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner. scontain.com , August 2018. Questions or Suggestions? Send email to to get access for evaluation version. The other environment variables are explained below. Also read Section Environment Variables for further details.","title":"Limitation"},{"location":"R/","text":"R We just added experimental support for R. For now, Rscript only support a single script as argument: no expressions or other arguments supported. Image Currently, we provide experimental support for R 3.5.0. You can pull this image as follows: docker pull sconecuratedimages/apps:R Running R To start R, just execute: docker run -it --rm sconecuratedimages/apps:R This will output something like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=4194304 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab (Thu Aug 16 16:45:05 2018 +0200) Branch: new-docker-images-cf (dirty) Configure options: --enable-shared --enable-debug --prefix=/home/christof/GIT/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 71e730d77fcae6fd37b80cd8669f2d75b8e58dbba80afa48929ae817bf263bb0 Warning message: failed to set alternate signal stack R version 3.5.0 (2018-04-23) -- Joy in Playing Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) Example Execute some first R program (taken from www.rexamples.com ): a - 42 A - a * 2 # R is case sensitive print ( a ) cat ( A , \\n ) # 84 is concatenated with \\n if ( A a ) # true, 84 42 { cat ( A , , a , \\n ) } This will result in an output: 84 42 Example 2 A somewhat more complex example from www.rexamples.com : #utility functions readinteger - function () { n - readline ( prompt = Enter an integer: ) if ( ! grepl ( ^[0-9]+$ , n )) { return ( readinteger ()) } return ( as.integer ( n )) } # real program start here num - round ( runif ( 1 ) * 100 , digits = 0 ) guess - -1 cat ( Guess a number between 0 and 100.\\n ) while ( guess != num ) { guess - readinteger () if ( guess == num ) { cat ( Congratulations, , num , is right.\\n ) } else if ( guess num ) { cat ( It s bigger!\\n ) } else if ( guess num ) { cat ( It s smaller!\\n ) } } This will result in an otherput as follows: Enter an integer: 50 It s bigger! Enter an integer: 75 It s bigger! Enter an integer: 87 It s smaller! Enter an integer: 82 It s smaller! Enter an integer: 78 It s bigger! Enter an integer: 80 It s bigger! Enter an integer: 81 Congratulations, 81 is right. scontain.com , August 2018. Questions or Suggestions?","title":"R"},{"location":"R/#r","text":"We just added experimental support for R. For now, Rscript only support a single script as argument: no expressions or other arguments supported.","title":"R"},{"location":"R/#image","text":"Currently, we provide experimental support for R 3.5.0. You can pull this image as follows: docker pull sconecuratedimages/apps:R","title":"Image"},{"location":"R/#running-r","text":"To start R, just execute: docker run -it --rm sconecuratedimages/apps:R This will output something like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=4194304 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab (Thu Aug 16 16:45:05 2018 +0200) Branch: new-docker-images-cf (dirty) Configure options: --enable-shared --enable-debug --prefix=/home/christof/GIT/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 71e730d77fcae6fd37b80cd8669f2d75b8e58dbba80afa48929ae817bf263bb0 Warning message: failed to set alternate signal stack R version 3.5.0 (2018-04-23) -- Joy in Playing Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit)","title":"Running R"},{"location":"R/#example","text":"Execute some first R program (taken from www.rexamples.com ): a - 42 A - a * 2 # R is case sensitive print ( a ) cat ( A , \\n ) # 84 is concatenated with \\n if ( A a ) # true, 84 42 { cat ( A , , a , \\n ) } This will result in an output: 84 42","title":"Example"},{"location":"R/#example-2","text":"A somewhat more complex example from www.rexamples.com : #utility functions readinteger - function () { n - readline ( prompt = Enter an integer: ) if ( ! grepl ( ^[0-9]+$ , n )) { return ( readinteger ()) } return ( as.integer ( n )) } # real program start here num - round ( runif ( 1 ) * 100 , digits = 0 ) guess - -1 cat ( Guess a number between 0 and 100.\\n ) while ( guess != num ) { guess - readinteger () if ( guess == num ) { cat ( Congratulations, , num , is right.\\n ) } else if ( guess num ) { cat ( It s bigger!\\n ) } else if ( guess num ) { cat ( It s smaller!\\n ) } } This will result in an otherput as follows: Enter an integer: 50 It s bigger! Enter an integer: 75 It s bigger! Enter an integer: 87 It s smaller! Enter an integer: 82 It s smaller! Enter an integer: 78 It s bigger! Enter an integer: 80 It s bigger! Enter an integer: 81 Congratulations, 81 is right. scontain.com , August 2018. Questions or Suggestions?","title":"Example 2"},{"location":"Rust/","text":"Rust SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves. To build Rust applications, we provide variants of the rustc and cargo command line utilities as part of image sconecuratedimages/crosscompilers : scone-rustc / scone rustc You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container sconecuratedimages/crosscompilers ): docker run -it sconecuratedimages/crosscompilers $ scone rustc --version rustc 1 .20.0 ( f3d6973f4 2017 -08-27 ) Let's try a simple hello world program. $ mkdir ~/projects $ cd ~/projects $ mkdir hello_world $ cd hello_world Let's try our rust program: $ cat main.rs EOF fn main() { println!( Hello, world! ); } EOF Let's compile the program for running inside of enclaves: $ scone rustc main.rs $ ls main main.rs Let's run main inside an enclave and print some debug information: $ SCONE_MODE = HW SCONE_VERSION = 1 ./main export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = no export SCONE_ALLOW_DLOPEN2 = no Hello, world! scone-cargo and scone cargo : You can build projects with scone cargo : $ scone cargo build --target = scone Alternatively, you can use scone-cargo if, for example, you need a command without a space. scone cargo has access to the SCONE-compiled rust standard library and the target file. --target=scone instructs it to use our target file - essentially triggering a cross-compiler build. Due to the cross-compilation, crates that depend on compiled (C) libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate. The following is an example of how an executable with openssl can be compiled: $ OPENSSL_LIB_DIR = /libressl-2.4.5 OPENSSL_INCLUDE_DIR = /libressl-2.4.5/include/ OPENSSL_STATIC = 1 PKG_CONFIG_ALLOW_CROSS = 1 scone-cargo build --target = scone In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library. scontain.com , December 2017. Questions or Suggestions?","title":"Rust"},{"location":"Rust/#rust","text":"SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves. To build Rust applications, we provide variants of the rustc and cargo command line utilities as part of image sconecuratedimages/crosscompilers :","title":"Rust"},{"location":"Rust/#scone-rustc-scone-rustc","text":"You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container sconecuratedimages/crosscompilers ): docker run -it sconecuratedimages/crosscompilers $ scone rustc --version rustc 1 .20.0 ( f3d6973f4 2017 -08-27 ) Let's try a simple hello world program. $ mkdir ~/projects $ cd ~/projects $ mkdir hello_world $ cd hello_world Let's try our rust program: $ cat main.rs EOF fn main() { println!( Hello, world! ); } EOF Let's compile the program for running inside of enclaves: $ scone rustc main.rs $ ls main main.rs Let's run main inside an enclave and print some debug information: $ SCONE_MODE = HW SCONE_VERSION = 1 ./main export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = no export SCONE_ALLOW_DLOPEN2 = no Hello, world!","title":"scone-rustc /  scone rustc"},{"location":"Rust/#scone-cargo-and-scone-cargo","text":"You can build projects with scone cargo : $ scone cargo build --target = scone Alternatively, you can use scone-cargo if, for example, you need a command without a space. scone cargo has access to the SCONE-compiled rust standard library and the target file. --target=scone instructs it to use our target file - essentially triggering a cross-compiler build. Due to the cross-compilation, crates that depend on compiled (C) libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate. The following is an example of how an executable with openssl can be compiled: $ OPENSSL_LIB_DIR = /libressl-2.4.5 OPENSSL_INCLUDE_DIR = /libressl-2.4.5/include/ OPENSSL_STATIC = 1 PKG_CONFIG_ALLOW_CROSS = 1 scone-cargo build --target = scone In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library. scontain.com , December 2017. Questions or Suggestions?","title":"scone-cargo and scone cargo:"},{"location":"SCONE_CAS/","text":"SCONE CAS SCONE CAS ( Configuration and Attestation Service ) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with their command line arguments their environment variables after the service was attested by the CAS. To attest a service in a swarm, the CAS requires the help of a local attestation service (LAS). As part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure. To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS. CAS Development Environment To set up a development environment with CAS, you can perform the following steps. We assume that you run the following commands inside of a container with the scone CLI like sconecuratedimages/sconecli . Also, we assume that you have access to a Docker swarm managed by some node called faye . First, check that the swarm is properly installed: $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE faye Ready Active Leader If scone swarm ls issues some warnings or there are no SGX-capable machines listed, you might want to run scone swarm check to update the labels of the swarm nodes. To run LAS on all nodes of a cluster managed by node faye , pull the newest LAS image and then execute on all nodes of a SWARM: $ scone service pull sconecuratedimages/services:las $ scone service create --detach = true --mode global --publish mode = host,target = 18766 ,published = 18766 --name = las localhost:5000/services:las Check that the service las is indeed running, check with command ps : $ scone service ps las ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS xvnsruar64qx las.q26sp44pp12uf81zlyhb5pnxf localhost:5000/services:las edna Running Running 3 minutes ago *:18766- 18766/tcp zfgx4t292ew6 las.cr3bxhy0nqmg77goxaih5sw8d localhost:5000/services:las dorothy Running Running 3 minutes ago *:18766- 18766/tcp n441o9qqmnwa las.os7ihjrel2jb4bplcn81h7f0i localhost:5000/services:las faye Running Running 3 minutes ago *:18766- 18766/tcp Now, we can start the CAS service as follows. We first pull the newest CAS image and run it on one node of the swarm. $ scone service pull sconecuratedimages/services:cas $ scone service create --name cas --detach = true --publish 8081 :8081 --publish 18765 :18765 localhost:5000/services:cas Let's check that CAS is indeed running: $ scone service ps cas ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS e6b0skuph9ve cas.1 localhost:5000/services:cas edna Running Running 3 hours ago scontain.com , June 2018. Questions or Suggestions?","title":"SCONE CAS"},{"location":"SCONE_CAS/#scone-cas","text":"SCONE CAS ( Configuration and Attestation Service ) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with their command line arguments their environment variables after the service was attested by the CAS. To attest a service in a swarm, the CAS requires the help of a local attestation service (LAS). As part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure. To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS.","title":"SCONE CAS"},{"location":"SCONE_CAS/#cas-development-environment","text":"To set up a development environment with CAS, you can perform the following steps. We assume that you run the following commands inside of a container with the scone CLI like sconecuratedimages/sconecli . Also, we assume that you have access to a Docker swarm managed by some node called faye . First, check that the swarm is properly installed: $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE faye Ready Active Leader If scone swarm ls issues some warnings or there are no SGX-capable machines listed, you might want to run scone swarm check to update the labels of the swarm nodes. To run LAS on all nodes of a cluster managed by node faye , pull the newest LAS image and then execute on all nodes of a SWARM: $ scone service pull sconecuratedimages/services:las $ scone service create --detach = true --mode global --publish mode = host,target = 18766 ,published = 18766 --name = las localhost:5000/services:las Check that the service las is indeed running, check with command ps : $ scone service ps las ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS xvnsruar64qx las.q26sp44pp12uf81zlyhb5pnxf localhost:5000/services:las edna Running Running 3 minutes ago *:18766- 18766/tcp zfgx4t292ew6 las.cr3bxhy0nqmg77goxaih5sw8d localhost:5000/services:las dorothy Running Running 3 minutes ago *:18766- 18766/tcp n441o9qqmnwa las.os7ihjrel2jb4bplcn81h7f0i localhost:5000/services:las faye Running Running 3 minutes ago *:18766- 18766/tcp Now, we can start the CAS service as follows. We first pull the newest CAS image and run it on one node of the swarm. $ scone service pull sconecuratedimages/services:cas $ scone service create --name cas --detach = true --publish 8081 :8081 --publish 18765 :18765 localhost:5000/services:cas Let's check that CAS is indeed running: $ scone service ps cas ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS e6b0skuph9ve cas.1 localhost:5000/services:cas edna Running Running 3 hours ago scontain.com , June 2018. Questions or Suggestions?","title":"CAS Development Environment"},{"location":"SCONE_CLI/","text":"SCONE CLI We maintain a single unified command line interface (CLI) scone that helps to to start and stop secure containers as well as secure applications. scone also provides functionality to install and monitor SCONE hosts. Use of docker CLI In many cases you can just use the docker CLI to run SCONE containers. This is particularly useful if you want to run some SCONE containers on your local machine. This removes, for example, the need to setup ssh . The scone command is structured similar as the docker CLI or the infinit CLI: One needs to specify an object (like host ) and a command (like install ) and some options. For some commands, some of the options are actually not optional but mandatory. You need to have access to SCONE container images You require permissions to be able to run the examples given in this section: please send an email with your free Docker ID to info@scontain.com . To use the scone CLI , you need to start it in a container. Assuming that you have a docker engine installed, you try the following examples by running the following container: docker run -it sconecuratedimages/sconecli Typically, you would run the scone CLI in a container on your developer machine. The containers you would run on a different machine. Hence, you will need some ssh credentials inside this container. ssh setup The scone CLI uses ssh to log into remote hosts. We assume that ssh is setup in such a way that you do not need passwords to log into these hosts. Read section ssh setup to learn how to use import ssh credentials inside of a container. Help scone has a built in help. To get a list of all objects , just type: $ scone --help To get a list of all commands for a given object (like host), execute: $ scone host --help To get a list of all options for a given object and command (e.g., host install) and some examples, just execute: $ scone host install --help bash auto-completion If you are using bash as your shell, scone supports auto-completion. This means that instead you can use the TAB key to see the options. For example, $ scone TAB will show all available objects. If you have already specified an object, auto-completion helps you to list all commands: $ scone host TAB If you also specified an command, it will provide you with a list of options (that you have not specified yet): $ scone host install TAB Of course, it also supports auto-completion: $ scone host install -n TAB will result in $ scone host install -name scontain.com , November 2017. Questions or Suggestions?","title":"SCONE CLI"},{"location":"SCONE_CLI/#scone-cli","text":"We maintain a single unified command line interface (CLI) scone that helps to to start and stop secure containers as well as secure applications. scone also provides functionality to install and monitor SCONE hosts. Use of docker CLI In many cases you can just use the docker CLI to run SCONE containers. This is particularly useful if you want to run some SCONE containers on your local machine. This removes, for example, the need to setup ssh . The scone command is structured similar as the docker CLI or the infinit CLI: One needs to specify an object (like host ) and a command (like install ) and some options. For some commands, some of the options are actually not optional but mandatory. You need to have access to SCONE container images You require permissions to be able to run the examples given in this section: please send an email with your free Docker ID to info@scontain.com . To use the scone CLI , you need to start it in a container. Assuming that you have a docker engine installed, you try the following examples by running the following container: docker run -it sconecuratedimages/sconecli Typically, you would run the scone CLI in a container on your developer machine. The containers you would run on a different machine. Hence, you will need some ssh credentials inside this container. ssh setup The scone CLI uses ssh to log into remote hosts. We assume that ssh is setup in such a way that you do not need passwords to log into these hosts. Read section ssh setup to learn how to use import ssh credentials inside of a container.","title":"SCONE CLI"},{"location":"SCONE_CLI/#help","text":"scone has a built in help. To get a list of all objects , just type: $ scone --help To get a list of all commands for a given object (like host), execute: $ scone host --help To get a list of all options for a given object and command (e.g., host install) and some examples, just execute: $ scone host install --help","title":"Help"},{"location":"SCONE_CLI/#bash-auto-completion","text":"If you are using bash as your shell, scone supports auto-completion. This means that instead you can use the TAB key to see the options. For example, $ scone TAB will show all available objects. If you have already specified an object, auto-completion helps you to list all commands: $ scone host TAB If you also specified an command, it will provide you with a list of options (that you have not specified yet): $ scone host install TAB Of course, it also supports auto-completion: $ scone host install -n TAB will result in $ scone host install -name scontain.com , November 2017. Questions or Suggestions?","title":"bash auto-completion"},{"location":"SCONE_Compose/","text":"Stack Files SCONE supports to run secure applications consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker stack file. Such an extended stack file defines for each process that runs inside of an enclave, a unique hash value ( MRENCLAVE ). During startup, SCONE performs an attestation for all these secure processes to ensure that the hash of the started program is as expected, i.e., is equal to MRENCLAVE . Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process. By default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field not_scone: \"true\" . In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE). Let's consider an example, that consists of a haproxy that runs in native mode and is directly started with Docker Swarm (indicated by line not_scone: \"true\" ) Moreover, we start myapp (defined in container image myapp-image ) and specify MRENCLAVE . Only after ensuring that the program started in the enclave has the expected MRENCLAVE the arguments and the environment variables are passed to myapp . version : 3.1.scone services : primary-service : image : myapp-image:latest command : /myapp arg1 arg2 arg3 $my_password mrenclave : 5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335 environment : - SECURE_ENV=value - MY_PIN=$my_pin - MY_PASSWORD=$my_password working_dir : / proxy : image : haproxy command : haproxy --read_config_from_environ not_scone : true environment : - HAPROXY_CONFIG=a=b,c=d,3=4 secrets : my_pin : kind : numeric length : 4 my_password : kind : ascii length : 8 The extended stack file is split by SCONE into a standard stack file and a configuration information that is stored in the SCONE Configruation and Attestation Service ( CAS ): In the current version of scone, we assume that the stack file is stored on a trusted host. The split functionality is part of the scone CLI , i.e., you can split a stack file via $ scone cas split STACK_FILE --stack STACK_ID The stack ID is a randomly chose unique ID. This creates a stack file that can be used to start a set of services in your Swarm. Read nginx example for some more details how to do this. To be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address $IP . If you have set up the CAS in the way specified in CAS Setup , set environment variable IP to the external IP address of the swarm manager node. You can login into CAS via: $ scone cas login -c CA.PEM YOURID CASALIAS --host $IP :8081:18765 Replace YOURID by your desired user id and CASALIAS by a name that you want to use to refer to this cas (see example ). See nginx example for some more details and read CAS Setup to learn how to run a CAS service for development. scontain.com , March 2018. Questions or Suggestions?","title":"SCONE Compose"},{"location":"SCONE_Compose/#stack-files","text":"SCONE supports to run secure applications consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker stack file. Such an extended stack file defines for each process that runs inside of an enclave, a unique hash value ( MRENCLAVE ). During startup, SCONE performs an attestation for all these secure processes to ensure that the hash of the started program is as expected, i.e., is equal to MRENCLAVE . Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process. By default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field not_scone: \"true\" . In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE). Let's consider an example, that consists of a haproxy that runs in native mode and is directly started with Docker Swarm (indicated by line not_scone: \"true\" ) Moreover, we start myapp (defined in container image myapp-image ) and specify MRENCLAVE . Only after ensuring that the program started in the enclave has the expected MRENCLAVE the arguments and the environment variables are passed to myapp . version : 3.1.scone services : primary-service : image : myapp-image:latest command : /myapp arg1 arg2 arg3 $my_password mrenclave : 5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335 environment : - SECURE_ENV=value - MY_PIN=$my_pin - MY_PASSWORD=$my_password working_dir : / proxy : image : haproxy command : haproxy --read_config_from_environ not_scone : true environment : - HAPROXY_CONFIG=a=b,c=d,3=4 secrets : my_pin : kind : numeric length : 4 my_password : kind : ascii length : 8 The extended stack file is split by SCONE into a standard stack file and a configuration information that is stored in the SCONE Configruation and Attestation Service ( CAS ): In the current version of scone, we assume that the stack file is stored on a trusted host. The split functionality is part of the scone CLI , i.e., you can split a stack file via $ scone cas split STACK_FILE --stack STACK_ID The stack ID is a randomly chose unique ID. This creates a stack file that can be used to start a set of services in your Swarm. Read nginx example for some more details how to do this. To be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address $IP . If you have set up the CAS in the way specified in CAS Setup , set environment variable IP to the external IP address of the swarm manager node. You can login into CAS via: $ scone cas login -c CA.PEM YOURID CASALIAS --host $IP :8081:18765 Replace YOURID by your desired user id and CASALIAS by a name that you want to use to refer to this cas (see example ). See nginx example for some more details and read CAS Setup to learn how to run a CAS service for development. scontain.com , March 2018. Questions or Suggestions?","title":"Stack Files"},{"location":"SCONE_Curated_Images/","text":"SCONE Curated Images We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub: Private images: 1 Image Name Description sconecuratedimages/crosscompilers a container image with all the SCONE crosscompilers. sconecuratedimages/crosscompilers:runtime a container image that can run dynamically linked applications inside of an enclave. sconecuratedimages/apps:python-2.7-alpine3.6 a container image including a python interpreter running inside of an enclave. sconecuratedimages/apps:mongodb-alpine MongoDB container image. sconecuratedimages/apps:vault-alpine Vault container image. sconecuratedimages/apps:memcached-alpine Memcached container image. sconecuratedimages/apps:node-8.9-alpine a container image for node running inside an enclave. sconecuratedimages/apps:nginx-1.13-alpine a container image for nginx running inside an enclave. sconecuratedimages/apps:8-jdk-alpine a container image for Java applications running inside an enclave. Login in Access to some SCONE images is restricted. First, create a new docker hub ID (- in case you do not yet have one). Second, get access to the private images for evaluation by sending email to scontain.com with your docker hub id and short statement what images you want to evaluate and what you plan to do with the images. Second, log into to docker hub via: docker login before you will be able to pull any of the private curated images. Scone Compilers To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer. Dynamically-Linked Binaries Even if you have no SGX CPU extension / no SGX driver installed on your computer, you can use a standard gcc compiler - as long as the requirements mentioned in SGX ToolChain are satisfied. docker pull sconecuratedimages/muslgcc Note that the binaries generated with the above image are just native binaries, i.e., they run outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library. To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image: docker pull sconecuratedimages/crosscompilers:runtime Statically-Linked Binaries To generate statically-linked secure binaries you need a cross compiler. You can pull this image from Docker hub (you need to be granted access rights for that): docker pull sconecuratedimages/crosscompilers Scone Hello World You can pull the following (private) image. This image only runs in hardware mode: docker pull sconecuratedimages/helloworld If you installed the patched Docker engine (see SCONE Host Setup ), run the helloworld program inside of an enclave via docker run sconecuratedimages/helloworld Hello World This command will fail in case you have the standard Docker engine installed: docker run sconecuratedimages/helloworld error opening sgx device: No such file or directory You can run on the standard Docker engine - if you have the SGX driver installed: docker run --device = /dev/isgx sconecuratedimages/helloworld Hello World If you do not have the SGX driver installed, you get an error message: docker run --device = /dev/isgx sconecuratedimages/helloworld docker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device /dev/isgx : no such file or directory. In this case, install the SGX driver and the patched Docker engine as described in SCONE Host Setup . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled. Screencast scontain.com , August 2018. Questions or Suggestions? send email to to get access.","title":"SCONE Curated Images"},{"location":"SCONE_Curated_Images/#scone-curated-images","text":"We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub: Private images: 1 Image Name Description sconecuratedimages/crosscompilers a container image with all the SCONE crosscompilers. sconecuratedimages/crosscompilers:runtime a container image that can run dynamically linked applications inside of an enclave. sconecuratedimages/apps:python-2.7-alpine3.6 a container image including a python interpreter running inside of an enclave. sconecuratedimages/apps:mongodb-alpine MongoDB container image. sconecuratedimages/apps:vault-alpine Vault container image. sconecuratedimages/apps:memcached-alpine Memcached container image. sconecuratedimages/apps:node-8.9-alpine a container image for node running inside an enclave. sconecuratedimages/apps:nginx-1.13-alpine a container image for nginx running inside an enclave. sconecuratedimages/apps:8-jdk-alpine a container image for Java applications running inside an enclave.","title":"SCONE Curated Images"},{"location":"SCONE_Curated_Images/#login-in","text":"Access to some SCONE images is restricted. First, create a new docker hub ID (- in case you do not yet have one). Second, get access to the private images for evaluation by sending email to scontain.com with your docker hub id and short statement what images you want to evaluate and what you plan to do with the images. Second, log into to docker hub via: docker login before you will be able to pull any of the private curated images.","title":"Login in"},{"location":"SCONE_Curated_Images/#scone-compilers","text":"To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer.","title":"Scone Compilers"},{"location":"SCONE_Curated_Images/#dynamically-linked-binaries","text":"Even if you have no SGX CPU extension / no SGX driver installed on your computer, you can use a standard gcc compiler - as long as the requirements mentioned in SGX ToolChain are satisfied. docker pull sconecuratedimages/muslgcc Note that the binaries generated with the above image are just native binaries, i.e., they run outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library. To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image: docker pull sconecuratedimages/crosscompilers:runtime","title":"Dynamically-Linked Binaries"},{"location":"SCONE_Curated_Images/#statically-linked-binaries","text":"To generate statically-linked secure binaries you need a cross compiler. You can pull this image from Docker hub (you need to be granted access rights for that): docker pull sconecuratedimages/crosscompilers","title":"Statically-Linked Binaries"},{"location":"SCONE_Curated_Images/#scone-hello-world","text":"You can pull the following (private) image. This image only runs in hardware mode: docker pull sconecuratedimages/helloworld If you installed the patched Docker engine (see SCONE Host Setup ), run the helloworld program inside of an enclave via docker run sconecuratedimages/helloworld Hello World This command will fail in case you have the standard Docker engine installed: docker run sconecuratedimages/helloworld error opening sgx device: No such file or directory You can run on the standard Docker engine - if you have the SGX driver installed: docker run --device = /dev/isgx sconecuratedimages/helloworld Hello World If you do not have the SGX driver installed, you get an error message: docker run --device = /dev/isgx sconecuratedimages/helloworld docker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device /dev/isgx : no such file or directory. In this case, install the SGX driver and the patched Docker engine as described in SCONE Host Setup . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled.","title":"Scone Hello World"},{"location":"SCONE_Curated_Images/#screencast","text":"scontain.com , August 2018. Questions or Suggestions? send email to to get access.","title":"Screencast"},{"location":"SCONE_Dockerfile/","text":"Dockerfile We show how to generate a first secure container image with the help of a Dockerfile. Prerequisites Ensure that the sgx driver is installed ls /dev/isgx /dev/isgx If the driver is not installed, read Section SCONE Host Setup to learn how to install the SGX driver. Ensure that the patched docker engine is installed We need docker build in this example. This command does not permit to map devices in the newly created containers. Hence, we provide a patched Docker engine SCONE Host Setup . Install the tutorial Clone the tutorial: git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git Access to SCONE Curated Images Right now, access to the curated images is still restricted. Please, send email to to request access. Generate HelloAgain image (dynamically-linked) We first generate a hello again container image with a dynamically-linked secure program: cd SCONE_TUTORIAL/DLDockerFile The Dockerfile to generate the new image looks like this: FROM sconecuratedimages/crosscompilers:runtime RUN mkdir /hello COPY dyn_hello_again /hello/ CMD SCONE_MODE=HW SCONE_ALPINE=1 SCONE_VERSION=1 /hello/dyn_hello_again This assumes that we already generated the dynamically linked binary with an appropriately configured gcc. We generate this with the provided gcc image: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_again.c -o dyn_hello_again We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials): ./generate.sh You can run this program inside of enclave (with the output of debug messages): docker run -it sconecuratedimages/helloworld:dynamic export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello Again This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment. Running on a docker engine without access to SGX, we get an error message: docker run -it sconecuratedimages/helloworld:dynamic [ Error ] Could not create enclave: Error opening SGX device Screencast Generate HelloAgain image (statically-linked) We generate a hello again container image. cd SCONE_TUTORIAL/DockerFile The Dockerfile is quite straight forward: FROM sconecuratedimages/crosscompilers MAINTAINER Christof Fetzer christof.fetzer@gmail.com RUN mkdir /hello COPY hello_again.c /hello/ RUN cd /hello scone-gcc hello_again.c -o again CMD [ /hello/again ] You can either execute all step manually (see below) or you can just execute docker login ./generate.sh and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub. We define the image name and tag that we want to generate: export TAG = again export FULLTAG = sconecuratedimages/helloworld: $TAG We build the image: docker build --pull -t $FULLTAG . docker run -it $FULLTAG We push it to docker hub (will fail unless you have the right to push $FULLTAG ): docker push $FULLTAG Please change the image name to a repository on docker hub to which you can write: export TAG = latest export IMAGE_NAME = myrepository/helloAgain Screencast scontain.com , November 2017. Questions or Suggestions?","title":"SCONE Dockerfile"},{"location":"SCONE_Dockerfile/#dockerfile","text":"We show how to generate a first secure container image with the help of a Dockerfile.","title":"Dockerfile"},{"location":"SCONE_Dockerfile/#prerequisites","text":"","title":"Prerequisites"},{"location":"SCONE_Dockerfile/#ensure-that-the-sgx-driver-is-installed","text":"ls /dev/isgx /dev/isgx If the driver is not installed, read Section SCONE Host Setup to learn how to install the SGX driver.","title":"Ensure that the sgx driver is installed"},{"location":"SCONE_Dockerfile/#ensure-that-the-patched-docker-engine-is-installed","text":"We need docker build in this example. This command does not permit to map devices in the newly created containers. Hence, we provide a patched Docker engine SCONE Host Setup .","title":"Ensure that the patched docker engine is installed"},{"location":"SCONE_Dockerfile/#install-the-tutorial","text":"Clone the tutorial: git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git","title":"Install the tutorial"},{"location":"SCONE_Dockerfile/#access-to-scone-curated-images","text":"Right now, access to the curated images is still restricted. Please, send email to to request access.","title":"Access to SCONE Curated Images"},{"location":"SCONE_Dockerfile/#generate-helloagain-image-dynamically-linked","text":"We first generate a hello again container image with a dynamically-linked secure program: cd SCONE_TUTORIAL/DLDockerFile The Dockerfile to generate the new image looks like this: FROM sconecuratedimages/crosscompilers:runtime RUN mkdir /hello COPY dyn_hello_again /hello/ CMD SCONE_MODE=HW SCONE_ALPINE=1 SCONE_VERSION=1 /hello/dyn_hello_again This assumes that we already generated the dynamically linked binary with an appropriately configured gcc. We generate this with the provided gcc image: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_again.c -o dyn_hello_again We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials): ./generate.sh You can run this program inside of enclave (with the output of debug messages): docker run -it sconecuratedimages/helloworld:dynamic export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello Again This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment. Running on a docker engine without access to SGX, we get an error message: docker run -it sconecuratedimages/helloworld:dynamic [ Error ] Could not create enclave: Error opening SGX device","title":"Generate HelloAgain image (dynamically-linked)"},{"location":"SCONE_Dockerfile/#screencast","text":"","title":"Screencast"},{"location":"SCONE_Dockerfile/#generate-helloagain-image-statically-linked","text":"We generate a hello again container image. cd SCONE_TUTORIAL/DockerFile The Dockerfile is quite straight forward: FROM sconecuratedimages/crosscompilers MAINTAINER Christof Fetzer christof.fetzer@gmail.com RUN mkdir /hello COPY hello_again.c /hello/ RUN cd /hello scone-gcc hello_again.c -o again CMD [ /hello/again ] You can either execute all step manually (see below) or you can just execute docker login ./generate.sh and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub. We define the image name and tag that we want to generate: export TAG = again export FULLTAG = sconecuratedimages/helloworld: $TAG We build the image: docker build --pull -t $FULLTAG . docker run -it $FULLTAG We push it to docker hub (will fail unless you have the right to push $FULLTAG ): docker push $FULLTAG Please change the image name to a repository on docker hub to which you can write: export TAG = latest export IMAGE_NAME = myrepository/helloAgain","title":"Generate HelloAgain image (statically-linked)"},{"location":"SCONE_Dockerfile/#screencast_1","text":"scontain.com , November 2017. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_EE2EE/","text":"Example: End-to-End Encryption We show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted. In what follows, we only interact with one swarm. Hence, we set environment variable SCONE_MANAGER to point to the manager node of the swarm. Say, in our case the manage is called faye : $ export SCONE_MANAGER = faye We have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files from the file system and decrypting these files inside the enclave. Let's assume that we have the newest image in our local swarm registry: $ scone service pull sconecuratedimages/sconetainer:fss ... fss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size: 1783 new tag: localhost:5000/sconetainer:fss We will start this service as a stack. Hence, we define a simple stack/compose file: $ cat compose.yml EOF version: 3.1.scone services: nginx: image: 127.0.0.1:5000/sconetainer:fss command: nginx -p /nginx -c nginx.conf mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb fspf_path: /nginx/fspf.pb fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag: 5930bffbd9ea2f1317e6872b032334db working_dir: / ports: - 8190:8080 - 8192:8082 EOF We explain in section MrEnclave how to determine mrenclave. For now, this file contains some metadata related to the encrypted files: fspf_path the path of the file system protection file fspf_key the key used to encrypt the file system protection file fspf_tag the tag (i.e., MAC) of the file system protection file We provide a low level for encrypting files with the help of scone fspf . (We will soon release a higher level support that will simplify the encryption of files). In this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps us to to pass some secrets like the fspf_key to an enclave and protecting both the confidentiality as well as the integrity of this secret. Clients can identify the CAS via its certificate. In this example, we explicitly specify the certificate of the CAS: $ cat ca.pem EOF -----BEGIN CERTIFICATE----- MIICFzCCAb2gAwIBAgIJAOawWIYrvd1oMAoGCCqGSM49BAMCMGgxCzAJBgNVBAYT AkRFMQ8wDQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoM C3Njb250YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTAe Fw0xODA0MDYxMDIyMzdaFw0yODA0MDMxMDIyMzdaMGgxCzAJBgNVBAYTAkRFMQ8w DQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoMC3Njb250 YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTBZMBMGByqG SM49AgEGCCqGSM49AwEHA0IABD9u85bK+nFdbnQVgZuR/rA9BmNmow3v4v5srS3M YGpVmRqpNbb/QYQ9iJN854N42L9T6mGyI402tKPYyfwz3k+jUDBOMB0GA1UdDgQW BBRhGJGv9MWohiZD3AySHz/otlxbyDAfBgNVHSMEGDAWgBRhGJGv9MWohiZD3AyS Hz/otlxbyDAMBgNVHRMEBTADAQH/MAoGCCqGSM49BAMCA0gAMEUCIQCWkKHgeDgn 4PrHfmfjYYerxyFyGmWOKjO5UcijrPqI9wIgUyhZ2OwuyzsjTEC6ofR4fzlnrUQJ XlkFOKY2/HqOPVE= -----END CERTIFICATE----- EOF and now log into the CAS: $ export IP = x.y.z.u $ mkdir -p ~/.config $ rm -f ~/.config/scone_cmd.conf $ scone cas login -c ca.pem christof cas --host $IP :8081:18765 We split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS: $ scone cas split compose.yml --stack 283299 For now, the stack ID is a randomly chose unique ID. If you want to modify this stack later, you need to use the same stack ID. Show public part that is sent to docker stack (which is the original stack file name appended with \"docker.yml\"): $ cat compose.yml.docker.yml --- services: nginx: command: nginx environment: SCONE_CAS_ADDR: x.y.z.u:18765 SCONE_CONFIG_ID: christof/283299/nginx SCONE_LAS_ADDR: 172.17.0.1:18766 image: 127.0.0.1:5000/sconetainer:fss ports: - 8190:8080 - 8192:8082 version: 3.1 Note: in the above example we assume that the host on which nginx is running is available at address 172.17.0.1 . Hence, the LAS is available at address \"172.17.0.1:18766 . In case you run a non-standard configuration, you might need to change SCONE_LAS_ADDR to the ip address and port where the LAS is available. The LAS must run on the same host for the attestation to work. We can now deploy the service with the help of docker/scone stack: $ scone stack deploy --compose-file compose.yml nginx Creating network nginx_default Creating service nginx_nginx Show running stack: $ scone stack ls NAME SERVICES nginx 1 We can get some more information about this stack: $ scone stack ps nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS h08r5hmeu9bt nginx_nginx.1 127 .0.0.1:5000/sconetainer:fss dorothy Running Starting 19 seconds ago File Encryption Let's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node dorothy and execute the following commands: sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b01ab65e1579 127 .0.0.1:5000/sconetainer:fss nginx 6 minutes ago Up 6 minutes 8080 /tcp, 8082 /tcp nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix ... Let's look inside the container: sudo docker exec -it b01ab65e1579 sh $ ls bin media proc srv dev mnt root sys etc nginx run tmp home nginx-etc sbin usr lib opt scone-test.key var The nginx configuration file as well as the html files should be encrypted. Let's verify this: $ cd nginx $ ls certificate.pem key.pem nginx.conf www_root fspf.pb mime.types nginx.pid $ head -c 80 nginx.conf ] P?\u025f 4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t ?2?L????????y??\u02f2?\u07cb Ok, the nginx configuration file seems to be encrypted. Now, look at the html files too: $ cd www_root $ ls 4K SCONE_TUTORIAL GO aboutScone Python appsecurity ... $ head -c 80 index.html ????V?\u036a?rhk? k???? $? .???k?\u042c?y ??\u0660n?+????P2?G;o_ .i?);?I??xFg[???[f? scontain.com , December 2017. Questions or Suggestions?","title":"Example E2E encryption"},{"location":"SCONE_EE2EE/#example-end-to-end-encryption","text":"We show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted. In what follows, we only interact with one swarm. Hence, we set environment variable SCONE_MANAGER to point to the manager node of the swarm. Say, in our case the manage is called faye : $ export SCONE_MANAGER = faye We have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files from the file system and decrypting these files inside the enclave. Let's assume that we have the newest image in our local swarm registry: $ scone service pull sconecuratedimages/sconetainer:fss ... fss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size: 1783 new tag: localhost:5000/sconetainer:fss We will start this service as a stack. Hence, we define a simple stack/compose file: $ cat compose.yml EOF version: 3.1.scone services: nginx: image: 127.0.0.1:5000/sconetainer:fss command: nginx -p /nginx -c nginx.conf mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb fspf_path: /nginx/fspf.pb fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag: 5930bffbd9ea2f1317e6872b032334db working_dir: / ports: - 8190:8080 - 8192:8082 EOF We explain in section MrEnclave how to determine mrenclave. For now, this file contains some metadata related to the encrypted files: fspf_path the path of the file system protection file fspf_key the key used to encrypt the file system protection file fspf_tag the tag (i.e., MAC) of the file system protection file We provide a low level for encrypting files with the help of scone fspf . (We will soon release a higher level support that will simplify the encryption of files). In this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps us to to pass some secrets like the fspf_key to an enclave and protecting both the confidentiality as well as the integrity of this secret. Clients can identify the CAS via its certificate. In this example, we explicitly specify the certificate of the CAS: $ cat ca.pem EOF -----BEGIN CERTIFICATE----- MIICFzCCAb2gAwIBAgIJAOawWIYrvd1oMAoGCCqGSM49BAMCMGgxCzAJBgNVBAYT AkRFMQ8wDQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoM C3Njb250YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTAe Fw0xODA0MDYxMDIyMzdaFw0yODA0MDMxMDIyMzdaMGgxCzAJBgNVBAYTAkRFMQ8w DQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoMC3Njb250 YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTBZMBMGByqG SM49AgEGCCqGSM49AwEHA0IABD9u85bK+nFdbnQVgZuR/rA9BmNmow3v4v5srS3M YGpVmRqpNbb/QYQ9iJN854N42L9T6mGyI402tKPYyfwz3k+jUDBOMB0GA1UdDgQW BBRhGJGv9MWohiZD3AySHz/otlxbyDAfBgNVHSMEGDAWgBRhGJGv9MWohiZD3AyS Hz/otlxbyDAMBgNVHRMEBTADAQH/MAoGCCqGSM49BAMCA0gAMEUCIQCWkKHgeDgn 4PrHfmfjYYerxyFyGmWOKjO5UcijrPqI9wIgUyhZ2OwuyzsjTEC6ofR4fzlnrUQJ XlkFOKY2/HqOPVE= -----END CERTIFICATE----- EOF and now log into the CAS: $ export IP = x.y.z.u $ mkdir -p ~/.config $ rm -f ~/.config/scone_cmd.conf $ scone cas login -c ca.pem christof cas --host $IP :8081:18765 We split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS: $ scone cas split compose.yml --stack 283299 For now, the stack ID is a randomly chose unique ID. If you want to modify this stack later, you need to use the same stack ID. Show public part that is sent to docker stack (which is the original stack file name appended with \"docker.yml\"): $ cat compose.yml.docker.yml --- services: nginx: command: nginx environment: SCONE_CAS_ADDR: x.y.z.u:18765 SCONE_CONFIG_ID: christof/283299/nginx SCONE_LAS_ADDR: 172.17.0.1:18766 image: 127.0.0.1:5000/sconetainer:fss ports: - 8190:8080 - 8192:8082 version: 3.1 Note: in the above example we assume that the host on which nginx is running is available at address 172.17.0.1 . Hence, the LAS is available at address \"172.17.0.1:18766 . In case you run a non-standard configuration, you might need to change SCONE_LAS_ADDR to the ip address and port where the LAS is available. The LAS must run on the same host for the attestation to work. We can now deploy the service with the help of docker/scone stack: $ scone stack deploy --compose-file compose.yml nginx Creating network nginx_default Creating service nginx_nginx Show running stack: $ scone stack ls NAME SERVICES nginx 1 We can get some more information about this stack: $ scone stack ps nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS h08r5hmeu9bt nginx_nginx.1 127 .0.0.1:5000/sconetainer:fss dorothy Running Starting 19 seconds ago","title":"Example: End-to-End Encryption"},{"location":"SCONE_EE2EE/#file-encryption","text":"Let's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node dorothy and execute the following commands: sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b01ab65e1579 127 .0.0.1:5000/sconetainer:fss nginx 6 minutes ago Up 6 minutes 8080 /tcp, 8082 /tcp nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix ... Let's look inside the container: sudo docker exec -it b01ab65e1579 sh $ ls bin media proc srv dev mnt root sys etc nginx run tmp home nginx-etc sbin usr lib opt scone-test.key var The nginx configuration file as well as the html files should be encrypted. Let's verify this: $ cd nginx $ ls certificate.pem key.pem nginx.conf www_root fspf.pb mime.types nginx.pid $ head -c 80 nginx.conf ] P?\u025f 4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t ?2?L????????y??\u02f2?\u07cb Ok, the nginx configuration file seems to be encrypted. Now, look at the html files too: $ cd www_root $ ls 4K SCONE_TUTORIAL GO aboutScone Python appsecurity ... $ head -c 80 index.html ????V?\u036a?rhk? k???? $? .???k?\u042c?y ??\u0660n?+????P2?G;o_ .i?);?I??xFg[???[f? scontain.com , December 2017. Questions or Suggestions?","title":"File Encryption"},{"location":"SCONE_ENV/","text":"SCONE Environment Variables To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service. Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters. SCONE Configuration File The location of the SCONE configuration file can be controlled with an environment variable: SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened, the default configuration file located in /etc/sgx-musl.conf is read instead. If the default configuration file cannot be read either, the program exits with an error message. Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves. The configuration file can define most of the behaviors that one can define via environment variables. However, the SCONE_... environment variables have higher priority than the settings from the config file. Format of SCONE Configuration File The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: Command Option(s) Description Q n defines the number of queues used to execute system calls n [number of queues] sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the SCONE_QUEUES environment variable H s defines the heap size in bytes s [heap size in bytes ] sets the size of heap, equivalent to setting SCONE_HEAP environment variable P N determines the backoff behavior of the queues N [spin number] equivalent to setting SCONE_SSPINS environment variable L S determines the backoff behavior of the queues S [sleep time] equivalent to setting SCONE_SSLEEP environment variable; s C Q R sthread serving system calls outside of an enclave C [core-no] if non-negative number: pin this sthread to core C ; if a negative number, do not pin this thread Q [queue-no] in [0..n] ; this sthreads serves this queue R [realtime] always set this to 0 e C Q R ethread running inside of enclave and executes application threads (which we call lthreads) C [core-no] non-negative number: pin to this core; negative number: no pinning ot this thread Q [queue-no] in [0..n] : this sthreads serves this queue R [realtime] always set this to 0 The number of sthreads is automatically increased if more sthreads are needed to process system calls. An sthread will block if it does not have any work left to do. Hence, we recommend to start exactly one sthread per ethread . ethreads will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one ethread per hyper-thread. Unless you want to limit the the number of CPU resources an enclave should use, the following is a good generic configuration file: $ sudo tee /etc/sgx-musl.conf EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF Run Mode SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode. Value Description SCONE_MODE=HW run program inside of enclave. If program fails to create an enclave, it will fail. SCONE_MODE=SIM run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode. SCONE_MODE=AUTO run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode Memory-Related Environment Variables SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave. Value Description SCONE_HEAP=s the requested heap size in bytes SCONE_HEAP=sK the requested heap size in KiloBytes SCONE_HEAP=sM the requested heap size in MegaBytes SCONE_HEAP=sG the requested heap size in GigaBytes SCONE_STACK : size of the stack allocated to threads spawned in the enclave. The default stack size is 128 KBytes. Please increase this if your expect your threads to require more than 128KBytes. For programs like MariaDB, MongoDB, Python, or node, we increase the stack size to 4 MBytes . Value Description SCONE_STACK=s the requested stack size in bytes SCONE_STACK=sK the requested stack size in KiloBytes SCONE_STACK=sM the requested stack size in MegaBytes Debug SCONE_VERSION if defined, SCONE will print the values of some of the SCONE environment variables during startup. SCONE_LOG set to value between 0 (no) and 7 (debug) to see more or less messages on stderr from the SCONE platform. Messages could be warnings if certain functions are not (yet) implemented inside of an enclave. Example output for SCONE_VERSION=1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_LOG = 6 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes Revision: e37dda73a6db435973f2e00347bd4cf462e4e027 ( Sat Aug 25 22 :59:30 2018 +0200 ) Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Dynamic library support: SCONE_ALLOW_DLOPEN=\"1\" : if defined, permit to load shared libraries after startup. These libraries must be authenticated or encrypted. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of MRENCLAVE . The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup. SCONE_ALLOW_DLOPEN=\"2\" : must be used for debugging only : this value enables loading of dynamic libraries after startup that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should eiter set SCONE_ALLOW_DLOPEN=\"1\" or you should not define SCONE_ALLOW_DLOPEN . Performance tuning variables: SCONE_QUEUES : number of systemcall queues to be used. SCONE_SLOTS : systemcall queue length: must be larger than the maximum number of lthreads . SCONE_SIGPIPE : if set to \"1\", SIGPIPE signals are delivered to the application SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. SCONE_SSLEEP : determines how fast we increase the backoff. Safety SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker. Dynamic link loader The dynamic link loader is part of image sconecuratedimages/crosscompilers:runtime ( see tutorial ). SCONE_LD_DEBUG : print the dynamically loaded libraries and their SHA-256 hashes LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries. LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself. SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave. scontain.com , November 2017. Questions or Suggestions?","title":"environment variables"},{"location":"SCONE_ENV/#scone-environment-variables","text":"To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service. Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters.","title":"SCONE Environment Variables"},{"location":"SCONE_ENV/#scone-configuration-file","text":"The location of the SCONE configuration file can be controlled with an environment variable: SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened, the default configuration file located in /etc/sgx-musl.conf is read instead. If the default configuration file cannot be read either, the program exits with an error message. Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves. The configuration file can define most of the behaviors that one can define via environment variables. However, the SCONE_... environment variables have higher priority than the settings from the config file.","title":"SCONE Configuration File"},{"location":"SCONE_ENV/#format-of-scone-configuration-file","text":"The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: Command Option(s) Description Q n defines the number of queues used to execute system calls n [number of queues] sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the SCONE_QUEUES environment variable H s defines the heap size in bytes s [heap size in bytes ] sets the size of heap, equivalent to setting SCONE_HEAP environment variable P N determines the backoff behavior of the queues N [spin number] equivalent to setting SCONE_SSPINS environment variable L S determines the backoff behavior of the queues S [sleep time] equivalent to setting SCONE_SSLEEP environment variable; s C Q R sthread serving system calls outside of an enclave C [core-no] if non-negative number: pin this sthread to core C ; if a negative number, do not pin this thread Q [queue-no] in [0..n] ; this sthreads serves this queue R [realtime] always set this to 0 e C Q R ethread running inside of enclave and executes application threads (which we call lthreads) C [core-no] non-negative number: pin to this core; negative number: no pinning ot this thread Q [queue-no] in [0..n] : this sthreads serves this queue R [realtime] always set this to 0 The number of sthreads is automatically increased if more sthreads are needed to process system calls. An sthread will block if it does not have any work left to do. Hence, we recommend to start exactly one sthread per ethread . ethreads will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one ethread per hyper-thread. Unless you want to limit the the number of CPU resources an enclave should use, the following is a good generic configuration file: $ sudo tee /etc/sgx-musl.conf EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF","title":"Format of SCONE Configuration File"},{"location":"SCONE_ENV/#run-mode","text":"SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode. Value Description SCONE_MODE=HW run program inside of enclave. If program fails to create an enclave, it will fail. SCONE_MODE=SIM run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode. SCONE_MODE=AUTO run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode","title":"Run Mode"},{"location":"SCONE_ENV/#memory-related-environment-variables","text":"SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave. Value Description SCONE_HEAP=s the requested heap size in bytes SCONE_HEAP=sK the requested heap size in KiloBytes SCONE_HEAP=sM the requested heap size in MegaBytes SCONE_HEAP=sG the requested heap size in GigaBytes SCONE_STACK : size of the stack allocated to threads spawned in the enclave. The default stack size is 128 KBytes. Please increase this if your expect your threads to require more than 128KBytes. For programs like MariaDB, MongoDB, Python, or node, we increase the stack size to 4 MBytes . Value Description SCONE_STACK=s the requested stack size in bytes SCONE_STACK=sK the requested stack size in KiloBytes SCONE_STACK=sM the requested stack size in MegaBytes","title":"Memory-Related Environment Variables"},{"location":"SCONE_ENV/#debug","text":"SCONE_VERSION if defined, SCONE will print the values of some of the SCONE environment variables during startup. SCONE_LOG set to value between 0 (no) and 7 (debug) to see more or less messages on stderr from the SCONE platform. Messages could be warnings if certain functions are not (yet) implemented inside of an enclave. Example output for SCONE_VERSION=1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_LOG = 6 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes Revision: e37dda73a6db435973f2e00347bd4cf462e4e027 ( Sat Aug 25 22 :59:30 2018 +0200 ) Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl","title":"Debug"},{"location":"SCONE_ENV/#dynamic-library-support","text":"SCONE_ALLOW_DLOPEN=\"1\" : if defined, permit to load shared libraries after startup. These libraries must be authenticated or encrypted. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of MRENCLAVE . The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup. SCONE_ALLOW_DLOPEN=\"2\" : must be used for debugging only : this value enables loading of dynamic libraries after startup that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should eiter set SCONE_ALLOW_DLOPEN=\"1\" or you should not define SCONE_ALLOW_DLOPEN .","title":"Dynamic library support:"},{"location":"SCONE_ENV/#performance-tuning-variables","text":"SCONE_QUEUES : number of systemcall queues to be used. SCONE_SLOTS : systemcall queue length: must be larger than the maximum number of lthreads . SCONE_SIGPIPE : if set to \"1\", SIGPIPE signals are delivered to the application SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. SCONE_SSLEEP : determines how fast we increase the backoff.","title":"Performance tuning variables:"},{"location":"SCONE_ENV/#safety","text":"SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker.","title":"Safety"},{"location":"SCONE_ENV/#dynamic-link-loader","text":"The dynamic link loader is part of image sconecuratedimages/crosscompilers:runtime ( see tutorial ). SCONE_LD_DEBUG : print the dynamically loaded libraries and their SHA-256 hashes LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries. LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself. SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave. scontain.com , November 2017. Questions or Suggestions?","title":"Dynamic link loader"},{"location":"SCONE_Fileshield/","text":"SCONE File Protection SCONE supports the transparent encryption and/or authentication of files. By transparent , we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection: a low-level interface intended to be used at the developer site. We assumet that the developer machine is sufficiently trust worthy. This is made available via command scone fspf and described in this document. a high-level interface simplifies the use of the file protection and it does and in particular, takes care of key management. ( The high-level interface is not yet available ). Concepts The underlying idea of SCONE file protection is that a user specifies that each file is either : authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity, encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or not-protected , i.e. SCONE reads and write the files without any extra protection mechanisms. For example, you might use not-protected if your application already encrypts its files or if you need direct access to devices. Marking all files individually as either authenticated , encrypted , or not-protected would not be very practical. Hence, we support to partition the filesystem into regions : regions do not overlap and each file belongs to exactly one region. A region is defined by a path. For example, region / is the root region and you could, for example, specify that all files in region / must be authenticated. You can define a second region, for example, region /data/db and that this region is encrypted. Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file /etc/db.conf would belong, in this case, to region / and file /data/db/table.db would belong to region /data/db . SCONE supports ephemeral regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted. When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. All files in an ephemeral region are lost when the application exits. All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a kernel region. For each region, you need to specify if the region is either ephemeral or kernel . Each region belongs to one of the following six classes: { ephemeral | kernel } X { not-protected | authenticated | encrypted } Example Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted. Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory volume ) and we will later mount this in the container as /data . The original (non-encrypted) files are stored in directory data-original . mkdir -p volume mkdir -p data-original Let's write some files in the data-original directory: cat data-original/hello.txt EOF Hello World EOF cat data-original/world.py EOF f = open( /data/hello.txt , r ) print str(f.read()) EOF Let's check that volume is empty and we print the hash values of the two files in data-original : ls volume shasum data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c data-original/world.py Now, we start the SCONE crosscompiler in a container to create the encrypted volume: docker run -it -v $PWD /volume:/data -v $PWD /data-original:/data-original sconecuratedimages/crosscompilers File System Protection File All the metadata required for checking the consistency of the files is stored in a file system protection file , or, short fspf . SCONE supports multiple *fspf*s. Let's start with a simple example with a single fspf . The fspf file is created via command scone fspf create and let us name this file fspf.pb . We execute the following commands inside the container (as indicated by the $ prompt): $ cd /data $ scone fspf create fspf.pb Created empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423 We can now split the file system in regions , a region is a subtree. You can add regions to a fspf with the help of command scone fspf addr . Each region has exactly one of the following properties: authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option --authenticated . encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option --encrypted . not-protected : files are neither authenticated nor encrypted. Specify command line option --not-protected . File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead. Say for now, that by default we do not protect files and we want to read files and write back changed files to the file system. To do so, we define that the root tree is --kernel as well as --not-protected : $ scone fspf addr fspf.pb / --kernel / --not-protected Added region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84 Let us add another region /data that should be encrypted and persisted. To encrypt the files, we specify option --encrypted . We specify option --kernel followed by a path (here, also /data ) to request that files in this region are written to the kernel file system into directory /data . $ scone fspf addr fspf.pb /data --encrypted --kernel /data Added region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7 The encryption key for a file is chosen at random and stored in fspf.pb . We use the Intel random number generator RdRand to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option --key-length 16 and --key-length 24 when creating a region with command scone fspf addr . Now, that we defined the regions, i.e., / and /data , we can add files to region /data . Let's just add all files in /data-original , encrypt these and write the encrypted files to /data . Note, the first /data argument specifies the protection region that determines the protection policy. The second, specifies where the encrypted files will be stored. That is, the command iterates over and reads the existing files in /data-original and encrypts them. The encrypted file content is written into the directory /data while the protection metadata of the individual files is added to the fsfp.pb file. $ scone fspf addf fspf.pb /data /data-original /data Added files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63 Note that if the /data region would have been only authenticated and not encrypted, the tool does not need to write out any (encrypted) files. It will only add the file names and the checksums (tags) of the files located in /data-original to the fspf.pb file. Thus, you could drop the last argument in this case. Coming back to the above example, we can now compare the hash values of the original files and the encrypted files: $ shasum /data/* 87fd97468024e3d2864516ff5840e15d9615340d /data/fspf.pb 31732914910f4a08b9832c442074b0932915476c /data/hello.txt 8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c /data/world.py $ shasum /data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a /data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c /data-original/world.py The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb $ scone fspf encrypt fspf.pb /data-original/keytag We store the random encryption key as well as the tag of file fspf.pb in file /data-original/keytag . We introduce a very simple program that reads the two files: $ cat example.c EOF #include stdio.h #include stdlib.h void printfile(const char* fn) { FILE *fp = fopen(fn, r ); char c; while((c=fgetc(fp))!=EOF ){ printf ( %c ,c ) ; } fclose ( fp ) ; } int main () { printfile ( /data/hello.txt ) ; printfile ( /data/world.py ) ; } EOF Let's crosscompile this program: scone gcc example.c -o example Executing this program results in an output like this: $./example R??C? q?z??E?? | \u042e? } \u00fc ?o $? ?!rga??\u0387* ` ?????????Gw? We need to activate the file system shield via environment variables by setting the location of the file system protection file (in SCONE_FSPF ), the encryption key of the file (in SCONE_FSPF_KEY ) and the tag of the fspf (in SCONE_FSPF_TAG ). We can extract the encryption key as well as the tag of fspf.pb from file /data-original/keytag : $ export SCONE_FSPF_KEY = $( cat /data-original/keytag | awk {print $11} ) $ export SCONE_FSPF_TAG = $( cat /data-original/keytag | awk {print $9} ) $ export SCONE_FSPF = /data/fspf.pb We can now execute this program again: $ ./example Hello World f = open ( /data/hello.txt , r ) print str ( f.read ()) Variables SCONE_FSPF_KEY , SCONE_FSPF_TAG and SCONE_FSPF should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please read the section about end-to-end encryption . Python Let's try a similar approach for Python. In the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file: docker run -it -v $PWD /volume:/data sconecuratedimages/apps:python-2.7-alpine3.6 bash The files /data/world.py and /data/hello.txt are encrypted: $ cat /data/world.py ? = ??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y?? f? Let's activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /data/fspf.pb We can now run the encrypted world.py program with the the Python interpreter: SCONE_HEAP = 100000000 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python /data/world.py export SCONE_QUEUES = 1 ... Hello World Protecting the Root Region Note that in the above example, Python will not be permitted to load dynamic libraries outside of the protected directory /data : a dynamic library must reside in either an authenticated or an encrypted region. To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries. Let us show how to authenticate all files in region / $ scone fspf addr fspf.pb / --kernel / --authenticated We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region / . Adding files from an existing container image We show how to add a subset of the files of container image sconecuratedimages/apps:python-2.7-alpine3.6 to our root region. To do so, we ensure that we have the newest images: docker pull sconecuratedimages/apps:python-2.7-alpine3.6 docker pull sconecuratedimages/crosscompilers How can we add all files in a container to the fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to /var/run/docker.sock : docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v $PWD /volume:/data -v $PWD /data-original:/data-original sconecuratedimages/crosscompilers Let us ensure that Docker is installed in this container: apt-get update apt-get install -y docker.io Now, we want to add all files of some target container. In our example, this is an instance of image sconecuratedimages/apps:python-2.7-alpine3.6 . We ensure that we pulled the latest image before we start the container: CONTAINER_ID = ` docker run -d sconecuratedimages/apps:python-2.7-alpine3.6 printf OK ` We can now copy all files from this container into a new directory rootvol : $ cd $ mkdir -p rootvol $ docker cp $CONTAINER_ID :/ ./rootvol Now that we have a copy of the files, we should not forget to garbage collect this container: docker rm $CONTAINER_ID Let's remove some directories that we do not want our program to access, like for example, /dev : $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++ Now, we create a root fspf : $ scone fspf create fspf.pb $ scone fspf addr fspf.pb / --kernel / --authenticated $ scone fspf addf fspf.pb / ./rootvol / $ scone fspf encrypt fspf.pb keytag We can now create a new container image with this file system protection file using this Dockerfile $ cat Dockerfile EOF FROM sconecuratedimages/apps:python-2.7-alpine3.6 COPY fspf.pb / EOF $ docker build -t sconecuratedimages/apps:python-2.7-alpine3.6-authenticated . We can run a container as follows: $ docker run -it sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh Let us activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb Let's run python with authenticated file system: SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python Checking the File System Shield Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in side of a python container: docker run -i sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh $ cat helloworld-manual.py EOF print Hello World EOF When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file. $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld-manual.py ( fails ) We can, however, add a new file via programs that have access to the key of the fspf . We can, for example, write a python program to add a new python program to the file system. By default, we disable that the root fspf is updated. We can enable updates by setting environment variable SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable SCONE_FSPF_MUTABLE=1 ). $ SCONE_HEAP = 1000000000 SCONE_FSPF_MUTABLE = 1 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python PYTHON f = open( helloworld.py , w ) f.write( print Hello World \\n ) f.close() PYTHON ``` The tag of the file system protection file is now changed. We can determine the new TAG with the help of command scone fspf show : $ export SCONE_FSPF_TAG = $( scone fspf show --tag /fspf.pb ) Now, we can run the new helloworld.py : $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld.py ... Hello World Extended Example To learn how to use multiple file system protection files, please have a look at the following screencast. Below is the script that is executed in the screencast: docker run -it -v $PWD :/mnt sconecuratedimages/crosscompilers $ mkdir -p /example $ mkdir -p /mnt/authenticated/ $ mkdir -p /mnt/encrypted/ $ cd /example $ mkdir -p .original $ scone fspf create fspf.pb $ scone fspf create authenticated.pb $ scone fspf create encrypted.pb # add protection regions $ scone fspf addr fspf.pb / -e --ephemeral $ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated $ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted # add files # enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated $ scone fspf addf authenticated.pb /mnt/authenticated ./original # enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted # the client will write the encrypted files to ./mnt/encrypted $ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted encrypted_key = ` scone fspf encrypt encrypted.pb | awk {print $11} ` $ echo encrypted.pb key: ${ encrypted_key } $ scone fspf addfspf fspf.pb authenticated.pb $ scone fspf addfspf fspf.pb encrypted.pb ${ encrypted_key } $ cat example.c EOF #include stdio.h int main() { FILE *fp = fopen( /mnt/authenticated/hello , w ); fprintf(fp, hello world\\n ); fclose(fp); fp = fopen( /mnt/encrypted/hello , w ); fprintf(fp, hello world\\n ); fclose(fp); } EOF $ scone gcc example.c -o sgxex $ cat /etc/sgx-musl.conf EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF $ SCONE_FSPF = fspf.pb ./sgxex $ cat /mnt/authenticated/hello $ cat /mnt/encrypted/hello $ cat cat.c EOF #include stdio.h int main() { char buf[80]; FILE *fp = fopen( /mnt/authenticated/hello , r ); fgets(buf, sizeof(buf), fp); fclose(fp); printf( read: %s \\n , buf); fp = fopen( /mnt/encrypted/hello , r ); fgets(buf, sizeof(buf), fp); fclose(fp); printf( read: %s \\n , buf); } EOF $ scone gcc cat.c -o native_cat $ ./native_cat $ scone gcc cat.c -o sgxcat $ SCONE_FSPF = fspf.pb ./sgxcat Notes The SCONE File Protection documentation is not yet completed and more information will be provided soon. scontain.com , 2018. Questions or Suggestions?","title":"scone fspf"},{"location":"SCONE_Fileshield/#scone-file-protection","text":"SCONE supports the transparent encryption and/or authentication of files. By transparent , we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection: a low-level interface intended to be used at the developer site. We assumet that the developer machine is sufficiently trust worthy. This is made available via command scone fspf and described in this document. a high-level interface simplifies the use of the file protection and it does and in particular, takes care of key management. ( The high-level interface is not yet available ).","title":"SCONE File Protection"},{"location":"SCONE_Fileshield/#concepts","text":"The underlying idea of SCONE file protection is that a user specifies that each file is either : authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity, encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or not-protected , i.e. SCONE reads and write the files without any extra protection mechanisms. For example, you might use not-protected if your application already encrypts its files or if you need direct access to devices. Marking all files individually as either authenticated , encrypted , or not-protected would not be very practical. Hence, we support to partition the filesystem into regions : regions do not overlap and each file belongs to exactly one region. A region is defined by a path. For example, region / is the root region and you could, for example, specify that all files in region / must be authenticated. You can define a second region, for example, region /data/db and that this region is encrypted. Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file /etc/db.conf would belong, in this case, to region / and file /data/db/table.db would belong to region /data/db . SCONE supports ephemeral regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted. When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. All files in an ephemeral region are lost when the application exits. All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a kernel region. For each region, you need to specify if the region is either ephemeral or kernel . Each region belongs to one of the following six classes: { ephemeral | kernel } X { not-protected | authenticated | encrypted }","title":"Concepts"},{"location":"SCONE_Fileshield/#example","text":"Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted. Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory volume ) and we will later mount this in the container as /data . The original (non-encrypted) files are stored in directory data-original . mkdir -p volume mkdir -p data-original Let's write some files in the data-original directory: cat data-original/hello.txt EOF Hello World EOF cat data-original/world.py EOF f = open( /data/hello.txt , r ) print str(f.read()) EOF Let's check that volume is empty and we print the hash values of the two files in data-original : ls volume shasum data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c data-original/world.py Now, we start the SCONE crosscompiler in a container to create the encrypted volume: docker run -it -v $PWD /volume:/data -v $PWD /data-original:/data-original sconecuratedimages/crosscompilers","title":"Example"},{"location":"SCONE_Fileshield/#file-system-protection-file","text":"All the metadata required for checking the consistency of the files is stored in a file system protection file , or, short fspf . SCONE supports multiple *fspf*s. Let's start with a simple example with a single fspf . The fspf file is created via command scone fspf create and let us name this file fspf.pb . We execute the following commands inside the container (as indicated by the $ prompt): $ cd /data $ scone fspf create fspf.pb Created empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423 We can now split the file system in regions , a region is a subtree. You can add regions to a fspf with the help of command scone fspf addr . Each region has exactly one of the following properties: authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option --authenticated . encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option --encrypted . not-protected : files are neither authenticated nor encrypted. Specify command line option --not-protected . File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead. Say for now, that by default we do not protect files and we want to read files and write back changed files to the file system. To do so, we define that the root tree is --kernel as well as --not-protected : $ scone fspf addr fspf.pb / --kernel / --not-protected Added region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84 Let us add another region /data that should be encrypted and persisted. To encrypt the files, we specify option --encrypted . We specify option --kernel followed by a path (here, also /data ) to request that files in this region are written to the kernel file system into directory /data . $ scone fspf addr fspf.pb /data --encrypted --kernel /data Added region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7 The encryption key for a file is chosen at random and stored in fspf.pb . We use the Intel random number generator RdRand to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option --key-length 16 and --key-length 24 when creating a region with command scone fspf addr . Now, that we defined the regions, i.e., / and /data , we can add files to region /data . Let's just add all files in /data-original , encrypt these and write the encrypted files to /data . Note, the first /data argument specifies the protection region that determines the protection policy. The second, specifies where the encrypted files will be stored. That is, the command iterates over and reads the existing files in /data-original and encrypts them. The encrypted file content is written into the directory /data while the protection metadata of the individual files is added to the fsfp.pb file. $ scone fspf addf fspf.pb /data /data-original /data Added files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63 Note that if the /data region would have been only authenticated and not encrypted, the tool does not need to write out any (encrypted) files. It will only add the file names and the checksums (tags) of the files located in /data-original to the fspf.pb file. Thus, you could drop the last argument in this case. Coming back to the above example, we can now compare the hash values of the original files and the encrypted files: $ shasum /data/* 87fd97468024e3d2864516ff5840e15d9615340d /data/fspf.pb 31732914910f4a08b9832c442074b0932915476c /data/hello.txt 8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c /data/world.py $ shasum /data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a /data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c /data-original/world.py The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb $ scone fspf encrypt fspf.pb /data-original/keytag We store the random encryption key as well as the tag of file fspf.pb in file /data-original/keytag . We introduce a very simple program that reads the two files: $ cat example.c EOF #include stdio.h #include stdlib.h void printfile(const char* fn) { FILE *fp = fopen(fn, r ); char c; while((c=fgetc(fp))!=EOF ){ printf ( %c ,c ) ; } fclose ( fp ) ; } int main () { printfile ( /data/hello.txt ) ; printfile ( /data/world.py ) ; } EOF Let's crosscompile this program: scone gcc example.c -o example Executing this program results in an output like this: $./example R??C? q?z??E?? | \u042e? } \u00fc ?o $? ?!rga??\u0387* ` ?????????Gw? We need to activate the file system shield via environment variables by setting the location of the file system protection file (in SCONE_FSPF ), the encryption key of the file (in SCONE_FSPF_KEY ) and the tag of the fspf (in SCONE_FSPF_TAG ). We can extract the encryption key as well as the tag of fspf.pb from file /data-original/keytag : $ export SCONE_FSPF_KEY = $( cat /data-original/keytag | awk {print $11} ) $ export SCONE_FSPF_TAG = $( cat /data-original/keytag | awk {print $9} ) $ export SCONE_FSPF = /data/fspf.pb We can now execute this program again: $ ./example Hello World f = open ( /data/hello.txt , r ) print str ( f.read ()) Variables SCONE_FSPF_KEY , SCONE_FSPF_TAG and SCONE_FSPF should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please read the section about end-to-end encryption .","title":"File System Protection File"},{"location":"SCONE_Fileshield/#python","text":"Let's try a similar approach for Python. In the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file: docker run -it -v $PWD /volume:/data sconecuratedimages/apps:python-2.7-alpine3.6 bash The files /data/world.py and /data/hello.txt are encrypted: $ cat /data/world.py ? = ??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y?? f? Let's activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /data/fspf.pb We can now run the encrypted world.py program with the the Python interpreter: SCONE_HEAP = 100000000 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python /data/world.py export SCONE_QUEUES = 1 ... Hello World","title":"Python"},{"location":"SCONE_Fileshield/#protecting-the-root-region","text":"Note that in the above example, Python will not be permitted to load dynamic libraries outside of the protected directory /data : a dynamic library must reside in either an authenticated or an encrypted region. To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries. Let us show how to authenticate all files in region / $ scone fspf addr fspf.pb / --kernel / --authenticated We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region / .","title":"Protecting the Root Region"},{"location":"SCONE_Fileshield/#adding-files-from-an-existing-container-image","text":"We show how to add a subset of the files of container image sconecuratedimages/apps:python-2.7-alpine3.6 to our root region. To do so, we ensure that we have the newest images: docker pull sconecuratedimages/apps:python-2.7-alpine3.6 docker pull sconecuratedimages/crosscompilers How can we add all files in a container to the fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to /var/run/docker.sock : docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v $PWD /volume:/data -v $PWD /data-original:/data-original sconecuratedimages/crosscompilers Let us ensure that Docker is installed in this container: apt-get update apt-get install -y docker.io Now, we want to add all files of some target container. In our example, this is an instance of image sconecuratedimages/apps:python-2.7-alpine3.6 . We ensure that we pulled the latest image before we start the container: CONTAINER_ID = ` docker run -d sconecuratedimages/apps:python-2.7-alpine3.6 printf OK ` We can now copy all files from this container into a new directory rootvol : $ cd $ mkdir -p rootvol $ docker cp $CONTAINER_ID :/ ./rootvol Now that we have a copy of the files, we should not forget to garbage collect this container: docker rm $CONTAINER_ID Let's remove some directories that we do not want our program to access, like for example, /dev : $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++ Now, we create a root fspf : $ scone fspf create fspf.pb $ scone fspf addr fspf.pb / --kernel / --authenticated $ scone fspf addf fspf.pb / ./rootvol / $ scone fspf encrypt fspf.pb keytag We can now create a new container image with this file system protection file using this Dockerfile $ cat Dockerfile EOF FROM sconecuratedimages/apps:python-2.7-alpine3.6 COPY fspf.pb / EOF $ docker build -t sconecuratedimages/apps:python-2.7-alpine3.6-authenticated . We can run a container as follows: $ docker run -it sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh Let us activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb Let's run python with authenticated file system: SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python","title":"Adding files from an existing container image"},{"location":"SCONE_Fileshield/#checking-the-file-system-shield","text":"Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in side of a python container: docker run -i sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh $ cat helloworld-manual.py EOF print Hello World EOF When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file. $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld-manual.py ( fails ) We can, however, add a new file via programs that have access to the key of the fspf . We can, for example, write a python program to add a new python program to the file system. By default, we disable that the root fspf is updated. We can enable updates by setting environment variable SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable SCONE_FSPF_MUTABLE=1 ). $ SCONE_HEAP = 1000000000 SCONE_FSPF_MUTABLE = 1 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python PYTHON f = open( helloworld.py , w ) f.write( print Hello World \\n ) f.close() PYTHON ``` The tag of the file system protection file is now changed. We can determine the new TAG with the help of command scone fspf show : $ export SCONE_FSPF_TAG = $( scone fspf show --tag /fspf.pb ) Now, we can run the new helloworld.py : $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld.py ... Hello World","title":"Checking the File System Shield"},{"location":"SCONE_Fileshield/#extended-example","text":"To learn how to use multiple file system protection files, please have a look at the following screencast. Below is the script that is executed in the screencast: docker run -it -v $PWD :/mnt sconecuratedimages/crosscompilers $ mkdir -p /example $ mkdir -p /mnt/authenticated/ $ mkdir -p /mnt/encrypted/ $ cd /example $ mkdir -p .original $ scone fspf create fspf.pb $ scone fspf create authenticated.pb $ scone fspf create encrypted.pb # add protection regions $ scone fspf addr fspf.pb / -e --ephemeral $ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated $ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted # add files # enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated $ scone fspf addf authenticated.pb /mnt/authenticated ./original # enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted # the client will write the encrypted files to ./mnt/encrypted $ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted encrypted_key = ` scone fspf encrypt encrypted.pb | awk {print $11} ` $ echo encrypted.pb key: ${ encrypted_key } $ scone fspf addfspf fspf.pb authenticated.pb $ scone fspf addfspf fspf.pb encrypted.pb ${ encrypted_key } $ cat example.c EOF #include stdio.h int main() { FILE *fp = fopen( /mnt/authenticated/hello , w ); fprintf(fp, hello world\\n ); fclose(fp); fp = fopen( /mnt/encrypted/hello , w ); fprintf(fp, hello world\\n ); fclose(fp); } EOF $ scone gcc example.c -o sgxex $ cat /etc/sgx-musl.conf EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF $ SCONE_FSPF = fspf.pb ./sgxex $ cat /mnt/authenticated/hello $ cat /mnt/encrypted/hello $ cat cat.c EOF #include stdio.h int main() { char buf[80]; FILE *fp = fopen( /mnt/authenticated/hello , r ); fgets(buf, sizeof(buf), fp); fclose(fp); printf( read: %s \\n , buf); fp = fopen( /mnt/encrypted/hello , r ); fgets(buf, sizeof(buf), fp); fclose(fp); printf( read: %s \\n , buf); } EOF $ scone gcc cat.c -o native_cat $ ./native_cat $ scone gcc cat.c -o sgxcat $ SCONE_FSPF = fspf.pb ./sgxcat","title":"Extended Example"},{"location":"SCONE_Fileshield/#notes","text":"The SCONE File Protection documentation is not yet completed and more information will be provided soon. scontain.com , 2018. Questions or Suggestions?","title":"Notes"},{"location":"SCONE_GENERATE_IMAGE/","text":"Generating Container Image with SCONE We show how to generate a Docker image that contains our hello world running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section. Prerequisites Check that all prerequisites from SCONE Tutorial are satisfied. Clone the SCONE_TUTORIAL before you start creating a hello world image. Generate HelloWorld image We generate a hello world container image. cd SCONE_TUTORIAL/CreateImage You can either execute all step manually by copy pasting all instructions or you can just execute docker login sudo ./Dockerfile.sh and watch the outputs. Please change the image name to a repository on docker hub to which you can write: export TAG = latest export IMAGE_NAME = sconecuratedimages/helloworld We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler: CONTAINER_ID = ` docker run -d -it --device = /dev/isgx -v $( pwd ) :/mnt sconecuratedimages/crosscompilers bash -c set -e printf Q 1\\ne 0 0 0\\ns 1 0 0\\n /etc/sgx-musl.conf sgxmusl-hw-async-gcc /mnt/hello_world.c -o /usr/local/bin/sgx_hello_world ` Note that above will fail if you do not have access to the SGX device /dev/isgx . Turn the container into an image: IMAGE_ID = $( docker commit -p -c CMD sgx_hello_world $CONTAINER_ID $IMAGE_NAME : $TAG ) You can run this image by executing: sudo docker run --device = /dev/isgx $IMAGE_NAME : $TAG You can push this image to Docker. However, ensure that you first login to docker: sudo docker login before you push the image to docker hub: sudo docker push $IMAGE_NAME : $TAG Note: this will fail in case you do not have the permission to push to this repository. Screencast scontain.com , November 2017. Questions or Suggestions?","title":"SCONE Create Image"},{"location":"SCONE_GENERATE_IMAGE/#generating-container-image-with-scone","text":"We show how to generate a Docker image that contains our hello world running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section.","title":"Generating Container Image with SCONE"},{"location":"SCONE_GENERATE_IMAGE/#prerequisites","text":"Check that all prerequisites from SCONE Tutorial are satisfied. Clone the SCONE_TUTORIAL before you start creating a hello world image.","title":"Prerequisites"},{"location":"SCONE_GENERATE_IMAGE/#generate-helloworld-image","text":"We generate a hello world container image. cd SCONE_TUTORIAL/CreateImage You can either execute all step manually by copy pasting all instructions or you can just execute docker login sudo ./Dockerfile.sh and watch the outputs. Please change the image name to a repository on docker hub to which you can write: export TAG = latest export IMAGE_NAME = sconecuratedimages/helloworld We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler: CONTAINER_ID = ` docker run -d -it --device = /dev/isgx -v $( pwd ) :/mnt sconecuratedimages/crosscompilers bash -c set -e printf Q 1\\ne 0 0 0\\ns 1 0 0\\n /etc/sgx-musl.conf sgxmusl-hw-async-gcc /mnt/hello_world.c -o /usr/local/bin/sgx_hello_world ` Note that above will fail if you do not have access to the SGX device /dev/isgx . Turn the container into an image: IMAGE_ID = $( docker commit -p -c CMD sgx_hello_world $CONTAINER_ID $IMAGE_NAME : $TAG ) You can run this image by executing: sudo docker run --device = /dev/isgx $IMAGE_NAME : $TAG You can push this image to Docker. However, ensure that you first login to docker: sudo docker login before you push the image to docker hub: sudo docker push $IMAGE_NAME : $TAG Note: this will fail in case you do not have the permission to push to this repository.","title":"Generate HelloWorld image"},{"location":"SCONE_GENERATE_IMAGE/#screencast","text":"scontain.com , November 2017. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_HOST/","text":"scone host This page describes the CLI scone host in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section SCONE Host Setup . Read section SCONE Command Line Interface to see how to install command scone . Commands scone host supports the following commands: check : checks that host is properly installed (patched docker engine and patched sgx driver) install : installs the patched SGX driver and patched docker engine reboot : reboots a host uninstall : uninstalls SGX driver and patched docker engine swarm : join a new or another swarm scone host install Command install installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm. NOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care. Options You must always specify option --name HOST , where, HOST is the name of the host that you want to install. You need to have ssh access to this host - without the need to type in a password. Example: To install host dorothy without joining any swarm, just execute $ scone host install --name dorothy If you want to install a host and make this host a manager of a new or an existing swarm, you need to add option --as-manager . Example: To install host faye and create a new swarm with faye as a manager, just execute $ scone host install --name faye --as-manager You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a worker , you have to specify option --join MANAGER Example: To install host edna and then join the swarm managed by manager faye , execute: $ scone host install --name edna --join faye You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye ` NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active 2 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a manager , you have to specify options --join MANAGER --as-manager Example: To install host edna and then join the swarm managed by manager faye as a manager, execute: $ scone host install --name edna --join faye --as-manager scone host swarm In the above example, we installed host dorothy without joining any swarm. Sometimes one wants to revise this decision via command swarm . You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options --join MANAGER and --as-manager as described for scone host install . Example: To add host dorothy to the swarm managed by host faye , just execute: $ scone host swarm --name dorothy --join faye Example: To add host dorothy to the swarm managed by host faye as a manager, just execute: $ scone host swarm --name dorothy --join faye --as-manager scone host check To check if a host is properly installed, you can use command check . Example: To check if host faye is properly installed, just execute: $ scone host check --name faye If the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host. scone host reboot Sometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility lsof : $ sudo lsof | grep dev/isgx Sometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with scone host reboot . You must specify options --name HOST to indicate which host to reboot. Moreover, scone host reboot will exit with an error unless you specify option --force . Example: To reboot host dorothy, just execute the following: $ scone host reboot --name dorothy --force If you want to wait until the host is again available, you can specify option --wait . Example: To reboot host dorothy and only return after the host has indeed rebooted, just execute the following: $ scone host reboot --name dorothy --force --wait scone host uninstall With the help of scone host uninstall you can force an host to leave any Docker swarm it might be part of uninstall the patched Docker engine uninstall the patched SGX driver NOTE: all containers that might run on this host, will be destroyed. Options You must define the name of the host to be uninstalled via option --name HOST . You must always give the --force option, otherwise, an error is issued. If a node is part of a swarm, you must explicitly specify the option --manager MANAGERHOST . If the node is not part of a swarm, you must specify the option --noswarm . Note: While other objects / commands support the use of environment variable SCONE_MANAGER as an implicit definition of --manager $SCONE_MANAGER , we decided that users must explicitly specify the manager of the swarm for command uninstall . Example: To uninstall a host edna that is part of a swarm managed by host faye , execute: $ scone host uninstall --name edna --force --manager faye Example: Let's assume that host edna is not part of a swarm. To uninstall host edna , execute: $ scone host uninstall --name edna --force --noswarm Example: To uninstall a host faye which is also manager can be tricky. First, make sure that all other nodes are removed from the swarm. To check the status, execute: $ scone host check --name faye $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If the output matches the above execute the following to uninstall host faye (both parameters --manager MANAGERHOST and --noswarm must be added): $ scone host uninstall --name faye --noswarm --manager faye --force General options --help (or, -h ): issue help message for object host . If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes scontain.com , December 2017. Questions or Suggestions?","title":"scone host"},{"location":"SCONE_HOST/#scone-host","text":"This page describes the CLI scone host in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section SCONE Host Setup . Read section SCONE Command Line Interface to see how to install command scone .","title":"scone host"},{"location":"SCONE_HOST/#commands","text":"scone host supports the following commands: check : checks that host is properly installed (patched docker engine and patched sgx driver) install : installs the patched SGX driver and patched docker engine reboot : reboots a host uninstall : uninstalls SGX driver and patched docker engine swarm : join a new or another swarm","title":"Commands"},{"location":"SCONE_HOST/#scone-host-install","text":"Command install installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm. NOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care.","title":"scone host install"},{"location":"SCONE_HOST/#options","text":"You must always specify option --name HOST , where, HOST is the name of the host that you want to install. You need to have ssh access to this host - without the need to type in a password. Example: To install host dorothy without joining any swarm, just execute $ scone host install --name dorothy If you want to install a host and make this host a manager of a new or an existing swarm, you need to add option --as-manager . Example: To install host faye and create a new swarm with faye as a manager, just execute $ scone host install --name faye --as-manager You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a worker , you have to specify option --join MANAGER Example: To install host edna and then join the swarm managed by manager faye , execute: $ scone host install --name edna --join faye You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye ` NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active 2 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a manager , you have to specify options --join MANAGER --as-manager Example: To install host edna and then join the swarm managed by manager faye as a manager, execute: $ scone host install --name edna --join faye --as-manager","title":"Options"},{"location":"SCONE_HOST/#scone-host-swarm","text":"In the above example, we installed host dorothy without joining any swarm. Sometimes one wants to revise this decision via command swarm . You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options --join MANAGER and --as-manager as described for scone host install . Example: To add host dorothy to the swarm managed by host faye , just execute: $ scone host swarm --name dorothy --join faye Example: To add host dorothy to the swarm managed by host faye as a manager, just execute: $ scone host swarm --name dorothy --join faye --as-manager","title":"scone host swarm"},{"location":"SCONE_HOST/#scone-host-check","text":"To check if a host is properly installed, you can use command check . Example: To check if host faye is properly installed, just execute: $ scone host check --name faye If the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host.","title":"scone host check"},{"location":"SCONE_HOST/#scone-host-reboot","text":"Sometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility lsof : $ sudo lsof | grep dev/isgx Sometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with scone host reboot . You must specify options --name HOST to indicate which host to reboot. Moreover, scone host reboot will exit with an error unless you specify option --force . Example: To reboot host dorothy, just execute the following: $ scone host reboot --name dorothy --force If you want to wait until the host is again available, you can specify option --wait . Example: To reboot host dorothy and only return after the host has indeed rebooted, just execute the following: $ scone host reboot --name dorothy --force --wait","title":"scone host reboot"},{"location":"SCONE_HOST/#scone-host-uninstall","text":"With the help of scone host uninstall you can force an host to leave any Docker swarm it might be part of uninstall the patched Docker engine uninstall the patched SGX driver NOTE: all containers that might run on this host, will be destroyed.","title":"scone host uninstall"},{"location":"SCONE_HOST/#options_1","text":"You must define the name of the host to be uninstalled via option --name HOST . You must always give the --force option, otherwise, an error is issued. If a node is part of a swarm, you must explicitly specify the option --manager MANAGERHOST . If the node is not part of a swarm, you must specify the option --noswarm . Note: While other objects / commands support the use of environment variable SCONE_MANAGER as an implicit definition of --manager $SCONE_MANAGER , we decided that users must explicitly specify the manager of the swarm for command uninstall . Example: To uninstall a host edna that is part of a swarm managed by host faye , execute: $ scone host uninstall --name edna --force --manager faye Example: Let's assume that host edna is not part of a swarm. To uninstall host edna , execute: $ scone host uninstall --name edna --force --noswarm Example: To uninstall a host faye which is also manager can be tricky. First, make sure that all other nodes are removed from the swarm. To check the status, execute: $ scone host check --name faye $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If the output matches the above execute the following to uninstall host faye (both parameters --manager MANAGERHOST and --noswarm must be added): $ scone host uninstall --name faye --noswarm --manager faye --force","title":"Options"},{"location":"SCONE_HOST/#general-options","text":"--help (or, -h ): issue help message for object host . If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes scontain.com , December 2017. Questions or Suggestions?","title":"General options"},{"location":"SCONE_HOST_SETUP/","text":"SCONE: Host Installation Guide Patched SGX Driver and Patched Docker Engine You only need the patched driver and patched docker engine in case you want to run Docker swarm. If you have already installed a SGX driver and a docker engine on your machine, you should be all set to go, i.e., no need to install the patched drivers. SCONE AUTO Mode If you just want to take SCONE for a test drive, you can run applications without installing an SGX driver. Note, however, that you need a reasonably new CPU to run the containers since we compile applications assuming that have access to most modern CPU extensions. This page describes how to set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and we remind you of how to set up your ssh configuration to be able to use your scone CLI. During this setup, we install a patched Intel SGX driver - required for better monitoring support, install a patched docker engine - to ensure that all containers have access to SGX, and start or join a docker swarm - if requested by command line options. Prerequisite : We assume that you have set up the scone command line interface . The scone CLI can run on your developer machine, a virtual machine or inside a container. The easiest way to get started is to run scone in a Docker container. No matter where scone is running, it requires that ssh be properly installed. We recommend for now to install Ubuntu 16.04 LTS on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. Also, Intel SGX driver still recommends Ubuntu 16.04 LTS and not Ubuntu 18.04 LTS. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size). The scone CLI uses *ssh to log into remote hosts. We assume that ssh is setup in such a way that you do not need passwords to log into these hosts. Please read section ssh setup to learn how to ensure this. Installation of a single host After you set up the scone CLI and your passwordless ssh works, you can install the scone related software on a new host, say, alice as follows: $ scone host install --name alice To verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute: $ scone host check --name alice This command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed. Installation of a swarm For a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that alice and bob should be managers but caroline a non-manager, execute the following: $ scone host install --name alice --as-manager $ scone host install --name bob --as-manager --join alice $ scone host install --name caroline --join alice Note that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects that they will be in the same local area network. Checking your Installation To test the installation, one can run a simple hello-world container: sudo docker run hello-world Background Information Patched Docker Engine (Moby) For an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves. Some docker commands (like docker run ) support an option --device (i.e., --device /dev/isgx ) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like docker build ) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx). With the help of this patched engine, we can use Dockerfiles to generate container images (see this Tutorial ). Right now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017). Patched SGX Driver We also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of available and free EPC (Extended Page Cache) pages. Right now, we provide a patched version of the latest Intel SGX driver (November 11, 2017). Note We have been updating SCONE such that SCONE does not need access to /dev/isgx during cross-compilation - which could, for example, be executed during docker build . Hence, we will soon switch to an unpatched Docker engine and SGX driver. Screencast This screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since scone host depends on systemd to start the swarm. In case systemd is not available, scone host will still be able to install the patched SGX driver and the patched Docker engine. Another issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by scone host . The reason for that is that typically that some process is still using the /dev/isgx device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case. scontain.com , March 2018. Questions or Suggestions?","title":"SCONE Host Setup"},{"location":"SCONE_HOST_SETUP/#scone-host-installation-guide","text":"Patched SGX Driver and Patched Docker Engine You only need the patched driver and patched docker engine in case you want to run Docker swarm. If you have already installed a SGX driver and a docker engine on your machine, you should be all set to go, i.e., no need to install the patched drivers. SCONE AUTO Mode If you just want to take SCONE for a test drive, you can run applications without installing an SGX driver. Note, however, that you need a reasonably new CPU to run the containers since we compile applications assuming that have access to most modern CPU extensions. This page describes how to set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and we remind you of how to set up your ssh configuration to be able to use your scone CLI. During this setup, we install a patched Intel SGX driver - required for better monitoring support, install a patched docker engine - to ensure that all containers have access to SGX, and start or join a docker swarm - if requested by command line options. Prerequisite : We assume that you have set up the scone command line interface . The scone CLI can run on your developer machine, a virtual machine or inside a container. The easiest way to get started is to run scone in a Docker container. No matter where scone is running, it requires that ssh be properly installed. We recommend for now to install Ubuntu 16.04 LTS on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. Also, Intel SGX driver still recommends Ubuntu 16.04 LTS and not Ubuntu 18.04 LTS. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size). The scone CLI uses *ssh to log into remote hosts. We assume that ssh is setup in such a way that you do not need passwords to log into these hosts. Please read section ssh setup to learn how to ensure this.","title":"SCONE: Host Installation Guide"},{"location":"SCONE_HOST_SETUP/#installation-of-a-single-host","text":"After you set up the scone CLI and your passwordless ssh works, you can install the scone related software on a new host, say, alice as follows: $ scone host install --name alice To verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute: $ scone host check --name alice This command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed.","title":"Installation of a single host"},{"location":"SCONE_HOST_SETUP/#installation-of-a-swarm","text":"For a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that alice and bob should be managers but caroline a non-manager, execute the following: $ scone host install --name alice --as-manager $ scone host install --name bob --as-manager --join alice $ scone host install --name caroline --join alice Note that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects that they will be in the same local area network.","title":"Installation of a swarm"},{"location":"SCONE_HOST_SETUP/#checking-your-installation","text":"To test the installation, one can run a simple hello-world container: sudo docker run hello-world","title":"Checking your Installation"},{"location":"SCONE_HOST_SETUP/#background-information","text":"","title":"Background Information"},{"location":"SCONE_HOST_SETUP/#patched-docker-engine-moby","text":"For an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves. Some docker commands (like docker run ) support an option --device (i.e., --device /dev/isgx ) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like docker build ) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx). With the help of this patched engine, we can use Dockerfiles to generate container images (see this Tutorial ). Right now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017).","title":"Patched Docker Engine (Moby)"},{"location":"SCONE_HOST_SETUP/#patched-sgx-driver","text":"We also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of available and free EPC (Extended Page Cache) pages. Right now, we provide a patched version of the latest Intel SGX driver (November 11, 2017).","title":"Patched SGX Driver"},{"location":"SCONE_HOST_SETUP/#note","text":"We have been updating SCONE such that SCONE does not need access to /dev/isgx during cross-compilation - which could, for example, be executed during docker build . Hence, we will soon switch to an unpatched Docker engine and SGX driver.","title":"Note"},{"location":"SCONE_HOST_SETUP/#screencast","text":"This screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since scone host depends on systemd to start the swarm. In case systemd is not available, scone host will still be able to install the patched SGX driver and the patched Docker engine. Another issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by scone host . The reason for that is that typically that some process is still using the /dev/isgx device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case. scontain.com , March 2018. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_OpenStack/","text":"","title":"SCONE OpenStack"},{"location":"SCONE_Publications/","text":"SCONE Related Publications SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016 This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system call interface. Authors : Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer Media : pdf , slides , audio , url Abstract : In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers. We describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput. Bibtex @inproceedings {199364, author = {Sergei Arnautov and Bohdan Trach and Franz Gregor and Thomas Knauth and Andre Martin and Christian Priebe and Joshua Lind and Divya Muthukumaran and Dan O{\\textquoteright}Keeffe and Mark L. Stillwell and David Goltzsche and Dave Eyers and R{\\ u}diger Kapitza and Peter Pietzuch and Christof Fetzer}, title = {{ SCONE }: Secure Linux Containers with Intel { SGX }} , booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)}, year = {2016}, isbn = {978-1-931971-33-1}, address = {Savannah, GA}, pages = {689--703}, url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/arnautov}, publisher = {{ USENIX } Association }, } Building Critical Applications Using Microservices, IEEE Security Privacy, Volume: 14 Issue: 6, December 2016 Author : Christof Fetzer Media : pdf , html Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves. Bibtex @ARTICLE{7782696, author={C. Fetzer}, journal={IEEE Security Privacy}, title={Building Critical Applications Using Microservices}, year={2016}, volume={14}, number={6}, pages={86-89}, keywords={trusted computing;critical applications;critical software correctness;legacy hardware;microservice confidentiality;microservice integrity;microservice-based system;secure enclaves;software components;trustworthy systems;Buildings;Containers;Kernel;Linux;Security;Linux;hardware;microservices;secure enclaves;security;software}, doi={10.1109/MSP.2016.129}, ISSN={1540-7993}, month={Nov}, } SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017 To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding the overheads, please see this paper. This won the best paper award of EuroSys 2017. Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer Media : pdf , html Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx. Bibtex @inproceedings{Kuvaiskii:2017:SMS:3064176.3064192, author = {Kuvaiskii, Dmitrii and Oleksenko, Oleksii and Arnautov, Sergei and Trach, Bohdan and Bhatotia, Pramod and Felber, Pascal and Fetzer, Christof}, title = {SGXBOUNDS: Memory Safety for Shielded Execution}, booktitle = {Proceedings of the Twelfth European Conference on Computer Systems}, series = {EuroSys 17}, year = {2017}, isbn = {978-1-4503-4938-3}, location = {Belgrade, Serbia}, pages = {205--221}, numpages = {17}, url = {http://doi.acm.org/10.1145/3064176.3064192}, doi = {10.1145/3064176.3064192}, acmid = {3064192}, publisher = {ACM}, address = {New York, NY, USA}, } FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017 This paper describes our new lock-free queue for our asynchronous system calls. Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach Media : pdf , html Abstract : With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability. Bibtex @INPROCEEDINGS{7967181, author={S. Arnautov and P. Felber and C. Fetzer and B. Trach}, booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, title={FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue}, year={2017}, volume={}, number={}, pages={907-916}, keywords={microprocessor chips;multi-threading;multiprocessing systems;operating systems (computers);queueing theory;FFQ;FIFO queue;consumer threads;fast single producer-multiple consumer concurrent FIFO queue;hardware threads;lock-free consumers;multicore architectures;multiple producer;operating systems;secure application;single producer variant;synchronization operations;Algorithm design and analysis;Context;Instruction sets;Message systems;Scalability;Synchronization;Throughput;FIFO queue;SPMC queue;concurrent FIFO queue;concurrent queue;lock-free algorithm;wait-free algorithm}, doi={10.1109/IPDPS.2017.41}, ISSN={}, month={May}, } PESOS: Policy Enhanced Secure Object Store, EuroSys 2018 Authors : Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden) Media : pdf , html Abstract : Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language. Pesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small. Bibtex @inproceedings{Krahn:2018:PEP:3190508.3190518, author = {Krahn, Robert and Trach, Bohdan and Vahldiek-Oberwagner, Anjo and Knauth, Thomas and Bhatotia, Pramod and Fetzer, Christof}, title = {Pesos: Policy Enhanced Secure Object Store}, booktitle = {Proceedings of the Thirteenth EuroSys Conference}, series = {EuroSys 18}, year = {2018}, isbn = {978-1-4503-5584-1}, location = {Porto, Portugal}, pages = {25:1--25:17}, articleno = {25}, numpages = {17}, url = {http://doi.acm.org/10.1145/3190508.3190518}, doi = {10.1145/3190508.3190518}, acmid = {3190518}, publisher = {ACM}, address = {New York, NY, USA}, keywords = {intel SGX, kinetic disks, policy language, storage security}, } ShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018 Authors : Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer Media : pdf , html , video Abstract : Middleboxes that process confidential data cannot be securely deployed in untrusted cloud environments. To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic. Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic. This motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers. ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate. Bibtex @inproceedings{Trach:2018:SSM:3185467.3185469, author = {Trach, Bohdan and Krohmer, Alfred and Gregor, Franz and Arnautov, Sergei and Bhatotia, Pramod and Fetzer, Christof}, title = {ShieldBox: Secure Middleboxes Using Shielded Execution}, booktitle = {Proceedings of the Symposium on SDN Research}, series = {SOSR 18}, year = {2018}, isbn = {978-1-4503-5664-0}, location = {Los Angeles, CA, USA}, pages = {2:1--2:14}, articleno = {2}, numpages = {14}, url = {http://doi.acm.org/10.1145/3185467.3185469}, doi = {10.1145/3185467.3185469}, acmid = {3185469}, publisher = {ACM}, address = {New York, NY, USA}, } Varys: Protecting SGX enclaves from practical side-channel attacks, USENIX ATC 2018 Authors : Oleksii Oleksenko, Bohdan Trach, Robert Krahn, and Andr\u00e9 Martin, TU Dresden; Mark Silberstein, Technion; Christof Fetzer, TU Dresden Media : pdf , slides , audio , url Bibtex @inproceedings {216033, author = {Oleksii Oleksenko and Bohdan Trach and Robert Krahn and Mark Silberstein and Christof Fetzer}, title = {Varys: Protecting {SGX} Enclaves from Practical Side-Channel Attacks}, booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)}, year = {2018}, isbn = {978-1-931971-44-7}, address = {Boston, MA}, pages = {227--240}, url = {https://www.usenix.org/conference/atc18/presentation/oleksenko}, publisher = {{USENIX} Association}, } scontain.com , June 2018. Questions or Suggestions?","title":"Publications"},{"location":"SCONE_Publications/#scone-related-publications","text":"","title":"SCONE Related Publications"},{"location":"SCONE_Publications/#scone-secure-linux-containers-with-intel-sgx-usenix-osdi-2016","text":"This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system call interface. Authors : Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer Media : pdf , slides , audio , url Abstract : In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers. We describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput. Bibtex @inproceedings {199364, author = {Sergei Arnautov and Bohdan Trach and Franz Gregor and Thomas Knauth and Andre Martin and Christian Priebe and Joshua Lind and Divya Muthukumaran and Dan O{\\textquoteright}Keeffe and Mark L. Stillwell and David Goltzsche and Dave Eyers and R{\\ u}diger Kapitza and Peter Pietzuch and Christof Fetzer}, title = {{ SCONE }: Secure Linux Containers with Intel { SGX }} , booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)}, year = {2016}, isbn = {978-1-931971-33-1}, address = {Savannah, GA}, pages = {689--703}, url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/arnautov}, publisher = {{ USENIX } Association }, }","title":"SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016"},{"location":"SCONE_Publications/#building-critical-applications-using-microservices-ieee-security-privacy-volume-14-issue-6-december-2016","text":"Author : Christof Fetzer Media : pdf , html Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves. Bibtex @ARTICLE{7782696, author={C. Fetzer}, journal={IEEE Security Privacy}, title={Building Critical Applications Using Microservices}, year={2016}, volume={14}, number={6}, pages={86-89}, keywords={trusted computing;critical applications;critical software correctness;legacy hardware;microservice confidentiality;microservice integrity;microservice-based system;secure enclaves;software components;trustworthy systems;Buildings;Containers;Kernel;Linux;Security;Linux;hardware;microservices;secure enclaves;security;software}, doi={10.1109/MSP.2016.129}, ISSN={1540-7993}, month={Nov}, }","title":"Building Critical Applications Using Microservices, IEEE Security &amp; Privacy, Volume: 14 Issue: 6, December 2016"},{"location":"SCONE_Publications/#sgxbounds-memory-safety-for-shielded-execution-eurosys-2017","text":"To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding the overheads, please see this paper. This won the best paper award of EuroSys 2017. Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer Media : pdf , html Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx. Bibtex @inproceedings{Kuvaiskii:2017:SMS:3064176.3064192, author = {Kuvaiskii, Dmitrii and Oleksenko, Oleksii and Arnautov, Sergei and Trach, Bohdan and Bhatotia, Pramod and Felber, Pascal and Fetzer, Christof}, title = {SGXBOUNDS: Memory Safety for Shielded Execution}, booktitle = {Proceedings of the Twelfth European Conference on Computer Systems}, series = {EuroSys 17}, year = {2017}, isbn = {978-1-4503-4938-3}, location = {Belgrade, Serbia}, pages = {205--221}, numpages = {17}, url = {http://doi.acm.org/10.1145/3064176.3064192}, doi = {10.1145/3064176.3064192}, acmid = {3064192}, publisher = {ACM}, address = {New York, NY, USA}, }","title":"SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017"},{"location":"SCONE_Publications/#ffq-a-fast-single-producermultiple-consumer-concurrent-fifo-queue-ipdps-2017","text":"This paper describes our new lock-free queue for our asynchronous system calls. Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach Media : pdf , html Abstract : With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability. Bibtex @INPROCEEDINGS{7967181, author={S. Arnautov and P. Felber and C. Fetzer and B. Trach}, booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, title={FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue}, year={2017}, volume={}, number={}, pages={907-916}, keywords={microprocessor chips;multi-threading;multiprocessing systems;operating systems (computers);queueing theory;FFQ;FIFO queue;consumer threads;fast single producer-multiple consumer concurrent FIFO queue;hardware threads;lock-free consumers;multicore architectures;multiple producer;operating systems;secure application;single producer variant;synchronization operations;Algorithm design and analysis;Context;Instruction sets;Message systems;Scalability;Synchronization;Throughput;FIFO queue;SPMC queue;concurrent FIFO queue;concurrent queue;lock-free algorithm;wait-free algorithm}, doi={10.1109/IPDPS.2017.41}, ISSN={}, month={May}, }","title":"FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017"},{"location":"SCONE_Publications/#pesos-policy-enhanced-secure-object-store-eurosys-2018","text":"Authors : Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden) Media : pdf , html Abstract : Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language. Pesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small. Bibtex @inproceedings{Krahn:2018:PEP:3190508.3190518, author = {Krahn, Robert and Trach, Bohdan and Vahldiek-Oberwagner, Anjo and Knauth, Thomas and Bhatotia, Pramod and Fetzer, Christof}, title = {Pesos: Policy Enhanced Secure Object Store}, booktitle = {Proceedings of the Thirteenth EuroSys Conference}, series = {EuroSys 18}, year = {2018}, isbn = {978-1-4503-5584-1}, location = {Porto, Portugal}, pages = {25:1--25:17}, articleno = {25}, numpages = {17}, url = {http://doi.acm.org/10.1145/3190508.3190518}, doi = {10.1145/3190508.3190518}, acmid = {3190518}, publisher = {ACM}, address = {New York, NY, USA}, keywords = {intel SGX, kinetic disks, policy language, storage security}, }","title":"PESOS: Policy Enhanced Secure Object Store, EuroSys 2018"},{"location":"SCONE_Publications/#shieldbox-secure-middleboxes-using-shielded-execution-sigcomm-sosr-2018","text":"Authors : Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer Media : pdf , html , video Abstract : Middleboxes that process confidential data cannot be securely deployed in untrusted cloud environments. To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic. Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic. This motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers. ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate. Bibtex @inproceedings{Trach:2018:SSM:3185467.3185469, author = {Trach, Bohdan and Krohmer, Alfred and Gregor, Franz and Arnautov, Sergei and Bhatotia, Pramod and Fetzer, Christof}, title = {ShieldBox: Secure Middleboxes Using Shielded Execution}, booktitle = {Proceedings of the Symposium on SDN Research}, series = {SOSR 18}, year = {2018}, isbn = {978-1-4503-5664-0}, location = {Los Angeles, CA, USA}, pages = {2:1--2:14}, articleno = {2}, numpages = {14}, url = {http://doi.acm.org/10.1145/3185467.3185469}, doi = {10.1145/3185467.3185469}, acmid = {3185469}, publisher = {ACM}, address = {New York, NY, USA}, }","title":"ShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018"},{"location":"SCONE_Publications/#varys-protecting-sgx-enclaves-from-practical-side-channel-attacks-usenix-atc-2018","text":"Authors : Oleksii Oleksenko, Bohdan Trach, Robert Krahn, and Andr\u00e9 Martin, TU Dresden; Mark Silberstein, Technion; Christof Fetzer, TU Dresden Media : pdf , slides , audio , url Bibtex @inproceedings {216033, author = {Oleksii Oleksenko and Bohdan Trach and Robert Krahn and Mark Silberstein and Christof Fetzer}, title = {Varys: Protecting {SGX} Enclaves from Practical Side-Channel Attacks}, booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)}, year = {2018}, isbn = {978-1-931971-44-7}, address = {Boston, MA}, pages = {227--240}, url = {https://www.usenix.org/conference/atc18/presentation/oleksenko}, publisher = {{USENIX} Association}, } scontain.com , June 2018. Questions or Suggestions?","title":"Varys: Protecting SGX enclaves from practical side-channel attacks, USENIX ATC 2018"},{"location":"SCONE_SERVICE/","text":"scone service scone service manages remote docker services. scone service is mainly a thin wrapper around docker service . However, instead of executing commands locally, it forwards the commands to the manager of a remote swarm. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER : this variable defines the default swarm to be used by scone service in case option --manager is not given. scone service supports all commands and all the options of docker service . The semantics of command create is, however, slightly modified: scone service create only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label SGX VERSION greater than 0. In addition to the docker defined commands, scone service supports two new commands: scone service Description registry checks if a registry service is running in the swarm and if it is not, it starts a new registry service. pull pulls an image from a repository and stores it in the registry of the swarm. scone service commands The following commands of scone service are implemented by docker service : scone service Description create Creates a new service on a SGX-enabled machine . inspect Display detailed information on one or more services logs Fetch the logs of a service or task ls List services ps List the tasks of one or more services rm Remove one or more services rollback Revert changes to a service's configuration scale Scale one or multiple replicated services update Update a service The options of the above commands are the same as for docker service with the exception that all commands support the new option --manager MANAGER . scone service registry [OPTIONS] Starts a local registry service in the swarm managed by node MANAGER . The identity of MANAGER is given via option --manager=MANAGER or via environment variable SCONE_MANAGER=MANAGER . Options Description --manager MANAGER manager of swarm (required unless SCONE_MANAGER is defined) --verbose print verbose messages --debug print debug messages --help print usage of this command scone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG] Pulls an image from a repository (typically, docker hub) and stores the image in the local registry. First, create this local repository with $ scone service registry --manager MANAGER It is expected that the name of the image to be pulled has the following format: REPOSITORY/IMAGE [ :TAG ] After pulling the image, it is locally available in the swarm. The name of the image id now: localhost:5000/IMAGE [ :TAG ] The identity of MANAGER is given via option --manager or via environment variable SCONE_MANAGER. Options Description --manager MANAGER manager of swarm (required) --verbose print verbose messages --debug print debug messages --help print usage of this command Example To ensure that a registry is running on the swarm managed by node faye , execute: $ export SCONE_MANAGER = faye $ scone service registry To pull image sconecuratedimages/sconetainer:shielded from docker hub and store it in the local registry, execute $ scone service pull sconecuratedimages/sconetainer:shielded You can then start this image as a service on the swarm managed by faye as follows: $ scone service create --name nginx-shielded --detach = true --publish 8090 :8080 --publish 8092 :8082 localhost:5000/sconetainer:shielded scontain.com , December 2017. Questions or Suggestions?","title":"scone service"},{"location":"SCONE_SERVICE/#scone-service","text":"scone service manages remote docker services. scone service is mainly a thin wrapper around docker service . However, instead of executing commands locally, it forwards the commands to the manager of a remote swarm. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER : this variable defines the default swarm to be used by scone service in case option --manager is not given. scone service supports all commands and all the options of docker service . The semantics of command create is, however, slightly modified: scone service create only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label SGX VERSION greater than 0. In addition to the docker defined commands, scone service supports two new commands: scone service Description registry checks if a registry service is running in the swarm and if it is not, it starts a new registry service. pull pulls an image from a repository and stores it in the registry of the swarm.","title":"scone service"},{"location":"SCONE_SERVICE/#scone-service-commands","text":"The following commands of scone service are implemented by docker service : scone service Description create Creates a new service on a SGX-enabled machine . inspect Display detailed information on one or more services logs Fetch the logs of a service or task ls List services ps List the tasks of one or more services rm Remove one or more services rollback Revert changes to a service's configuration scale Scale one or multiple replicated services update Update a service The options of the above commands are the same as for docker service with the exception that all commands support the new option --manager MANAGER .","title":"scone service commands"},{"location":"SCONE_SERVICE/#scone-service-registry-options","text":"Starts a local registry service in the swarm managed by node MANAGER . The identity of MANAGER is given via option --manager=MANAGER or via environment variable SCONE_MANAGER=MANAGER . Options Description --manager MANAGER manager of swarm (required unless SCONE_MANAGER is defined) --verbose print verbose messages --debug print debug messages --help print usage of this command","title":"scone service registry [OPTIONS]"},{"location":"SCONE_SERVICE/#scone-service-pull-options-repositoryimagetag","text":"Pulls an image from a repository (typically, docker hub) and stores the image in the local registry. First, create this local repository with $ scone service registry --manager MANAGER It is expected that the name of the image to be pulled has the following format: REPOSITORY/IMAGE [ :TAG ] After pulling the image, it is locally available in the swarm. The name of the image id now: localhost:5000/IMAGE [ :TAG ] The identity of MANAGER is given via option --manager or via environment variable SCONE_MANAGER. Options Description --manager MANAGER manager of swarm (required) --verbose print verbose messages --debug print debug messages --help print usage of this command","title":"scone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG]"},{"location":"SCONE_SERVICE/#example","text":"To ensure that a registry is running on the swarm managed by node faye , execute: $ export SCONE_MANAGER = faye $ scone service registry To pull image sconecuratedimages/sconetainer:shielded from docker hub and store it in the local registry, execute $ scone service pull sconecuratedimages/sconetainer:shielded You can then start this image as a service on the swarm managed by faye as follows: $ scone service create --name nginx-shielded --detach = true --publish 8090 :8080 --publish 8092 :8082 localhost:5000/sconetainer:shielded scontain.com , December 2017. Questions or Suggestions?","title":"Example"},{"location":"SCONE_STACK/","text":"scone stack scone stack manages remote docker stacks. scone stack is mainly a thin wrapper around docker stack that forwards the commands to a remote swarm - instead of executing the commands locally. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given. scone stack commands scone stack Description deploy deploy a new stack or update an existing stack ls list stacks ps list the tasks in the stack rm remove one or more stacks services list the services in the stack scone stack deploy The command deploy is used to create a set of services specified by a stack file . scone stack deploy expects the stack file specified via option -compose-file or -c . We expect this file to reside on your local machine/container (i.e., where you start scone stack deploy ). scone stack deploy copies the stack file to the remote swarm manager before executing docker stack deploy on the swarm manager. Example To deploy a stack that is called nginx on a remote swarm managed by node faye , execute the following: $ scone stack deploy --compose-file compose.yml --manager faye nginx In case you set the environment variable SCONE_MANAGER to faye , you can drop option --manager : $ export SCONE_MANAGER = faye $ scone stack deploy --compose-file compose.yml nginx scontain.com , December 2017. Questions or Suggestions?","title":"scone stack"},{"location":"SCONE_STACK/#scone-stack","text":"scone stack manages remote docker stacks. scone stack is mainly a thin wrapper around docker stack that forwards the commands to a remote swarm - instead of executing the commands locally. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given.","title":"scone stack"},{"location":"SCONE_STACK/#scone-stack-commands","text":"scone stack Description deploy deploy a new stack or update an existing stack ls list stacks ps list the tasks in the stack rm remove one or more stacks services list the services in the stack","title":"scone stack commands"},{"location":"SCONE_STACK/#scone-stack-deploy","text":"The command deploy is used to create a set of services specified by a stack file . scone stack deploy expects the stack file specified via option -compose-file or -c . We expect this file to reside on your local machine/container (i.e., where you start scone stack deploy ). scone stack deploy copies the stack file to the remote swarm manager before executing docker stack deploy on the swarm manager.","title":"scone stack deploy"},{"location":"SCONE_STACK/#example","text":"To deploy a stack that is called nginx on a remote swarm managed by node faye , execute the following: $ scone stack deploy --compose-file compose.yml --manager faye nginx In case you set the environment variable SCONE_MANAGER to faye , you can drop option --manager : $ export SCONE_MANAGER = faye $ scone stack deploy --compose-file compose.yml nginx scontain.com , December 2017. Questions or Suggestions?","title":"Example"},{"location":"SCONE_SWARM/","text":"scone swarm Command scone swarm ls lists the nodes of a docker swarm. This is an extension of docker node ls in the sense that one can list the nodes of a swarm managed by some remote host, and several SCONE-related attributes of the nodes of a swarm are also printed. The SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands. Commands scone swarm supports the following commands: check : checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct. ls : lists all nodes of a swarm and their SCONE-related labels. Attributes As we mentioned above, scone introduces multiple new attributes: NODENO : each host in the swarm has a unique number in the range [1, number of swarm nodes ]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO. SGX VERSION : denotes the SGX version of the CPU of a host: 0 : the host does not support SGX (or, does not have a SGX driver installed) 1 : the host supports SGX version 1 2 : the host supports SGX version 2 (CPUs are not yet available) DOCKER-ENGINE : shows the version of the Docker engine that is installed. It will show SCONE if the latest patched Docker engine is installed. SGX-DRIVER : shows the version of the SGX driver. It will show SCONE if the latest patched Intel driver is installed. scone swarm ls You can list all nodes of a swarm and their attributes with the help of command scone swarm ls . This command requires you to specify a manager of the swarm with the help of option --manager MANAGER Example: To list all nodes of a swarm managed by host faye , execute $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader To list the nodes of the swarm managed by host alice , execute $ scone swarm ls --manager alice NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE alice Ready Active Leader 2 1 SCONE SCONE beatrix Ready Active 3 1 SCONE SCONE caroline Ready Active Environment Variable In case you mainly work with one swarm, you can set environment variable SCONE_MANAGER . If option --manager is not specified and SCONE_MANAGER is defined, the value stored in SCONE_MANAGER is used as the name of the manager. Example: To list all nodes of a swarm managed by host faye : $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader scone swarm check scone stores the attributes of a node using Docker: the attributes of a node are stored as node labels . For example, the attributes if a node supports SGX ( SGX VERSION ), if the patched docker engine ( DOCKER-ENGINE ) and the patched Intel driver ( SGX-DRIVER ) is installed are all stored as labels. Attributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued. To check and update the labels of the swarm nodes, you can execute command scone swarm check . You must specify the manager of the swarm by defining option --manager MANAGER or by defining environment variable SCONE_MANAGER . Example: To check the labels of the swarm managed by node faye , execute: $ scone swarm check --manager faye --verbose General options --help (or, -h ): issue help message for object **host*. If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes Screencast scontain.com , December 2017. Questions or Suggestions?","title":"scone swarm"},{"location":"SCONE_SWARM/#scone-swarm","text":"Command scone swarm ls lists the nodes of a docker swarm. This is an extension of docker node ls in the sense that one can list the nodes of a swarm managed by some remote host, and several SCONE-related attributes of the nodes of a swarm are also printed. The SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands.","title":"scone swarm"},{"location":"SCONE_SWARM/#commands","text":"scone swarm supports the following commands: check : checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct. ls : lists all nodes of a swarm and their SCONE-related labels.","title":"Commands"},{"location":"SCONE_SWARM/#attributes","text":"As we mentioned above, scone introduces multiple new attributes: NODENO : each host in the swarm has a unique number in the range [1, number of swarm nodes ]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO. SGX VERSION : denotes the SGX version of the CPU of a host: 0 : the host does not support SGX (or, does not have a SGX driver installed) 1 : the host supports SGX version 1 2 : the host supports SGX version 2 (CPUs are not yet available) DOCKER-ENGINE : shows the version of the Docker engine that is installed. It will show SCONE if the latest patched Docker engine is installed. SGX-DRIVER : shows the version of the SGX driver. It will show SCONE if the latest patched Intel driver is installed.","title":"Attributes"},{"location":"SCONE_SWARM/#scone-swarm-ls","text":"You can list all nodes of a swarm and their attributes with the help of command scone swarm ls . This command requires you to specify a manager of the swarm with the help of option --manager MANAGER Example: To list all nodes of a swarm managed by host faye , execute $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader To list the nodes of the swarm managed by host alice , execute $ scone swarm ls --manager alice NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE alice Ready Active Leader 2 1 SCONE SCONE beatrix Ready Active 3 1 SCONE SCONE caroline Ready Active","title":"scone swarm ls"},{"location":"SCONE_SWARM/#environment-variable","text":"In case you mainly work with one swarm, you can set environment variable SCONE_MANAGER . If option --manager is not specified and SCONE_MANAGER is defined, the value stored in SCONE_MANAGER is used as the name of the manager. Example: To list all nodes of a swarm managed by host faye : $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader","title":"Environment Variable"},{"location":"SCONE_SWARM/#scone-swarm-check","text":"scone stores the attributes of a node using Docker: the attributes of a node are stored as node labels . For example, the attributes if a node supports SGX ( SGX VERSION ), if the patched docker engine ( DOCKER-ENGINE ) and the patched Intel driver ( SGX-DRIVER ) is installed are all stored as labels. Attributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued. To check and update the labels of the swarm nodes, you can execute command scone swarm check . You must specify the manager of the swarm by defining option --manager MANAGER or by defining environment variable SCONE_MANAGER . Example: To check the labels of the swarm managed by node faye , execute: $ scone swarm check --manager faye --verbose","title":"scone swarm check"},{"location":"SCONE_SWARM/#general-options","text":"--help (or, -h ): issue help message for object **host*. If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes","title":"General options"},{"location":"SCONE_SWARM/#screencast","text":"scontain.com , December 2017. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_Swarm_Example/","text":"Starting a SCONE Application on a Swarm We show how to run a secure nginx version, i.e., one that runs inside an enclave in a docker swarm with automatic restarts. To simplify the running of services, we provide a simple wrapper around the docker service command: the scone service executes docker service commands on the manager of a swarm. The manager is either specified via an option --manager or via environment variable SCONE_MANAGER . This is done in the same way as for command scone swarm . In what follows, we assume that SCONE_MANAGER is set to the leader of the swarm. The scone commands are typically executed in a container running at the developer site . Prerequisites Registry support For running an application in a Docker Swarm, you need to set up a local registry to ensure that all nodes get access to the same container image. The scone CLI expects the registry to be available at localhost:5000 . You can start a default registry with the help of scone : $ scone service registry --verbose Registry is already running in swarm beatrix To simplify pushing images to the local registry, the scone CLI includes a scone service pull command to pull an image from docker hub and then to push this image to the local registry. For example, to pull image sconecuratedimages/sconetainer:noshielding and store it as localhost:5000/sconetainer:noshielding , just execute: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding SGX Support Services are automatically restarted. In case, there is a persistent failure in some service ha , we would see repeated restarts like: $ scone service ps ha ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS f65id6ow5n6w ha.1 sconecuratedimages/nginx beatrix Ready Ready 1 second ago jt6wj5e3lso4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 3 seconds ago task: non-zero exit (1) sspou3mcis8m \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 9 seconds ago task: non-zero exit (1) p3bw780pu63b \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 15 seconds ago task: non-zero exit (1) 75zjsesil5k4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 22 seconds ago task: non-zero exit (1) Reasons for such failures might be that that the containers might not have access to the sgx device. There are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute scone swarm check : $ scone swarm check warning: sgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0) ( Line numer: 198 ) warning: --device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0) ( Line numer: 199 ) To get a summary view of a swarm after you performed a check, just executed: $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 2 1 SCONE SCONE caroline Ready Active Reachable 3 1 SCONE SCONE dorothy Ready Active 4 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE beatrix Ready Active Leader Starting a Service After pulling an image into the local registry (see above), we can start a service in the swarm via scone service create . Docker swarm will start the image and it also takes care of failures by restarting failed services. For the next steps, make sure that all nodes have access to image sconecuratedimages/sconetainer:noshielding and pull this image via: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding We start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes: $ scone service create --name sconeweb --detach = true --publish 80 :80 --publish 443 :443 --replicas = 2 localhost:5000/sconetainer:noshielding In case the service starts up correctly, you will see a status like this: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ba3odjkz6mx2 sconeweb.1 localhost:5000/nginx:noshielding alice Running Running 8 minutes ago x2xq1c3aede7 sconeweb.2 localhost:5000/nginx:noshielding beatrix Running Running 8 minutes ago If, for example, an image is not available on all nodes, you might see the following status: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o79714pw2fpn sconeweb.1 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago t0byepte0fzj \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 mg4xdq868syq \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 ry1pqen9jgan \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 q05ti7gkxc7r \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 zxj74inh2zdf sconeweb.2 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago Stopping the service Stop the service via: $ scone service rm sconeweb Updating the image of a service Say, there is a new version of the sconetainer image available. We can update this image in our local registry as follows: $ scone pull sconecuratedimages/sconetainer:noshielding We can now update the service as follows: $ scone service update --image localhost:5000/sconetainer sconeweb Draining a node To be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node: sudo docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 91a1vvex4dgozfrzy1y136gmg * alice Ready Active Leader jhrayos9ylu02egwvkxpqtbwb beatrix Ready Active You can now take node alice out of service by executing: sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg To put the node back in service by executing: sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg scontain.com , November 2017. Questions or Suggestions?","title":"SCONE Swarm Example"},{"location":"SCONE_Swarm_Example/#starting-a-scone-application-on-a-swarm","text":"We show how to run a secure nginx version, i.e., one that runs inside an enclave in a docker swarm with automatic restarts. To simplify the running of services, we provide a simple wrapper around the docker service command: the scone service executes docker service commands on the manager of a swarm. The manager is either specified via an option --manager or via environment variable SCONE_MANAGER . This is done in the same way as for command scone swarm . In what follows, we assume that SCONE_MANAGER is set to the leader of the swarm. The scone commands are typically executed in a container running at the developer site .","title":"Starting a SCONE Application on a Swarm"},{"location":"SCONE_Swarm_Example/#prerequisites","text":"","title":"Prerequisites"},{"location":"SCONE_Swarm_Example/#registry-support","text":"For running an application in a Docker Swarm, you need to set up a local registry to ensure that all nodes get access to the same container image. The scone CLI expects the registry to be available at localhost:5000 . You can start a default registry with the help of scone : $ scone service registry --verbose Registry is already running in swarm beatrix To simplify pushing images to the local registry, the scone CLI includes a scone service pull command to pull an image from docker hub and then to push this image to the local registry. For example, to pull image sconecuratedimages/sconetainer:noshielding and store it as localhost:5000/sconetainer:noshielding , just execute: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding","title":"Registry support"},{"location":"SCONE_Swarm_Example/#sgx-support","text":"Services are automatically restarted. In case, there is a persistent failure in some service ha , we would see repeated restarts like: $ scone service ps ha ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS f65id6ow5n6w ha.1 sconecuratedimages/nginx beatrix Ready Ready 1 second ago jt6wj5e3lso4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 3 seconds ago task: non-zero exit (1) sspou3mcis8m \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 9 seconds ago task: non-zero exit (1) p3bw780pu63b \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 15 seconds ago task: non-zero exit (1) 75zjsesil5k4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 22 seconds ago task: non-zero exit (1) Reasons for such failures might be that that the containers might not have access to the sgx device. There are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute scone swarm check : $ scone swarm check warning: sgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0) ( Line numer: 198 ) warning: --device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0) ( Line numer: 199 ) To get a summary view of a swarm after you performed a check, just executed: $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 2 1 SCONE SCONE caroline Ready Active Reachable 3 1 SCONE SCONE dorothy Ready Active 4 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE beatrix Ready Active Leader","title":"SGX Support"},{"location":"SCONE_Swarm_Example/#starting-a-service","text":"After pulling an image into the local registry (see above), we can start a service in the swarm via scone service create . Docker swarm will start the image and it also takes care of failures by restarting failed services. For the next steps, make sure that all nodes have access to image sconecuratedimages/sconetainer:noshielding and pull this image via: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding We start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes: $ scone service create --name sconeweb --detach = true --publish 80 :80 --publish 443 :443 --replicas = 2 localhost:5000/sconetainer:noshielding In case the service starts up correctly, you will see a status like this: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ba3odjkz6mx2 sconeweb.1 localhost:5000/nginx:noshielding alice Running Running 8 minutes ago x2xq1c3aede7 sconeweb.2 localhost:5000/nginx:noshielding beatrix Running Running 8 minutes ago If, for example, an image is not available on all nodes, you might see the following status: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o79714pw2fpn sconeweb.1 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago t0byepte0fzj \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 mg4xdq868syq \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 ry1pqen9jgan \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 q05ti7gkxc7r \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago No such image: sconecuratedim\u2026 zxj74inh2zdf sconeweb.2 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago","title":"Starting a Service"},{"location":"SCONE_Swarm_Example/#stopping-the-service","text":"Stop the service via: $ scone service rm sconeweb","title":"Stopping the service"},{"location":"SCONE_Swarm_Example/#updating-the-image-of-a-service","text":"Say, there is a new version of the sconetainer image available. We can update this image in our local registry as follows: $ scone pull sconecuratedimages/sconetainer:noshielding We can now update the service as follows: $ scone service update --image localhost:5000/sconetainer sconeweb","title":"Updating the image of a service"},{"location":"SCONE_Swarm_Example/#draining-a-node","text":"To be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node: sudo docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 91a1vvex4dgozfrzy1y136gmg * alice Ready Active Leader jhrayos9ylu02egwvkxpqtbwb beatrix Ready Active You can now take node alice out of service by executing: sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg To put the node back in service by executing: sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg scontain.com , November 2017. Questions or Suggestions?","title":"Draining a node"},{"location":"SCONE_TUTORIAL/","text":"SCONE Hello World Install sgxmusl cross compiler image Ensure that you installed the various sconecuratedimages/crosscompilers container image: docker image ls sconecuratedimages/* REPOSITORY TAG IMAGE ID CREATED SIZE sconecuratedimages/crosscompilers latest dff7975b7f32 7 hours ago 1 .57GB sconecuratedimages/crosscompilers scone dff7975b7f32 7 hours ago 1 .57GB If the cross compiler image is not yet installed, read Section SCONE Curated Container Images to learn how to install the SCONE cross compiler image. If the docker command fails, please ensure that docker is indeed installed (see SCONE Host Setup . Also, on some systems you might need to use sodo to run docker commands. Install the tutorial Clone the tutorial: git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git Native Hello World Ensure that hello world runs natively on your machine: cd SCONE_TUTORIAL/HelloWorld/ gcc hello_world.c -o native_hello_world ./native_hello_world Hello World Note that the generated executable, i.e., sim_hello_world , will only run on Linux. Statically-Linked Hello World The default cross compiler variant that runs hello world inside of an enclave is scone gcc and you can find this in container sconecuratedimages/crosscompilers . This variant requires access to the SGX device. In Linux, the SGX device is made available as /dev/isgx and we can give the cross compiler inside of an container access via option --device=/dev/isgx : docker run --rm --device = /dev/isgx -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers scone-gcc hello_world.c -o sgx_hello_world This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically linked binary since it is wrapped in a dynamically linked loader program: ldd ./sgx_hello_world linux-vdso.so.1 = ( 0x00007ffcf73ad000 ) libpthread.so.0 = /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007f7c2a0e9000 ) libc.so.6 = /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007f7c29d1f000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007f7c2a306000 ) Ensure that file /etc/sgx-musl.conf exists. If not, store some default file like: printf Q 1\\ne 0 0 0\\ns 1 0 0\\n | sudo tee /etc/sgx-musl.conf To run sgx_hello_world , in an enclave, just execute: ./sgx_hello_world Hello World To see some more debug messages, set environment variable SCONE_VERSION=1 : SCONE_VERSION = 1 ./sgx_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_ALLOW_DLOPEN = no Revision: 9b355b99170ad434010353bb9f4dca24e532b1b7 Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Hello World The debug outputs SCONE_MODE=hw shows that sgx_hello_world runs in hardware mode, i.e., inside an SGX enclave. Note. The compilation as well as the hello world program will fail in case you do not have an SGX driver installed. Dynamically-Linked Hello World docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_world.c -o dyn_hello_world To run this natively, just execute the following: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable SCONE_VERSION=1 : Hardware Mode vs Simulation Mode For debugging, we support three different modes for execution: hardware, simulation, and automatic : hardware : by setting environment variable to SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave. simulation : by setting environment variable to SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable. automatic : by setting environment variable to SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode) NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave. Execution on a SGX-capable machine docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello World Execution on a non-SGX machine If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message: docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world [ Error ] Could not create enclave: Error opening SGX device You could run this in simulation mode as follows: docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = SIM -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 Hello World Alternatively, you could run this program in automatic mode: docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = AUTO -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 HelloWorld Run STRACE Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows: docker run --cap-add SYS_PTRACE -it --rm --device = /dev/isgx -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace -f /usr/src/myapp/sgx_hello_world strace.log Hello World head strace.log execve ( /usr/src/myapp/sgx_hello_world , [ /usr/src/myapp/sgx_hello_world ] , [ /* 10 vars */ ]) = 0 brk ( NULL ) = 0x10e8000 access ( /etc/ld.so.nohwcap , F_OK ) = -1 ENOENT ( No such file or directory ) mmap ( NULL, 8192 , PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 ) = 0x7f17f07f1000 access ( /etc/ld.so.preload , R_OK ) = -1 ENOENT ( No such file or directory ) open ( /etc/ld.so.cache , O_RDONLY | O_CLOEXEC ) = 3 fstat ( 3 , { st_mode = S_IFREG | 0644 , st_size = 18506 , ... }) = 0 mmap ( NULL, 18506 , PROT_READ, MAP_PRIVATE, 3 , 0 ) = 0x7f17f07ec000 close ( 3 ) = 0 access ( /etc/ld.so.nohwcap , F_OK ) = -1 ENOENT ( No such file or directory ) Screencast scontain.com , November 2017. Questions or Suggestions?","title":"SCONE Hello World"},{"location":"SCONE_TUTORIAL/#scone-hello-world","text":"","title":"SCONE Hello World"},{"location":"SCONE_TUTORIAL/#install-sgxmusl-cross-compiler-image","text":"Ensure that you installed the various sconecuratedimages/crosscompilers container image: docker image ls sconecuratedimages/* REPOSITORY TAG IMAGE ID CREATED SIZE sconecuratedimages/crosscompilers latest dff7975b7f32 7 hours ago 1 .57GB sconecuratedimages/crosscompilers scone dff7975b7f32 7 hours ago 1 .57GB If the cross compiler image is not yet installed, read Section SCONE Curated Container Images to learn how to install the SCONE cross compiler image. If the docker command fails, please ensure that docker is indeed installed (see SCONE Host Setup . Also, on some systems you might need to use sodo to run docker commands.","title":"Install sgxmusl cross compiler image"},{"location":"SCONE_TUTORIAL/#install-the-tutorial","text":"Clone the tutorial: git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git","title":"Install the tutorial"},{"location":"SCONE_TUTORIAL/#native-hello-world","text":"Ensure that hello world runs natively on your machine: cd SCONE_TUTORIAL/HelloWorld/ gcc hello_world.c -o native_hello_world ./native_hello_world Hello World Note that the generated executable, i.e., sim_hello_world , will only run on Linux.","title":"Native Hello World"},{"location":"SCONE_TUTORIAL/#statically-linked-hello-world","text":"The default cross compiler variant that runs hello world inside of an enclave is scone gcc and you can find this in container sconecuratedimages/crosscompilers . This variant requires access to the SGX device. In Linux, the SGX device is made available as /dev/isgx and we can give the cross compiler inside of an container access via option --device=/dev/isgx : docker run --rm --device = /dev/isgx -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers scone-gcc hello_world.c -o sgx_hello_world This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically linked binary since it is wrapped in a dynamically linked loader program: ldd ./sgx_hello_world linux-vdso.so.1 = ( 0x00007ffcf73ad000 ) libpthread.so.0 = /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007f7c2a0e9000 ) libc.so.6 = /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007f7c29d1f000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007f7c2a306000 ) Ensure that file /etc/sgx-musl.conf exists. If not, store some default file like: printf Q 1\\ne 0 0 0\\ns 1 0 0\\n | sudo tee /etc/sgx-musl.conf To run sgx_hello_world , in an enclave, just execute: ./sgx_hello_world Hello World To see some more debug messages, set environment variable SCONE_VERSION=1 : SCONE_VERSION = 1 ./sgx_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_ALLOW_DLOPEN = no Revision: 9b355b99170ad434010353bb9f4dca24e532b1b7 Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Hello World The debug outputs SCONE_MODE=hw shows that sgx_hello_world runs in hardware mode, i.e., inside an SGX enclave. Note. The compilation as well as the hello world program will fail in case you do not have an SGX driver installed.","title":"Statically-Linked Hello World"},{"location":"SCONE_TUTORIAL/#dynamically-linked-hello-world","text":"docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_world.c -o dyn_hello_world To run this natively, just execute the following: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable SCONE_VERSION=1 :","title":"Dynamically-Linked Hello World"},{"location":"SCONE_TUTORIAL/#hardware-mode-vs-simulation-mode","text":"For debugging, we support three different modes for execution: hardware, simulation, and automatic : hardware : by setting environment variable to SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave. simulation : by setting environment variable to SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable. automatic : by setting environment variable to SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode) NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave.","title":"Hardware Mode vs Simulation Mode"},{"location":"SCONE_TUTORIAL/#execution-on-a-sgx-capable-machine","text":"docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello World","title":"Execution on a SGX-capable machine"},{"location":"SCONE_TUTORIAL/#execution-on-a-non-sgx-machine","text":"If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message: docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world [ Error ] Could not create enclave: Error opening SGX device You could run this in simulation mode as follows: docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = SIM -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 Hello World Alternatively, you could run this program in automatic mode: docker run --rm -v $PWD :/usr/src/myapp -e SCONE_MODE = AUTO -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 HelloWorld","title":"Execution on a non-SGX machine"},{"location":"SCONE_TUTORIAL/#run-strace","text":"Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows: docker run --cap-add SYS_PTRACE -it --rm --device = /dev/isgx -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace -f /usr/src/myapp/sgx_hello_world strace.log Hello World head strace.log execve ( /usr/src/myapp/sgx_hello_world , [ /usr/src/myapp/sgx_hello_world ] , [ /* 10 vars */ ]) = 0 brk ( NULL ) = 0x10e8000 access ( /etc/ld.so.nohwcap , F_OK ) = -1 ENOENT ( No such file or directory ) mmap ( NULL, 8192 , PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 ) = 0x7f17f07f1000 access ( /etc/ld.so.preload , R_OK ) = -1 ENOENT ( No such file or directory ) open ( /etc/ld.so.cache , O_RDONLY | O_CLOEXEC ) = 3 fstat ( 3 , { st_mode = S_IFREG | 0644 , st_size = 18506 , ... }) = 0 mmap ( NULL, 18506 , PROT_READ, MAP_PRIVATE, 3 , 0 ) = 0x7f17f07ec000 close ( 3 ) = 0 access ( /etc/ld.so.nohwcap , F_OK ) = -1 ENOENT ( No such file or directory )","title":"Run STRACE"},{"location":"SCONE_TUTORIAL/#screencast","text":"scontain.com , November 2017. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_VOLUME/","text":"scone volume scone volume provides functionality to create volumes that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to volumes. The underlying technology of scone volume is infinit - a software created by a Docker Inc subsidiary. Please read the infinit documentation to understand the infinit concepts of user , network , and silo . NOTE: infinit is labeled as alpha software: use this only for development or testing . The performance of infinit needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given. scone volume commands scone volume Description install install infinit on all nodes of a swarm. create create new volume and install infinit storage platform if required check check that all volumes are available on all hosts delete delete a volume scone volume install [OPTIONS] scone volume install ensures that infinit is installed on all nodes of a swarm. Options Description --manager MANAGER manager of swarm (required) --as user run as user (default random id) --network name name of network to use/create (default scone-network) --silo name name of silo to use/create (default scone-silo) --capacity sz size of silo in GB (default 10) --help show this help message Notes: If you do not specify a default user via option --user USER , a random default user is created. If you do not specify a network name via option --network name , the default network name is set to scone-network . You can specify a silo name via potion --silo name . The default silo name is \"scone-silo\" . The capacity of the silo (in GB) is given via --capacity name . The default capacity is 10GB. **Examples: ** To install infinit with a storage capacity of 15 GB on each node, execute the following: $ scone volume install --verbose --manager alice --capacity 15 If you want to keep explicit control over user and network names, execute the following: $ scone volume install --verbose --manager alice --as scone --capacity 5 --silo my-silo --network scone-networkg If you want to see the user and network names for a given swarm, execute: $ scone volume install --manager alice --help scone volume create scone volume create creates a new infinit volume for a given user (option --as USER ) and with a given volume name (via option --name VOLUME ). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the --fast option to avoid checks and reinstallation of infinit. This volume will be available on all hosts of a swarm at location: /mnt/infinit/USER/VOLUME Options Description --manager HOST manager of swarm (required) --name volume name of volume to create (required) --help show this help message --as user run as user (default ) --network name name of network to use/create (default ) --silo name name of silo to use/create (default ) --capacity sz size of silo in GB (default ) --fast create volume without reinstalling all software (default=false) --verbose print location of created volume **Example: ** In case you already installed infinit on your swarm and you just want to create a new volume, execute the following: $ scone volume create --verbose --fast --name new-volume --verbose In case you do not know if infinit is already installed when you create a volume and your want to keep control over the names used by infinit, execute the following: $ scone volume create --verbose --manager alice --as scone --name my-volume --capacity 5 --silo my-silo --network scone-network --export config scone volume check Checks if all created volumes are available on all swarm nodes. Note: the current implementation of **scone volume check uses the metadata stored in the container/environment in which scone volume check executes. In case you created some volumes in a different container, the checks will complain.** Will plan to fix this issue in a future version of scone volume check . Options Description --manager HOST manager of swarm (required) Example: $ scone volume check --verbose --manager alice scone volume delete scone volume delete deletes a volume. This removes the volume from all nodes of a swarm. Options Description --manager HOST manager of swarm (required) --name volume name of volume to be deleted (required) Example: $ scone volume delete --verbose --manager alice --volume new-volume Example To install infinit on all nodes of a swarm managed by node faye and reserve a capacity of 15GB per node, execute the following: $ scone volume install --verbose --manager faye --capacity 15 To create a volume named test_scone_volume for some arbitrary infinit user test_scone_user , execute the following: $ scone volume create --verbose --manager faye --as test_scone_user --name test_scone_volume Note that the infinit user test_scone_user is automatically created if it does not yet exists. After completion of this command, you will have access to the newly created volume at location /mnt/infinit/test_scone_user/test_scone_volume on all nodes of the swarm. You can give a service access to this volume with the help of its stack file. Say, the service needs to access this mapped at /mnt/vol as follows: myservice : volumes : - /mnt/infinit/test_scone_user/test_scone_volume:/mnt/ vol Note that scone volume create will check that infinit is properly installed and will reinstall parts that are missing. If you are sure that infinit is properly installed, you can add option --fast to omit the checks that infinit is properly installed. To create a volume test_scone2_volume on a swarm managed by faye when you know that infinit is installed, just executed $ scone volume create --verbose --fast --manager faye --as test_scone_user --name test_scone2_volume To check that the volumes on the swarm are properly installed, execute command check : $ scone volume check --verbose --manager faye To delete a volume, we provide command delete . For example, to delete volume test_scone2_volume , execute: $ scone volume delete --verbose --manager faye --as test_scone_user --name test_scone2_volume scontain.com , December 2017. Questions or Suggestions?","title":"scone volume"},{"location":"SCONE_VOLUME/#scone-volume","text":"scone volume provides functionality to create volumes that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to volumes. The underlying technology of scone volume is infinit - a software created by a Docker Inc subsidiary. Please read the infinit documentation to understand the infinit concepts of user , network , and silo . NOTE: infinit is labeled as alpha software: use this only for development or testing . The performance of infinit needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given.","title":"scone volume"},{"location":"SCONE_VOLUME/#scone-volume-commands","text":"scone volume Description install install infinit on all nodes of a swarm. create create new volume and install infinit storage platform if required check check that all volumes are available on all hosts delete delete a volume","title":"scone volume commands"},{"location":"SCONE_VOLUME/#scone-volume-install-options","text":"scone volume install ensures that infinit is installed on all nodes of a swarm. Options Description --manager MANAGER manager of swarm (required) --as user run as user (default random id) --network name name of network to use/create (default scone-network) --silo name name of silo to use/create (default scone-silo) --capacity sz size of silo in GB (default 10) --help show this help message Notes: If you do not specify a default user via option --user USER , a random default user is created. If you do not specify a network name via option --network name , the default network name is set to scone-network . You can specify a silo name via potion --silo name . The default silo name is \"scone-silo\" . The capacity of the silo (in GB) is given via --capacity name . The default capacity is 10GB. **Examples: ** To install infinit with a storage capacity of 15 GB on each node, execute the following: $ scone volume install --verbose --manager alice --capacity 15 If you want to keep explicit control over user and network names, execute the following: $ scone volume install --verbose --manager alice --as scone --capacity 5 --silo my-silo --network scone-networkg If you want to see the user and network names for a given swarm, execute: $ scone volume install --manager alice --help","title":"scone volume install [OPTIONS]"},{"location":"SCONE_VOLUME/#scone-volume-create","text":"scone volume create creates a new infinit volume for a given user (option --as USER ) and with a given volume name (via option --name VOLUME ). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the --fast option to avoid checks and reinstallation of infinit. This volume will be available on all hosts of a swarm at location: /mnt/infinit/USER/VOLUME Options Description --manager HOST manager of swarm (required) --name volume name of volume to create (required) --help show this help message --as user run as user (default ) --network name name of network to use/create (default ) --silo name name of silo to use/create (default ) --capacity sz size of silo in GB (default ) --fast create volume without reinstalling all software (default=false) --verbose print location of created volume **Example: ** In case you already installed infinit on your swarm and you just want to create a new volume, execute the following: $ scone volume create --verbose --fast --name new-volume --verbose In case you do not know if infinit is already installed when you create a volume and your want to keep control over the names used by infinit, execute the following: $ scone volume create --verbose --manager alice --as scone --name my-volume --capacity 5 --silo my-silo --network scone-network --export config","title":"scone volume create"},{"location":"SCONE_VOLUME/#scone-volume-check","text":"Checks if all created volumes are available on all swarm nodes. Note: the current implementation of **scone volume check uses the metadata stored in the container/environment in which scone volume check executes. In case you created some volumes in a different container, the checks will complain.** Will plan to fix this issue in a future version of scone volume check . Options Description --manager HOST manager of swarm (required) Example: $ scone volume check --verbose --manager alice","title":"scone volume check"},{"location":"SCONE_VOLUME/#scone-volume-delete","text":"scone volume delete deletes a volume. This removes the volume from all nodes of a swarm. Options Description --manager HOST manager of swarm (required) --name volume name of volume to be deleted (required) Example: $ scone volume delete --verbose --manager alice --volume new-volume","title":"scone volume delete"},{"location":"SCONE_VOLUME/#example","text":"To install infinit on all nodes of a swarm managed by node faye and reserve a capacity of 15GB per node, execute the following: $ scone volume install --verbose --manager faye --capacity 15 To create a volume named test_scone_volume for some arbitrary infinit user test_scone_user , execute the following: $ scone volume create --verbose --manager faye --as test_scone_user --name test_scone_volume Note that the infinit user test_scone_user is automatically created if it does not yet exists. After completion of this command, you will have access to the newly created volume at location /mnt/infinit/test_scone_user/test_scone_volume on all nodes of the swarm. You can give a service access to this volume with the help of its stack file. Say, the service needs to access this mapped at /mnt/vol as follows: myservice : volumes : - /mnt/infinit/test_scone_user/test_scone_volume:/mnt/ vol Note that scone volume create will check that infinit is properly installed and will reinstall parts that are missing. If you are sure that infinit is properly installed, you can add option --fast to omit the checks that infinit is properly installed. To create a volume test_scone2_volume on a swarm managed by faye when you know that infinit is installed, just executed $ scone volume create --verbose --fast --manager faye --as test_scone_user --name test_scone2_volume To check that the volumes on the swarm are properly installed, execute command check : $ scone volume check --verbose --manager faye To delete a volume, we provide command delete . For example, to delete volume test_scone2_volume , execute: $ scone volume delete --verbose --manager faye --as test_scone_user --name test_scone2_volume scontain.com , December 2017. Questions or Suggestions?","title":"Example"},{"location":"SCONE_toolchain/","text":"SCONE SGX Toolchain SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran. The objective of these (cross-)compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves. To simplify the use of these (cross-)compilers, SCONE maintains curated container image that includes these cross-compilers. Compiler variants Depending on if you want to generate a dynamically-linked or a statically-linked binary, you can use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. Portability of SCONE programs Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code base, i.e., it includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we recommend both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs one does not change the build process when moving to SCONE. Loading Shared Libraries after startup SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java and Python. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2). For production enclaves, you will need to protect the integrity of the shared libraries with the help of the SCONE file shield . Dynamically-Linked Binaries The easiest to get started is to compile your programs such that the generated code is position independent ( -fPIC ), the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ), your binary is dynamically linked (i.e., do not use -static ), and link against musl as your libc (i.e., not glibc or any other libc). When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the file system shield . To simplify the compiling of your programs for scone, we make available a docker image sconecuratedimages/muslgcc which includes gcc and g++ support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options. Statically-Linked Binaries For statically linked binaries, we make available a (private) docker image sconecuratedimages/crosscompilers which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves. Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program web-srv-go , you will still see dynamic dependencies: $ ldd web-srv-go linux-vdso.so.1 = ( 0x00007ffe423fd000 ) libpthread.so.0 = /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007effa344f000 ) libc.so.6 = /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007effa3085000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007effa366c000 ) The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked loader program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked. Using the cross compiler container How to use the compiler: use this as a base image and build your programs inside of a container we a Dockerfile ), or map volumes such that the compiler can compile files living outside the container (see SCONE Tutorial ). For an example how to use the crosscompilers, see how to compile a programs written in GO . Example Note on some systems you will need to run docker with root permissions, i.e., in this case you should prefix a docker ... command with sudo , i.e., you execute sudo docker ... One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is in file myapp.c in your current directory ( $PWD ). You can compile this code as follows: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c This call will generate a binary a.out in your working directory. This binary is dynamically linked against musl: ldd a.out /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) libc.musl-x86_64.so.1 = /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our hello world in Section SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of Dockerfiles . In most cases, you might just set to use one of our crosscompilers in your configure script or Makefile . A simple way is to use the Docker image sconecuratedimages/crosscompilers as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of cargo which is scone-cargo . Debugger support We also support gdb to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image sconecuratedimages/crosscompilers as scone-gdb . For example on how to use the debugger, see how to debug a program written in GO . scontain.com , November 2017. Questions or Suggestions?","title":"SCONE SGX toolchain"},{"location":"SCONE_toolchain/#scone-sgx-toolchain","text":"SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran. The objective of these (cross-)compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves. To simplify the use of these (cross-)compilers, SCONE maintains curated container image that includes these cross-compilers.","title":"SCONE SGX Toolchain"},{"location":"SCONE_toolchain/#compiler-variants","text":"Depending on if you want to generate a dynamically-linked or a statically-linked binary, you can use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. Portability of SCONE programs Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code base, i.e., it includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we recommend both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs one does not change the build process when moving to SCONE. Loading Shared Libraries after startup SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java and Python. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2). For production enclaves, you will need to protect the integrity of the shared libraries with the help of the SCONE file shield .","title":"Compiler variants"},{"location":"SCONE_toolchain/#dynamically-linked-binaries","text":"The easiest to get started is to compile your programs such that the generated code is position independent ( -fPIC ), the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ), your binary is dynamically linked (i.e., do not use -static ), and link against musl as your libc (i.e., not glibc or any other libc). When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the file system shield . To simplify the compiling of your programs for scone, we make available a docker image sconecuratedimages/muslgcc which includes gcc and g++ support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options.","title":"Dynamically-Linked Binaries"},{"location":"SCONE_toolchain/#statically-linked-binaries","text":"For statically linked binaries, we make available a (private) docker image sconecuratedimages/crosscompilers which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves. Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program web-srv-go , you will still see dynamic dependencies: $ ldd web-srv-go linux-vdso.so.1 = ( 0x00007ffe423fd000 ) libpthread.so.0 = /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007effa344f000 ) libc.so.6 = /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007effa3085000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007effa366c000 ) The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked loader program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked.","title":"Statically-Linked Binaries"},{"location":"SCONE_toolchain/#using-the-cross-compiler-container","text":"How to use the compiler: use this as a base image and build your programs inside of a container we a Dockerfile ), or map volumes such that the compiler can compile files living outside the container (see SCONE Tutorial ). For an example how to use the crosscompilers, see how to compile a programs written in GO .","title":"Using the cross compiler container"},{"location":"SCONE_toolchain/#example","text":"Note on some systems you will need to run docker with root permissions, i.e., in this case you should prefix a docker ... command with sudo , i.e., you execute sudo docker ... One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is in file myapp.c in your current directory ( $PWD ). You can compile this code as follows: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c This call will generate a binary a.out in your working directory. This binary is dynamically linked against musl: ldd a.out /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) libc.musl-x86_64.so.1 = /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container: docker run --rm -v $PWD :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our hello world in Section SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of Dockerfiles . In most cases, you might just set to use one of our crosscompilers in your configure script or Makefile . A simple way is to use the Docker image sconecuratedimages/crosscompilers as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of cargo which is scone-cargo .","title":"Example"},{"location":"SCONE_toolchain/#debugger-support","text":"We also support gdb to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image sconecuratedimages/crosscompilers as scone-gdb . For example on how to use the debugger, see how to debug a program written in GO . scontain.com , November 2017. Questions or Suggestions?","title":"Debugger support"},{"location":"aboutScone/","text":"About SCONE The SCONE platform is commercially supported by scontain.com . The SCONE platform has been developed at the Systems Engineering group at TU Dresden in the context of the following EU H2020 projects : Sereca which investigates how to use Intel SGX enclave in the context of reactive programs written in Vert.x . Secure Cloud which focuses on the processing of big data in untrusted clouds. The first paper about SCONE has been published in OSDI 2016 with our colleagues from Imperial College London, Technische Universit\u00e4t Braunschweig and University of Otago. More papers about SCONE can be found here . We investigate use cases and extensions of SCONE in the context of the following EU H2020 projects : SELIS : we investigate how to secure data processing within a Shared European Logistics Intelligent Information Space with the help of SCONE. ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform. LEGATO : a new EU project in which we address high integrity computations inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves. Computing Resources scontain.com can provide access to SGX-capable machines. Consulting Services scontain.com provides consulting services as well as helping you to port your applications to SGX. Contact If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at info@scontain.com . Legal Notice The content of this documentation is maintained by scontain.com . scontain.com , June 2018. Questions or Suggestions?","title":"About"},{"location":"aboutScone/#about-scone","text":"The SCONE platform is commercially supported by scontain.com . The SCONE platform has been developed at the Systems Engineering group at TU Dresden in the context of the following EU H2020 projects : Sereca which investigates how to use Intel SGX enclave in the context of reactive programs written in Vert.x . Secure Cloud which focuses on the processing of big data in untrusted clouds. The first paper about SCONE has been published in OSDI 2016 with our colleagues from Imperial College London, Technische Universit\u00e4t Braunschweig and University of Otago. More papers about SCONE can be found here . We investigate use cases and extensions of SCONE in the context of the following EU H2020 projects : SELIS : we investigate how to secure data processing within a Shared European Logistics Intelligent Information Space with the help of SCONE. ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform. LEGATO : a new EU project in which we address high integrity computations inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves.","title":"About SCONE"},{"location":"aboutScone/#computing-resources","text":"scontain.com can provide access to SGX-capable machines.","title":"Computing Resources"},{"location":"aboutScone/#consulting-services","text":"scontain.com provides consulting services as well as helping you to port your applications to SGX.","title":"Consulting Services"},{"location":"aboutScone/#contact","text":"If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at info@scontain.com .","title":"Contact"},{"location":"aboutScone/#legal-notice","text":"The content of this documentation is maintained by scontain.com . scontain.com , June 2018. Questions or Suggestions?","title":"Legal Notice"},{"location":"appsecurity/","text":"Application-Oriented Security SCONE supports developers and service providers , to protect the confidentiality and integrity of their applications - even when running in environments that cannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like microservice-based applications as well as cloud-native applications . However, SCONE can protect most programs running on top of Linux. SCONE supports developers and service providers to ensure end-to-end encryption in the sense that data is always encrypted , i.e., while being transmitted, while being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect. For example, a service that operates only on encrypted data might not need to be protected with SGX. Keep it simple Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.* Ease of Use SCONE supports strong application-oriented security with a workflow like Docker, i.e., SCONE supports Dockerfiles as well as extended Docker compose files. This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container. The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Docker Swarm can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Docker Swarm, failed containers will automatically be restarted on an appropriate host. scontain.com , March 2018. Questions or Suggestions?","title":"Application-Oriented Security"},{"location":"appsecurity/#application-oriented-security","text":"SCONE supports developers and service providers , to protect the confidentiality and integrity of their applications - even when running in environments that cannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like microservice-based applications as well as cloud-native applications . However, SCONE can protect most programs running on top of Linux. SCONE supports developers and service providers to ensure end-to-end encryption in the sense that data is always encrypted , i.e., while being transmitted, while being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect. For example, a service that operates only on encrypted data might not need to be protected with SGX. Keep it simple Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.*","title":"Application-Oriented Security"},{"location":"appsecurity/#ease-of-use","text":"SCONE supports strong application-oriented security with a workflow like Docker, i.e., SCONE supports Dockerfiles as well as extended Docker compose files. This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container. The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Docker Swarm can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Docker Swarm, failed containers will automatically be restarted on an appropriate host. scontain.com , March 2018. Questions or Suggestions?","title":"Ease of Use"},{"location":"background/","text":"SCONE Background Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data. Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack isolated hosts or on OpenStack baremetal clouds . Problem: Defender's Dilemma Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems. A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities. For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected. To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect. SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release: Linux Lines of Code (StefanPohl, CC0, original } OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines): OpenStack Lines of Code (OpenHub original ) To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code: Docker Lines of Code (OpenHub original ) Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too. SCONE Approach The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team. A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves. Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices. Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts. scontain.com , March 2018. Questions or Suggestions?","title":"SCONE Background"},{"location":"background/#scone-background","text":"Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data. Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack isolated hosts or on OpenStack baremetal clouds .","title":"SCONE Background"},{"location":"background/#problem-defenders-dilemma","text":"Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems. A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities. For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected. To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect. SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release: Linux Lines of Code (StefanPohl, CC0, original } OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines): OpenStack Lines of Code (OpenHub original ) To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code: Docker Lines of Code (OpenHub original ) Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too.","title":"Problem: Defender's Dilemma"},{"location":"background/#scone-approach","text":"The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team. A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves. Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices. Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts. scontain.com , March 2018. Questions or Suggestions?","title":"SCONE Approach"},{"location":"configuration/","text":"SCONE Configuration File SCONE permits users to tune certain internal parameters. These parameters affect performance but do not - at least directly - impact confidentiality nor integrity. These tuning parameters are defined by default in file /etc/sgx-musl.conf . When executing a program, one can change this default via environment variable SCONE_CONFIG . SCONE runs four different types of threads: ETHREADS : this are operating systems threads that run inside an enclave STHREADS : these are operating systems threads that run outside an enclave. SCONE_ETHREADS, SCONE_STHREADS, and SCONE_ESPINS, SCONE_ESLEEP variables are gone. As a replacement for SCONE_ETHREADS and SCONE_STHREADS variable, there is now SCONE_QUEUES variable, and a configurations file (in which you can set most of environment variables). The path to default config is /etc/sgx-musl.conf, you can also specify it via SCONE_CONFIG option. A reasonable default configuration file is already installed on sgx1-sgx3 machines. The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: - Q - sets the number of syscall-return queue pairs to allocate, equivalent to setting SCONE_QUEUES env variable; - H - sets the size of heap, equivalent to setting SCONE_HEAP env variable; - P - equivalent to setting SCONE_SSPINS env variable; - L - equivalent to setting SCONE_SSLEEP env variable; - s - creates an sthread pinned to core , serving queue pair with index , if is non-zero - sets SCHED_FIFO scheduler for the thread. - e - same as before, but creates an ethread. SCONE_* env variables have higher priority than settings from the config file. application interface SCONE is developed for legacy compatible code. Sometimes though it is wanted to adapt legacy code to benefit from SGX specific functionalities. SCONE exposes a set of these functionalities to the application in the scone.h header file. The source code can make sure that it is acutally executed with SCONE by testing for the __SCONE_LIBC__ macro defined in features.h . scontain.com , November 2017. Questions or Suggestions?","title":"SCONE Configuration File"},{"location":"configuration/#scone-configuration-file","text":"SCONE permits users to tune certain internal parameters. These parameters affect performance but do not - at least directly - impact confidentiality nor integrity. These tuning parameters are defined by default in file /etc/sgx-musl.conf . When executing a program, one can change this default via environment variable SCONE_CONFIG . SCONE runs four different types of threads: ETHREADS : this are operating systems threads that run inside an enclave STHREADS : these are operating systems threads that run outside an enclave. SCONE_ETHREADS, SCONE_STHREADS, and SCONE_ESPINS, SCONE_ESLEEP variables are gone. As a replacement for SCONE_ETHREADS and SCONE_STHREADS variable, there is now SCONE_QUEUES variable, and a configurations file (in which you can set most of environment variables). The path to default config is /etc/sgx-musl.conf, you can also specify it via SCONE_CONFIG option. A reasonable default configuration file is already installed on sgx1-sgx3 machines. The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: - Q - sets the number of syscall-return queue pairs to allocate, equivalent to setting SCONE_QUEUES env variable; - H - sets the size of heap, equivalent to setting SCONE_HEAP env variable; - P - equivalent to setting SCONE_SSPINS env variable; - L - equivalent to setting SCONE_SSLEEP env variable; - s - creates an sthread pinned to core , serving queue pair with index , if is non-zero - sets SCHED_FIFO scheduler for the thread. - e - same as before, but creates an ethread. SCONE_* env variables have higher priority than settings from the config file.","title":"SCONE Configuration File"},{"location":"configuration/#application-interface","text":"SCONE is developed for legacy compatible code. Sometimes though it is wanted to adapt legacy code to benefit from SGX specific functionalities. SCONE exposes a set of these functionalities to the application in the scone.h header file. The source code can make sure that it is acutally executed with SCONE by testing for the __SCONE_LIBC__ macro defined in features.h . scontain.com , November 2017. Questions or Suggestions?","title":"application interface"},{"location":"dockerfileexample/","text":"Dockerfile Example We show now how to create a container image that contains an hello world program running inside an enclave. Warning This build should not be pushed to a public repository since it contains the complete SCONE platform. Build your container images with a multi-stage build such that they only contain your binaries. Here is the dockerfile: cat Dockerfile EOF FROM sconecuratedimages/crosscompilers RUN echo #include stdio.h helloworld.c \\ echo int main() { helloworld.c \\ echo printf(\\ Hello World!\\n\\ ); } helloworld.c RUN gcc -o helloworld helloworld.c CMD bash -c SCONE_VERSION=1 /helloworld EOF Let's generate an image ( helloworld ) with this Dockerfile: docker build --pull -t helloworld . Let's run the image as follows: docker run --device = /dev/isgx --rm helloworld The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 597cdef086651d46652cab78a89386b790ed058427ce1a5feacc3da7bc731902 Hello World! Note In case you do not have a SGX driver installed, run the program in simulation mode by executing docker run --rm helloworld scontain.com , August 2018. Questions or Suggestions?","title":"Dockerfile Example"},{"location":"dockerfileexample/#dockerfile-example","text":"We show now how to create a container image that contains an hello world program running inside an enclave. Warning This build should not be pushed to a public repository since it contains the complete SCONE platform. Build your container images with a multi-stage build such that they only contain your binaries. Here is the dockerfile: cat Dockerfile EOF FROM sconecuratedimages/crosscompilers RUN echo #include stdio.h helloworld.c \\ echo int main() { helloworld.c \\ echo printf(\\ Hello World!\\n\\ ); } helloworld.c RUN gcc -o helloworld helloworld.c CMD bash -c SCONE_VERSION=1 /helloworld EOF Let's generate an image ( helloworld ) with this Dockerfile: docker build --pull -t helloworld . Let's run the image as follows: docker run --device = /dev/isgx --rm helloworld The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 597cdef086651d46652cab78a89386b790ed058427ce1a5feacc3da7bc731902 Hello World! Note In case you do not have a SGX driver installed, run the program in simulation mode by executing docker run --rm helloworld scontain.com , August 2018. Questions or Suggestions?","title":"Dockerfile Example"},{"location":"dockerinstall/","text":"Installation of Docker Engine To install the docker engine, you can follow the official description . Alternatively, you can install the docker engine on Ubuntu as follows: curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_docker.sh | bash scontain.com , July 2018. Questions or Suggestions?","title":"Installing docker"},{"location":"dockerinstall/#installation-of-docker-engine","text":"To install the docker engine, you can follow the official description . Alternatively, you can install the docker engine on Ubuntu as follows: curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_docker.sh | bash scontain.com , July 2018. Questions or Suggestions?","title":"Installation of Docker Engine"},{"location":"faq/","text":"Frequently Asked Questions Attestation How can I verify the authenticity and integrity of my running running in enclave, if I access it remotely One approach is to store the encrypted certificate in an encrypted file region and SCONE CAS gives the service access to the file key only after a successful attestation: see file shield example . If the service can authenticate itself with the correct certificate, this means that it passed the attestation. How do I bind an interpreter (Python, JavaScript, Java) with its scripts/programs in a secure container? The problem is that the scripts/programs are not measured during the attestation of the enclave, i.e., it is not included in MrEnclave . This code must be protected with the SCONE file shield, i.e., this must be encrypted and/or authenticated. The current state of the file system is checked during attestation and this ensures the integrity of the code (i.e., no modification), the freshness (i.e., no old version is used), and the confidentiality (i.e., an adversary cannot see the code - in case the code is stored an encrypted file region). In the next version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. How can one restrict the initial script that the protected interpreter executes? The arguments of the code executed inside of an enclave must be passed via SCONE CAS. In other words, the initial script is protected by CAS, i.e., by passing the arguments to Python only after attestation via a secure channel directly into the enclave. In the next version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. This means that only code that is already include in an encrypted / authenticated file region can be executed. How can I pass secrets to my enclave? You could pass these secrets as environment variables via CAS or you can store these in encrypted files. SCONE will pass the environment variables and the file encryption key only after a successful attestation to the application via TLS. Simulation Mode Program crashes/stops in simulation mode Simulation mode assumes a modern CPU with instructions like AESNI and SSE and AVX extensions. If they are not available, your program will exit with an error message. Has my CPU sufficient features for simulation mode? Just run the following program to check if your CPU has sufficient features for simulation mode: docker run --rm sconecuratedimages/apps:check_cpuid Memory related issues My process/enclave is getting killed without any error message Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. This means that the enclave might request so much memory that the Linux Out-Of-Memory-Killer might kill the process in which this enclave runs. Ensure that you have enough free main memory such that your enclave can start. My machine has lots of main memory but still processes are getting killed Try to figure out - using tools like top - which processes are using up your memory. Sometimes, you might have too many active docker containers running: check this with docker ps -a . Try to run docker containers with docker run --rm to enforce automatic cleanup after a container exits. My program does not start up Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. We cannot estimate how much memory your application needs. Hence, for SGX version 1, we provide environment variables SCONE_HEAP and SCONE_STACK . For Java and GO programs, set SCONE_HEAP to at least 1G . The default stack size is 128KBytes. This is too little for some applications like Python and MariaDB. A good value for applications that use lots of stack seem to be 4MBytes, i.e., set export SCONE_STACK=4M . My program has a very large VIRT memory footprint SCONE reserves 64GB of the virtual address space for each enclave using the SGX driver. Hence, when monitoring a process, e.g., with top , VIRT is reported as at least 64g . Note that the important measure is the physical memory used: top reports this in column RES (given in KiB)). Crash Failures My program crashes / gets killed SGX version 1: check that your program has a sufficiently large heap by setting environment variable SCONE_HEAP ( see above ) SGX version 1: check that your program has sufficiently large stacks by setting environment variable SCONE_STACK ( see above ). SGX version 1: check that you machine has sufficient main memory see above ) Run your program with scone-gdb to determine where your program crashes Running inside of enclaves How do I know that I run inside of an enclave? Set environment variable SCONE_VERSION=1 : you will see in what mode your program is running. How can I experimentally show that I run inside of an enclave? Please check out our memory dump tutorial on this. How can I enforce/verify that a service/program runs inside of an enclave? You need to attest that your program runs inside of an enclave. SCONE supports transparent attestation with the help of CAS . On the application level, one often does not want to perform an explicit attestation but an implicit attestation with the help of TLS to reduce/eliminate the amount of reengineering that is needed. The idea is that a service can only provide the correct TLS certificate if it runs inside an enclave. To do so, one would give the enclave an encrypted TLS private key in the file system (can be generate by Scone CAS if this is requested) and the enclave gets only access to the encryption key after a successful attestation. The decryption of the TLS private key is done transparently by SCONE. Does SCONE support enclaves in production mode? Note that by default SCONE runs in debug or pre-release mode. For production enclaves, you still need to get an enclave signer key from Intel. This will change when Intel makes flexible launch control available. Shielding Does SCONE ensure the security of incoming and outgoing TCP connections to/from a service running in an enclave? Please use TLS in your service. If your service does not support TLS out of the box, use our TCP shield. How do I encrypt stdin/stdout/stderr? Send us an email on how to use the terminal shield How do I encrypt pipes? Send us an email regarding the encryption of pipes. How do I encrypt TCP connections? Context: Services like memcached do not support TLS out of the box. Send us an email on how to use the TCP shield. How do I transparently encrypt/decrypt files? Enable the file shield: see file shield example . Are all files in a container encrypted? No, only those in an encrypted file region: see file shield example . What do I need to do to protect the files my service needs, e.g. HTTPS encryption key and certificate You need to encrypt them when you build you container image: see file shield example . This will soon become a little easier to do. Dynamic Libraries What are protected dynamic libraries? The dynamic libraries loaded during start up of program are included in the hash of the enclave, i.e., any modification of any of these libraries will change MrEnclave . In that sense, the dynamic libraries are integrity protected since any modifications will result in a failed attestation. One can determine the dynamic libraries which are loaded during startup with command ldd for native binaries. Note that depending how the binary is build (static linking, dynamic linking, for Alpine Linux or for Ubuntu), ldd might only print the dynamic libraries used to start the enclave. To enable the loading of protected dynamic libraries, set environment variable SCONE_ALLOW_DLOPEN=1 . If you want to disallow this, undefine SCONE_ALLOW_DLOPEN . SCONE_ALLOW_DLOPEN=2 enables both protected as well as unprotected shared libraries. What are unprotected dynamic libraries? An unprotected shared library is loaded after the program has started by the application with a call to function dlopen (or similar). These libraries are not integrity protected by MrEnclave since they are loaded after MrEnclave was computed. To ensure the integrity of these shared library the have to be located in an authenticated or an encrypted file region. In future, we plan to enforce this property. To enable the loading of protected dynamic libraries after startup, set environment variable SCONE_ALLOW_DLOPEN=2 . This will also enable loading libraries during startup. Encrypted Code and Libraries One can encrypt code by pushing your main code in a shared library that is stored in an encrypted file region. In this way, you can protect the integrity as well as the confidentiality of your code. Resource Usage CPU utilization / number of threads is higher than in native mode By default, we use multiple threads to serve inside the enclave and to serve system calls outside the enclave. If theses threads have no work to do, they go to sleep. You can reduce the number of threads / CPU utilization by specifying a SCONE configuration file which uses less threads. For example, you could use one thread inside the enclave and one outside the enclave with this configuration file: cat /etc/sgx-musl.conf EOF Q 1 e -1 0 0 s -1 0 0 EOF The CPU utilization is still higher than in native mode In our experience, then newest version of SCONE needs less than 1-2% CPU utilization when a service is idling. If the CPU utilization of a service is higher than the native version during idle periods, you could try to tune the the backoff behavior of the queues by setting parameters L and P appropriately. Note that the standard values should in most cases do not need any tuning. The memory usage is higher than in native mode The issue is that SGX v1 enclaves must allocate all memory at startup since enclaves are fixed (- this will change with SGXv2). You can reduce memory consumption by setting SCONE_HEAP and SCONE_STACK . In SGX v2 we will allocate memory on demand, i.e., we will be more memory efficient. Note that in SGX v2, we will be able to dynamically adjust the size of the heap and the stack sizes automatically, i.e., one does not need to allocate all memory in the beginning. This will also reduce the startup times. High startup times Since in SGX v1 one needs to allocate all memory at the start of an enclave, startup times can be very large. In SGX v2, we will be able to dynamically scale the size of an enclave during runtime. Since we will allocate less memory during startup time, this will reduce the startup times. Side-Channel Protection The newest microcode of new Intel CPUs protects against L1TF side channels when hyperthreading is disabled. Please ensure that your CPU microcode is up-to-date: You can follow the following instruction to update the microcode of your CPU . Unimplemented Functions Some function returns ENOSYS Sometimes it is difficult to diagnose why a function fails. In most cases, the issue is that we do not yet support fork . Note that SCONE supports vfork . You can check which functions might not be supported by running your application with environment variable SCONE_LOG=7 . scontain.com , August 2018. Questions or Suggestions?","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#attestation","text":"","title":"Attestation"},{"location":"faq/#how-can-i-verify-the-authenticity-and-integrity-of-my-running-running-in-enclave-if-i-access-it-remotely","text":"One approach is to store the encrypted certificate in an encrypted file region and SCONE CAS gives the service access to the file key only after a successful attestation: see file shield example . If the service can authenticate itself with the correct certificate, this means that it passed the attestation.","title":"How can I verify the authenticity and integrity of my running running in enclave, if I access it remotely"},{"location":"faq/#how-do-i-bind-an-interpreter-python-javascript-java-with-its-scriptsprograms-in-a-secure-container","text":"The problem is that the scripts/programs are not measured during the attestation of the enclave, i.e., it is not included in MrEnclave . This code must be protected with the SCONE file shield, i.e., this must be encrypted and/or authenticated. The current state of the file system is checked during attestation and this ensures the integrity of the code (i.e., no modification), the freshness (i.e., no old version is used), and the confidentiality (i.e., an adversary cannot see the code - in case the code is stored an encrypted file region). In the next version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail.","title":"How do I bind an interpreter (Python, JavaScript, Java) with its scripts/programs in a secure container?"},{"location":"faq/#how-can-one-restrict-the-initial-script-that-the-protected-interpreter-executes","text":"The arguments of the code executed inside of an enclave must be passed via SCONE CAS. In other words, the initial script is protected by CAS, i.e., by passing the arguments to Python only after attestation via a secure channel directly into the enclave. In the next version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. This means that only code that is already include in an encrypted / authenticated file region can be executed.","title":"How can one restrict the initial script that the protected interpreter executes?"},{"location":"faq/#how-can-i-pass-secrets-to-my-enclave","text":"You could pass these secrets as environment variables via CAS or you can store these in encrypted files. SCONE will pass the environment variables and the file encryption key only after a successful attestation to the application via TLS.","title":"How can I pass secrets to my enclave?"},{"location":"faq/#simulation-mode","text":"","title":"Simulation Mode"},{"location":"faq/#program-crashesstops-in-simulation-mode","text":"Simulation mode assumes a modern CPU with instructions like AESNI and SSE and AVX extensions. If they are not available, your program will exit with an error message.","title":"Program crashes/stops in simulation mode"},{"location":"faq/#has-my-cpu-sufficient-features-for-simulation-mode","text":"Just run the following program to check if your CPU has sufficient features for simulation mode: docker run --rm sconecuratedimages/apps:check_cpuid","title":"Has my CPU sufficient features for simulation mode?"},{"location":"faq/#memory-related-issues","text":"","title":"Memory related issues"},{"location":"faq/#my-processenclave-is-getting-killed-without-any-error-message","text":"Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. This means that the enclave might request so much memory that the Linux Out-Of-Memory-Killer might kill the process in which this enclave runs. Ensure that you have enough free main memory such that your enclave can start.","title":"My process/enclave is getting killed without any error message"},{"location":"faq/#my-machine-has-lots-of-main-memory-but-still-processes-are-getting-killed","text":"Try to figure out - using tools like top - which processes are using up your memory. Sometimes, you might have too many active docker containers running: check this with docker ps -a . Try to run docker containers with docker run --rm to enforce automatic cleanup after a container exits.","title":"My machine has lots of main memory but still processes are getting killed"},{"location":"faq/#my-program-does-not-start-up","text":"Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. We cannot estimate how much memory your application needs. Hence, for SGX version 1, we provide environment variables SCONE_HEAP and SCONE_STACK . For Java and GO programs, set SCONE_HEAP to at least 1G . The default stack size is 128KBytes. This is too little for some applications like Python and MariaDB. A good value for applications that use lots of stack seem to be 4MBytes, i.e., set export SCONE_STACK=4M .","title":"My program does not start up"},{"location":"faq/#my-program-has-a-very-large-virt-memory-footprint","text":"SCONE reserves 64GB of the virtual address space for each enclave using the SGX driver. Hence, when monitoring a process, e.g., with top , VIRT is reported as at least 64g . Note that the important measure is the physical memory used: top reports this in column RES (given in KiB)).","title":"My program has a very large VIRT memory footprint"},{"location":"faq/#crash-failures","text":"","title":"Crash Failures"},{"location":"faq/#my-program-crashes-gets-killed","text":"SGX version 1: check that your program has a sufficiently large heap by setting environment variable SCONE_HEAP ( see above ) SGX version 1: check that your program has sufficiently large stacks by setting environment variable SCONE_STACK ( see above ). SGX version 1: check that you machine has sufficient main memory see above ) Run your program with scone-gdb to determine where your program crashes","title":"My program crashes / gets killed"},{"location":"faq/#running-inside-of-enclaves","text":"","title":"Running inside of enclaves"},{"location":"faq/#how-do-i-know-that-i-run-inside-of-an-enclave","text":"Set environment variable SCONE_VERSION=1 : you will see in what mode your program is running.","title":"How do I know that I run inside of an enclave?"},{"location":"faq/#how-can-i-experimentally-show-that-i-run-inside-of-an-enclave","text":"Please check out our memory dump tutorial on this.","title":"How can I experimentally show that I run inside of an enclave?"},{"location":"faq/#how-can-i-enforceverify-that-a-serviceprogram-runs-inside-of-an-enclave","text":"You need to attest that your program runs inside of an enclave. SCONE supports transparent attestation with the help of CAS . On the application level, one often does not want to perform an explicit attestation but an implicit attestation with the help of TLS to reduce/eliminate the amount of reengineering that is needed. The idea is that a service can only provide the correct TLS certificate if it runs inside an enclave. To do so, one would give the enclave an encrypted TLS private key in the file system (can be generate by Scone CAS if this is requested) and the enclave gets only access to the encryption key after a successful attestation. The decryption of the TLS private key is done transparently by SCONE.","title":"How can I enforce/verify that a service/program runs inside of an enclave?"},{"location":"faq/#does-scone-support-enclaves-in-production-mode","text":"Note that by default SCONE runs in debug or pre-release mode. For production enclaves, you still need to get an enclave signer key from Intel. This will change when Intel makes flexible launch control available.","title":"Does SCONE support enclaves in production mode?"},{"location":"faq/#shielding","text":"","title":"Shielding"},{"location":"faq/#does-scone-ensure-the-security-of-incoming-and-outgoing-tcp-connections-tofrom-a-service-running-in-an-enclave","text":"Please use TLS in your service. If your service does not support TLS out of the box, use our TCP shield.","title":"Does SCONE ensure the security of incoming and outgoing TCP connections to/from a service running in an enclave?"},{"location":"faq/#how-do-i-encrypt-stdinstdoutstderr","text":"Send us an email on how to use the terminal shield","title":"How do I encrypt stdin/stdout/stderr?"},{"location":"faq/#how-do-i-encrypt-pipes","text":"Send us an email regarding the encryption of pipes.","title":"How do I encrypt pipes?"},{"location":"faq/#how-do-i-encrypt-tcp-connections","text":"Context: Services like memcached do not support TLS out of the box. Send us an email on how to use the TCP shield.","title":"How do I encrypt TCP connections?"},{"location":"faq/#how-do-i-transparently-encryptdecrypt-files","text":"Enable the file shield: see file shield example .","title":"How do I transparently encrypt/decrypt files?"},{"location":"faq/#are-all-files-in-a-container-encrypted","text":"No, only those in an encrypted file region: see file shield example .","title":"Are all files in a container encrypted?"},{"location":"faq/#what-do-i-need-to-do-to-protect-the-files-my-service-needs-eg-https-encryption-key-and-certificate","text":"You need to encrypt them when you build you container image: see file shield example . This will soon become a little easier to do.","title":"What do I need to do to protect the files my service needs, e.g. HTTPS encryption key and certificate"},{"location":"faq/#dynamic-libraries","text":"","title":"Dynamic Libraries"},{"location":"faq/#what-are-protected-dynamic-libraries","text":"The dynamic libraries loaded during start up of program are included in the hash of the enclave, i.e., any modification of any of these libraries will change MrEnclave . In that sense, the dynamic libraries are integrity protected since any modifications will result in a failed attestation. One can determine the dynamic libraries which are loaded during startup with command ldd for native binaries. Note that depending how the binary is build (static linking, dynamic linking, for Alpine Linux or for Ubuntu), ldd might only print the dynamic libraries used to start the enclave. To enable the loading of protected dynamic libraries, set environment variable SCONE_ALLOW_DLOPEN=1 . If you want to disallow this, undefine SCONE_ALLOW_DLOPEN . SCONE_ALLOW_DLOPEN=2 enables both protected as well as unprotected shared libraries.","title":"What are protected dynamic libraries?"},{"location":"faq/#what-are-unprotected-dynamic-libraries","text":"An unprotected shared library is loaded after the program has started by the application with a call to function dlopen (or similar). These libraries are not integrity protected by MrEnclave since they are loaded after MrEnclave was computed. To ensure the integrity of these shared library the have to be located in an authenticated or an encrypted file region. In future, we plan to enforce this property. To enable the loading of protected dynamic libraries after startup, set environment variable SCONE_ALLOW_DLOPEN=2 . This will also enable loading libraries during startup.","title":"What are unprotected dynamic libraries?"},{"location":"faq/#encrypted-code-and-libraries","text":"One can encrypt code by pushing your main code in a shared library that is stored in an encrypted file region. In this way, you can protect the integrity as well as the confidentiality of your code.","title":"Encrypted Code and Libraries"},{"location":"faq/#resource-usage","text":"","title":"Resource Usage"},{"location":"faq/#cpu-utilization-number-of-threads-is-higher-than-in-native-mode","text":"By default, we use multiple threads to serve inside the enclave and to serve system calls outside the enclave. If theses threads have no work to do, they go to sleep. You can reduce the number of threads / CPU utilization by specifying a SCONE configuration file which uses less threads. For example, you could use one thread inside the enclave and one outside the enclave with this configuration file: cat /etc/sgx-musl.conf EOF Q 1 e -1 0 0 s -1 0 0 EOF","title":"CPU utilization / number of threads is higher than in native mode"},{"location":"faq/#the-cpu-utilization-is-still-higher-than-in-native-mode","text":"In our experience, then newest version of SCONE needs less than 1-2% CPU utilization when a service is idling. If the CPU utilization of a service is higher than the native version during idle periods, you could try to tune the the backoff behavior of the queues by setting parameters L and P appropriately. Note that the standard values should in most cases do not need any tuning.","title":"The CPU utilization is still higher than in native mode"},{"location":"faq/#the-memory-usage-is-higher-than-in-native-mode","text":"The issue is that SGX v1 enclaves must allocate all memory at startup since enclaves are fixed (- this will change with SGXv2). You can reduce memory consumption by setting SCONE_HEAP and SCONE_STACK . In SGX v2 we will allocate memory on demand, i.e., we will be more memory efficient. Note that in SGX v2, we will be able to dynamically adjust the size of the heap and the stack sizes automatically, i.e., one does not need to allocate all memory in the beginning. This will also reduce the startup times.","title":"The memory usage is higher than in native mode"},{"location":"faq/#high-startup-times","text":"Since in SGX v1 one needs to allocate all memory at the start of an enclave, startup times can be very large. In SGX v2, we will be able to dynamically scale the size of an enclave during runtime. Since we will allocate less memory during startup time, this will reduce the startup times.","title":"High startup times"},{"location":"faq/#side-channel-protection","text":"The newest microcode of new Intel CPUs protects against L1TF side channels when hyperthreading is disabled. Please ensure that your CPU microcode is up-to-date: You can follow the following instruction to update the microcode of your CPU .","title":"Side-Channel Protection"},{"location":"faq/#unimplemented-functions","text":"","title":"Unimplemented Functions"},{"location":"faq/#some-function-returns-enosys","text":"Sometimes it is difficult to diagnose why a function fails. In most cases, the issue is that we do not yet support fork . Note that SCONE supports vfork . You can check which functions might not be supported by running your application with environment variable SCONE_LOG=7 . scontain.com , August 2018. Questions or Suggestions?","title":"Some function returns ENOSYS"},{"location":"firstcontainer/","text":"Running Your First SCONE program Hello World in Simulation Mode Let's start with a simple hello world program that we run inside a container on top of SCONE. We first need to start the SCONE crosscompiler. The crosscompiler container image is hosted in a private repository on Docker hub and can be started with the help of docker: docker run -it sconecuratedimages/crosscompilers A docker engine must be installed and you need access to sconecuratedimages/crosscompilers You need to install a docker engine . In some docker installations, you might have to replace \"docker\" by \"sudo docker\". Send us an email to get access to the image. Even in simulation mode, we require some CPU features. Please ensure that your CPU has the right features by executing: docker run --rm sconecuratedimages/apps:check_cpuid This should output (amongst other messages): CPU has all features to run SCONE in Simulation Mode. Now execute the following command inside the container to create the hello world program: cat helloworld . c EOF #include stdio.h int main () { printf ( Hello World \\n ); } EOF Compile the program with the SCONE crosscompiler (i.e., gcc): gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we did not give the container access to SGX, the program runs in simulation mode , i.e., the SCONE software runs but we do not use Intel SGX enclaves. Use simulation mode only for development and debugging This mode must not be used for production since programs do not run inside of enclaves . Simulation mode will run on modern Intel CPUs - even those without Intel SGX. It might, however, fail on old CPUs without AES hardware support. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 501194b1da9d4e86828353349cc7f9ef310b0dd1 Enclave hash: a01127f2190ed5ecd21f9fd432e4d07f7f250ad1e1808d9c0305e75505383c44 Hello World The output shows that SCONE is running in simulation mode : export SCONE_MODE=sim Background Info The most convenient way to use SCONE for development is to enable automatic (a.k.a. AUTO ) mode 1 . In AUTO mode, you neither need access to SGX-capable CPUs nor do you need to install any new software on your host: you only need to have access to a Docker engine. If you have access to an SGX-capable CPU and you give the container access to the sgx device, SCONE will run applications inside of SGX enclaves. Otherwise, the applications will run in simulation mode. Let's see in the next chapter how to run the hello world program inside an Intel SGX enclave. scontain.com , August 2018. Questions or Suggestions? This is the default mode: see description of environment variable SCONE_MODE . Just send an email with your free Docker ID to info@scontain.com .","title":"Simulation Mode"},{"location":"firstcontainer/#running-your-first-scone-program","text":"","title":"Running Your First SCONE program"},{"location":"firstcontainer/#hello-world-in-simulation-mode","text":"Let's start with a simple hello world program that we run inside a container on top of SCONE. We first need to start the SCONE crosscompiler. The crosscompiler container image is hosted in a private repository on Docker hub and can be started with the help of docker: docker run -it sconecuratedimages/crosscompilers A docker engine must be installed and you need access to sconecuratedimages/crosscompilers You need to install a docker engine . In some docker installations, you might have to replace \"docker\" by \"sudo docker\". Send us an email to get access to the image. Even in simulation mode, we require some CPU features. Please ensure that your CPU has the right features by executing: docker run --rm sconecuratedimages/apps:check_cpuid This should output (amongst other messages): CPU has all features to run SCONE in Simulation Mode. Now execute the following command inside the container to create the hello world program: cat helloworld . c EOF #include stdio.h int main () { printf ( Hello World \\n ); } EOF Compile the program with the SCONE crosscompiler (i.e., gcc): gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we did not give the container access to SGX, the program runs in simulation mode , i.e., the SCONE software runs but we do not use Intel SGX enclaves. Use simulation mode only for development and debugging This mode must not be used for production since programs do not run inside of enclaves . Simulation mode will run on modern Intel CPUs - even those without Intel SGX. It might, however, fail on old CPUs without AES hardware support. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 501194b1da9d4e86828353349cc7f9ef310b0dd1 Enclave hash: a01127f2190ed5ecd21f9fd432e4d07f7f250ad1e1808d9c0305e75505383c44 Hello World The output shows that SCONE is running in simulation mode : export SCONE_MODE=sim","title":"Hello World in Simulation Mode"},{"location":"firstcontainer/#background-info","text":"The most convenient way to use SCONE for development is to enable automatic (a.k.a. AUTO ) mode 1 . In AUTO mode, you neither need access to SGX-capable CPUs nor do you need to install any new software on your host: you only need to have access to a Docker engine. If you have access to an SGX-capable CPU and you give the container access to the sgx device, SCONE will run applications inside of SGX enclaves. Otherwise, the applications will run in simulation mode. Let's see in the next chapter how to run the hello world program inside an Intel SGX enclave. scontain.com , August 2018. Questions or Suggestions? This is the default mode: see description of environment variable SCONE_MODE . Just send an email with your free Docker ID to info@scontain.com .","title":"Background Info"},{"location":"glossary/","text":"Glossary attestation Process of proving the integrity and authenticity of the attestee's software or hardware component to a verifier. This includes validating if a given software is executed on a given hardware. local attestation Attestation executed locally, e.g., one software component validates the integrity and authenticity of another software component which is executed on the same hardware. In Intel SGX, the CPU creates a report containing integrity information of the attested enclave whose keyed-MAC can only be verified, and changed for that matter, by the verifier enclave running on the same platform. remote attestation Attestation executed remotely, i.e., the component which does the validation and the component which is validated are executed on different machines. In Intel SGX, the report received during local attestation is signed with the quoting enclave's private key making the integrity of the quote - the signed report - remotely verifiable. Configuration and Attestation Service (CAS) The Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, SCONE-enabled executables, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client. Local Attestation Service (LAS) A per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave. booting secure boot A boot procedure which allows only the execution of firmware, bootloaders and operating systems which are digitally signed by a (well) defined set of acceptable signers. measured boot A boot procedure which measures the state of the system at each boot step. This measurement can be accessed to verify the current state of a given system. Compared to secure boot, measured boot will not prevent an \"insecure\" state of the system. cloud-native application An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers. cloud provider An entity providing cloud services (PaaS, IaaS, MaaS etc.) to its customers. It is assumed that a cloud provider is in physical or logical control of the hardware and system software used to provide the cloud services. container An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions. curated image A container image of a popular service maintained by scontain.com. enclave . This is an alias for SGX enclave . Docker image A snapshot of a container's state that can be used to initialize new containers with the same state. Docker registry A Docker registry stores Docker images for the purpose of easy distribution comparable to the app stores of Android or iOS. EPC . A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads. microservice A rather small component which offers a single service. service provider A company operating an application - typically, making these available via the Internet. We use this a general term that includes different models like Software as a Service (SaaS) providers as well as Hosted service providers etc. SCONE A software framework, jointly developed by the SERECA and SecureCloud EU projects, allowing the trustworthy execution of unmodified x86 source code within Intel SGX enclaves. It consists of components enabling the execution inside enclaves such as the SCONE runtime and the C, C++, Fortran, Go, and Rust SCONE cross-compilers, components ensuring the trustworthiness of this execution and deployment in clouds such as the CAS and SCONE client. SCONE Docker image A SCONE Docker image is a Docker image that contains an SCONE-enabled executable and is additionally annotated via image labels with metadata allowing the attestation of the started SCONE-enabled executable and the image's file system content. SCONE container A SCONE container is a running instance of a SCONE Docker image. SCONE microservice Microservice which is a SCONE-enabled executable. SCONE-enabled executable An executable created by a SCONE cross-compiler. The actual program will be executed within an enclave and utilises the SCONE runtime. SCONE runtime The runtime environment necessary to execute a SCONE-enabled executable. At the moment this consists of a modified C-library based on the musl library. SCONE cross-compiler Compilers for various programming languages such as C, C++, Rust, Go, and Fortran which compile source code into a SCONE-enabled executable. SCONE client A program that is used to configure SCONE-enabled executables. It allows the user to push confidential configurations to the CAS and encrypt files to ensure their content is only accessible by specific SCONE-enabled executables executed inside enclaves. SCONE infrastructure The SCONE infrastructure summarises all components necessary to deploy and run a SCONE-enabled executable. This includes Docker components like the Docker daemon and the Docker registry as well as the SCONE client and additional services like CAS and LAS. Secure container . A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE. Secure program . A program that executes inside an enclave. SGX (Software Guard eXtension) . A CPU extension by Intel that permits to create SGX enclaves. SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC. threading . SCONE uses different kind of threads: ethread : a thread that executes application threads inside of an enclave lthread : an application thread. Typically, created by the application directly or indirectly via a pthread_create call. In SCONE, this pthread_create call will create a lthread . The lthread is executed by some ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly. sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave trusted computing base (TCB) The set of hardware and software components which can break the security policy. Therefore one has to trust that these components are not malicious or faulty. trusted execution environment (TEE) A piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity. scontain.com and SecureCloud , March 2018. Questions or Suggestions?","title":"Glossary"},{"location":"glossary/#glossary","text":"attestation Process of proving the integrity and authenticity of the attestee's software or hardware component to a verifier. This includes validating if a given software is executed on a given hardware. local attestation Attestation executed locally, e.g., one software component validates the integrity and authenticity of another software component which is executed on the same hardware. In Intel SGX, the CPU creates a report containing integrity information of the attested enclave whose keyed-MAC can only be verified, and changed for that matter, by the verifier enclave running on the same platform. remote attestation Attestation executed remotely, i.e., the component which does the validation and the component which is validated are executed on different machines. In Intel SGX, the report received during local attestation is signed with the quoting enclave's private key making the integrity of the quote - the signed report - remotely verifiable. Configuration and Attestation Service (CAS) The Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, SCONE-enabled executables, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client. Local Attestation Service (LAS) A per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave. booting secure boot A boot procedure which allows only the execution of firmware, bootloaders and operating systems which are digitally signed by a (well) defined set of acceptable signers. measured boot A boot procedure which measures the state of the system at each boot step. This measurement can be accessed to verify the current state of a given system. Compared to secure boot, measured boot will not prevent an \"insecure\" state of the system. cloud-native application An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers. cloud provider An entity providing cloud services (PaaS, IaaS, MaaS etc.) to its customers. It is assumed that a cloud provider is in physical or logical control of the hardware and system software used to provide the cloud services. container An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions. curated image A container image of a popular service maintained by scontain.com. enclave . This is an alias for SGX enclave . Docker image A snapshot of a container's state that can be used to initialize new containers with the same state. Docker registry A Docker registry stores Docker images for the purpose of easy distribution comparable to the app stores of Android or iOS. EPC . A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads. microservice A rather small component which offers a single service. service provider A company operating an application - typically, making these available via the Internet. We use this a general term that includes different models like Software as a Service (SaaS) providers as well as Hosted service providers etc. SCONE A software framework, jointly developed by the SERECA and SecureCloud EU projects, allowing the trustworthy execution of unmodified x86 source code within Intel SGX enclaves. It consists of components enabling the execution inside enclaves such as the SCONE runtime and the C, C++, Fortran, Go, and Rust SCONE cross-compilers, components ensuring the trustworthiness of this execution and deployment in clouds such as the CAS and SCONE client. SCONE Docker image A SCONE Docker image is a Docker image that contains an SCONE-enabled executable and is additionally annotated via image labels with metadata allowing the attestation of the started SCONE-enabled executable and the image's file system content. SCONE container A SCONE container is a running instance of a SCONE Docker image. SCONE microservice Microservice which is a SCONE-enabled executable. SCONE-enabled executable An executable created by a SCONE cross-compiler. The actual program will be executed within an enclave and utilises the SCONE runtime. SCONE runtime The runtime environment necessary to execute a SCONE-enabled executable. At the moment this consists of a modified C-library based on the musl library. SCONE cross-compiler Compilers for various programming languages such as C, C++, Rust, Go, and Fortran which compile source code into a SCONE-enabled executable. SCONE client A program that is used to configure SCONE-enabled executables. It allows the user to push confidential configurations to the CAS and encrypt files to ensure their content is only accessible by specific SCONE-enabled executables executed inside enclaves. SCONE infrastructure The SCONE infrastructure summarises all components necessary to deploy and run a SCONE-enabled executable. This includes Docker components like the Docker daemon and the Docker registry as well as the SCONE client and additional services like CAS and LAS. Secure container . A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE. Secure program . A program that executes inside an enclave. SGX (Software Guard eXtension) . A CPU extension by Intel that permits to create SGX enclaves. SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC. threading . SCONE uses different kind of threads: ethread : a thread that executes application threads inside of an enclave lthread : an application thread. Typically, created by the application directly or indirectly via a pthread_create call. In SCONE, this pthread_create call will create a lthread . The lthread is executed by some ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly. sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave trusted computing base (TCB) The set of hardware and software components which can break the security policy. Therefore one has to trust that these components are not malicious or faulty. trusted execution environment (TEE) A piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity. scontain.com and SecureCloud , March 2018. Questions or Suggestions?","title":"Glossary"},{"location":"groupcacheUseCase/","text":"groupcache Example groupcache is a memcached-like library written in GO: it implements a peer-to-peer caching service. We use groupcache to show how to build a little more complex GO program with SCONE. Typically, one would build a container image for groupcache with a Dockerfile . Since our emphasis is to explain how to build such programs, we first show the individual steps to compile and execute groupcache and second, please read how to build container images with a multi-stage build . Note You might want to read how to compile GO programs with SCONE first. Building groupcache - without shielding First, start a crosscompiler container. Give it access to the sgx device if your host has this device - otherwise, just drop argument --device=/dev/isgx : docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Second, install the go command to simplify the building of groupcache : apk update apk add go git curl Now you can build the groupcache library as follows: go get -compiler gccgo -u github.com/golang/groupcache Note that flag -compiler gccgo is required to ensure that the scone-gccgo is used to compile groupcache . That's it! Example OK, we should show how to use group cache. We show this for a simple application from fiorix : cat groupcache.go EOF // Simple groupcache example: https://github.com/golang/groupcache // Running 3 instances: // go run groupcache.go -addr=:8080 -pool=http://127.0.0.1:8080,http://127.0.0.1:8081,http://127.0.0.1:8082 // go run groupcache.go -addr=:8081 -pool=http://127.0.0.1:8081,http://127.0.0.1:8080,http://127.0.0.1:8082 // go run groupcache.go -addr=:8082 -pool=http://127.0.0.1:8082,http://127.0.0.1:8080,http://127.0.0.1:8081 // Testing: // curl localhost:8080/color?name=red package main import ( errors flag log net/http strings github.com/golang/groupcache ) var Store = map[string][]byte{ red : []byte( #FF0000 ), green : []byte( #00FF00 ), blue : []byte( #0000FF ), } var Group = groupcache.NewGroup( foobar , 64 20, groupcache.GetterFunc( func(ctx groupcache.Context, key string, dest groupcache.Sink) error { log.Println( looking up , key) v, ok := Store[key] if !ok { return errors.New( color not found ) } dest.SetBytes(v) return nil }, )) func main() { addr := flag.String( addr , :8080 , server address ) peers := flag.String( pool , http://localhost:8080 , server pool list ) flag.Parse() http.HandleFunc( /color , func(w http.ResponseWriter, r *http.Request) { color := r.FormValue( name ) var b []byte err := Group.Get(nil, color, groupcache.AllocatingByteSliceSink( b)) if err != nil { http.Error(w, err.Error(), http.StatusNotFound) return } w.Write(b) w.Write([]byte{ \\n }) }) p := strings.Split(*peers, , ) pool := groupcache.NewHTTPPool(p[0]) pool.Set(p...) http.ListenAndServe(*addr, nil) } EOF Let's compile this with scone-gccgo : export SCONE_HEAP = 1G go build -compiler gccgo -buildmode = exe -gccgoflags -g groupcache.go Run groupcache in the background: ./groupcache -addr = :8080 -pool = http://127.0.0.1:8080 And let's query groupcache: curl localhost:8080/color?name = green #00FF00 curl localhost:8080/color?name = red #FF0000 Shielding While the above code runs inside of an enclave, there are multiple security issues if that code would run in an untrusted environment: The peers communicate via http instead of https. This means that executing in an enclave does not improve the security since an attacker can just look into the network traffic of groupcache. The use of https instead of http would require a certificate and a private key. The arguments (i.e., -addr and -pool) are passed via command line, i.e., we can neither trust the integrity nor the confidentiality of these arguments. This groupcache service logs error messages on stderr. How can we be sure if the code runs indeed in an enclave? After all, SCONE supports simulation mode. Alternative: Manual Modifications One could manually modify the program to use https instead of http , one could encrypt the output with AES. However, this would require that we have to change groupcache not only to encrypt all output but also to manage the key for encrypting the output. The private key of the certificate is typically stored in an unencrypted file and protected via the access control of the file system. Since we do not trust the operating system, we would need to encrypt the private key in the file system. We would need to attest the groupcache and after successful attestation pass the encryption keys and the arguments to groupcache via a secure channel. Alternative: SCONE shielding Many programs would require such or similar changes as groupcache . Hence, SCONE provides a way to shield programs without the need to modify these programs. This main advantages of that approach is that one does not have to put in the engineering to modify the code - which is difficult and bug-prone one can easily keep up with upstream code changes without the need to continuously patch the upstream code one does not risk a lock-in by having SGX-specific or SCONE-specific code modifications We will show in later sections how we can shield this application with the help of SCONE such that no source code changes are necessary, and we only need to define a description what shields should be activated In this way, we can address all of the above issues that we described. scontain.com , July 2018. Questions or Suggestions?","title":"GO example"},{"location":"groupcacheUseCase/#groupcache-example","text":"groupcache is a memcached-like library written in GO: it implements a peer-to-peer caching service. We use groupcache to show how to build a little more complex GO program with SCONE. Typically, one would build a container image for groupcache with a Dockerfile . Since our emphasis is to explain how to build such programs, we first show the individual steps to compile and execute groupcache and second, please read how to build container images with a multi-stage build . Note You might want to read how to compile GO programs with SCONE first.","title":"groupcache Example"},{"location":"groupcacheUseCase/#building-groupcache-without-shielding","text":"First, start a crosscompiler container. Give it access to the sgx device if your host has this device - otherwise, just drop argument --device=/dev/isgx : docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Second, install the go command to simplify the building of groupcache : apk update apk add go git curl Now you can build the groupcache library as follows: go get -compiler gccgo -u github.com/golang/groupcache Note that flag -compiler gccgo is required to ensure that the scone-gccgo is used to compile groupcache . That's it!","title":"Building groupcache - without shielding"},{"location":"groupcacheUseCase/#example","text":"OK, we should show how to use group cache. We show this for a simple application from fiorix : cat groupcache.go EOF // Simple groupcache example: https://github.com/golang/groupcache // Running 3 instances: // go run groupcache.go -addr=:8080 -pool=http://127.0.0.1:8080,http://127.0.0.1:8081,http://127.0.0.1:8082 // go run groupcache.go -addr=:8081 -pool=http://127.0.0.1:8081,http://127.0.0.1:8080,http://127.0.0.1:8082 // go run groupcache.go -addr=:8082 -pool=http://127.0.0.1:8082,http://127.0.0.1:8080,http://127.0.0.1:8081 // Testing: // curl localhost:8080/color?name=red package main import ( errors flag log net/http strings github.com/golang/groupcache ) var Store = map[string][]byte{ red : []byte( #FF0000 ), green : []byte( #00FF00 ), blue : []byte( #0000FF ), } var Group = groupcache.NewGroup( foobar , 64 20, groupcache.GetterFunc( func(ctx groupcache.Context, key string, dest groupcache.Sink) error { log.Println( looking up , key) v, ok := Store[key] if !ok { return errors.New( color not found ) } dest.SetBytes(v) return nil }, )) func main() { addr := flag.String( addr , :8080 , server address ) peers := flag.String( pool , http://localhost:8080 , server pool list ) flag.Parse() http.HandleFunc( /color , func(w http.ResponseWriter, r *http.Request) { color := r.FormValue( name ) var b []byte err := Group.Get(nil, color, groupcache.AllocatingByteSliceSink( b)) if err != nil { http.Error(w, err.Error(), http.StatusNotFound) return } w.Write(b) w.Write([]byte{ \\n }) }) p := strings.Split(*peers, , ) pool := groupcache.NewHTTPPool(p[0]) pool.Set(p...) http.ListenAndServe(*addr, nil) } EOF Let's compile this with scone-gccgo : export SCONE_HEAP = 1G go build -compiler gccgo -buildmode = exe -gccgoflags -g groupcache.go Run groupcache in the background: ./groupcache -addr = :8080 -pool = http://127.0.0.1:8080 And let's query groupcache: curl localhost:8080/color?name = green #00FF00 curl localhost:8080/color?name = red #FF0000","title":"Example"},{"location":"groupcacheUseCase/#shielding","text":"While the above code runs inside of an enclave, there are multiple security issues if that code would run in an untrusted environment: The peers communicate via http instead of https. This means that executing in an enclave does not improve the security since an attacker can just look into the network traffic of groupcache. The use of https instead of http would require a certificate and a private key. The arguments (i.e., -addr and -pool) are passed via command line, i.e., we can neither trust the integrity nor the confidentiality of these arguments. This groupcache service logs error messages on stderr. How can we be sure if the code runs indeed in an enclave? After all, SCONE supports simulation mode.","title":"Shielding"},{"location":"groupcacheUseCase/#alternative-manual-modifications","text":"One could manually modify the program to use https instead of http , one could encrypt the output with AES. However, this would require that we have to change groupcache not only to encrypt all output but also to manage the key for encrypting the output. The private key of the certificate is typically stored in an unencrypted file and protected via the access control of the file system. Since we do not trust the operating system, we would need to encrypt the private key in the file system. We would need to attest the groupcache and after successful attestation pass the encryption keys and the arguments to groupcache via a secure channel.","title":"Alternative: Manual Modifications"},{"location":"groupcacheUseCase/#alternative-scone-shielding","text":"Many programs would require such or similar changes as groupcache . Hence, SCONE provides a way to shield programs without the need to modify these programs. This main advantages of that approach is that one does not have to put in the engineering to modify the code - which is difficult and bug-prone one can easily keep up with upstream code changes without the need to continuously patch the upstream code one does not risk a lock-in by having SGX-specific or SCONE-specific code modifications We will show in later sections how we can shield this application with the help of SCONE such that no source code changes are necessary, and we only need to define a description what shields should be activated In this way, we can address all of the above issues that we described. scontain.com , July 2018. Questions or Suggestions?","title":"Alternative: SCONE shielding"},{"location":"hardwaremode/","text":"Running \"Hello World\" inside of an enclave So far , we showed how to run a hello world program using simulation mode . Let's show how to run this program in hardware mode , i.e., the hello world program runs inside an Intel SGX enclave. Actually, the only change is to give the container access to the SGX device via option --device=/dev/isgx . Detailed Description We first need to start a container which includes the SCONE crosscompiler and give the container access to the Intel SGX driver: docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers The docker engine and the Intel SGX driver must be installed. Read about how to install a docker engine and to install the Intel SGX driver . In some installations, you might have to replace \"docker\" by \"sudo docker\". To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Hardware mode is only supported in Linux, since the Intel SGX driver is only available on Linux. Now execute the following command inside the container to create the hello world program: cat helloworld . c EOF #include stdio.h int main () { printf ( Hello World \\n ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we have given the container access to the SGX driver, this runs in hardware mode . Use this mode only for development and debugging The program runs inside of a hardware enclave . However, the enclave is in debug mode, i.e., one can actually introspect the content of the enclave. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 9d601c360ce9b6100e35dc42ec2800c1c20478328a0d4450d8d5163c00289dea Hello World The output shows that SCONE is running in hardware mode : export SCONE_MODE=hw Background Info For ease of use, we create all Docker images such that applications run inside of enclaves if enclaves are available ( AUTO mode ). If SGX is not available, they run in simulation mode, i.e., outside of an enclave but all SCONE software is running. To disable simulation mode, you can set environment variable SCONE_MODE=HW : docker run --device=/dev/isgx -e SCONE_MODE=HW -it sconecuratedimages/crosscompilers If you would start your container in hardware mode but forget to give it access to the sgx device, i.e., docker run -e SCONE_MODE=HW -it sconecuratedimages/crosscompilers compilation of the helloworld will succeed but running the helloworld program will fail: bash-4.4# echo $SCONE_MODE HW bash-4.4# ls -l /dev/isgx ls: /dev/isgx: No such file or directory bash-4.4# ./helloworld [SCONE|ERROR] ./tools/starter-exec.c:993:_dl_main(): Could not create enclave: Error opening SGX device When running your software in operations, you would force the programs to run inside of enclaves: this can be enforced with the help of SCONE configuration and attestation service . Environment Variables To simplify the development with SCONE, you can control the behavior of SCONE with a set of environment variables, i.e., variables defined by your shell. Section SCONE Environment Variables describes these in details. scontain.com , August 2018. Questions or Suggestions?","title":"Hardware Mode"},{"location":"hardwaremode/#running-hello-world-inside-of-an-enclave","text":"So far , we showed how to run a hello world program using simulation mode . Let's show how to run this program in hardware mode , i.e., the hello world program runs inside an Intel SGX enclave. Actually, the only change is to give the container access to the SGX device via option --device=/dev/isgx .","title":"Running \"Hello World\" inside of  an enclave"},{"location":"hardwaremode/#detailed-description","text":"We first need to start a container which includes the SCONE crosscompiler and give the container access to the Intel SGX driver: docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers The docker engine and the Intel SGX driver must be installed. Read about how to install a docker engine and to install the Intel SGX driver . In some installations, you might have to replace \"docker\" by \"sudo docker\". To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Hardware mode is only supported in Linux, since the Intel SGX driver is only available on Linux. Now execute the following command inside the container to create the hello world program: cat helloworld . c EOF #include stdio.h int main () { printf ( Hello World \\n ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we have given the container access to the SGX driver, this runs in hardware mode . Use this mode only for development and debugging The program runs inside of a hardware enclave . However, the enclave is in debug mode, i.e., one can actually introspect the content of the enclave. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 9d601c360ce9b6100e35dc42ec2800c1c20478328a0d4450d8d5163c00289dea Hello World The output shows that SCONE is running in hardware mode : export SCONE_MODE=hw","title":"Detailed Description"},{"location":"hardwaremode/#background-info","text":"For ease of use, we create all Docker images such that applications run inside of enclaves if enclaves are available ( AUTO mode ). If SGX is not available, they run in simulation mode, i.e., outside of an enclave but all SCONE software is running. To disable simulation mode, you can set environment variable SCONE_MODE=HW : docker run --device=/dev/isgx -e SCONE_MODE=HW -it sconecuratedimages/crosscompilers If you would start your container in hardware mode but forget to give it access to the sgx device, i.e., docker run -e SCONE_MODE=HW -it sconecuratedimages/crosscompilers compilation of the helloworld will succeed but running the helloworld program will fail: bash-4.4# echo $SCONE_MODE HW bash-4.4# ls -l /dev/isgx ls: /dev/isgx: No such file or directory bash-4.4# ./helloworld [SCONE|ERROR] ./tools/starter-exec.c:993:_dl_main(): Could not create enclave: Error opening SGX device When running your software in operations, you would force the programs to run inside of enclaves: this can be enforced with the help of SCONE configuration and attestation service . Environment Variables To simplify the development with SCONE, you can control the behavior of SCONE with a set of environment variables, i.e., variables defined by your shell. Section SCONE Environment Variables describes these in details. scontain.com , August 2018. Questions or Suggestions?","title":"Background Info"},{"location":"hostexample/","text":"Host Execution We now show how one can compile a simple hello world program in a container and how to execute the program in the container in simulation mode and on the host in hardware mode . Installation In this example, we assume that you run on a host and have installed the Intel SGX driver and a docker engine . Driver installation is not strictly necessary: without the driver, the program will be automatically be executed in simulation mode on the host. Detailed Description We first need to start a container which includes the SCONE crosscompiler which is based on Ubuntu 1 : docker run -it -v $PWD :/src sconecuratedimages/crosscompilers:ubuntu We map the local directory of the host into the container (via option -v ) to be able to executed the generated binary on the host. Now execute the following command inside the container to create the hello world program: cd / src cat helloworld . c EOF #include stdio.h int main () { printf ( Hello World \\n ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program with some debug output in the container: SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World The output shows that SCONE is running in simulation mode : \" export SCONE_MODE=sim \" Note Our patched docker engine automatically maps the sgx device inside of containers. In this case, the program would actually be executed in hardware mode. Execution on host Now, exit the container by executing: exit On the host, you can now execute the generated helloworld program 2 . First, we need to ensure that there exists a SCONE configuration file . We store this in the local directory: cat sgx-musl.conf EOF Q 1 e -1 0 0 s -1 0 0 EOF Now, we start the program with debug messages and using the configuration file in the local directory: SCONE_CONFIG = $PWD /sgx-musl.conf SCONE_VERSION = 1 ./helloworld In case your host has the Intel sgx driver installed, the output will show that it is executed in hardware mode on the host: ... export SCONE_MODE=hw ... Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World If you do not have the sgx driver installed, the program runs in simulation mode. scontain.com , August 2018. Questions or Suggestions? The Ubuntu crosscompiler creates static binaries that can run on a Linux host. The host must run Linux.","title":"Host Execution"},{"location":"hostexample/#host-execution","text":"We now show how one can compile a simple hello world program in a container and how to execute the program in the container in simulation mode and on the host in hardware mode .","title":"Host Execution"},{"location":"hostexample/#installation","text":"In this example, we assume that you run on a host and have installed the Intel SGX driver and a docker engine . Driver installation is not strictly necessary: without the driver, the program will be automatically be executed in simulation mode on the host.","title":"Installation"},{"location":"hostexample/#detailed-description","text":"We first need to start a container which includes the SCONE crosscompiler which is based on Ubuntu 1 : docker run -it -v $PWD :/src sconecuratedimages/crosscompilers:ubuntu We map the local directory of the host into the container (via option -v ) to be able to executed the generated binary on the host. Now execute the following command inside the container to create the hello world program: cd / src cat helloworld . c EOF #include stdio.h int main () { printf ( Hello World \\n ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program with some debug output in the container: SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World The output shows that SCONE is running in simulation mode : \" export SCONE_MODE=sim \" Note Our patched docker engine automatically maps the sgx device inside of containers. In this case, the program would actually be executed in hardware mode.","title":"Detailed Description"},{"location":"hostexample/#execution-on-host","text":"Now, exit the container by executing: exit On the host, you can now execute the generated helloworld program 2 . First, we need to ensure that there exists a SCONE configuration file . We store this in the local directory: cat sgx-musl.conf EOF Q 1 e -1 0 0 s -1 0 0 EOF Now, we start the program with debug messages and using the configuration file in the local directory: SCONE_CONFIG = $PWD /sgx-musl.conf SCONE_VERSION = 1 ./helloworld In case your host has the Intel sgx driver installed, the output will show that it is executed in hardware mode on the host: ... export SCONE_MODE=hw ... Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World If you do not have the sgx driver installed, the program runs in simulation mode. scontain.com , August 2018. Questions or Suggestions? The Ubuntu crosscompiler creates static binaries that can run on a Linux host. The host must run Linux.","title":"Execution on host"},{"location":"installation/","text":"SCONE Installation Depending on how you want to use SCONE, you need to install some additional software: Simulation mode: if you want to run SCONE inside a container in simulation mode, you just need to install the docker engine on your host. Hardware mode: If you want to run your applications in hardware mode inside of containers, you need to install the docker engine and install the Intel SGX driver on your host. Running on a host: Running you applications on a host, you need to install the Intel SGX driver on the host. Running on a VM: Running you applications on a virtual machine, you need to install the Intel SGX driver on the VM and you have to ensure that your hypervisor supports SGX: You might want to install a patched version of KVM . Running in Docker Swarm: Since Docker Swarm does not permit to map devices to containers in the swarm, you need to install a patched docker engine . Ensure that your CPU runs the newest microcode by updating the CPU microcode: . scontain.com , July 2018. Questions or Suggestions?","title":"Installation overview"},{"location":"installation/#scone-installation","text":"Depending on how you want to use SCONE, you need to install some additional software: Simulation mode: if you want to run SCONE inside a container in simulation mode, you just need to install the docker engine on your host. Hardware mode: If you want to run your applications in hardware mode inside of containers, you need to install the docker engine and install the Intel SGX driver on your host. Running on a host: Running you applications on a host, you need to install the Intel SGX driver on the host. Running on a VM: Running you applications on a virtual machine, you need to install the Intel SGX driver on the VM and you have to ensure that your hypervisor supports SGX: You might want to install a patched version of KVM . Running in Docker Swarm: Since Docker Swarm does not permit to map devices to containers in the swarm, you need to install a patched docker engine . Ensure that your CPU runs the newest microcode by updating the CPU microcode: . scontain.com , July 2018. Questions or Suggestions?","title":"SCONE Installation"},{"location":"memory_dump/","text":"Finding Secrets... One of the main advantages of SGX technology is that it cannot only protect against external attackers but also against internal attackers that have root access. We introduce a simple approach to demonstrate that your application is indeed running inside of an enclave and that its secrets are not accessible. Note To ensure that your SCONE application runs inside of an enclave and to distribute secrets, please to use the SCONE configuration and attestation service . To demonstrate how one can check that SGX/SCONE can protect against users with root access 1 , let's consider a simplistic example : we store some secret in a variable secret 2 . By default, binaries are not encrypted. Hence, do not store any secrets in a binary as it is done in this simplistic example. If you really need to to protect your binaries - for example, since you have some legacy code with embedded secrets - SCONE can protect these secrets by permitting to encrypt shared libraries. Example We use the SCONE crosscompiler image and start an container with access to the SGX device: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Let's create some program that stores a secret ( MYBIGS ) in a local variable secret : cat mysecret.c EOF #include stdio.h #include unistd.h const char *code_is_not_encrypted= THIS_IS_NOT_SECRET ; int main() { char secret[7]; secret[0] = M ; secret[1] = Y ; secret[2] = B ; secret[3] = I ; secret[4] = G ; secret[5] = S ; secret[6] =0; printf( %s SECRET at %lx\\n , secret, secret); printf( Kill with Ctrl-C.\\n ); for(;;) sleep(1); // loop forever } EOF Compile this program with the SCONE crosscompiler (i.e., gcc): gcc -g -o mysecret mysecret.c Simulation Mode You can run this program in SIMULATION MODE , i.e., this program does not protect your secrets: SCONE_VERSION = 1 SCONE_MODE = SIM SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Log into a different terminal on your host . Let us figure out the process ID of the mysecret program: SPID = $( ps -a | grep -v grep | grep mysecret | awk {print $1} ) Now, we can dump the memory of this process via the /proc filesystem. You can determine the different memory regions of your process via cat /proc/$SPID/maps and the memory is stored in /proc/$SPID/mem . We can use the following Python program to write all pages to stdout : cat dumpstack.py EOF import sys, os, string, re pid = sys.argv[1] maps_file = open( /proc/%s/maps % pid, r ) mem_file = open( /proc/%s/mem % pid, r ) r=0 for line in maps_file.readlines(): # for each mapped region w=line.rsplit(None, 1)[-1] # last word if w != /dev/isgx and w != [vvar] and w != [vdso] and w != [vsyscall] : m = re.match(r ([0-9A-Fa-f]+)-([0-9A-Fa-f]+) ([-r]) , line) r += 1 p = 0 if m.group(3) == r : # if this is a readable region start = int(m.group(1), 16) end = int(m.group(2), 16) while start end: try: mem_file.seek(start) # seek to region start chunk = mem_file.read(4096) # read region contents sys.stdout.write(chunk) p += 1 if p 1000: sys.stderr.write( region = %02d, index=%x \\r % (r,start)) p = 0 start += 4096 except: pass sys.stderr.write( \\n ) EOF Run this program and grep for the prefix of our secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will take some time but eventually it will print the full secret MYBIGS . Hardware Mode Let us now run the program in hardware mode. First, ensure that you kill the original program by typing control-C . Let's start mysecret in an enclave, i.e., in hardware mode inside the container: SCONE_VERSION = 1 SCONE_MODE = HW SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Update environment variable SPID in a second terminal on your host: SPID = $( ps -a | grep -v grep | grep mysecret | awk {print $1} ) and then try to find the secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will run for much less time and in particular, it will not print any secrets. Note, however, that secrets stored in the binaries can be found because the binary is not encrypted: a copy of the original binary - which is used to start the enclave - stays in main memory outside the enclave. Let's look for the string THIS_IS_NOT_SECRET in our example application. We can find this secret as follows: sudo python dumpstack.py $SPID | strings -n 5 | grep THIS_IS_NOT_SECRET scontain.com , July 2018. Questions or Suggestions? Note that the enclave runs in this example runs in debug mode, i.e., one can still attach to this enclave with scone-gdb . To prevent access via the debugger, you need to run your enclave in production mode . Note that an adversary could analyse the binary and figure out the secret. The standard way to provide an enclave with secrets is to use SCONE CAS .","title":"Finding Secrets"},{"location":"memory_dump/#finding-secrets","text":"One of the main advantages of SGX technology is that it cannot only protect against external attackers but also against internal attackers that have root access. We introduce a simple approach to demonstrate that your application is indeed running inside of an enclave and that its secrets are not accessible. Note To ensure that your SCONE application runs inside of an enclave and to distribute secrets, please to use the SCONE configuration and attestation service . To demonstrate how one can check that SGX/SCONE can protect against users with root access 1 , let's consider a simplistic example : we store some secret in a variable secret 2 . By default, binaries are not encrypted. Hence, do not store any secrets in a binary as it is done in this simplistic example. If you really need to to protect your binaries - for example, since you have some legacy code with embedded secrets - SCONE can protect these secrets by permitting to encrypt shared libraries.","title":"Finding Secrets..."},{"location":"memory_dump/#example","text":"We use the SCONE crosscompiler image and start an container with access to the SGX device: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Let's create some program that stores a secret ( MYBIGS ) in a local variable secret : cat mysecret.c EOF #include stdio.h #include unistd.h const char *code_is_not_encrypted= THIS_IS_NOT_SECRET ; int main() { char secret[7]; secret[0] = M ; secret[1] = Y ; secret[2] = B ; secret[3] = I ; secret[4] = G ; secret[5] = S ; secret[6] =0; printf( %s SECRET at %lx\\n , secret, secret); printf( Kill with Ctrl-C.\\n ); for(;;) sleep(1); // loop forever } EOF Compile this program with the SCONE crosscompiler (i.e., gcc): gcc -g -o mysecret mysecret.c","title":"Example"},{"location":"memory_dump/#simulation-mode","text":"You can run this program in SIMULATION MODE , i.e., this program does not protect your secrets: SCONE_VERSION = 1 SCONE_MODE = SIM SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Log into a different terminal on your host . Let us figure out the process ID of the mysecret program: SPID = $( ps -a | grep -v grep | grep mysecret | awk {print $1} ) Now, we can dump the memory of this process via the /proc filesystem. You can determine the different memory regions of your process via cat /proc/$SPID/maps and the memory is stored in /proc/$SPID/mem . We can use the following Python program to write all pages to stdout : cat dumpstack.py EOF import sys, os, string, re pid = sys.argv[1] maps_file = open( /proc/%s/maps % pid, r ) mem_file = open( /proc/%s/mem % pid, r ) r=0 for line in maps_file.readlines(): # for each mapped region w=line.rsplit(None, 1)[-1] # last word if w != /dev/isgx and w != [vvar] and w != [vdso] and w != [vsyscall] : m = re.match(r ([0-9A-Fa-f]+)-([0-9A-Fa-f]+) ([-r]) , line) r += 1 p = 0 if m.group(3) == r : # if this is a readable region start = int(m.group(1), 16) end = int(m.group(2), 16) while start end: try: mem_file.seek(start) # seek to region start chunk = mem_file.read(4096) # read region contents sys.stdout.write(chunk) p += 1 if p 1000: sys.stderr.write( region = %02d, index=%x \\r % (r,start)) p = 0 start += 4096 except: pass sys.stderr.write( \\n ) EOF Run this program and grep for the prefix of our secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will take some time but eventually it will print the full secret MYBIGS .","title":"Simulation Mode"},{"location":"memory_dump/#hardware-mode","text":"Let us now run the program in hardware mode. First, ensure that you kill the original program by typing control-C . Let's start mysecret in an enclave, i.e., in hardware mode inside the container: SCONE_VERSION = 1 SCONE_MODE = HW SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Update environment variable SPID in a second terminal on your host: SPID = $( ps -a | grep -v grep | grep mysecret | awk {print $1} ) and then try to find the secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will run for much less time and in particular, it will not print any secrets. Note, however, that secrets stored in the binaries can be found because the binary is not encrypted: a copy of the original binary - which is used to start the enclave - stays in main memory outside the enclave. Let's look for the string THIS_IS_NOT_SECRET in our example application. We can find this secret as follows: sudo python dumpstack.py $SPID | strings -n 5 | grep THIS_IS_NOT_SECRET scontain.com , July 2018. Questions or Suggestions? Note that the enclave runs in this example runs in debug mode, i.e., one can still attach to this enclave with scone-gdb . To prevent access via the debugger, you need to run your enclave in production mode . Note that an adversary could analyse the binary and figure out the secret. The standard way to provide an enclave with secrets is to use SCONE CAS .","title":"Hardware Mode"},{"location":"microcode/","text":"L1TF (a.k.a. Foreshadow) Mitigation Intel SGX is susceptible to a side channel attack in which an attacker can read the secrets of an enclave E by starting an enclave A on the same core as E . This attack only works for secrets of E stored in a L1 cache and if A and E run on the same core. This attack is referred to as enclave-to-enclave (E2E) attack. E2E does not work if E and A run on different cores since each core has its own dedicated L1 cache. The current Intel microcode ensures that the L1 cache of a core is flushed when a thread leaves the enclave. In this way, if A runs after E , it will not be able to read any secrets from L1 . Note that flushing L1 is an expensive operation. SCONE keeps the threads running inside of an enclave and in this way avoids the cost of flushing L1 on each system call. The microcode update by Intel protects against L1TF only if hyperthreading is disabled! If enclave A would run on one hyperthread and E on the second hyperthread of the same core, A could still read the values of E stored in L1 using the L1TF side channel. Hence, you must also disable the hyperthreading inside of your BIOS 1 . If hyperthreading is switched on/off, is checked during attestation. In other words, one can ensure during attestation that the enclave is protected against L1TF. According to the Intel documentation , an update microcode combined with switched off hyperthreading will fully mitigate L1TF and E2E for Intel SGX . Microcode Update on Ubuntu Note that the BIOS loads the microcode during boot. The operating system can load a newer version of the microcode, for example, in case there is no new microcode update available yet for your BIOS. In this case, the OS has to load the new microcode on each new boot. On Ubuntu, you can upgrade your CPU to the newest microcode as follows: TMPDIR = $( mktemp -d ) cd $TMPDIR curl -o ucode.tgz https://downloadmirror.intel.com/28039/eng/microcode-20180807.tgz tar -xzf ucode.tgz sudo apt-get update sudo apt-get install -y intel-microcode if [ -f /sys/devices/system/cpu/microcode/reload ] ; then if [ -d /lib/firmware ] ; then mkdir -p OLD cp -rf /lib/firmware/intel-ucode OLD sudo cp -rf intel-ucode /lib/firmware echo 1 | sudo tee /sys/devices/system/cpu/microcode/reload else echo Error: microcode directory does not exist fi else echo Error: is intel-micrcode really installed? fi You can check that a new microcode was updated by executing: dmesg | grep microcode This will result in an output like [ 108.121907] microcode: CPU0 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129335] microcode: CPU0 updated to revision 0xc6, date = 2018-04-17 [ 108.129406] microcode: CPU1 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129484] microcode: CPU1 updated to revision 0xc6, date = 2018-04-17 [ 108.129539] microcode: CPU2 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129611] microcode: CPU2 updated to revision 0xc6, date = 2018-04-17 [ 108.129696] microcode: CPU3 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129768] microcode: CPU3 updated to revision 0xc6, date = 2018-04-17 [ 108.129816] microcode: CPU4 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129905] microcode: CPU4 updated to revision 0xc6, date = 2018-04-17 [ 108.129953] microcode: CPU5 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.130025] microcode: CPU5 updated to revision 0xc6, date = 2018-04-17 [ 108.130070] microcode: CPU6 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.130142] microcode: CPU6 updated to revision 0xc6, date = 2018-04-17 [ 108.130224] microcode: CPU7 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.130296] microcode: CPU7 updated to revision 0xc6, date = 2018-04-17 [ 108.130298] microcode: Enabling Indirect Branch Prediction Barrier [ 108.130300] microcode: Enabling Indirect Branch Restricted Speculation When you are sure that this works out, you can load this microcode during reboot as follows: cat /tmp/load-intel-ucode.sh EOF #!/bin/bash echo 1 | sudo tee /sys/devices/system/cpu/microcode/reload EOF sudo mv /tmp/load-intel-ucode.sh /lib/firmware/load-intel-ucode.sh chmod a+x /lib/firmware/load-intel-ucode.sh # the following cmd pops up the editor: add the @reboot... via the editor crontab -e @reboot /lib/firmware/load-intel-ucode.sh Checking CPU Features CPU microcode updates that protects against side channels like L1TF are not available for all CPUs. You can check that your microcode has up to date protection features enabled by executing the following: docker run --rm sconecuratedimages/apps:check_cpuid scontain.com , August 2018. Questions or Suggestions? We have published an alternative to switching off hyperthreading in Usenix ATC 2018 : we ensure that both hyperthreads of a core run inside of the same enclave.","title":"Updating CPU microcode"},{"location":"microcode/#l1tf-aka-foreshadow-mitigation","text":"Intel SGX is susceptible to a side channel attack in which an attacker can read the secrets of an enclave E by starting an enclave A on the same core as E . This attack only works for secrets of E stored in a L1 cache and if A and E run on the same core. This attack is referred to as enclave-to-enclave (E2E) attack. E2E does not work if E and A run on different cores since each core has its own dedicated L1 cache. The current Intel microcode ensures that the L1 cache of a core is flushed when a thread leaves the enclave. In this way, if A runs after E , it will not be able to read any secrets from L1 . Note that flushing L1 is an expensive operation. SCONE keeps the threads running inside of an enclave and in this way avoids the cost of flushing L1 on each system call. The microcode update by Intel protects against L1TF only if hyperthreading is disabled! If enclave A would run on one hyperthread and E on the second hyperthread of the same core, A could still read the values of E stored in L1 using the L1TF side channel. Hence, you must also disable the hyperthreading inside of your BIOS 1 . If hyperthreading is switched on/off, is checked during attestation. In other words, one can ensure during attestation that the enclave is protected against L1TF. According to the Intel documentation , an update microcode combined with switched off hyperthreading will fully mitigate L1TF and E2E for Intel SGX .","title":"L1TF (a.k.a. Foreshadow) Mitigation"},{"location":"microcode/#microcode-update-on-ubuntu","text":"Note that the BIOS loads the microcode during boot. The operating system can load a newer version of the microcode, for example, in case there is no new microcode update available yet for your BIOS. In this case, the OS has to load the new microcode on each new boot. On Ubuntu, you can upgrade your CPU to the newest microcode as follows: TMPDIR = $( mktemp -d ) cd $TMPDIR curl -o ucode.tgz https://downloadmirror.intel.com/28039/eng/microcode-20180807.tgz tar -xzf ucode.tgz sudo apt-get update sudo apt-get install -y intel-microcode if [ -f /sys/devices/system/cpu/microcode/reload ] ; then if [ -d /lib/firmware ] ; then mkdir -p OLD cp -rf /lib/firmware/intel-ucode OLD sudo cp -rf intel-ucode /lib/firmware echo 1 | sudo tee /sys/devices/system/cpu/microcode/reload else echo Error: microcode directory does not exist fi else echo Error: is intel-micrcode really installed? fi You can check that a new microcode was updated by executing: dmesg | grep microcode This will result in an output like [ 108.121907] microcode: CPU0 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129335] microcode: CPU0 updated to revision 0xc6, date = 2018-04-17 [ 108.129406] microcode: CPU1 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129484] microcode: CPU1 updated to revision 0xc6, date = 2018-04-17 [ 108.129539] microcode: CPU2 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129611] microcode: CPU2 updated to revision 0xc6, date = 2018-04-17 [ 108.129696] microcode: CPU3 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129768] microcode: CPU3 updated to revision 0xc6, date = 2018-04-17 [ 108.129816] microcode: CPU4 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.129905] microcode: CPU4 updated to revision 0xc6, date = 2018-04-17 [ 108.129953] microcode: CPU5 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.130025] microcode: CPU5 updated to revision 0xc6, date = 2018-04-17 [ 108.130070] microcode: CPU6 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.130142] microcode: CPU6 updated to revision 0xc6, date = 2018-04-17 [ 108.130224] microcode: CPU7 sig=0x506e3, pf=0x2, revision=0xc2 [ 108.130296] microcode: CPU7 updated to revision 0xc6, date = 2018-04-17 [ 108.130298] microcode: Enabling Indirect Branch Prediction Barrier [ 108.130300] microcode: Enabling Indirect Branch Restricted Speculation When you are sure that this works out, you can load this microcode during reboot as follows: cat /tmp/load-intel-ucode.sh EOF #!/bin/bash echo 1 | sudo tee /sys/devices/system/cpu/microcode/reload EOF sudo mv /tmp/load-intel-ucode.sh /lib/firmware/load-intel-ucode.sh chmod a+x /lib/firmware/load-intel-ucode.sh # the following cmd pops up the editor: add the @reboot... via the editor crontab -e @reboot /lib/firmware/load-intel-ucode.sh","title":"Microcode Update on Ubuntu"},{"location":"microcode/#checking-cpu-features","text":"CPU microcode updates that protects against side channels like L1TF are not available for all CPUs. You can check that your microcode has up to date protection features enabled by executing the following: docker run --rm sconecuratedimages/apps:check_cpuid scontain.com , August 2018. Questions or Suggestions? We have published an alternative to switching off hyperthreading in Usenix ATC 2018 : we ensure that both hyperthreads of a core run inside of the same enclave.","title":"Checking CPU Features"},{"location":"multistagebuild/","text":"Multi-Stage Build As we mentioned in the context of the dockerfile example , that you should not include the SCONE platform in the images you build - at least if you intent to push you images to public repositories. The easiest way to achieve this, is to use multi-stage builds. The idea is to build you application with the scone cross-compiler image (i.e., sconecuratedimages/crosscompilers ) image and then copy the application to another container with a different base image. You must ensure that you copy all parts of your application are included. If you use static linking , this can be easier than using dynamic linking. We show how to generate a Docker image of a dynamically linked application: we show this for groupcache . We do want to make sure that the image is as small as possible and in particular, that the image must not contain the SCONE crosscompilers. Hence, we use a multi-stage build during which we copy all dependencies of groupcache : cat Dockerfile EOF FROM sconecuratedimages/crosscompilers RUN apk update \\ apk add git curl go \\ go get -compiler gccgo -u github.com/golang/groupcache \\ curl -fsSL --output groupcache.go https://gist.githubusercontent.com/fiorix/816117cfc7573319b72d/raw/797d2ed5b567dcffb8ebd8896a3d7671b1a44b31/groupcache.go \\ export SCONE_HEAP=1G \\ go build -compiler gccgo -buildmode=exe groupcache.go FROM alpine:latest COPY --from=0 /groupcache / COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.11 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.11 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 COPY --from=0 /opt/scone/lib/ld-scone-x86_64.so.1 /opt/scone/lib/ld-scone-x86_64.so.1 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 COPY --from=0 /etc/sgx-musl.conf /etc/sgx-musl.conf CMD sh -c SCONE_HEAP=1G /groupcache EOF Note that one can figure out the libraries to copy with command ldd groupcache . Let's generate an image groupcache with this Dockerfile: docker build --pull -t groupcache . The size of the groupcache image is about 65MB. You can run this container by executing: docker run --rm --publish 8080 :8080 groupcache You can now query this service from a different terminal on the host this service, e.g.,: curl localhost:8080/color?name = green Warning This service has multiple security issues : we show how to address these with the help of the SCONE shields in a later section. scontain.com , August 2018. Questions or Suggestions?","title":"Multi-stage build"},{"location":"multistagebuild/#multi-stage-build","text":"As we mentioned in the context of the dockerfile example , that you should not include the SCONE platform in the images you build - at least if you intent to push you images to public repositories. The easiest way to achieve this, is to use multi-stage builds. The idea is to build you application with the scone cross-compiler image (i.e., sconecuratedimages/crosscompilers ) image and then copy the application to another container with a different base image. You must ensure that you copy all parts of your application are included. If you use static linking , this can be easier than using dynamic linking. We show how to generate a Docker image of a dynamically linked application: we show this for groupcache . We do want to make sure that the image is as small as possible and in particular, that the image must not contain the SCONE crosscompilers. Hence, we use a multi-stage build during which we copy all dependencies of groupcache : cat Dockerfile EOF FROM sconecuratedimages/crosscompilers RUN apk update \\ apk add git curl go \\ go get -compiler gccgo -u github.com/golang/groupcache \\ curl -fsSL --output groupcache.go https://gist.githubusercontent.com/fiorix/816117cfc7573319b72d/raw/797d2ed5b567dcffb8ebd8896a3d7671b1a44b31/groupcache.go \\ export SCONE_HEAP=1G \\ go build -compiler gccgo -buildmode=exe groupcache.go FROM alpine:latest COPY --from=0 /groupcache / COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.11 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.11 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 COPY --from=0 /opt/scone/lib/ld-scone-x86_64.so.1 /opt/scone/lib/ld-scone-x86_64.so.1 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 COPY --from=0 /etc/sgx-musl.conf /etc/sgx-musl.conf CMD sh -c SCONE_HEAP=1G /groupcache EOF Note that one can figure out the libraries to copy with command ldd groupcache . Let's generate an image groupcache with this Dockerfile: docker build --pull -t groupcache . The size of the groupcache image is about 65MB. You can run this container by executing: docker run --rm --publish 8080 :8080 groupcache You can now query this service from a different terminal on the host this service, e.g.,: curl localhost:8080/color?name = green Warning This service has multiple security issues : we show how to address these with the help of the SCONE shields in a later section. scontain.com , August 2018. Questions or Suggestions?","title":"Multi-Stage Build"},{"location":"news/","text":"SCONE News NEWS (2018-06-01) SCONE adds a new curated image for PySpark . Also, check out our other usescases . NEWS (2018-03-17): SCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine NEWS (2018-03-07): SCONE adds an updated curated Node image (sconecuratedimages/apps:node-8.9.4-alpine) NEWS (2018-02-02): SCONE adds support for Zookeeper (sconecuratedimages/apps:zookeeper-alpine). NEWS (2018-02-02): SCONE adds support for Python 3.6.4 (sconecuratedimages/apps:python-3.5-alpine). NEWS (2018-01-27): SCONE adds support for Java . NEWS (2018-01-27): SCONE adds curated Node image (sconecuratedimages/apps:node-8-alpine). NEWS (2018-01-27): SCONE adds curated MongoDB image (sconecuratedimages/apps:mongodb-alpine). NEWS (2018-01-27): SCONE adds curated Memcached image (sconecuratedimages/apps:memcached-alpine). NEWS (2018-01-27): SCONE adds curated Vault image (sconecuratedimages/apps:vault-alpine). SOON TO COME: While SGX is affected by Spectre, SCONE adds protection against Spectre (variants 1 and 2)","title":"News Log"},{"location":"news/#scone-news","text":"NEWS (2018-06-01) SCONE adds a new curated image for PySpark . Also, check out our other usescases . NEWS (2018-03-17): SCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine NEWS (2018-03-07): SCONE adds an updated curated Node image (sconecuratedimages/apps:node-8.9.4-alpine) NEWS (2018-02-02): SCONE adds support for Zookeeper (sconecuratedimages/apps:zookeeper-alpine). NEWS (2018-02-02): SCONE adds support for Python 3.6.4 (sconecuratedimages/apps:python-3.5-alpine). NEWS (2018-01-27): SCONE adds support for Java . NEWS (2018-01-27): SCONE adds curated Node image (sconecuratedimages/apps:node-8-alpine). NEWS (2018-01-27): SCONE adds curated MongoDB image (sconecuratedimages/apps:mongodb-alpine). NEWS (2018-01-27): SCONE adds curated Memcached image (sconecuratedimages/apps:memcached-alpine). NEWS (2018-01-27): SCONE adds curated Vault image (sconecuratedimages/apps:vault-alpine). SOON TO COME: While SGX is affected by Spectre, SCONE adds protection against Spectre (variants 1 and 2)","title":"SCONE News"},{"location":"outline/","text":"Getting Started To simplify not only getting started with SCONE but also using SCONE, we support multiple ways to develop and run SCONE-based applications: simulation mode inside of a container: use this mode to check out SCONE or to develop software on machines without Intel SGX support (e.g., a Mac). hardware mode inside of a container: this mode requires that you install the Intel SGX driver on your host. host mode : compile inside of a container and execute on the host. Dockerfile : build program and container image with the help of a Dockerfile. swarm mode: you can run your application on a cluster of machines. To use a swarm, we provide a SCONE CLI and a patched engine and patched SGX driver to make up for some implementation limitations of Docker swarm. Language Support After gaining access to the SCONE container images 1 , you can compile and run the hello world program as shown in this section . After that, check out how you could automate the compilation with the help of a Dockerfile . Next, you could run an example application in a Swarm . If you want to run applications languages not supported by GNU gcc compiler, please read the descriptions in the appropriate section in menu Language Support : in addition to C , C++ , Fortran , GO and Rust , we also support Python , Java , and JavaScript/ Node.js . Please send us an email if you need access to another programming language. Terminology We might use some terms in the SCONE documentation that we do not explicitly introduce. We maintain a Glossary that defines some of the important terms that we use within the SCONE technical documentation. scontain.com , August 2018. Questions or Suggestions? You need to gain permissions to be able to run the examples given in this documentation. Please send an email with your free Docker ID to info@scontain.com .","title":"Getting Started"},{"location":"outline/#getting-started","text":"To simplify not only getting started with SCONE but also using SCONE, we support multiple ways to develop and run SCONE-based applications: simulation mode inside of a container: use this mode to check out SCONE or to develop software on machines without Intel SGX support (e.g., a Mac). hardware mode inside of a container: this mode requires that you install the Intel SGX driver on your host. host mode : compile inside of a container and execute on the host. Dockerfile : build program and container image with the help of a Dockerfile. swarm mode: you can run your application on a cluster of machines. To use a swarm, we provide a SCONE CLI and a patched engine and patched SGX driver to make up for some implementation limitations of Docker swarm.","title":"Getting Started"},{"location":"outline/#language-support","text":"After gaining access to the SCONE container images 1 , you can compile and run the hello world program as shown in this section . After that, check out how you could automate the compilation with the help of a Dockerfile . Next, you could run an example application in a Swarm . If you want to run applications languages not supported by GNU gcc compiler, please read the descriptions in the appropriate section in menu Language Support : in addition to C , C++ , Fortran , GO and Rust , we also support Python , Java , and JavaScript/ Node.js . Please send us an email if you need access to another programming language. Terminology We might use some terms in the SCONE documentation that we do not explicitly introduce. We maintain a Glossary that defines some of the important terms that we use within the SCONE technical documentation. scontain.com , August 2018. Questions or Suggestions? You need to gain permissions to be able to run the examples given in this documentation. Please send an email with your free Docker ID to info@scontain.com .","title":"Language Support"},{"location":"performance/","text":"Performance of SCONE-based Programs The performance of running programs inside of enclaves depends on various factors. The main factors are the following: Locality of memory accesses : if the working set of a process does not fit in the EPC (extended page cache), the process will suffer page faults. During a page fault, a page in the EPC will be selected by the SGX driver, re-encrypted and stored in main memory. The overhead of a process inside of an enclave grows with the page fault rate. If a program has an option to reduce the memory footprint (not include debug symbols in binaries, compiled to reduce size, etc), this will often result in better performance. system calls : each system call requires that memory-based arguments are copied from the enclave to the outside memory and memory-based return values are copied from the main memory into the enclave. Exiting and returning to an enclave takes at least 8000 cycles 1 . SCONE uses an asynchronous system call interface that ensures that threads do not need to exit the enclave to perform a system call. threading : SCONE provides application-level threading. This ensures that in case an application thread waits for some event like the return of a system call, we can switch to a new application thread without the need to exit the enclave. The normalized PyPy performance shows that overheads are for the standard Python benchmarks are acceptable. For getting optimal performance in your applications, SCONE provides multiple tuning parameters ( see here ): The default SCONE configuration file often provides reasonable performance but optimal performance - at least for SGXv1 hardware - can often only be achieved by selecting the right number of ethreads , sthreads and heap size . scontain.com , July 2018. Questions or Suggestions? Recent CPU microcode updates have increased this number further.","title":"Performance"},{"location":"performance/#performance-of-scone-based-programs","text":"The performance of running programs inside of enclaves depends on various factors. The main factors are the following: Locality of memory accesses : if the working set of a process does not fit in the EPC (extended page cache), the process will suffer page faults. During a page fault, a page in the EPC will be selected by the SGX driver, re-encrypted and stored in main memory. The overhead of a process inside of an enclave grows with the page fault rate. If a program has an option to reduce the memory footprint (not include debug symbols in binaries, compiled to reduce size, etc), this will often result in better performance. system calls : each system call requires that memory-based arguments are copied from the enclave to the outside memory and memory-based return values are copied from the main memory into the enclave. Exiting and returning to an enclave takes at least 8000 cycles 1 . SCONE uses an asynchronous system call interface that ensures that threads do not need to exit the enclave to perform a system call. threading : SCONE provides application-level threading. This ensures that in case an application thread waits for some event like the return of a system call, we can switch to a new application thread without the need to exit the enclave. The normalized PyPy performance shows that overheads are for the standard Python benchmarks are acceptable. For getting optimal performance in your applications, SCONE provides multiple tuning parameters ( see here ): The default SCONE configuration file often provides reasonable performance but optimal performance - at least for SGXv1 hardware - can often only be achieved by selecting the right number of ethreads , sthreads and heap size . scontain.com , July 2018. Questions or Suggestions? Recent CPU microcode updates have increased this number further.","title":"Performance of SCONE-based Programs"},{"location":"pypyscone/","text":"PyPy for SCONE PyPy is a Just In Time compiler for Python. We maintain a container image that includes PyPy running inside of an enclave. The speed of pypy inside of an enclave is in many cases faster than running natively (i.e., outside an enclave) using the standard CPython interpreter. To compare the performance we are using the standard python / pypy speed center and add pypy for SCONE. Below you can see a graph that gives an overview of the performance of different Python variants. This depicts normalized performance using pypy as the baseline, i.e., smaller values show better performance. For example, a ratio of 65 means that the program is indeed 65 times slower than pypy running in native mode (i.e., these are not percent values). We can see in this graph that pypy can achieve for some benchmarks dramatic speedups. pypy running inside of an enclave is in almost all benchmarks faster than native CPython: You can compare the performance in more details using the speedcenter website maintained by UFCG .","title":"PyPy"},{"location":"pypyscone/#pypy-for-scone","text":"PyPy is a Just In Time compiler for Python. We maintain a container image that includes PyPy running inside of an enclave. The speed of pypy inside of an enclave is in many cases faster than running natively (i.e., outside an enclave) using the standard CPython interpreter. To compare the performance we are using the standard python / pypy speed center and add pypy for SCONE. Below you can see a graph that gives an overview of the performance of different Python variants. This depicts normalized performance using pypy as the baseline, i.e., smaller values show better performance. For example, a ratio of 65 means that the program is indeed 65 times slower than pypy running in native mode (i.e., these are not percent values). We can see in this graph that pypy can achieve for some benchmarks dramatic speedups. pypy running inside of an enclave is in almost all benchmarks faster than native CPython: You can compare the performance in more details using the speedcenter website maintained by UFCG .","title":"PyPy for SCONE"},{"location":"pyspark/","text":"PySpark Use Case PySpark is the Python API to Spark . We will add some more documentation about the curated PySpark image later. Until then, you can have a look at our PySpark screencast: If you want to evaluate the PySpark image, send us an email . scontain.com , June 2018. Questions or Suggestions?","title":"PySpark"},{"location":"pyspark/#pyspark-use-case","text":"PySpark is the Python API to Spark . We will add some more documentation about the curated PySpark image later. Until then, you can have a look at our PySpark screencast: If you want to evaluate the PySpark image, send us an email . scontain.com , June 2018. Questions or Suggestions?","title":"PySpark Use Case"},{"location":"scone_file_shield/","text":"SCONE IMAGE Objective: Simplify the shielding of files and the execution of services inside of enclaves. We want to be able to convert an image such that the files in the image are authenticated and optionally, be encrypted binaries inside of the image a converted to run inside of enclaves can given access to the encryption key to potential users A client that runs a container of this image, can simply encryption of all volumes that are mapped into the container simply map encrypted volumes inside of containers and also ensure the freshness of the files a client can simply access the secret information from the producer of the image Note: In many applications, we might split the file shielding along the volumes that we map into a container. There is a top-level FSPF that gets its information from the stack file of that container. Example scone image protect : we can generate a fspf for an existing image and update the image with the help of a stack file like this: aptsigner : image : aptsigner target_num_containers : 1 command : aptsign ... volumes : - /infinit/myself/etc-mysql:/ packages - /infinit/myself/apt-keyring:/ keyring The stack file gives us the image names and the volumes that might be mapped into the containers. We can authenticate the root FSPF with a public key and optionally, encrypt the fspf with a symmetric key. The symmetric key would permit customers to decrypt the FSPF. scone image protect # Description --encrypt 0-1 encrypt all files currently stored in this image --signer KEY 0-1 certificate use to sign FSPF (optionally) --mrsigner MRSIGNER 0-1 key to sign enclave (optionally) --service NAME 0-1 default - encrypt all services in the stack file --intag TAG 0-1 default - scone --stack FILE 1 stackfile used to define volumes --binary CMD 0-n ensure that this binary runs inside an enclave We store for each image the signer key, the tag and the encryption key in CAS. We can give access to these keys via a token. A token has a limited lifetime, typically, one year. Encrypt Volume A user can run a container by specifying a stack file. This stack file contains all information for SCONE to figure out the signatures to run the services / images. However, we need to run the stack in the context of the same session in which the volume was encrypted (or, the session was given access to these keys). Encrypt volume and store tag in CAS: scone volume encrypt --as tenant --name vol This call will fail if the volume is already encrypted. To simplify encryption, you will be able to create an already encrypted volume (via flag --encrypt ). The call will also fail, if no session is currently active. The key is stored in the current session as volume/tenant/vol/key and the tag as volume/tenant/vol/tag . Updating TAG In case the volume has been updated and you want to map it into a container, you need first to update the TAG in CAS. To update the tag, perform the following scone volume encrypt --as tenant --name vol --update-tag --no-check scone volume encrypt Description --update-tag update the TAG in CAS --reencrypt change the encryption key --check do not perform integrity check if volume is already encrypted A container can update the TAG and store the update TAG in a file and encrypt the TAG with a shared key volume/tenant/vol/tagkey . Example: nginx sconedocs : image : 127.0 . 0.1 : 5000 / sconetainer : fss command : /bin/nginx -p / nginx - c nginx . conf volumes : - /infinit/scone/sconedocs:/ nginx - /infinit/scone/sconedocs-etc:/ nginx - etc","title":"SCONE IMAGE"},{"location":"scone_file_shield/#scone-image","text":"Objective: Simplify the shielding of files and the execution of services inside of enclaves. We want to be able to convert an image such that the files in the image are authenticated and optionally, be encrypted binaries inside of the image a converted to run inside of enclaves can given access to the encryption key to potential users A client that runs a container of this image, can simply encryption of all volumes that are mapped into the container simply map encrypted volumes inside of containers and also ensure the freshness of the files a client can simply access the secret information from the producer of the image Note: In many applications, we might split the file shielding along the volumes that we map into a container. There is a top-level FSPF that gets its information from the stack file of that container.","title":"SCONE IMAGE"},{"location":"scone_file_shield/#example","text":"scone image protect : we can generate a fspf for an existing image and update the image with the help of a stack file like this: aptsigner : image : aptsigner target_num_containers : 1 command : aptsign ... volumes : - /infinit/myself/etc-mysql:/ packages - /infinit/myself/apt-keyring:/ keyring The stack file gives us the image names and the volumes that might be mapped into the containers. We can authenticate the root FSPF with a public key and optionally, encrypt the fspf with a symmetric key. The symmetric key would permit customers to decrypt the FSPF. scone image protect # Description --encrypt 0-1 encrypt all files currently stored in this image --signer KEY 0-1 certificate use to sign FSPF (optionally) --mrsigner MRSIGNER 0-1 key to sign enclave (optionally) --service NAME 0-1 default - encrypt all services in the stack file --intag TAG 0-1 default - scone --stack FILE 1 stackfile used to define volumes --binary CMD 0-n ensure that this binary runs inside an enclave We store for each image the signer key, the tag and the encryption key in CAS. We can give access to these keys via a token. A token has a limited lifetime, typically, one year.","title":"Example"},{"location":"scone_file_shield/#encrypt-volume","text":"A user can run a container by specifying a stack file. This stack file contains all information for SCONE to figure out the signatures to run the services / images. However, we need to run the stack in the context of the same session in which the volume was encrypted (or, the session was given access to these keys). Encrypt volume and store tag in CAS: scone volume encrypt --as tenant --name vol This call will fail if the volume is already encrypted. To simplify encryption, you will be able to create an already encrypted volume (via flag --encrypt ). The call will also fail, if no session is currently active. The key is stored in the current session as volume/tenant/vol/key and the tag as volume/tenant/vol/tag .","title":"Encrypt Volume"},{"location":"scone_file_shield/#updating-tag","text":"In case the volume has been updated and you want to map it into a container, you need first to update the TAG in CAS. To update the tag, perform the following scone volume encrypt --as tenant --name vol --update-tag --no-check scone volume encrypt Description --update-tag update the TAG in CAS --reencrypt change the encryption key --check do not perform integrity check if volume is already encrypted A container can update the TAG and store the update TAG in a file and encrypt the TAG with a shared key volume/tenant/vol/tagkey .","title":"Updating TAG"},{"location":"scone_file_shield/#example-nginx","text":"sconedocs : image : 127.0 . 0.1 : 5000 / sconetainer : fss command : /bin/nginx -p / nginx - c nginx . conf volumes : - /infinit/scone/sconedocs:/ nginx - /infinit/scone/sconedocs-etc:/ nginx - etc","title":"Example: nginx"},{"location":"scone_session/","text":"scone session A session is a unique namespace that is used for the management of keys associated with the execution of an application. scone session manages keys and their access rights in the context of scone-based applications. Motivation The management of keys is one of the important problems to be addressed when building secure cloud applications. Here are some typical questions that need to be addressed: how to generate keys in a secure fashion? how to share keys between different applications? how to prevent unauthorized access to keys? how to ensure that keys stay confidential - despite, for example, administrator churn. how to ensure that no human can read keys? More advanced questions that we need to address is on how to ensure company policies, like how to enforce company policy regarding which versions of a binary are permitted to be executed? how to restrict the teams that can certain applications? Another related topic is the management of the private key of certificates. how to provide enclaves with certificates and the associated private key in a secure fashion? Related question that we need to address is on how to make this practical to use. In particular, we need to address the following question: how can we achieve all this without the need to re-engineer our applications? Approach scone session helps you to address the above questions. Before we describe the details of the scone session command line interface, we motivate its design from the threat model that we must address. Problem: No trusted party to manage access rights Traditionally, most systems have the concept of an administrator that can manage the access rights to resources. For example, a root user in an operating system or a database can determine which resources a user can access. In SCONE, we do not want to require an administrator that controls the access to secrets. Instead, we would like to empower each user such that this user can control access to her resources without any administrator being able to overwrite the access control policy of the user. However, a user can decide to share resources with others. For example, a user could share access to the key of an encrypted volume with other users. Approach: Session Concept Our objective is to provide key management without introducing the need to have a trusted party that is responsible to administrate access rights. The main idea is that each user can create a session (an alternative name would be a namespace ) in which the user has complete control over access rights to the keys created within this session. Problem: No Trusted Input and Output One of the issues that we face in the context of secure trusted computing is that we can neither trust the cloud computers and in many cases, not even our development machines. In some cases, the cloud machines might even be more secure than our development machines. Hence, we need to address the following problem: Problem: Any key that is received via an input device or output via an output device cannot be considered to be confidential anymore (- unless the key itself is encrypted) Approach: Keys are always encrypted To address this problem, we need to ensure that keys are never input or output as plain text. To achieve this, we support generation of keys and management of keys that never require keys to be transferred as plain text. Problem: Authorization If there is no trusted input and output, how can we know that certain requests / commands were indeed issued by a specific user? For example, a command is issued by a user O to give user U access to a key K that O has created. How do we ensure that it is indeed the intention of O to give U access to K ? After all, the input for issuing this command my be inserted by U or some other interested party. Approach: Multi-factor authentication and authorization While we do not trust that the input is confidential and under the control of the user, we assume that we have a reasonably secure way to authenticate the user and to ask for an authorization from the user. For the authentication and authorization, we can specify multiple devices that perform the multi-factor authentication and authorization. The authentication and authorization will be performed as a combination of face ID, finger print ID, and PIN codes running on one or more devices. Command line interface: scone session scone session create scone session create starts a new session and activates this session, i.e., makes this the default session. Outputs the name of the session on success. Options scone session create # Description --approve URL 0-n URL for approval of certain operations --fingerprint FP 1 per URL defines the fingerprint of the public key for the preceeding --approve URL --tenant TENANT 1 name of tenant --name NAME 1 name to identify session --no [OP,] 0-1 define what operations that do not require approval.(default: ALL = all operations require approval) We support the following commands / operations: Operation (OP) Approval? Description create required create a session. Approval to verify the arguments of the command and to ensure that issued to correct CAS key required create a key. Approval to verify the arguments of the command and to ensure that issued to correct CAS share required share the keys of this session with another session. Verify that command was issued by owner and that arguments are correct. join required join another session (if that session shared all its key with this session). Verify that command was issued by owner and that arguments are correct. checkpoint required checkpoint session information. Verify that command was issued by owner and that arguments are correct. restore required restores a session checkpoint. Verify that command was issued by owner and that arguments are correct. Caveats scone session create creates credentials to identify this session and to be able to modify the session. The session credentials are stored in file $HOME/.scone-session/TENANT/NAME/SESSIONID.cred : this file is encrypted with the seal key of scone session . If this file is corrupted or deleted, access to the session keys is not possible anymore . Hence, this file should be backed up. In case the scone CLI runs inside of a container, this file should be mapped into a volume. Since this file is encrypted with the seal key of scone session , if the CPU that created this file fails or the CPU microcode is updated, this file cannot be accessed anymore . Hence, the owner of the session should share the access rights on multiple machines. We support this with the help of scone session share and scone session join . Alternative: only support scone session join and authorize via approval only. This would help with software upgrade too. Authorize new client via session join. Session information is also stored in a CAS. You can copy the session information from one CAS to another CAS with the help of scone session checkpoint and scone session restore . Moreover, you can use these two commands to move a session from one CAS to another CAS. To be able to support updated scone clients, you must enable that a session permits additional clients with different MRENCLAVEs. Also, to support updated CAS. We support this via scone session join to permit new clients to get existing session information. To support new CAS, a user has to perform a checkpoint and restore operation. Example: scone session create --tenant myself --name www \\ --approve ap.com/myself --fingerprint 43 :51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\ --approve ap2.com/myboss --fingerprint 55 :45:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\ scone session key Creates a key or a certificate and makes it available at a given key name. scone session key --name KEYNAME [ --replace ] \u2014-type TYPE [ --access [ session ] :MRENCLAVE ]] Options scone session key Description --scope SCOPE limits visibility of key: final, session, full. final = only participants specified with option --access are permitted to receive the key. session = one can add more enclaves that can access the key but it is not possible to extend this beyond the current session. full no limit to what extend the user can share this key with other sessions. -- type TYPE determines the type of key. TYPE=[DIGIT:N,ALPHA:N,ALPHANUM:N,PRINT:N,CERT:N] scone session share --key KEYNAME --gentoken --session SESSIONID Use cases: 1) We share the public key and the private key of an image with a client scone session share --key image/NAME/fspf-key --pubkey signer --gentoken --duration 366 2) We share the complete session with another client: scone session share --gentoken --all 3) Share all keys related to a stack that was created scone session share --stack FILE --gentoken Prints a token that can be used to access all keys to be able to run this stack scone session join --session SESSION-ID --token TOKEN Use case: Replace the current session by another session: scone session join --token TOKEN We effectively join a already existing session ID with a new client. scone session import --token TOKEN scone session import --token TOKEN [ --replace ] Import all keys related to the TOKEN. If --replace is given, keys are updated in case they already exist. Otherwise, an error message is issued. scone session checkpoint Copies the current state of a session from the CAS and stores it encrypted on the local file system. scone session checkpoint scone session restore --session SESSIONID --CAS URL Take the checkpoint of session SESSIONID and stores it to CAS referred to via URL. If the session already exists, this will result in a failure. scone session remove --force Removes a current session from the CAS. scone session activate --session SESSIONID Switches session to another session with ID SESSIONID Example: Sconedocs website Root ... We need to specify the path, the key and the tag of the file system protection file: image : aptsigner fspf_path : /nginx/ fspf . pb fspf_key : 970 f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag : 5930 bffbd9ea2f1317e6872b032334db We get this information from the CAS for the current session. Key directory is /image/aptsigner/fspf_path , ... Example: Signing APT packages Our objective is to sign Debian/Ubuntu packages without ever having the private key be visible in the clear. In particular, we do not want be able ourselves to change our minds and make the keys visible to us or others. Approach: We package the software in a container image that contains all code required to sign packages. To do so, we have two volumes: 1) packages : contains the package to be signed 2) keyring : Sharing Keys Access to keys: public key: share with everybody encrpytion key: share access with token generate token: with limited lifetime scontain.com , December 2017. Questions or Suggestions?","title":"scone session"},{"location":"scone_session/#scone-session","text":"A session is a unique namespace that is used for the management of keys associated with the execution of an application. scone session manages keys and their access rights in the context of scone-based applications.","title":"scone session"},{"location":"scone_session/#motivation","text":"The management of keys is one of the important problems to be addressed when building secure cloud applications. Here are some typical questions that need to be addressed: how to generate keys in a secure fashion? how to share keys between different applications? how to prevent unauthorized access to keys? how to ensure that keys stay confidential - despite, for example, administrator churn. how to ensure that no human can read keys? More advanced questions that we need to address is on how to ensure company policies, like how to enforce company policy regarding which versions of a binary are permitted to be executed? how to restrict the teams that can certain applications? Another related topic is the management of the private key of certificates. how to provide enclaves with certificates and the associated private key in a secure fashion? Related question that we need to address is on how to make this practical to use. In particular, we need to address the following question: how can we achieve all this without the need to re-engineer our applications?","title":"Motivation"},{"location":"scone_session/#approach","text":"scone session helps you to address the above questions. Before we describe the details of the scone session command line interface, we motivate its design from the threat model that we must address.","title":"Approach"},{"location":"scone_session/#problem-no-trusted-party-to-manage-access-rights","text":"Traditionally, most systems have the concept of an administrator that can manage the access rights to resources. For example, a root user in an operating system or a database can determine which resources a user can access. In SCONE, we do not want to require an administrator that controls the access to secrets. Instead, we would like to empower each user such that this user can control access to her resources without any administrator being able to overwrite the access control policy of the user. However, a user can decide to share resources with others. For example, a user could share access to the key of an encrypted volume with other users.","title":"Problem: No trusted party to manage access rights"},{"location":"scone_session/#approach-session-concept","text":"Our objective is to provide key management without introducing the need to have a trusted party that is responsible to administrate access rights. The main idea is that each user can create a session (an alternative name would be a namespace ) in which the user has complete control over access rights to the keys created within this session.","title":"Approach: Session Concept"},{"location":"scone_session/#problem-no-trusted-input-and-output","text":"One of the issues that we face in the context of secure trusted computing is that we can neither trust the cloud computers and in many cases, not even our development machines. In some cases, the cloud machines might even be more secure than our development machines. Hence, we need to address the following problem: Problem: Any key that is received via an input device or output via an output device cannot be considered to be confidential anymore (- unless the key itself is encrypted)","title":"Problem: No Trusted Input and Output"},{"location":"scone_session/#approach-keys-are-always-encrypted","text":"To address this problem, we need to ensure that keys are never input or output as plain text. To achieve this, we support generation of keys and management of keys that never require keys to be transferred as plain text.","title":"Approach: Keys are always encrypted"},{"location":"scone_session/#problem-authorization","text":"If there is no trusted input and output, how can we know that certain requests / commands were indeed issued by a specific user? For example, a command is issued by a user O to give user U access to a key K that O has created. How do we ensure that it is indeed the intention of O to give U access to K ? After all, the input for issuing this command my be inserted by U or some other interested party.","title":"Problem: Authorization"},{"location":"scone_session/#approach-multi-factor-authentication-and-authorization","text":"While we do not trust that the input is confidential and under the control of the user, we assume that we have a reasonably secure way to authenticate the user and to ask for an authorization from the user. For the authentication and authorization, we can specify multiple devices that perform the multi-factor authentication and authorization. The authentication and authorization will be performed as a combination of face ID, finger print ID, and PIN codes running on one or more devices.","title":"Approach: Multi-factor authentication and authorization"},{"location":"scone_session/#command-line-interface-scone-session","text":"","title":"Command line interface: scone session"},{"location":"scone_session/#scone-session-create","text":"scone session create starts a new session and activates this session, i.e., makes this the default session. Outputs the name of the session on success.","title":"scone session create"},{"location":"scone_session/#options","text":"scone session create # Description --approve URL 0-n URL for approval of certain operations --fingerprint FP 1 per URL defines the fingerprint of the public key for the preceeding --approve URL --tenant TENANT 1 name of tenant --name NAME 1 name to identify session --no [OP,] 0-1 define what operations that do not require approval.(default: ALL = all operations require approval) We support the following commands / operations: Operation (OP) Approval? Description create required create a session. Approval to verify the arguments of the command and to ensure that issued to correct CAS key required create a key. Approval to verify the arguments of the command and to ensure that issued to correct CAS share required share the keys of this session with another session. Verify that command was issued by owner and that arguments are correct. join required join another session (if that session shared all its key with this session). Verify that command was issued by owner and that arguments are correct. checkpoint required checkpoint session information. Verify that command was issued by owner and that arguments are correct. restore required restores a session checkpoint. Verify that command was issued by owner and that arguments are correct.","title":"Options"},{"location":"scone_session/#caveats","text":"scone session create creates credentials to identify this session and to be able to modify the session. The session credentials are stored in file $HOME/.scone-session/TENANT/NAME/SESSIONID.cred : this file is encrypted with the seal key of scone session . If this file is corrupted or deleted, access to the session keys is not possible anymore . Hence, this file should be backed up. In case the scone CLI runs inside of a container, this file should be mapped into a volume. Since this file is encrypted with the seal key of scone session , if the CPU that created this file fails or the CPU microcode is updated, this file cannot be accessed anymore . Hence, the owner of the session should share the access rights on multiple machines. We support this with the help of scone session share and scone session join . Alternative: only support scone session join and authorize via approval only. This would help with software upgrade too. Authorize new client via session join. Session information is also stored in a CAS. You can copy the session information from one CAS to another CAS with the help of scone session checkpoint and scone session restore . Moreover, you can use these two commands to move a session from one CAS to another CAS. To be able to support updated scone clients, you must enable that a session permits additional clients with different MRENCLAVEs. Also, to support updated CAS. We support this via scone session join to permit new clients to get existing session information. To support new CAS, a user has to perform a checkpoint and restore operation.","title":"Caveats"},{"location":"scone_session/#example","text":"scone session create --tenant myself --name www \\ --approve ap.com/myself --fingerprint 43 :51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\ --approve ap2.com/myboss --fingerprint 55 :45:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\","title":"Example:"},{"location":"scone_session/#scone-session-key","text":"Creates a key or a certificate and makes it available at a given key name. scone session key --name KEYNAME [ --replace ] \u2014-type TYPE [ --access [ session ] :MRENCLAVE ]]","title":"scone session key"},{"location":"scone_session/#options_1","text":"scone session key Description --scope SCOPE limits visibility of key: final, session, full. final = only participants specified with option --access are permitted to receive the key. session = one can add more enclaves that can access the key but it is not possible to extend this beyond the current session. full no limit to what extend the user can share this key with other sessions. -- type TYPE determines the type of key. TYPE=[DIGIT:N,ALPHA:N,ALPHANUM:N,PRINT:N,CERT:N]","title":"Options"},{"location":"scone_session/#scone-session-share-key-keyname-gentoken-session-sessionid","text":"Use cases: 1) We share the public key and the private key of an image with a client scone session share --key image/NAME/fspf-key --pubkey signer --gentoken --duration 366 2) We share the complete session with another client: scone session share --gentoken --all 3) Share all keys related to a stack that was created scone session share --stack FILE --gentoken Prints a token that can be used to access all keys to be able to run this stack","title":"scone session share --key KEYNAME  --gentoken --session SESSIONID"},{"location":"scone_session/#scone-session-join-session-session-id-token-token","text":"Use case: Replace the current session by another session: scone session join --token TOKEN We effectively join a already existing session ID with a new client.","title":"scone session join --session SESSION-ID --token TOKEN"},{"location":"scone_session/#scone-session-import-token-token","text":"scone session import --token TOKEN [ --replace ] Import all keys related to the TOKEN. If --replace is given, keys are updated in case they already exist. Otherwise, an error message is issued.","title":"scone session import --token TOKEN"},{"location":"scone_session/#scone-session-checkpoint","text":"Copies the current state of a session from the CAS and stores it encrypted on the local file system. scone session checkpoint","title":"scone session checkpoint"},{"location":"scone_session/#scone-session-restore-session-sessionid-cas-url","text":"Take the checkpoint of session SESSIONID and stores it to CAS referred to via URL. If the session already exists, this will result in a failure.","title":"scone session restore --session SESSIONID --CAS URL"},{"location":"scone_session/#scone-session-remove-force","text":"Removes a current session from the CAS.","title":"scone session remove --force"},{"location":"scone_session/#scone-session-activate-session-sessionid","text":"Switches session to another session with ID SESSIONID","title":"scone session activate --session SESSIONID"},{"location":"scone_session/#example-sconedocs-website","text":"Root ... We need to specify the path, the key and the tag of the file system protection file: image : aptsigner fspf_path : /nginx/ fspf . pb fspf_key : 970 f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag : 5930 bffbd9ea2f1317e6872b032334db We get this information from the CAS for the current session. Key directory is /image/aptsigner/fspf_path , ...","title":"Example: Sconedocs website"},{"location":"scone_session/#example-signing-apt-packages","text":"Our objective is to sign Debian/Ubuntu packages without ever having the private key be visible in the clear. In particular, we do not want be able ourselves to change our minds and make the keys visible to us or others. Approach: We package the software in a container image that contains all code required to sign packages. To do so, we have two volumes: 1) packages : contains the package to be signed 2) keyring :","title":"Example: Signing APT packages"},{"location":"scone_session/#sharing-keys","text":"Access to keys: public key: share with everybody encrpytion key: share access with token generate token: with limited lifetime scontain.com , December 2017. Questions or Suggestions?","title":"Sharing Keys"},{"location":"sconeinstall/","text":"Installing SCONE Typically, you will use the SCONE crosscompilers and CLI inside of a container. Hence, just start a container that contains SCONE in simulation mode or in hardware mode . In case you want to executed your programs on a host , you can compile in a container and move the generated binary to the host and execute on this host. In case you want to distribute your programs in container images and execute them in containers, you probably want to use a Dockerfile to compile your program and generate your container image . scontain.com , June 2018. Questions or Suggestions?","title":"Installing SCONE"},{"location":"sconeinstall/#installing-scone","text":"Typically, you will use the SCONE crosscompilers and CLI inside of a container. Hence, just start a container that contains SCONE in simulation mode or in hardware mode . In case you want to executed your programs on a host , you can compile in a container and move the generated binary to the host and execute on this host. In case you want to distribute your programs in container images and execute them in containers, you probably want to use a Dockerfile to compile your program and generate your container image . scontain.com , June 2018. Questions or Suggestions?","title":"Installing SCONE"},{"location":"sgxinstall/","text":"Installation of SGX driver To install the SGX 2.0 driver on Linux distributions, follow the official description . Alternatively, on a modern Ubuntu system, you could execute the following (assuming that you have sudo access): curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_sgx_driver.sh | bash Patched Driver As an alternative, we support to install a patched and slightly enhanced driver via the SCONE command line . This patched driver and patched docker-engine is only needed in case you want to use Docker Swarm. Check if the SGX driver is installed Check on the host as well as inside your containers that the SGX device /dev/isgx is visible: ls /dev/isgx /dev/null 2 1 echo SGX Driver installed || echo SGX Driver NOT installed If the driver is not installed, read Section SCONE Host Setup to learn how to install the SGX driver with the help of the SCONE CLI. Checking availability of SGX device inside of containers Docker does not automatically map the SGX device inside of containers. We provide, however, a patched Docker engine and a patched SGX driver that together permit to automatically map the sgx device inside of containers. You can run the checks to see if the SGX device gets mapped automatically into a container, i.e., if you run the patched docker engine. # preferred alternative: required for swarms to work: SGX device is available in all containers by default docker run --rm sconecuratedimages/checksgx || echo SGX device is not automatically mapped inside of container If the SGX device is not automatically mapped into the container, you can try to map the device as follows into the container: # alternative: use --device option without --privileged flag docker run --device = /dev/isgx --rm sconecuratedimages/checksgx || echo --device=/dev/isgx: failed to map SGX device inside of container In the unlikely case that the device is not mapped in the container, you can try to see if the container must be privileged or if we might need to remap the device ids: # last alternative: use --device option without --privileged flag sudo docker run -v /dev/isgx:/dev/isgx --privileged --rm sconecuratedimages/checksgx || echo SGX device NOT available inside of container Use the first alternative that works in your installation to give containers access to the SGX device. SCONE CLI If your node is part of a Docker swarm, we provide a simple check as part of the scone CLI . Say, the leader node of your swarm is called beatrix , then you could just execute on your development machine (see scone swarm manual for details): scone swarm check --verbose --manager beatrix scontain.com , June 2018. Questions or Suggestions?","title":"Installing Intel SGX driver"},{"location":"sgxinstall/#installation-of-sgx-driver","text":"To install the SGX 2.0 driver on Linux distributions, follow the official description . Alternatively, on a modern Ubuntu system, you could execute the following (assuming that you have sudo access): curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_sgx_driver.sh | bash","title":"Installation of SGX driver"},{"location":"sgxinstall/#patched-driver","text":"As an alternative, we support to install a patched and slightly enhanced driver via the SCONE command line . This patched driver and patched docker-engine is only needed in case you want to use Docker Swarm.","title":"Patched Driver"},{"location":"sgxinstall/#check-if-the-sgx-driver-is-installed","text":"Check on the host as well as inside your containers that the SGX device /dev/isgx is visible: ls /dev/isgx /dev/null 2 1 echo SGX Driver installed || echo SGX Driver NOT installed If the driver is not installed, read Section SCONE Host Setup to learn how to install the SGX driver with the help of the SCONE CLI.","title":"Check if the SGX driver is installed"},{"location":"sgxinstall/#checking-availability-of-sgx-device-inside-of-containers","text":"Docker does not automatically map the SGX device inside of containers. We provide, however, a patched Docker engine and a patched SGX driver that together permit to automatically map the sgx device inside of containers. You can run the checks to see if the SGX device gets mapped automatically into a container, i.e., if you run the patched docker engine. # preferred alternative: required for swarms to work: SGX device is available in all containers by default docker run --rm sconecuratedimages/checksgx || echo SGX device is not automatically mapped inside of container If the SGX device is not automatically mapped into the container, you can try to map the device as follows into the container: # alternative: use --device option without --privileged flag docker run --device = /dev/isgx --rm sconecuratedimages/checksgx || echo --device=/dev/isgx: failed to map SGX device inside of container In the unlikely case that the device is not mapped in the container, you can try to see if the container must be privileged or if we might need to remap the device ids: # last alternative: use --device option without --privileged flag sudo docker run -v /dev/isgx:/dev/isgx --privileged --rm sconecuratedimages/checksgx || echo SGX device NOT available inside of container Use the first alternative that works in your installation to give containers access to the SGX device.","title":"Checking availability of SGX device inside of containers"},{"location":"sgxinstall/#scone-cli","text":"If your node is part of a Docker swarm, we provide a simple check as part of the scone CLI . Say, the leader node of your swarm is called beatrix , then you could just execute on your development machine (see scone swarm manual for details): scone swarm check --verbose --manager beatrix scontain.com , June 2018. Questions or Suggestions?","title":"SCONE CLI"},{"location":"ssh/","text":"ssh Setup ssh is standard way to log securely into remote hosts. The SCONE CLI requires that you can log into all hosts of your docker swarm(s) without the need for providing a password. We describe in this section how you could set this up. scone The scone utility executes commands via ssh on the SGX-capable machine to install software as well as to deploy and monitor containers. Since we potentially execute many ssh commands, you need to configure ssh such that you can log into the SGX machines without having to type a password, you can use the basename of your SGX machines to login, and ssh is permitted to reuse connections to reduce the execution time of the scone commands. To do so, you need to configure ssh on your development machine and/or your container in which you run scone as well as on the sgx hosts that you are using. Development machine For each SGX host inside of your swarm, you should add a host alias to your ssh configuration. Host alias To reduce your typing overhead, scone assumes that each host has a unique basename and that you configured ssh such that you can log into the host via this basename. For example, instead of typing node2.my.very.long.domain.com , you must configure ssh such that ssh node2 is a shortcut for ssh node2.my.very.long.domain.com . As a caveat, this basename must be sufficient for other hosts in the same swarm to reach node2 . In the above figure, manager must be able to resolve node2 to the IP address of node2.my.very.long.domain.com . By default most swarms are setup this way. On your development machine (or, more precisely in your development container), you need to add an alias node2 for node2.my.very.long.domain.com , you could add the following lines to your ssh config (stored in $HOME/.ssh/config ): Host node2 HostName node2.my.very.long.domain.com Port 22 User scone IdentityFile ~/.ssh/id_rsa ssh connection reuse To be able to reuse a ssh connection, you must configure ssh appropriately. You should set ControlMaster to auto and you have to specify a control path via option ControlPath . You can define a generic path like ~/.ssh/ssh_mux_%h_%p_%r - this can be the same for all hosts. For example, for some host alice you might add the following lines to $HOME/.ssh/config : Host alice ControlMaster auto ControlPath ~/.ssh/ssh_mux_%h_%p_%r user ubuntu port 10101 hostname sshproxy.cloudprovider.com Container Configuration To simplify the ssh setup inside of containers, you might want to map your ssh configuration residing in your home directory into the containers in which you run the scone CLI. Since you probably have a different user ID inside and outside the container, you might want to copy the original ssh configuration: docker run -it -v $HOME /.ssh:/root/.xssh sconecuratedimages/sconecli Inside the container, copy the external ssh configuration: $ cp -rf $HOME /.xssh/* $HOME /.ssh SSH Agent In the container running the scone CLI, you need to start a ssh-agent in the container in which you run the scone CLI: $ SA = $( ssh-agent ) $ eval $SA and add your public key by executing: $ ssh-add Ensure that you are now able to log into all hosts of your swarm. SGX Host Setup scone expects to have password-less access to all SGX hosts that you want to use for Scone. To do so, you need to add one of your public ssh keys to ~/.ssh/authorized_keys on all SGX host. For example, you could add your public key ~/.ssh/id_rsa.pub to file ~/.ssh/authorized_keys on each of these hosts. Some commands are required to be executed with sudo . We assume that the user has the right to perform a password-less sudo on the SGX hosts. In case the user does not yet have this right, the user should be added to /etc/sudoers . Typically, one would add the user with the help of command visudo . The entry for user alice might look like this: alice ALL =( ALL ) NOPASSWD: ALL SSH Credentials In case you do not want to use your standard credentials inside of a container, you need to ensure that you have a pair of authentication keys inside the container. If there exists no public key $HOME/.ssh/id_rsa.pub (often the case if you use a container), you can generate a new pair by executing: $ ssh-keygen -b 4096 -t rsa Append the generated public key HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/authorized_keys on the SGX hosts for which you to be able to log in without a password. Also, ensure that your $HOME/.ssh/config contains an entry for each host of your swarm (see above). scontain.com , March 2018. Questions or Suggestions?","title":"ssh setup"},{"location":"ssh/#ssh-setup","text":"ssh is standard way to log securely into remote hosts. The SCONE CLI requires that you can log into all hosts of your docker swarm(s) without the need for providing a password. We describe in this section how you could set this up.","title":"ssh Setup"},{"location":"ssh/#scone","text":"The scone utility executes commands via ssh on the SGX-capable machine to install software as well as to deploy and monitor containers. Since we potentially execute many ssh commands, you need to configure ssh such that you can log into the SGX machines without having to type a password, you can use the basename of your SGX machines to login, and ssh is permitted to reuse connections to reduce the execution time of the scone commands. To do so, you need to configure ssh on your development machine and/or your container in which you run scone as well as on the sgx hosts that you are using.","title":"scone"},{"location":"ssh/#development-machine","text":"For each SGX host inside of your swarm, you should add a host alias to your ssh configuration.","title":"Development machine"},{"location":"ssh/#host-alias","text":"To reduce your typing overhead, scone assumes that each host has a unique basename and that you configured ssh such that you can log into the host via this basename. For example, instead of typing node2.my.very.long.domain.com , you must configure ssh such that ssh node2 is a shortcut for ssh node2.my.very.long.domain.com . As a caveat, this basename must be sufficient for other hosts in the same swarm to reach node2 . In the above figure, manager must be able to resolve node2 to the IP address of node2.my.very.long.domain.com . By default most swarms are setup this way. On your development machine (or, more precisely in your development container), you need to add an alias node2 for node2.my.very.long.domain.com , you could add the following lines to your ssh config (stored in $HOME/.ssh/config ): Host node2 HostName node2.my.very.long.domain.com Port 22 User scone IdentityFile ~/.ssh/id_rsa","title":"Host alias"},{"location":"ssh/#ssh-connection-reuse","text":"To be able to reuse a ssh connection, you must configure ssh appropriately. You should set ControlMaster to auto and you have to specify a control path via option ControlPath . You can define a generic path like ~/.ssh/ssh_mux_%h_%p_%r - this can be the same for all hosts. For example, for some host alice you might add the following lines to $HOME/.ssh/config : Host alice ControlMaster auto ControlPath ~/.ssh/ssh_mux_%h_%p_%r user ubuntu port 10101 hostname sshproxy.cloudprovider.com","title":"ssh connection reuse"},{"location":"ssh/#container-configuration","text":"To simplify the ssh setup inside of containers, you might want to map your ssh configuration residing in your home directory into the containers in which you run the scone CLI. Since you probably have a different user ID inside and outside the container, you might want to copy the original ssh configuration: docker run -it -v $HOME /.ssh:/root/.xssh sconecuratedimages/sconecli Inside the container, copy the external ssh configuration: $ cp -rf $HOME /.xssh/* $HOME /.ssh","title":"Container Configuration"},{"location":"ssh/#ssh-agent","text":"In the container running the scone CLI, you need to start a ssh-agent in the container in which you run the scone CLI: $ SA = $( ssh-agent ) $ eval $SA and add your public key by executing: $ ssh-add Ensure that you are now able to log into all hosts of your swarm.","title":"SSH Agent"},{"location":"ssh/#sgx-host-setup","text":"scone expects to have password-less access to all SGX hosts that you want to use for Scone. To do so, you need to add one of your public ssh keys to ~/.ssh/authorized_keys on all SGX host. For example, you could add your public key ~/.ssh/id_rsa.pub to file ~/.ssh/authorized_keys on each of these hosts. Some commands are required to be executed with sudo . We assume that the user has the right to perform a password-less sudo on the SGX hosts. In case the user does not yet have this right, the user should be added to /etc/sudoers . Typically, one would add the user with the help of command visudo . The entry for user alice might look like this: alice ALL =( ALL ) NOPASSWD: ALL","title":"SGX Host Setup"},{"location":"ssh/#ssh-credentials","text":"In case you do not want to use your standard credentials inside of a container, you need to ensure that you have a pair of authentication keys inside the container. If there exists no public key $HOME/.ssh/id_rsa.pub (often the case if you use a container), you can generate a new pair by executing: $ ssh-keygen -b 4096 -t rsa Append the generated public key HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/authorized_keys on the SGX hosts for which you to be able to log in without a password. Also, ensure that your $HOME/.ssh/config contains an entry for each host of your swarm (see above). scontain.com , March 2018. Questions or Suggestions?","title":"SSH Credentials"},{"location":"tensorflowlite/","text":"TensorFlow Lite Use Case TensorFlow Lite was designed for on-device machine learning inference with low latency and a small binary size . Hence, TensorFlow Lite is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We will add some more documentation about the curated TensorFlow Lite image later. Until then, you can have a look at our TensorFlow Lite screencast: If you want to evaluate the TensorFlow Lite image, send us an email . scontain.com , June 2018. Questions or Suggestions?","title":"TensorFlow Lite"},{"location":"tensorflowlite/#tensorflow-lite-use-case","text":"TensorFlow Lite was designed for on-device machine learning inference with low latency and a small binary size . Hence, TensorFlow Lite is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We will add some more documentation about the curated TensorFlow Lite image later. Until then, you can have a look at our TensorFlow Lite screencast: If you want to evaluate the TensorFlow Lite image, send us an email . scontain.com , June 2018. Questions or Suggestions?","title":"TensorFlow Lite Use Case"},{"location":"usecases/","text":"SCONE Use Cases Overview In this chapter, we introduce some use cases of how SCONE can be used. Most of these use cases will be based on curated images 1 that users can subscribe to. So far, we have the following use cases demos: PySpark : Apache Spark with Python inside of Intel SGX enclaves with the help of SCONE. TensorFlow Lite : TensorFlow Lite inside of Intel SGX enclaves with the help of SCONE. We will add more use cases over time. scontain.com , June 2018. Questions or Suggestions? A curated image is a container image of a popular service maintained by scontain.com.","title":"Introduction"},{"location":"usecases/#scone-use-cases-overview","text":"In this chapter, we introduce some use cases of how SCONE can be used. Most of these use cases will be based on curated images 1 that users can subscribe to. So far, we have the following use cases demos: PySpark : Apache Spark with Python inside of Intel SGX enclaves with the help of SCONE. TensorFlow Lite : TensorFlow Lite inside of Intel SGX enclaves with the help of SCONE. We will add more use cases over time. scontain.com , June 2018. Questions or Suggestions? A curated image is a container image of a popular service maintained by scontain.com.","title":"SCONE Use Cases Overview"}]}