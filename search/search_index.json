{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Confidential Computing with SCONE Kubernetes Tutorial added (2020-05-12) SCONE supports confidential computing on top of Kubernetes. We explain the basic concepts and provide a deep-dive tutorial to show how to build and run an encrypted Python program in a Kubernetes deployment. This shows also some new features such as policy-based certificate generation and injection . We will add a 2 nd part that will show not only how to simplify this but also provide extra security using the commercial version of SCONE. New Performance Features of SCONE Platform (2020-05-10) SCONE has excellent performance and excellent security. For some applications like TensorFlow Lite and MariaDB it can actually be used as an accelerator , i.e., it runs faster inside SGX enclaves than natively outside (because we provide, e.g., optimized thread management and synchronization). For confidential High Performance Computing (cHPC) we added more CPU affinity features to support the tuning options used in this domain. Introduced semantic versioning for SCONE Platform (2020-04-29) We started to release new verions of the SCONE platform with a fixed period (right now every week). Scone semantic versioning permits clients/users of the SCONE platform to upgrade to new versions in their own speed. Provide a performance monitoring tool (TEEMon) for SGX-based applications (2020-02-02) We develop TEEMon - a real-time performance monitoring and analysis tool for Intel SGX-based applications. We integrate TEEMon with Kubernetes to monitor the performance of an application with 1000s SGX enclaves. Support for most major AI Frameworks and GPU support (2019-11-02) SCONE supports not only TensorFlow and TensorFlow Lite inside SGX enclaves but also most other frameworks like OpenVino and PyTorch and Scikit-Learn etc. You can use in combinations with GPUs if your objective is to protect your models / Python code. Send us an email to learn more. SCONE Executive Summary The SCONE confidential computing platform facilitates always encrypted execution : one can run services and applications such that neither the data nor the code is ever accessible in clear text - not even for root users. Only the application code itself can access the unencrypted data and code . SCONE simplifies the task of encrypting the input, executing the service/application in encrypted memory on an untrusted host, transparently encrypting the output and shipping the output back to the client. SCONE ( Secure CONtainer Environment ) supports the execution of confidential applications inside of containers running inside a Kubernetes cluster ( example ). SCONE also supports the execution of confidential applications inside of VMs (e.g., on top of Windows10 ) as well as directly on a host ( baremetal ). SCONE supports all common programming languages. It also supports air-gapped systems both with SGXv1 as well as SGXv2. The memory size of SCONE-based applications can be up to 32GB on current SGX-capable CPUs. The specifications published by Intel shows that upcoming CPUs will support even larger enclaves and SCONE will - on these CPUs - support applications with basically unlimited memory sizes . SCONE helps to ensure that data, communications, code and the main memory is always encrypted . To do so, SCONE needs to verify (i.e., attest) that the expected application code is running in a trusted execution environment on a potentially untrusted host. Read our secure remote execution tutorial to see how to perform an encrypted remote execution in a single step. In this way, one can even execute encrypted code. We show how to execute encrypted Python scripts in the context of blender , an encrypted wordcount and a hello world program . SCONE can help you to encrypt your input and output data on your local computer. The keys are managed with the help of SCONE CAS (Configuration and Attestation Service). SCONE CAS itself runs, of course, inside an enclave. It can either run on the client side or on a remote host. It can even be operated by an untrusted entity and still be trusted by CAS clients. SCONE supports multiple stakeholders ( confidential multiparty computation ) that do not necessarily trust each other. SCONE supports users, service providers, application providers, data providers and infrastructure providers. They can all work together and SCONE can ensuring that each party can protect its own intellectual property. Some of the services, like SCONE CAS, can be actually operated by not necessarily trusted stakeholders since clients can verify that the services are in the correct state. If you are interested in confidential multi-party computations , we can give you access to a proof of concept that shows how to protect AI models and provide access control to the model, e.g., can only be executed on certain machines and only certain arguments can be provided by the user - depending on a given SCONE policy. Just send us an email .","title":"Executive summary"},{"location":"#confidential-computing-with-scone","text":"Kubernetes Tutorial added (2020-05-12) SCONE supports confidential computing on top of Kubernetes. We explain the basic concepts and provide a deep-dive tutorial to show how to build and run an encrypted Python program in a Kubernetes deployment. This shows also some new features such as policy-based certificate generation and injection . We will add a 2 nd part that will show not only how to simplify this but also provide extra security using the commercial version of SCONE. New Performance Features of SCONE Platform (2020-05-10) SCONE has excellent performance and excellent security. For some applications like TensorFlow Lite and MariaDB it can actually be used as an accelerator , i.e., it runs faster inside SGX enclaves than natively outside (because we provide, e.g., optimized thread management and synchronization). For confidential High Performance Computing (cHPC) we added more CPU affinity features to support the tuning options used in this domain. Introduced semantic versioning for SCONE Platform (2020-04-29) We started to release new verions of the SCONE platform with a fixed period (right now every week). Scone semantic versioning permits clients/users of the SCONE platform to upgrade to new versions in their own speed. Provide a performance monitoring tool (TEEMon) for SGX-based applications (2020-02-02) We develop TEEMon - a real-time performance monitoring and analysis tool for Intel SGX-based applications. We integrate TEEMon with Kubernetes to monitor the performance of an application with 1000s SGX enclaves. Support for most major AI Frameworks and GPU support (2019-11-02) SCONE supports not only TensorFlow and TensorFlow Lite inside SGX enclaves but also most other frameworks like OpenVino and PyTorch and Scikit-Learn etc. You can use in combinations with GPUs if your objective is to protect your models / Python code. Send us an email to learn more.","title":"Confidential Computing with SCONE"},{"location":"#scone-executive-summary","text":"The SCONE confidential computing platform facilitates always encrypted execution : one can run services and applications such that neither the data nor the code is ever accessible in clear text - not even for root users. Only the application code itself can access the unencrypted data and code . SCONE simplifies the task of encrypting the input, executing the service/application in encrypted memory on an untrusted host, transparently encrypting the output and shipping the output back to the client. SCONE ( Secure CONtainer Environment ) supports the execution of confidential applications inside of containers running inside a Kubernetes cluster ( example ). SCONE also supports the execution of confidential applications inside of VMs (e.g., on top of Windows10 ) as well as directly on a host ( baremetal ). SCONE supports all common programming languages. It also supports air-gapped systems both with SGXv1 as well as SGXv2. The memory size of SCONE-based applications can be up to 32GB on current SGX-capable CPUs. The specifications published by Intel shows that upcoming CPUs will support even larger enclaves and SCONE will - on these CPUs - support applications with basically unlimited memory sizes . SCONE helps to ensure that data, communications, code and the main memory is always encrypted . To do so, SCONE needs to verify (i.e., attest) that the expected application code is running in a trusted execution environment on a potentially untrusted host. Read our secure remote execution tutorial to see how to perform an encrypted remote execution in a single step. In this way, one can even execute encrypted code. We show how to execute encrypted Python scripts in the context of blender , an encrypted wordcount and a hello world program . SCONE can help you to encrypt your input and output data on your local computer. The keys are managed with the help of SCONE CAS (Configuration and Attestation Service). SCONE CAS itself runs, of course, inside an enclave. It can either run on the client side or on a remote host. It can even be operated by an untrusted entity and still be trusted by CAS clients. SCONE supports multiple stakeholders ( confidential multiparty computation ) that do not necessarily trust each other. SCONE supports users, service providers, application providers, data providers and infrastructure providers. They can all work together and SCONE can ensuring that each party can protect its own intellectual property. Some of the services, like SCONE CAS, can be actually operated by not necessarily trusted stakeholders since clients can verify that the services are in the correct state. If you are interested in confidential multi-party computations , we can give you access to a proof of concept that shows how to protect AI models and provide access control to the model, e.g., can only be executed on certain machines and only certain arguments can be provided by the user - depending on a given SCONE policy. Just send us an email .","title":"SCONE Executive Summary"},{"location":"C++/","text":"C++ Program Language Support SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation. Cross-compilation is required to support, in particular, statically linked binaries. This page focuses on the SCONE C++ cross compiler scone g++ (a.k.a. scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++. Image Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Help If you need some help, just execute in the container: $ scone g++ --help Usage: x86_64-linux-musl-g++ [ options ] file... Options: ... Example Let's try to compile a simple program: cat > sqrt.cc << EOF #include <iostream> #include <cmath> using namespace std; int main() { int x = 0; while(x < 10) { double y = sqrt((double)x); cout << \"The square root of \" << x << \" is \" << y << endl; x++; } return 0; } EOF We compile the program with scone gcc or scone-gcc : scone g++ sqrt.cc -o sqrt Let's execute the binary and switch on debug outputs: SCONE_VERSION = 1 ./sqrt The output will look like: xport SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: ebf98279a2cae1179366f8b5a0fc007decdc5dd3dec2b92ddbf121c2e2bf22f4 The square root of 0 is 0 The square root of 1 is 1 The square root of 2 is 1.41421 The square root of 3 is 1.73205 The square root of 4 is 2 The square root of 5 is 2.23607 The square root of 6 is 2.44949 The square root of 7 is 2.64575 The square root of 8 is 2.82843 The square root of 9 is 3 Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. Screencast","title":"C++"},{"location":"C++/#c-program-language-support","text":"SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation. Cross-compilation is required to support, in particular, statically linked binaries. This page focuses on the SCONE C++ cross compiler scone g++ (a.k.a. scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++.","title":"C++ Program Language Support"},{"location":"C++/#image","text":"Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Image"},{"location":"C++/#help","text":"If you need some help, just execute in the container: $ scone g++ --help Usage: x86_64-linux-musl-g++ [ options ] file... Options: ...","title":"Help"},{"location":"C++/#example","text":"Let's try to compile a simple program: cat > sqrt.cc << EOF #include <iostream> #include <cmath> using namespace std; int main() { int x = 0; while(x < 10) { double y = sqrt((double)x); cout << \"The square root of \" << x << \" is \" << y << endl; x++; } return 0; } EOF We compile the program with scone gcc or scone-gcc : scone g++ sqrt.cc -o sqrt Let's execute the binary and switch on debug outputs: SCONE_VERSION = 1 ./sqrt The output will look like: xport SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: ebf98279a2cae1179366f8b5a0fc007decdc5dd3dec2b92ddbf121c2e2bf22f4 The square root of 0 is 0 The square root of 1 is 1 The square root of 2 is 1.41421 The square root of 3 is 1.73205 The square root of 4 is 2 The square root of 5 is 2.23607 The square root of 6 is 2.44949 The square root of 7 is 2.64575 The square root of 8 is 2.82843 The square root of 9 is 3","title":"Example"},{"location":"C++/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs.","title":"Debugging"},{"location":"C++/#screencast","text":"","title":"Screencast"},{"location":"C/","text":"C Program Language Support SCONE supports native compilation combined with dynamic linking as well as cross-compilation with static as well as dynamic linking. This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc. Image Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Help If you need some help, just execute in the container: $ scone gcc --help Usage: x86_64-linux-musl-gcc [ options ] file... Options: ... Example Let's try to compile a simple program: cat > fib.c << EOF #include <stdio.h> #include <stdlib.h> int main(int argc, char** argv) { int n=0, first = 0, second = 1, next = 0, c; if (argc > 1) n=atoi(argv[1]); printf(\"fib(%d)= 1\",n); for ( c = 1 ; c < n ; c++ ) { next = first + second; first = second; second = next; printf(\", %d\",next); } printf(\"\\n\"); } EOF We compile the program with scone gcc or scone-gcc or just gcc (all equivalent): scone gcc fib.c -o fib To compute fib(23), execute: SCONE_VERSION = 1 ./fib 23 The last line of the output should look as follows: fib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657 Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. Screencast","title":"C"},{"location":"C/#c-program-language-support","text":"SCONE supports native compilation combined with dynamic linking as well as cross-compilation with static as well as dynamic linking. This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc.","title":"C Program Language Support"},{"location":"C/#image","text":"Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Image"},{"location":"C/#help","text":"If you need some help, just execute in the container: $ scone gcc --help Usage: x86_64-linux-musl-gcc [ options ] file... Options: ...","title":"Help"},{"location":"C/#example","text":"Let's try to compile a simple program: cat > fib.c << EOF #include <stdio.h> #include <stdlib.h> int main(int argc, char** argv) { int n=0, first = 0, second = 1, next = 0, c; if (argc > 1) n=atoi(argv[1]); printf(\"fib(%d)= 1\",n); for ( c = 1 ; c < n ; c++ ) { next = first + second; first = second; second = next; printf(\", %d\",next); } printf(\"\\n\"); } EOF We compile the program with scone gcc or scone-gcc or just gcc (all equivalent): scone gcc fib.c -o fib To compute fib(23), execute: SCONE_VERSION = 1 ./fib 23 The last line of the output should look as follows: fib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657","title":"Example"},{"location":"C/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs.","title":"Debugging"},{"location":"C/#screencast","text":"","title":"Screencast"},{"location":"CAS/","text":"SOMETHING WENT WRONG if you can see this, something went wrong... you should see something different!","title":"SOMETHING WENT WRONG"},{"location":"CAS/#something-went-wrong","text":"if you can see this, something went wrong... you should see something different!","title":"SOMETHING WENT WRONG"},{"location":"CASConfiguration/","text":"Configuration CAS's configuration is read from a TOML -formatted file. CAS, as most services, needs to be told how to behave with a configuration. This configuration is under the control of the CAS owner. Thus, CAS users are protected from malicious intend of the owner. Note Any root CAS has its initial configuration being part of its MrEnclave. This configuration is encrytped during the first startup with the secret seal key of CAS. In other words, attesting a root CAS includes attesting that the CAS was initialized with the expected configuration file. Database CAS stores its data in a SQLite database. The location of which is determined in by the path option of the configuration in the database section: [database] path = \"/etc/cas/database.sqlite\" Relative paths are interpreted relative to the configuration file's location. [database] path = \"database.sqlite\" The special :memory: option can be used to use a transient in-memory database that will be removed once the CAS process terminates. [database] path = \":memory:\" Note During attestation we need to attest a root CAS which is guaranteed to have an encrypted database, runs in a production enclave and checks its configuration file during attestation. Interfaces At the moment, CAS offers two interfaces: a client API with which clients can upload, upgrade and query sessions, and an enclave interface with which enclaves communicate with CAS. These configuration options will determine on which network interfaces and ports CAS will listen for incoming connection. Interfaces are defined with values satisfying std::net::SocketAddr 's FromStr implementation . The client API is configured via the api_listen option while the enclave interface is configured through the enclave_listen option: [api] enclave_listen = \"0.0.0.0:18765\" api_listen = \"0.0.0.0:8080\" Note, a configuration with an IP address of 0.0.0.0 will make the CAS listen on any available network interface. IAS Client The IAS client enables CAS to verify enclave EPID quotes using Intel SGX Attestation Service . Its configuration consists of the Service Provider ID (SPID), linkability setting and the TLS identity used to authenticated to the IAS service. spid has to be a 32 character or 16 byte long hex string. linkable_quotes is either true or false . api_url is the url endpoint at which the client will query Intel's attestation service. This option is optional and will point to https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3 by default. You have to adapt this setting if you use an IAS production identity. The TLS identity is configured through the ias.identity option. Refer to the section about TLS identities for details on how to specify it. [ias] spid = \"2EC5BE64E242BCC9E4670D49E5C7091E\" linkable_quotes = true api_url = \"https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3\" [ias.identity] cert = \"ias_client.pem\" key = \"ias_client.key\" Security: The confidentiality of the IAS client configuration, in particular of the private key, is of concern for the CAS owner/operator. Anybody obtaining the private key can impersonate as the CAS owner/operator to Intel's service. CAS users do not have to trust this configuration. Intel's service signs responses. Attestation There are a couple of attestation related configuration options defined in the attestation dictionary. Security: These options are security sensitive as they define how trust is established into enclaves. As these are configured via a file in the file system which is not authenticated by default the trusted computing base of the CAS users is at the mercy of the CAS owner/operator which himself is at the mercy of the platform provider! Note These options are security sensitive as they define how trust is established into enclaves. As these are configured via the config file in the file system, the config file must be part of the attestation. Policy The attestation policy determines by which method enclave are attested. You can choose from without , intel , and scone . Policy Description Without Connecting enclaves are not attested, thus any connection is trusted. Intel Connecting enclaves are attested using Intel's attestation scheme. Scone Connecting enclaves are attested using SCONE's optimized attestation scheme decreasing attestation latency. Scone attestation policy is recommended. attestation.policy = \"scone\" Trusting Imperfect Platforms By default only enclaves running on perfectly trustworthy platforms will be trusted by CAS. However, maintaining a platform perfectly trustworthy 24/7 is a high burden or might not be desired due to the involved performance penalty. Thus, CAS can be configured to trust platforms with minor imperfections enabling a user defined trade-off. This options are only regarded if the attestation policy is either scone or intel . Outdated Platforms An update has been released for at least one component of the platforms trust stack which is not installed. The platform has not been identified as compromised and thus is not revoked, but depending on the nature of the update the platform's trust state might be at risk. Note that, due to slow or discontinued support from its manufacturer and/or Intel some platforms might be only be updatable with considerable delay or even impossible to update to the most recent state. Eventually, this is a trade off between security and availability of the system. Setting attestation.trust_group_out_of_date to true will trust the affected platforms, while setting it to false will do the contrary attestation.trust_group_out_of_date = true Imperfect Configured Platforms Platforms with enabled Hyper-Threading allow certain types of side-channel attacks to be mounted against enclaves. This can be prevented by turning of Hyper-Threading in the BIOS which might introduce performance penalities. CAS can be configured to trust platforms with Hyper-Threading by setting attestation.trust_configuration_needed to true . attestation.trust_configuration_needed = false SCONE Quoting Enclave Configuration SCONE's attestation scheme is built around a separate quoting enclave into which trust has to be established. This trust can either be established using Intel's attestation scheme or manually. To establish trust using Intel's attestation scheme the expected SCONE quoting enclaves' measurement value has to be specified. CAS will use its IAS credentials to establish the necessary trust. # List of trusted MRENCLAVE hashes for SCONE attestation. # When CAS receives a SCONE quote signed with an unknown public key it will use # Intel's attestation mechanism to attest the SCONE QE learning its MRENCLAVE # hash. Only enclaves with hash contained in this list will be trusted. # The list has to contain string with the hex-encoded hash values. The values # shall be 32 bytes in size. # If this list is empty SCONE attestation will only accept quotes from # whitelisted public keys. scone_qe_mrenclave_whitelist = [ # quoting enclave built with sdk v2.5 (debug mode) w/o HT \"C2CA1D8166C45EB6063764413CDFFA8D9BBAB44CE6CDA6837000D703368F338B\" ] Alternatively, individual SCONE Quoting Enclaves can be configured to be trustworthy. [attestation] # List of trusted public keys for SCONE attestation. # SCONE quotes signed with one of this public keys are always trusted by CAS. # The list has to contain string with the hex-encoded public keys. The keys # shall be 32 bytes in size. scone_qe_pubkey_whitelist = [ \"E37F149AE30896E314A2859874C5A9C7803FB3187B99F5D08E526B1C0396507C\" ] Both the quoting enclave's platform-independent measurement value, as well as, its platform-specific public key can be found in LAS' log output: [ 10000 :INFO@21.06.2019/20:39:15 ] APP: Creating LAS target information message. [ 10000 :INFO@21.06.2019/20:39:16 ] STARTER: SCONE QE has MRENCLAVE C2CA1D8166C45EB6063764413CDFFA8D9BBAB44CE6CDA6837000D703368F338B [ 10000 :INFO@21.06.2019/20:39:16 ] STARTER: SCONE QE has public key E37F149AE30896E314A2859874C5A9C7803FB3187B99F5D08E526B1C0396507C [ 10000 :INFO@21.06.2019/20:39:16 ] STARTER: LAS is listening on 0 .0.0.0:18766 TLS Identity A TLS identity consists of a certificate and the associated private key. Certificate and private key have to be provided as file system paths where CAS can open and read the certificate resp. private key. CAS will ensure that certificate and private key belong together. cert = \"certificate.pem\" key = \"private_key.pem\" While this is easy to set up and modify, this has major disadvantages like: this certificate is disconnected from the CAS certificate chain, and the key is not protected at all. The production CAS generates the TLS certificate/key on-the-fly, signing it by CAS (instance) CA certificate, and storing it in the palaemon.db . The palaemon.db is protected as we show above. Private Keys We support RSA and PKCS8 encoded private keys. By default, openssl will encode elliptic curve keys in the incompatible format specified in SEC 1: Elliptic Curve Cryptography . You can use the openssl cli to convert these keys into PKCS8 : openssl pkcs8 -in pkey.pem -topk8 --nocrypt -out pkey.pkcs8.pem","title":"Configuration"},{"location":"CASConfiguration/#configuration","text":"CAS's configuration is read from a TOML -formatted file. CAS, as most services, needs to be told how to behave with a configuration. This configuration is under the control of the CAS owner. Thus, CAS users are protected from malicious intend of the owner. Note Any root CAS has its initial configuration being part of its MrEnclave. This configuration is encrytped during the first startup with the secret seal key of CAS. In other words, attesting a root CAS includes attesting that the CAS was initialized with the expected configuration file.","title":"Configuration"},{"location":"CASConfiguration/#database","text":"CAS stores its data in a SQLite database. The location of which is determined in by the path option of the configuration in the database section: [database] path = \"/etc/cas/database.sqlite\" Relative paths are interpreted relative to the configuration file's location. [database] path = \"database.sqlite\" The special :memory: option can be used to use a transient in-memory database that will be removed once the CAS process terminates. [database] path = \":memory:\" Note During attestation we need to attest a root CAS which is guaranteed to have an encrypted database, runs in a production enclave and checks its configuration file during attestation.","title":"Database"},{"location":"CASConfiguration/#interfaces","text":"At the moment, CAS offers two interfaces: a client API with which clients can upload, upgrade and query sessions, and an enclave interface with which enclaves communicate with CAS. These configuration options will determine on which network interfaces and ports CAS will listen for incoming connection. Interfaces are defined with values satisfying std::net::SocketAddr 's FromStr implementation . The client API is configured via the api_listen option while the enclave interface is configured through the enclave_listen option: [api] enclave_listen = \"0.0.0.0:18765\" api_listen = \"0.0.0.0:8080\" Note, a configuration with an IP address of 0.0.0.0 will make the CAS listen on any available network interface.","title":"Interfaces"},{"location":"CASConfiguration/#ias-client","text":"The IAS client enables CAS to verify enclave EPID quotes using Intel SGX Attestation Service . Its configuration consists of the Service Provider ID (SPID), linkability setting and the TLS identity used to authenticated to the IAS service. spid has to be a 32 character or 16 byte long hex string. linkable_quotes is either true or false . api_url is the url endpoint at which the client will query Intel's attestation service. This option is optional and will point to https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3 by default. You have to adapt this setting if you use an IAS production identity. The TLS identity is configured through the ias.identity option. Refer to the section about TLS identities for details on how to specify it. [ias] spid = \"2EC5BE64E242BCC9E4670D49E5C7091E\" linkable_quotes = true api_url = \"https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3\" [ias.identity] cert = \"ias_client.pem\" key = \"ias_client.key\" Security: The confidentiality of the IAS client configuration, in particular of the private key, is of concern for the CAS owner/operator. Anybody obtaining the private key can impersonate as the CAS owner/operator to Intel's service. CAS users do not have to trust this configuration. Intel's service signs responses.","title":"IAS Client"},{"location":"CASConfiguration/#attestation","text":"There are a couple of attestation related configuration options defined in the attestation dictionary. Security: These options are security sensitive as they define how trust is established into enclaves. As these are configured via a file in the file system which is not authenticated by default the trusted computing base of the CAS users is at the mercy of the CAS owner/operator which himself is at the mercy of the platform provider! Note These options are security sensitive as they define how trust is established into enclaves. As these are configured via the config file in the file system, the config file must be part of the attestation.","title":"Attestation"},{"location":"CASConfiguration/#policy","text":"The attestation policy determines by which method enclave are attested. You can choose from without , intel , and scone . Policy Description Without Connecting enclaves are not attested, thus any connection is trusted. Intel Connecting enclaves are attested using Intel's attestation scheme. Scone Connecting enclaves are attested using SCONE's optimized attestation scheme decreasing attestation latency. Scone attestation policy is recommended. attestation.policy = \"scone\"","title":"Policy"},{"location":"CASConfiguration/#trusting-imperfect-platforms","text":"By default only enclaves running on perfectly trustworthy platforms will be trusted by CAS. However, maintaining a platform perfectly trustworthy 24/7 is a high burden or might not be desired due to the involved performance penalty. Thus, CAS can be configured to trust platforms with minor imperfections enabling a user defined trade-off. This options are only regarded if the attestation policy is either scone or intel .","title":"Trusting Imperfect Platforms"},{"location":"CASConfiguration/#outdated-platforms","text":"An update has been released for at least one component of the platforms trust stack which is not installed. The platform has not been identified as compromised and thus is not revoked, but depending on the nature of the update the platform's trust state might be at risk. Note that, due to slow or discontinued support from its manufacturer and/or Intel some platforms might be only be updatable with considerable delay or even impossible to update to the most recent state. Eventually, this is a trade off between security and availability of the system. Setting attestation.trust_group_out_of_date to true will trust the affected platforms, while setting it to false will do the contrary attestation.trust_group_out_of_date = true","title":"Outdated Platforms"},{"location":"CASConfiguration/#imperfect-configured-platforms","text":"Platforms with enabled Hyper-Threading allow certain types of side-channel attacks to be mounted against enclaves. This can be prevented by turning of Hyper-Threading in the BIOS which might introduce performance penalities. CAS can be configured to trust platforms with Hyper-Threading by setting attestation.trust_configuration_needed to true . attestation.trust_configuration_needed = false","title":"Imperfect Configured Platforms"},{"location":"CASConfiguration/#scone-quoting-enclave-configuration","text":"SCONE's attestation scheme is built around a separate quoting enclave into which trust has to be established. This trust can either be established using Intel's attestation scheme or manually. To establish trust using Intel's attestation scheme the expected SCONE quoting enclaves' measurement value has to be specified. CAS will use its IAS credentials to establish the necessary trust. # List of trusted MRENCLAVE hashes for SCONE attestation. # When CAS receives a SCONE quote signed with an unknown public key it will use # Intel's attestation mechanism to attest the SCONE QE learning its MRENCLAVE # hash. Only enclaves with hash contained in this list will be trusted. # The list has to contain string with the hex-encoded hash values. The values # shall be 32 bytes in size. # If this list is empty SCONE attestation will only accept quotes from # whitelisted public keys. scone_qe_mrenclave_whitelist = [ # quoting enclave built with sdk v2.5 (debug mode) w/o HT \"C2CA1D8166C45EB6063764413CDFFA8D9BBAB44CE6CDA6837000D703368F338B\" ] Alternatively, individual SCONE Quoting Enclaves can be configured to be trustworthy. [attestation] # List of trusted public keys for SCONE attestation. # SCONE quotes signed with one of this public keys are always trusted by CAS. # The list has to contain string with the hex-encoded public keys. The keys # shall be 32 bytes in size. scone_qe_pubkey_whitelist = [ \"E37F149AE30896E314A2859874C5A9C7803FB3187B99F5D08E526B1C0396507C\" ] Both the quoting enclave's platform-independent measurement value, as well as, its platform-specific public key can be found in LAS' log output: [ 10000 :INFO@21.06.2019/20:39:15 ] APP: Creating LAS target information message. [ 10000 :INFO@21.06.2019/20:39:16 ] STARTER: SCONE QE has MRENCLAVE C2CA1D8166C45EB6063764413CDFFA8D9BBAB44CE6CDA6837000D703368F338B [ 10000 :INFO@21.06.2019/20:39:16 ] STARTER: SCONE QE has public key E37F149AE30896E314A2859874C5A9C7803FB3187B99F5D08E526B1C0396507C [ 10000 :INFO@21.06.2019/20:39:16 ] STARTER: LAS is listening on 0 .0.0.0:18766","title":"SCONE Quoting Enclave Configuration"},{"location":"CASConfiguration/#tls-identity","text":"A TLS identity consists of a certificate and the associated private key. Certificate and private key have to be provided as file system paths where CAS can open and read the certificate resp. private key. CAS will ensure that certificate and private key belong together. cert = \"certificate.pem\" key = \"private_key.pem\" While this is easy to set up and modify, this has major disadvantages like: this certificate is disconnected from the CAS certificate chain, and the key is not protected at all. The production CAS generates the TLS certificate/key on-the-fly, signing it by CAS (instance) CA certificate, and storing it in the palaemon.db . The palaemon.db is protected as we show above.","title":"TLS Identity"},{"location":"CASConfiguration/#private-keys","text":"We support RSA and PKCS8 encoded private keys. By default, openssl will encode elliptic curve keys in the incompatible format specified in SEC 1: Elliptic Curve Cryptography . You can use the openssl cli to convert these keys into PKCS8 : openssl pkcs8 -in pkey.pem -topk8 --nocrypt -out pkey.pkcs8.pem","title":"Private Keys"},{"location":"CASOverview/","text":"SCONE Configuration and Attestation Service (CAS) SCONE CAS manages the secrets - in particular, the keys - of an application. The application is in complete control of the secrets: only services given explicit permission by the application's policy get access to keys, encrypted data, encrypted code and policies. Key generation . SCONE CAS can generate keys on behalf of an application. The generation is performed inside of a trusted execution environment . Access to keys is controlled by a security policy controlled by the application. Neither root users nor SCONE CAS admins can access the keys nor the security policies. So far, SCONE CAS runs inside of SGX enclave s. Isolation . Users can run their own instances of SCONE CAS , i.e., one can isolate the secrets of different users and the secrets of different applications. Secure key and configuration provisioning without the need to change the source code of applications: secrets, keys, and configuration parameters are securely provisioned via command line arguments, environment variables and via transparently encrypted files. Access control . To modify or read a policy, a client needs to prove, via TLS, that it knows the private key belonging to a public key specified in the policy. SCONE CAS grants - without any exception - only such clients access to this policy. The client's access to a private key is typically also controlled by a policy - possibly, even the same policy. Note that only after a successful attestation, will a client can get access to its private keys. Management . The management of SCONE CAS can be delegated to a third party. The confidentiality and integrity of the policies and their secrets are ensured by CAS itself. Since the entity creating a policy has complete control over who can read or modify this policy, no admin managing SCONE CAS can overwrite the application's access control to a policy. SCONE CAS supports peer-to-peer based attestation of services operated by mutually distrusting peers. Encrypted Code . One can create images with encrypted Python code or Java or JavaScript or C# or any other JIT or interpreted code on a trusted host. Alternatively, this code could als be generated inside of an enclave. One can transparently attest and decrypt the code inside of an enclave . This can be done without the need to change the Python engine or the Java/... virtual machine. Note that SCONE CAS attests both the Python engine as well as the Python code.","title":"Overview"},{"location":"CASOverview/#scone-configuration-and-attestation-service-cas","text":"SCONE CAS manages the secrets - in particular, the keys - of an application. The application is in complete control of the secrets: only services given explicit permission by the application's policy get access to keys, encrypted data, encrypted code and policies. Key generation . SCONE CAS can generate keys on behalf of an application. The generation is performed inside of a trusted execution environment . Access to keys is controlled by a security policy controlled by the application. Neither root users nor SCONE CAS admins can access the keys nor the security policies. So far, SCONE CAS runs inside of SGX enclave s. Isolation . Users can run their own instances of SCONE CAS , i.e., one can isolate the secrets of different users and the secrets of different applications. Secure key and configuration provisioning without the need to change the source code of applications: secrets, keys, and configuration parameters are securely provisioned via command line arguments, environment variables and via transparently encrypted files. Access control . To modify or read a policy, a client needs to prove, via TLS, that it knows the private key belonging to a public key specified in the policy. SCONE CAS grants - without any exception - only such clients access to this policy. The client's access to a private key is typically also controlled by a policy - possibly, even the same policy. Note that only after a successful attestation, will a client can get access to its private keys. Management . The management of SCONE CAS can be delegated to a third party. The confidentiality and integrity of the policies and their secrets are ensured by CAS itself. Since the entity creating a policy has complete control over who can read or modify this policy, no admin managing SCONE CAS can overwrite the application's access control to a policy. SCONE CAS supports peer-to-peer based attestation of services operated by mutually distrusting peers. Encrypted Code . One can create images with encrypted Python code or Java or JavaScript or C# or any other JIT or interpreted code on a trusted host. Alternatively, this code could als be generated inside of an enclave. One can transparently attest and decrypt the code inside of an enclave . This can be done without the need to change the Python engine or the Java/... virtual machine. Note that SCONE CAS attests both the Python engine as well as the Python code.","title":"SCONE Configuration and Attestation Service (CAS)"},{"location":"CAS_for_development/","text":"CAS for Development We explain how to start a SCONE CAS instance for development on your local machine. This CAS instance runs inside a debug enclave, i.e., do not use this in production . For setting up a production mode CAS, send us an email. For running CAS in a Kubernetes Cluster, please set up CAS with helm . Public CAS for development We have setup a CAS instance for testing and development, which is available for general use at scone-cas.cf . This instance runs in pre-release mode, i.e., do not use this instance for production . Pulling CAS Image To start CAS, you first pull CAS Docker image to your local registry. To be able to do so, please ask us via email for access to CAS. We will give you access to a private Docker repository and we will send you the name of the private repository. For this tutorial, please set the environment variable CAS to the name of the Docker repository - typically this might look something like this: export CAS = sconecuratedimages/services:cas Pull the CAS image like this: docker pull $CAS If this fails, ensure that you are logged into docker (via docker login ) and that you set environment variable CAS properly. Starting and Stopping CAS The easiest way to start CAS is to use a simple Docker compose file. To do so, create a new directory for the Docker compose file: mkdir -p CAS cd CAS Create a compose file that exposes the ports of CAS to the host: cat > docker-compose.yml <<EOF version: '3.2' services: cas: command: sh -c \"SCONE_HEAP=1G cas -c /etc/cas/cas.toml\" environment: - SCONE_LOG=7 - SCONE_MODE=HW image: $CAS devices: - \"/dev/isgx\" ports: - target: 8081 published: 8081 protocol: tcp mode: host - target: 18765 published: 18765 protocol: tcp mode: host EOF Now start CAS in the background as follows: docker-compose up -d cas By executing docker-compose logs cas you will see the output of CAS. You can check if CAS is still running by executing: docker-compose up -d cas This will result in an output like cas_cas_1 is up-to-date You can stop CAS by executing: docker-compose stop cas","title":"Starting CAS"},{"location":"CAS_for_development/#cas-for-development","text":"We explain how to start a SCONE CAS instance for development on your local machine. This CAS instance runs inside a debug enclave, i.e., do not use this in production . For setting up a production mode CAS, send us an email. For running CAS in a Kubernetes Cluster, please set up CAS with helm . Public CAS for development We have setup a CAS instance for testing and development, which is available for general use at scone-cas.cf . This instance runs in pre-release mode, i.e., do not use this instance for production .","title":"CAS for Development"},{"location":"CAS_for_development/#pulling-cas-image","text":"To start CAS, you first pull CAS Docker image to your local registry. To be able to do so, please ask us via email for access to CAS. We will give you access to a private Docker repository and we will send you the name of the private repository. For this tutorial, please set the environment variable CAS to the name of the Docker repository - typically this might look something like this: export CAS = sconecuratedimages/services:cas Pull the CAS image like this: docker pull $CAS If this fails, ensure that you are logged into docker (via docker login ) and that you set environment variable CAS properly.","title":"Pulling CAS Image"},{"location":"CAS_for_development/#starting-and-stopping-cas","text":"The easiest way to start CAS is to use a simple Docker compose file. To do so, create a new directory for the Docker compose file: mkdir -p CAS cd CAS Create a compose file that exposes the ports of CAS to the host: cat > docker-compose.yml <<EOF version: '3.2' services: cas: command: sh -c \"SCONE_HEAP=1G cas -c /etc/cas/cas.toml\" environment: - SCONE_LOG=7 - SCONE_MODE=HW image: $CAS devices: - \"/dev/isgx\" ports: - target: 8081 published: 8081 protocol: tcp mode: host - target: 18765 published: 18765 protocol: tcp mode: host EOF Now start CAS in the background as follows: docker-compose up -d cas By executing docker-compose logs cas you will see the output of CAS. You can check if CAS is still running by executing: docker-compose up -d cas This will result in an output like cas_cas_1 is up-to-date You can stop CAS by executing: docker-compose stop cas","title":"Starting and Stopping CAS"},{"location":"EncryptedWordCount/","text":"Encrypted Python Programs and Encrypted Input With the help of a simple wordcount Python program, we show how to execute encrypted Python code inside of an SGX enclave. In this example, we also show how to encrypt an input file. The Python engine itself also runs inside the same enclave. We put both the Python code as well as the input file of the wordcount in the same encrypted filesystem. Typically, we would put the Python code in the encrypted filesystem of the image and the encrypted input and output files in one or more encrypted volumes mapped into the container. This Python image that we use is not intended for production usage The Python libraries in the Python base image **are not encrypted* . Moreover, the Python engine runs inside of a debug enclave. Contact us , if you need a production-ready Python engine with encrypted Python libraries.* You require access to the images This demo uses private docker and github repos. To get access to these repos, please send us an email . Getting the Code After you gotten access to the repos, you can get the code via: git clone https://github.com/scontain/EncryptedWordCount.git and then enter directory EncryptedWordCount cd EncryptedWordCount Files The Python source code and the input file is stored in directory native-files/ . The wordcount.py code is just some very simple Python code to count the frequency of words in a given file: #!/usr/bin/python import sys file = open ( sys . argv [ 1 ], \"r+\" ) wordcount = {} for word in file . read () . split (): if word not in wordcount : wordcount [ word ] = 1 else : wordcount [ word ] += 1 for k , v in wordcount . items (): print ( k , v ) Creating Image with Encrypted wordcount The repository includes a script create_image.sh that encrypts the files in directory native-files/ : the files are integrity , confidentiality and rollback protected attests that CAS is a proper CAS running inside an enclave and then stores the TLS certificate of the CAS in the local file system generates a docker image containing a Python engine which runs inside of an enclave pushes a security policy to CAS via TLS (checking the certificate of CAS) that includes (a) the encryption key of the encrypted files, and (b) limits access to the generated image access to the security policy is controlled via a self-generated key pair Perform all of the above steps by executing the following shell script: ./create_image.sh This creates an image encryptedwordcount stored in the local registry and a session in a SCONE Configuration and Attestation Service (CAS) running on a remote site. We assume in this demo that the creation of the wordcount image is performed on a trusted host. The execution of the wordcount can be performed on an untrusted host. For simplicity, we execute on the same host. Running the wordcount image First, read some environment variables set by ./create_image.sh source myenv and then start the wordcount.py with docker-compose : docker-compose up You will see quite some log output ending with ... python_1 | scone 1 python_1 | region. 3 python_1 | Hence, 1 python_1 | {ephemeral 1 python_1 | without 1 python_1 | command 1 python_1 | File 1 python_1 | mechanisms. 1 python_1 | region: 1 python_1 | the 19 python_1 | path. 1 python_1 | document. 1 encryptedwordcount_python_1 exited with code 0 Stop the compose file with control-C. You can run it again by executing docker-compose up again. Cleanup Cleanup afterwards by executing: ./cleanup.sh before starting it with ./create_image.sh and docker-compose up again. Note: Please read Hello World tutorial to learn more about the technical details.","title":"Encrypted Code and Input"},{"location":"EncryptedWordCount/#encrypted-python-programs-and-encrypted-input","text":"With the help of a simple wordcount Python program, we show how to execute encrypted Python code inside of an SGX enclave. In this example, we also show how to encrypt an input file. The Python engine itself also runs inside the same enclave. We put both the Python code as well as the input file of the wordcount in the same encrypted filesystem. Typically, we would put the Python code in the encrypted filesystem of the image and the encrypted input and output files in one or more encrypted volumes mapped into the container. This Python image that we use is not intended for production usage The Python libraries in the Python base image **are not encrypted* . Moreover, the Python engine runs inside of a debug enclave. Contact us , if you need a production-ready Python engine with encrypted Python libraries.* You require access to the images This demo uses private docker and github repos. To get access to these repos, please send us an email .","title":"Encrypted Python Programs and Encrypted Input"},{"location":"EncryptedWordCount/#getting-the-code","text":"After you gotten access to the repos, you can get the code via: git clone https://github.com/scontain/EncryptedWordCount.git and then enter directory EncryptedWordCount cd EncryptedWordCount","title":"Getting the Code"},{"location":"EncryptedWordCount/#files","text":"The Python source code and the input file is stored in directory native-files/ . The wordcount.py code is just some very simple Python code to count the frequency of words in a given file: #!/usr/bin/python import sys file = open ( sys . argv [ 1 ], \"r+\" ) wordcount = {} for word in file . read () . split (): if word not in wordcount : wordcount [ word ] = 1 else : wordcount [ word ] += 1 for k , v in wordcount . items (): print ( k , v )","title":"Files"},{"location":"EncryptedWordCount/#creating-image-with-encrypted-wordcount","text":"The repository includes a script create_image.sh that encrypts the files in directory native-files/ : the files are integrity , confidentiality and rollback protected attests that CAS is a proper CAS running inside an enclave and then stores the TLS certificate of the CAS in the local file system generates a docker image containing a Python engine which runs inside of an enclave pushes a security policy to CAS via TLS (checking the certificate of CAS) that includes (a) the encryption key of the encrypted files, and (b) limits access to the generated image access to the security policy is controlled via a self-generated key pair Perform all of the above steps by executing the following shell script: ./create_image.sh This creates an image encryptedwordcount stored in the local registry and a session in a SCONE Configuration and Attestation Service (CAS) running on a remote site. We assume in this demo that the creation of the wordcount image is performed on a trusted host. The execution of the wordcount can be performed on an untrusted host. For simplicity, we execute on the same host.","title":"Creating Image with Encrypted wordcount"},{"location":"EncryptedWordCount/#running-the-wordcount-image","text":"First, read some environment variables set by ./create_image.sh source myenv and then start the wordcount.py with docker-compose : docker-compose up You will see quite some log output ending with ... python_1 | scone 1 python_1 | region. 3 python_1 | Hence, 1 python_1 | {ephemeral 1 python_1 | without 1 python_1 | command 1 python_1 | File 1 python_1 | mechanisms. 1 python_1 | region: 1 python_1 | the 19 python_1 | path. 1 python_1 | document. 1 encryptedwordcount_python_1 exited with code 0 Stop the compose file with control-C. You can run it again by executing docker-compose up again.","title":"Running the wordcount image"},{"location":"EncryptedWordCount/#cleanup","text":"Cleanup afterwards by executing: ./cleanup.sh before starting it with ./create_image.sh and docker-compose up again. Note: Please read Hello World tutorial to learn more about the technical details.","title":"Cleanup"},{"location":"Fortran/","text":"Fortran SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler scone gfortran (a.k.a. scone-gfortran , a.k.a. gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran. Image Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Help If you need some help, just execute in the container: $ scone gfortran --help Usage: x86_64-linux-musl-gfortran [ options ] file... Options: ... Let's try a simple Fortran program. cat > gcd.f << EOF * euclid.f (FORTRAN 77) * Find greatest common divisor using the Euclidean algorithm PROGRAM EUCLID PRINT *, 'A?' READ *, NA IF (NA.LE.0) THEN PRINT *, 'A must be a positive integer.' STOP END IF PRINT *, 'B?' READ *, NB IF (NB.LE.0) THEN PRINT *, 'B must be a positive integer.' STOP END IF PRINT *, 'The GCD of', NA, ' and', NB, ' is', NGCD(NA, NB), '.' STOP END FUNCTION NGCD(NA, NB) IA = NA IB = NB 1 IF (IB.NE.0) THEN ITEMP = IA IA = IB IB = MOD(ITEMP, IB) GOTO 1 END IF NGCD = IA RETURN END EOF We compile the program with scone gfortran (a.k.a. scone-gfortran ): scone gfortran gcd.f -o gcd We can now run this program as follows: SCONE_VERSION = 1 ./gcd << EOF 10 15 EOF The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: e9c60984724c6d5ffc8cc8a6ba4377910e63c8534ef24b87d0727e712809ba50 A? B? The GCD of 10 and 15 is 5 . Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. Screencast","title":"Fortran"},{"location":"Fortran/#fortran","text":"SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler scone gfortran (a.k.a. scone-gfortran , a.k.a. gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran.","title":"Fortran"},{"location":"Fortran/#image","text":"Ensure that you have the newest SCONE cross compiler image: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Image"},{"location":"Fortran/#help","text":"If you need some help, just execute in the container: $ scone gfortran --help Usage: x86_64-linux-musl-gfortran [ options ] file... Options: ... Let's try a simple Fortran program. cat > gcd.f << EOF * euclid.f (FORTRAN 77) * Find greatest common divisor using the Euclidean algorithm PROGRAM EUCLID PRINT *, 'A?' READ *, NA IF (NA.LE.0) THEN PRINT *, 'A must be a positive integer.' STOP END IF PRINT *, 'B?' READ *, NB IF (NB.LE.0) THEN PRINT *, 'B must be a positive integer.' STOP END IF PRINT *, 'The GCD of', NA, ' and', NB, ' is', NGCD(NA, NB), '.' STOP END FUNCTION NGCD(NA, NB) IA = NA IB = NB 1 IF (IB.NE.0) THEN ITEMP = IA IA = IB IB = MOD(ITEMP, IB) GOTO 1 END IF NGCD = IA RETURN END EOF We compile the program with scone gfortran (a.k.a. scone-gfortran ): scone gfortran gcd.f -o gcd We can now run this program as follows: SCONE_VERSION = 1 ./gcd << EOF 10 15 EOF The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: e9c60984724c6d5ffc8cc8a6ba4377910e63c8534ef24b87d0727e712809ba50 A? B? The GCD of 10 and 15 is 5 .","title":"Help"},{"location":"Fortran/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs.","title":"Debugging"},{"location":"Fortran/#screencast","text":"","title":"Screencast"},{"location":"GO/","text":"GO SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image sconecuratedimages/crosscompilers . Example Start the SCONE crosscompiler container: docker run --device = /dev/isgx -it -p 8080 :8080 sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Lets consider a simple GO program (take from a GO tutorial ): cat > web-srv.go << EOF package main import ( \"os\" \"fmt\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hi there, I love %s!\\n\", r.URL.Path[1:]) if r.URL.Path[1:] == \"EXIT\" { os.Exit(0) } } func main() { http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } EOF You can cross-compile this program as follows: SCONE_HEAP = 1G scone-gccgo web-srv.go -O3 -o web-srv-go -g You can start the compiled program (and enable some debug messages) as follows: SCONE_VERSION = 1 ./web-srv-go & The output should look as follows: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: dea1dadce2884bbfd642c10f436c1d02db7ac0f4e4f3abe5d2fde031056405dd You can now connect to port 8080, for example, with curl : curl localhost:8080/SCONE The output should be as follows: Hi there, I love SCONE! You can terminate the server with curl localhost:8080/EXIT This will output the following text: curl: (52) Empty reply from server Building Dependencies Building larger applications that include external dependencies can be difficult when using scone-gccgo alone. To simplify the building of complex applications, we recommend the use of the go command. First, install go inside a sconecuratedimages/crosscompilers container as follows: $ apk update $ apk add go You can then build your dependencies with the help of go and the SCONE go crosscompiler: $ go build -compiler gccgo -buildmode=exe Note you need to specify gccgo not scone-gccgo : gccgo is an alias of scone-gccgo . For a more detailed example, please read how we compile groupcache . Debugging SCONE supports debugging of programs running inside of an enclave with the help of gdb. Debugging inside of a container Standard containers have not sufficient rights to use the debugger. Hence, you must start a container with SYS_PTRACE capability. For example: docker run --cap-add SYS_PTRACE -it -p 8080 :8080 -v \" $PWD \" /EXAMPLE:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers Handling Illegal instructions Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. scone-gdb ./web-srv-go This will produce the following output: GNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git Copyright (C) 2017 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"x86_64-linux-gnu\". Type \"show configuration\" for configuration details. For bug reporting instructions, please see: <http://www.gnu.org/software/gdb/bugs/>. Find the GDB manual and other documentation resources online at: <http://www.gnu.org/software/gdb/documentation/>. For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\"... Source directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd Setting environment variable \"LD_PRELOAD\" to null value. Reading symbols from ./web-srv-go...done. [SCONE] Initializing... If your program contains some illegal instructions, you need to ask the debugger to forward the signals, that these illegal instructions cause, to the program via handle SIGILL nostop pass : # (gdb) handle SIGILL nostop pass This will produce the following output: Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction (gdb) Since we do not patch the CPUID instructions in this run, run you will see something like this: Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [Thread debugging using libthread_db enabled] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\". [SCONE] Enclave base: 1000000000 [SCONE] Loaded debug symbols [New Thread 0x7f1786d26700 (LWP 105)] [New Thread 0x7f1786525700 (LWP 106)] [New Thread 0x7f1785d24700 (LWP 107)] [New Thread 0x7f1785523700 (LWP 108)] [New Thread 0x7f1787502700 (LWP 109)] [New Thread 0x7f17874fa700 (LWP 110)] [New Thread 0x7f17874f2700 (LWP 111)] [New Thread 0x7f17874ea700 (LWP 112)] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 8 \"web-srv-go\" received signal SIGILL, Illegal instruction. You could interrupt this execution via control c: ^C Thread 1 \"web-srv-go\" received signal SIGINT, Interrupt. 0x00007f17870f69dd in pthread_join ( threadid = 139739022911232 , thread_return = 0x7ffe1c807928 ) at pthread_join.c:90 90 pthread_join.c: No such file or directory. ( gdb ) where #0 0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90 #1 0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764 ( gdb ) cont Continuing. Breakpoints scone-gdb support breakpoints. Say, we want to get control in the debugger whenever a request is being processed by the handler. We would set a breakpoint at function main.handler as follows: scone-gdb ./web-srv-go ... [ SCONE ] Initializing... ( gdb ) handle SIGILL nostop pass Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction ( gdb ) break main.handler Function \"main.handler\" not defined. Make breakpoint pending on future shared library load? ( y or [ n ]) y Breakpoint 1 ( main.handler ) pending. ( gdb ) run Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\" . [ SCONE ] Enclave base: 1000000000 [ SCONE ] Loaded debug symbols [ New Thread 0x7fb6cad32700 ( LWP 243 )] [ New Thread 0x7fb6ca531700 ( LWP 244 )] [ New Thread 0x7fb6c9d30700 ( LWP 245 )] [ New Thread 0x7fb6c952f700 ( LWP 246 )] [ New Thread 0x7fb6cb50e700 ( LWP 247 )] [ New Thread 0x7fb6cb506700 ( LWP 248 )] [ New Thread 0x7fb6cb4fe700 ( LWP 249 )] [ New Thread 0x7fb6cb4f6700 ( LWP 250 )] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Note that at the time when we are setting the breakpoint, the symbols of the code running inside of the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on. We are now sending a request with the help of curl from a different window. This triggers the breakpoint: [ Switching to Thread 0x7fb6cb506700 ( LWP 248 )] Thread 7 \"web-srv-go\" hit Breakpoint 1 , main.handler ( w = ..., r = 0x100909e300 ) at web-srv.go:8 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) n 9 fmt.Fprintf ( w, \"Hi there, I love %s!\" , r.URL.Path [ 1 : ]) ( gdb ) n 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) c Continuing. Screencast","title":"GO"},{"location":"GO/#go","text":"SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image sconecuratedimages/crosscompilers .","title":"GO"},{"location":"GO/#example","text":"Start the SCONE crosscompiler container: docker run --device = /dev/isgx -it -p 8080 :8080 sconecuratedimages/crosscompilers Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Lets consider a simple GO program (take from a GO tutorial ): cat > web-srv.go << EOF package main import ( \"os\" \"fmt\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hi there, I love %s!\\n\", r.URL.Path[1:]) if r.URL.Path[1:] == \"EXIT\" { os.Exit(0) } } func main() { http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } EOF You can cross-compile this program as follows: SCONE_HEAP = 1G scone-gccgo web-srv.go -O3 -o web-srv-go -g You can start the compiled program (and enable some debug messages) as follows: SCONE_VERSION = 1 ./web-srv-go & The output should look as follows: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: dea1dadce2884bbfd642c10f436c1d02db7ac0f4e4f3abe5d2fde031056405dd You can now connect to port 8080, for example, with curl : curl localhost:8080/SCONE The output should be as follows: Hi there, I love SCONE! You can terminate the server with curl localhost:8080/EXIT This will output the following text: curl: (52) Empty reply from server","title":"Example"},{"location":"GO/#building-dependencies","text":"Building larger applications that include external dependencies can be difficult when using scone-gccgo alone. To simplify the building of complex applications, we recommend the use of the go command. First, install go inside a sconecuratedimages/crosscompilers container as follows: $ apk update $ apk add go You can then build your dependencies with the help of go and the SCONE go crosscompiler: $ go build -compiler gccgo -buildmode=exe Note you need to specify gccgo not scone-gccgo : gccgo is an alias of scone-gccgo . For a more detailed example, please read how we compile groupcache .","title":"Building Dependencies"},{"location":"GO/#debugging","text":"SCONE supports debugging of programs running inside of an enclave with the help of gdb.","title":"Debugging"},{"location":"GO/#debugging-inside-of-a-container","text":"Standard containers have not sufficient rights to use the debugger. Hence, you must start a container with SYS_PTRACE capability. For example: docker run --cap-add SYS_PTRACE -it -p 8080 :8080 -v \" $PWD \" /EXAMPLE:/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers","title":"Debugging inside of a container"},{"location":"GO/#handling-illegal-instructions","text":"Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. scone-gdb ./web-srv-go This will produce the following output: GNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git Copyright (C) 2017 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"x86_64-linux-gnu\". Type \"show configuration\" for configuration details. For bug reporting instructions, please see: <http://www.gnu.org/software/gdb/bugs/>. Find the GDB manual and other documentation resources online at: <http://www.gnu.org/software/gdb/documentation/>. For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\"... Source directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd Setting environment variable \"LD_PRELOAD\" to null value. Reading symbols from ./web-srv-go...done. [SCONE] Initializing... If your program contains some illegal instructions, you need to ask the debugger to forward the signals, that these illegal instructions cause, to the program via handle SIGILL nostop pass : # (gdb) handle SIGILL nostop pass This will produce the following output: Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction (gdb) Since we do not patch the CPUID instructions in this run, run you will see something like this: Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [Thread debugging using libthread_db enabled] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\". [SCONE] Enclave base: 1000000000 [SCONE] Loaded debug symbols [New Thread 0x7f1786d26700 (LWP 105)] [New Thread 0x7f1786525700 (LWP 106)] [New Thread 0x7f1785d24700 (LWP 107)] [New Thread 0x7f1785523700 (LWP 108)] [New Thread 0x7f1787502700 (LWP 109)] [New Thread 0x7f17874fa700 (LWP 110)] [New Thread 0x7f17874f2700 (LWP 111)] [New Thread 0x7f17874ea700 (LWP 112)] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 8 \"web-srv-go\" received signal SIGILL, Illegal instruction. You could interrupt this execution via control c: ^C Thread 1 \"web-srv-go\" received signal SIGINT, Interrupt. 0x00007f17870f69dd in pthread_join ( threadid = 139739022911232 , thread_return = 0x7ffe1c807928 ) at pthread_join.c:90 90 pthread_join.c: No such file or directory. ( gdb ) where #0 0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90 #1 0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764 ( gdb ) cont Continuing.","title":"Handling Illegal instructions"},{"location":"GO/#breakpoints","text":"scone-gdb support breakpoints. Say, we want to get control in the debugger whenever a request is being processed by the handler. We would set a breakpoint at function main.handler as follows: scone-gdb ./web-srv-go ... [ SCONE ] Initializing... ( gdb ) handle SIGILL nostop pass Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction ( gdb ) break main.handler Function \"main.handler\" not defined. Make breakpoint pending on future shared library load? ( y or [ n ]) y Breakpoint 1 ( main.handler ) pending. ( gdb ) run Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\" . [ SCONE ] Enclave base: 1000000000 [ SCONE ] Loaded debug symbols [ New Thread 0x7fb6cad32700 ( LWP 243 )] [ New Thread 0x7fb6ca531700 ( LWP 244 )] [ New Thread 0x7fb6c9d30700 ( LWP 245 )] [ New Thread 0x7fb6c952f700 ( LWP 246 )] [ New Thread 0x7fb6cb50e700 ( LWP 247 )] [ New Thread 0x7fb6cb506700 ( LWP 248 )] [ New Thread 0x7fb6cb4fe700 ( LWP 249 )] [ New Thread 0x7fb6cb4f6700 ( LWP 250 )] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Note that at the time when we are setting the breakpoint, the symbols of the code running inside of the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on. We are now sending a request with the help of curl from a different window. This triggers the breakpoint: [ Switching to Thread 0x7fb6cb506700 ( LWP 248 )] Thread 7 \"web-srv-go\" hit Breakpoint 1 , main.handler ( w = ..., r = 0x100909e300 ) at web-srv.go:8 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) n 9 fmt.Fprintf ( w, \"Hi there, I love %s!\" , r.URL.Path [ 1 : ]) ( gdb ) n 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) c Continuing.","title":"Breakpoints"},{"location":"GO/#screencast","text":"","title":"Screencast"},{"location":"Java/","text":"Java SCONE supports Java: we maintain an image containing OpenJDK8. Ensure that you have the newest version of this image cached: docker pull sconecuratedimages/apps:8-jdk-alpine In case the pull fails, ask us for access to evalutate SCONE. Now, run the image. On some hosts, JVM needs --privileged access: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:8-jdk-alpine Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed. Example Let us start with a simple hello world example: cat > HelloWorld.java << EOF public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); } } EOF We can compile as follows: javac HelloWorld.java and run inside of an enclave as follows: java HelloWorld This might produce some warnings (since we disable access to the proc filesystem ): / # java HelloWorld Picked up JAVA_TOOL_OPTIONS: -Xmx256m OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World We can run Java with some additional log messages to see what accesses are blocked: SCONE_VERSION = 1 SCONE_LOG = 7 java HelloWorld which results in the following output export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_HEAP=2147483648 export SCONE_STACK=81920 export SCONE_LOG=7 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) # export SCONE_DISABLE_EDMM=yes export SCONE_MPROTECT=yes Revision: 9a3643062da22bc2eed5812d94b5363e0f81422d (Tue Feb 5 23:32:58 2019 +0000) Branch: master Configure options: --enable-shared --enable-debug --prefix=/home/ubuntu/dlequoc/java/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: cf60b6d0cf3f154ada58aea485444eddb857f37a62c13b768f232022560e8754 Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World This correctly prints Hello World despite some few warnings that the heap is exhausted. Screencast Environment variables SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM, container or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . You can increase the heap to ensure that JVM does not run out of heap memory: SCONE_LOG = 7 SCONE_HEAP = 12G java HelloWorld will result in the following output: Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World Example: Zookeeper The above image runs Zookeeper without any modification. Our Zookeeper image is available here: sconecuratedimages/apps:zookeeper-alpine . Zookeeper inside of an enclave runs zk-smoketest without any issues.","title":"Java"},{"location":"Java/#java","text":"SCONE supports Java: we maintain an image containing OpenJDK8. Ensure that you have the newest version of this image cached: docker pull sconecuratedimages/apps:8-jdk-alpine In case the pull fails, ask us for access to evalutate SCONE. Now, run the image. On some hosts, JVM needs --privileged access: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:8-jdk-alpine Please drop argument --device=/dev/isgx in case you do not have an SGX driver installed.","title":"Java"},{"location":"Java/#example","text":"Let us start with a simple hello world example: cat > HelloWorld.java << EOF public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); } } EOF We can compile as follows: javac HelloWorld.java and run inside of an enclave as follows: java HelloWorld This might produce some warnings (since we disable access to the proc filesystem ): / # java HelloWorld Picked up JAVA_TOOL_OPTIONS: -Xmx256m OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World We can run Java with some additional log messages to see what accesses are blocked: SCONE_VERSION = 1 SCONE_LOG = 7 java HelloWorld which results in the following output export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_HEAP=2147483648 export SCONE_STACK=81920 export SCONE_LOG=7 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) # export SCONE_DISABLE_EDMM=yes export SCONE_MPROTECT=yes Revision: 9a3643062da22bc2eed5812d94b5363e0f81422d (Tue Feb 5 23:32:58 2019 +0000) Branch: master Configure options: --enable-shared --enable-debug --prefix=/home/ubuntu/dlequoc/java/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: cf60b6d0cf3f154ada58aea485444eddb857f37a62c13b768f232022560e8754 Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World This correctly prints Hello World despite some few warnings that the heap is exhausted.","title":"Example"},{"location":"Java/#screencast","text":"","title":"Screencast"},{"location":"Java/#environment-variables","text":"SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM, container or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . You can increase the heap to ensure that JVM does not run out of heap memory: SCONE_LOG = 7 SCONE_HEAP = 12G java HelloWorld will result in the following output: Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World","title":"Environment variables"},{"location":"Java/#example-zookeeper","text":"The above image runs Zookeeper without any modification. Our Zookeeper image is available here: sconecuratedimages/apps:zookeeper-alpine . Zookeeper inside of an enclave runs zk-smoketest without any issues.","title":"Example: Zookeeper"},{"location":"LASIntro/","text":"LAS for Development We explain how to start a Local Attestation Service (LAS) instance for development. LAS is need to perform a local attestation (i.e., this creates a quote that can be verified by CAS). Note that this LAS runs inside a debug enclave, i.e., do not use this LAS instance in production . For setting up a production mode LAS, send us an email. Pulling LAS Image To start LAS, you first pull LAS to your local registry. To do so, please set the environment variable LAS to the image repository that we given you access to. The standard LAS image name is defined as follows: export LAS = sconecuratedimages/services:las Pull the image from Docker hub like this: docker pull $LAS If this fails, ensure that you are logged into docker (via docker login ) and that we granted you access to that image. Starting and Stopping LAS The easiest way to start LAS is to use a simple Docker compose file. Please create a separate directory for that: mkdir -p LAS cd LAS Create the following compose file: cat > docker-compose.yml <<EOF version: '3.2' services: las: image: sconecuratedimages/services:las devices: - \"/dev/isgx\" ports: - target: 18766 published: 18766 protocol: tcp mode: host EOF Now start LAS in the background as follows: docker-compose up -d las By executing docker-compose logs las you will see the output of LAS. You can check if LAS is still running by executing: docker-compose up -d las This will result in an output like las_las_1 is up-to-date You can stop LAS by executing: docker-compose stop","title":"Starting LAS"},{"location":"LASIntro/#las-for-development","text":"We explain how to start a Local Attestation Service (LAS) instance for development. LAS is need to perform a local attestation (i.e., this creates a quote that can be verified by CAS). Note that this LAS runs inside a debug enclave, i.e., do not use this LAS instance in production . For setting up a production mode LAS, send us an email.","title":"LAS for Development"},{"location":"LASIntro/#pulling-las-image","text":"To start LAS, you first pull LAS to your local registry. To do so, please set the environment variable LAS to the image repository that we given you access to. The standard LAS image name is defined as follows: export LAS = sconecuratedimages/services:las Pull the image from Docker hub like this: docker pull $LAS If this fails, ensure that you are logged into docker (via docker login ) and that we granted you access to that image.","title":"Pulling LAS Image"},{"location":"LASIntro/#starting-and-stopping-las","text":"The easiest way to start LAS is to use a simple Docker compose file. Please create a separate directory for that: mkdir -p LAS cd LAS Create the following compose file: cat > docker-compose.yml <<EOF version: '3.2' services: las: image: sconecuratedimages/services:las devices: - \"/dev/isgx\" ports: - target: 18766 published: 18766 protocol: tcp mode: host EOF Now start LAS in the background as follows: docker-compose up -d las By executing docker-compose logs las you will see the output of LAS. You can check if LAS is still running by executing: docker-compose up -d las This will result in an output like las_las_1 is up-to-date You can stop LAS by executing: docker-compose stop","title":"Starting and Stopping LAS"},{"location":"MrEnclave/","text":"Determining MrEnclave An enclave is identified by a hash value which is called MrEnclave . This hash is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave . To determine MrEnclave , we provide a simple way to determine MrEnclave on the developer site via environment variable SCONE_HASH=1 . Example: MrEnclave of Python Let us determine MrEnclave of our python interpreter. We start the container and then set environment variable SCONE_HASH=1 to ask SCONE to print MrEnclave and then terminate and SCONE_ALPINE=1 to ensure that the application is indeed started with SCONE. Note When setting SCONE_HASH=1 the program is not executed - only MrEnclave is printed on stdout.** > docker run -it sconecuratedimages/apps:python-2.7-alpine3.6 sh $ SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541 Now, let us change the heap size via environment variable SCONE_HEAP by asking for a 2GB heap: $ SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python aa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465 By default, SCONE does not permit to load dynamic libraries after startup. By setting SCONE_ALLOW_DLOPEN=1 , we permit to load dynamic libraries during runtime. This changes MrEnclave : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Using debug output As an alternative to SCONE_HASH=1 is to print MrEnclave via debug messages by setting SCONE_VERSION=1 : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_VERSION = 1 SCONE_ALPINE = 1 /usr/local/bin/python export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( protected ) export SCONE_MPROTECT = no Revision: b6a40e091e2adb253f019401723d2a734e887a74 ( Fri Jan 26 07 :44:44 2018 +0100 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Enclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Python 2 .7.14 ( default, Jan 10 2018 , 05 :35:30 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information.","title":"MrEnclave"},{"location":"MrEnclave/#determining-mrenclave","text":"An enclave is identified by a hash value which is called MrEnclave . This hash is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave . To determine MrEnclave , we provide a simple way to determine MrEnclave on the developer site via environment variable SCONE_HASH=1 .","title":"Determining MrEnclave"},{"location":"MrEnclave/#example-mrenclave-of-python","text":"Let us determine MrEnclave of our python interpreter. We start the container and then set environment variable SCONE_HASH=1 to ask SCONE to print MrEnclave and then terminate and SCONE_ALPINE=1 to ensure that the application is indeed started with SCONE. Note When setting SCONE_HASH=1 the program is not executed - only MrEnclave is printed on stdout.** > docker run -it sconecuratedimages/apps:python-2.7-alpine3.6 sh $ SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541 Now, let us change the heap size via environment variable SCONE_HEAP by asking for a 2GB heap: $ SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python aa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465 By default, SCONE does not permit to load dynamic libraries after startup. By setting SCONE_ALLOW_DLOPEN=1 , we permit to load dynamic libraries during runtime. This changes MrEnclave : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508","title":"Example: MrEnclave of Python"},{"location":"MrEnclave/#using-debug-output","text":"As an alternative to SCONE_HASH=1 is to print MrEnclave via debug messages by setting SCONE_VERSION=1 : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_VERSION = 1 SCONE_ALPINE = 1 /usr/local/bin/python export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( protected ) export SCONE_MPROTECT = no Revision: b6a40e091e2adb253f019401723d2a734e887a74 ( Fri Jan 26 07 :44:44 2018 +0100 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Enclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Python 2 .7.14 ( default, Jan 10 2018 , 05 :35:30 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information.","title":"Using debug output"},{"location":"Nodejs/","text":"Node We provide Node 8.9.4 image that runs inside of an enclave: docker pull sconecuratedimages/apps:node-8.9.4-alpine Example Let's look at a little hello world program. First, we need to start a node container: docker run -it --device = /dev/isgx -p3000:3000 sconecuratedimages/apps:node-8.9.4-alpine sh In case you have no sgx driver installed, you can drop option --device=/dev/isgx for testing. The programs will then run in simulation mode , i.e., the SCONE software runs but in native mode and not inside an enclave. Inside of the container, we first add npm : apk add --no-cache nodejs-npm Ensure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB: export SCONE_HEAP=1G We create a new application myapp : mkdir myapp cd myapp cat > package.json << EOF { \"name\": \"myapp\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"app.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } EOF We install express with the help of npm : npm install express --save Let's store the hello world code: cat > app.js << EOF var express = require('express'); var app = express(); app.get('/', function (req, res) { res.send('Hello World!'); }); app.listen(3000, function () { console.log('Example app listening on port 3000!'); }); EOF We can run this application inside of an enclave with node . We can also enable some debug messages by setting environment variable SCONE_VERSION=1 to print that we run inside of an enclave: SCONE_VERSION = 1 node app.js This results in an output like this: export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 4294967296 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: e349ed6e4821f0cbfe895413c616409848216173 ( Wed Feb 28 19 :28:04 2018 +0100 ) Branch: master Configure options: --enable-shared --enable-debug --prefix = /builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb Example app listening on port 3000 ! You can now try to send a request to myapp from another shell in the container. Assuming that you did not start a new container in meantime, execute in another shell of your host: docker exec -it $( docker ps -l -q ) sh Inside of the container, first install curl and then query myapp : apk add --no-cache curl curl localhost:3000/ This results in an output like this: Hello World!/ # Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. errno ENOSYS SCONE does not yet support the fork system call (- this will happen later this year). If you spawn processes, there will be some error message like: npm ERR! spawn ENOSYS Environment variables SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . Environment variable SCONE_VERSION=1 prints debug messages - to show that the program runs inside of an enclave. SCONE_MODE=hw enforce that program runs in hardware enclave. By default, we set SCONE_MODE=auto which uses hardware enclave if available and software emulation otherwise. Dockerfile The above example, you could more easily put the following text in a Dockerfile: FROM sconecuratedimages/apps:node-8.9.4-alpine ENV SCONE_HEAP = 1G EXPOSE 3000 RUN apk add --no-cache nodejs-npm \\ && mkdir myapp \\ && cd myapp \\ && echo \"{\" > package.json \\ && echo '\"name\": \"myapp\",' >> package.json \\ && echo '\"version\": \"1.0.0\",' >> package.json \\ && echo '\"description\": \"\",' >> package.json \\ && echo '\"main\": \"app.js\",' >> package.json \\ && echo '\"scripts\":' >> package.json { \\ && echo ' \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"' >> package.json \\ && echo '},' >> package.json \\ && echo '\"author\": \"\",' >> package.json \\ && echo '\"license\": \"ISC\"' >> package.json \\ && echo '}' >> package.json \\ && npm install express --save \\ && echo \"var express = require('express');\" > app.js \\ && echo \"var app = express();\" >> app.js \\ && echo \"app.get('/', function (req, res) {\" >> app.js \\ && echo \" res.send('Hello World!');\" >> app.js \\ && echo \"});\" >> app.js \\ && echo \"app.listen(3000, function () {\" >> app.js \\ && echo \" console.log('Example app listening on port 3000!');\" >> app.js \\ && echo \"});\" >> app.js CMD SCONE_VERSION = 1 node /myapp/app.js Now create an image myapp as follows: docker build -t myapp . You can run a container of this image as a daemon as follows: docker run -d -p 3000 :3000 myapp You can now query myapp as follows: curl localhost:3000 This results in an output like this: Hello World! Screencast","title":"Node"},{"location":"Nodejs/#node","text":"We provide Node 8.9.4 image that runs inside of an enclave: docker pull sconecuratedimages/apps:node-8.9.4-alpine","title":"Node"},{"location":"Nodejs/#example","text":"Let's look at a little hello world program. First, we need to start a node container: docker run -it --device = /dev/isgx -p3000:3000 sconecuratedimages/apps:node-8.9.4-alpine sh In case you have no sgx driver installed, you can drop option --device=/dev/isgx for testing. The programs will then run in simulation mode , i.e., the SCONE software runs but in native mode and not inside an enclave. Inside of the container, we first add npm : apk add --no-cache nodejs-npm Ensure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB: export SCONE_HEAP=1G We create a new application myapp : mkdir myapp cd myapp cat > package.json << EOF { \"name\": \"myapp\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"app.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } EOF We install express with the help of npm : npm install express --save Let's store the hello world code: cat > app.js << EOF var express = require('express'); var app = express(); app.get('/', function (req, res) { res.send('Hello World!'); }); app.listen(3000, function () { console.log('Example app listening on port 3000!'); }); EOF We can run this application inside of an enclave with node . We can also enable some debug messages by setting environment variable SCONE_VERSION=1 to print that we run inside of an enclave: SCONE_VERSION = 1 node app.js This results in an output like this: export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 4294967296 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: e349ed6e4821f0cbfe895413c616409848216173 ( Wed Feb 28 19 :28:04 2018 +0100 ) Branch: master Configure options: --enable-shared --enable-debug --prefix = /builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb Example app listening on port 3000 ! You can now try to send a request to myapp from another shell in the container. Assuming that you did not start a new container in meantime, execute in another shell of your host: docker exec -it $( docker ps -l -q ) sh Inside of the container, first install curl and then query myapp : apk add --no-cache curl curl localhost:3000/ This results in an output like this: Hello World!/ # Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. errno ENOSYS SCONE does not yet support the fork system call (- this will happen later this year). If you spawn processes, there will be some error message like: npm ERR! spawn ENOSYS","title":"Example"},{"location":"Nodejs/#environment-variables","text":"SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . Environment variable SCONE_VERSION=1 prints debug messages - to show that the program runs inside of an enclave. SCONE_MODE=hw enforce that program runs in hardware enclave. By default, we set SCONE_MODE=auto which uses hardware enclave if available and software emulation otherwise.","title":"Environment variables"},{"location":"Nodejs/#dockerfile","text":"The above example, you could more easily put the following text in a Dockerfile: FROM sconecuratedimages/apps:node-8.9.4-alpine ENV SCONE_HEAP = 1G EXPOSE 3000 RUN apk add --no-cache nodejs-npm \\ && mkdir myapp \\ && cd myapp \\ && echo \"{\" > package.json \\ && echo '\"name\": \"myapp\",' >> package.json \\ && echo '\"version\": \"1.0.0\",' >> package.json \\ && echo '\"description\": \"\",' >> package.json \\ && echo '\"main\": \"app.js\",' >> package.json \\ && echo '\"scripts\":' >> package.json { \\ && echo ' \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"' >> package.json \\ && echo '},' >> package.json \\ && echo '\"author\": \"\",' >> package.json \\ && echo '\"license\": \"ISC\"' >> package.json \\ && echo '}' >> package.json \\ && npm install express --save \\ && echo \"var express = require('express');\" > app.js \\ && echo \"var app = express();\" >> app.js \\ && echo \"app.get('/', function (req, res) {\" >> app.js \\ && echo \" res.send('Hello World!');\" >> app.js \\ && echo \"});\" >> app.js \\ && echo \"app.listen(3000, function () {\" >> app.js \\ && echo \" console.log('Example app listening on port 3000!');\" >> app.js \\ && echo \"});\" >> app.js CMD SCONE_VERSION = 1 node /myapp/app.js Now create an image myapp as follows: docker build -t myapp . You can run a container of this image as a daemon as follows: docker run -d -p 3000 :3000 myapp You can now query myapp as follows: curl localhost:3000 This results in an output like this: Hello World!","title":"Dockerfile"},{"location":"Nodejs/#screencast","text":"","title":"Screencast"},{"location":"ProductionModeCAS/","text":"Outline We describe the following aspects of CAS: CAS Production Version CAS Attestation Policy Access Control PALAEMON Operations on sessions Automatic Volume Creation Palaemon Files Service Certificates CAS Production Version The SCONE CAS (Configuration and Attestation Service) - a.k.a. Palaemon - manages secrets and hence, must be protected by running inside of a production enclave and encrypting its database which is stored in the filesystem. Encrypting files is transparent to applications running in the context of a CAS: CAS attests the application and then passes its filesystem keys to the application. Actually, it passes the keys to the SCONE runtime of the application and the runtime transparently encrypts files on writes and decrypts files on reads. CAS can run in the context of a parent CAS to manage the filesystem keys of the child CAS. This is the preferred way to run CAS . However, there always exists a CAS that does not have a parent CAS: we call this a root CAS . We describe the principles of operating a root CAS below. Root CAS Scontain.com operates its own root CAS . Customers might, however, want to operate their own rootCAS . This root CAS runs inside an enclave and uses an encrypted filesystem. Starting up a root CAS is different from a child CAS: A root CAS uses the SGX sealing key (mrenclave-based) to encrypt its database. This can be enabled by setting sealed to true in the CAS config file, in the database section. If this option is enabled during the first start of CAS, it will create a volume.fspf with an encrypted region for the database file ( palaemon.db ), encrypt the volume.fspf with the sealing key and store the fspf file in the fs, initialize the FS shield with the newly created volume.fspf , and create an empty palaemon.db in this protected volume. Note that the creation is done inside of the enclave, i.e., only the encrypted DB and volume.fspf is visible outside of the enclave. During subsequent runs, CAS will detect that the volume/database already exist, and will simply initialize the FS shield using already existing volume.fspf. Limitations: The root CAS does no have rollback protection: Intel has stopped supporting monotonic counters which are implemented by the Intel Management Engine (ME) when using Linux. We plan to add rollback protection in a later version of CAS using an alternative mechanism. CAS Configuration Protection As we mentioned above, the CAS configuration file will need to set sealed to true . An attacker could try to attack the initial configuration of CAS. Hence, the CAS configuration file needs to be protected too, as it contains security relevant configuration options. For example, it contains hashes of trusted SCONE quoting enclaves. One possibility would be to hardcode the config into CAS itself, however this is not sufficiently flexible since modifications of a config requires CAS recompilation. We use a different approach: we tie the fs tag with the mrenclave by adding the tag into enclave_parms structure. This can be done by setting SCONE_FSPF_TAG during enclave signing to provide fspf tag that shall be added to enclave_parms, e.g. $ SCONE_FSPF_TAG = 84cae7af24a94f52f5de5dd36c1843dc scone-signer sign /myapp Since mrenclave with this approach depends on the fspf tag, a client can make sure that the configuration file has not been manipulated by performing CAS attestation. If an application was signed with SCONE_FSPF_TAG , environment variable SCONE_BOOTSTRAP_FSPF shall be used to provide path to FSPF (the runtime will terminate execution if it detects that it has an expected fspf tag, but no FSPF was provided): SCONE_BOOTSTRAP_FSPF = /fspf.pb /myapp During FS shield initialization, fspf tag from enclave_parms must match the tag of the FSPF provided via SCONE_BOOTSTRAP_FSPF, otherwise execution will be terminated. Note that when SCONE_BOOTSTRAP_FSPF is used, no application arguments or environment variables will be passed to the application, as they cannot be trusted (this would require application-level modifications to check whether args and env are sane). To protect CAS, we create a FSPF covering everything that needs to be integrity protected, and use SCONE_FSPF_TAG to add the tag into mrenclave of CAS. Limitations: - read-only file system - no args/env - no secrets in the initial fs state CAS Attestation To establish trust into a root CAS, the client needs to perform a remote attestation of the root CAS. CAS provides a REST endpoint ( /v1/attest ) to request its quote, which can then be validated by a remote party. The quote will contain a hash of the CAS's certificate chain (in the report data), allowing the client to verify whether the hash of the certificates presented by CAS matches the hash in the report data, and use these certificates for future communication with CAS without the need for re-attestation. CAS Attestation API The following types of attestation information are provided by CAS: IAS verification report ( IASReport ) - verification report provided by the Intel Attestation Service (IAS) EPID quote ( EPID ) - unverified quote produced by a quoting enclave. Such quote needs to be verified by the requester via IAS To request CAS attestation information, make a GET request to v1/attest , for example: GET /v1/attest Query parameters Name Type Description type string Type of requested attestation information. Accepted values: IASReport , EPID . If no type is specified, IASReport is assumed. This is an optional field nonce 16 byte hex string Nonce to include in the request. This is an optional field spid 16 byte hex string Identifier of the IAS user. Valid only for EPID requests. This is a mandatory field (if type is EPID ) linkable boolean Whether linkable or unlinkable signature scheme should be used. Valid only for EPID requests. This is a mandatory field (if type is EPID ) Response Succesfull calls will be responded with a JSON map with the following fields: field name description proof_data Attestation result proving the software identity of CAS additional_data Additional data integrity protected via proof Proof Data proof_data is a JSON map. The returned proof data depends on the requested attestation method. Accordingly, \"IASReport\", and \"EPID\" proof data might be returned. The returned type of the proof data is specified in the type field. IASReport The data returned for an IASReport is equivalent to the data obtained from Intel's IAS. Please refer to Intel Attestation Service API documentation, section 3.2 for description of the individual data fields. Data obtained from headers are put into the following JSON fields: - request_id - report_signature - signing_certificate - advisory_url - advisory_ids - body The IAS response's body is found in the body field. See the example below. EPID The proof data returned for an EPID quote is a JSON map with the following fields: field description type Will be \" EPID \" quote Base64 encoded EPID quote spid 16-byte hex encoded IAS SPID specified in the request linkable true or false as specified in the request spid and linkable merely reflect the provided query parameters of the request. For verification, quote has to be sent to Intel's IAS service. IAS has to be queried with the specified spid and linkable settings to obtain verification reports successfully. Additional Data The additional_data value of the response's JSON contains additional data that is integrity protected and sent along with the attestation proof. This data is typically used to, but not limited to, transmit identification information and replay protection that is cryptographically bound to the attestation proof. The additional_data values is a JSON map with the following fields: field name description type Must be \" CasAttestationDataV1 \" nonce random number used during report request, making this data distinguishable from other attestation responses and, thus, protects against replay cert_chain certificate chain used by to secure network communication To protect the additional_data 's integrity it is hashed and its hash value is embedded in the proof field in an attestation method specific manner (see the Proof Data section below). The hash is created by hashing additional_data 's unprettified JSON representation with SHA512 yielding a 64-byte long hash value. Response Status Codes 200 ok - no error 400 bad request - invalid argument 500 - internal server error Example: CAS Attestation with IASReport method Request GET /v1/attest?type=IASReport&nonce=12121212121212121212121212121211 HTTP/1.1 Host: cas:8081 User-Agent: curl/7.58.0 Accept: */* Response HTTP / 1.1 200 OK content-type : application/json content-length : 6713 date : Fri, 19 Jul 2019 14:31:47 GMT { \"proof_data\" : { \"type\" : \"IASReport\" , \"request_id\" : \"04b865cb3b414e5ebfd960b72483993d\" , \"report_signature\" : \"L/JnqRwLET03VNJbw75Bss5bRuJ3p3SEb1CG8ZHwRqyiE87B7zzwY2MWtoV/zhIxIYyhm4Hw/mzvzlSJRDbXizJuhe3Flg8mRulytTpjOxW5nVqnZRBRFSN+jl+vbd9WKxXZgeSuwhwV+rUqaJqnH+eZj1hVTjO+icK0mJ8dODLl3mFkJh4ykMetYJOXNBuJ+GXlMOcUOV01gHcRiY25ecKMax+e9GeZFrur+JgMHhyi15xL+aPRLVMXem35e2NKVB8B2mMJw7WzeDywQTE0p8QOBXKhjGwk9t9yc5I8JJPTJcyixo619xagHNGSxdm+muJEDAyzAcJumU9xs6m54A==\" , \"signing_certificate\" : \"-----BEGIN CERTIFICATE-----\\nMIIEoTCCAwmgAwIBAgIJANEHdl0yo7CWMA0GCSqGSIb3DQEBCwUAMH4xCzAJBgNV\\nBAYTAlVTMQswCQYDVQQIDAJDQTEUMBIGA1UEBwwLU2FudGEgQ2xhcmExGjAYBgNV\\nBAoMEUludGVsIENvcnBvcmF0aW9uMTAwLgYDVQQDDCdJbnRlbCBTR1ggQXR0ZXN0\\nYXRpb24gUmVwb3J0IFNpZ25pbmcgQ0EwHhcNMTYxMTIyMDkzNjU4WhcNMjYxMTIw\\nMDkzNjU4WjB7MQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFDASBgNVBAcMC1Nh\\nbnRhIENsYXJhMRowGAYDVQQKDBFJbnRlbCBDb3Jwb3JhdGlvbjEtMCsGA1UEAwwk\\nSW50ZWwgU0dYIEF0dGVzdGF0aW9uIFJlcG9ydCBTaWduaW5nMIIBIjANBgkqhkiG\\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqXot4OZuphR8nudFrAFiaGxxkgma/Es/BA+t\\nbeCTUR106AL1ENcWA4FX3K+E9BBL0/7X5rj5nIgX/R/1ubhkKWw9gfqPG3KeAtId\\ncv/uTO1yXv50vqaPvE1CRChvzdS/ZEBqQ5oVvLTPZ3VEicQjlytKgN9cLnxbwtuv\\nLUK7eyRPfJW/ksddOzP8VBBniolYnRCD2jrMRZ8nBM2ZWYwnXnwYeOAHV+W9tOhA\\nImwRwKF/95yAsVwd21ryHMJBcGH70qLagZ7Ttyt++qO/6+KAXJuKwZqjRlEtSEz8\\ngZQeFfVYgcwSfo96oSMAzVr7V0L6HSDLRnpb6xxmbPdqNol4tQIDAQABo4GkMIGh\\nMB8GA1UdIwQYMBaAFHhDe3amfrzQr35CN+s1fDuHAVE8MA4GA1UdDwEB/wQEAwIG\\nwDAMBgNVHRMBAf8EAjAAMGAGA1UdHwRZMFcwVaBToFGGT2h0dHA6Ly90cnVzdGVk\\nc2VydmljZXMuaW50ZWwuY29tL2NvbnRlbnQvQ1JML1NHWC9BdHRlc3RhdGlvblJl\\ncG9ydFNpZ25pbmdDQS5jcmwwDQYJKoZIhvcNAQELBQADggGBAGcIthtcK9IVRz4r\\nRq+ZKE+7k50/OxUsmW8aavOzKb0iCx07YQ9rzi5nU73tME2yGRLzhSViFs/LpFa9\\nlpQL6JL1aQwmDR74TxYGBAIi5f4I5TJoCCEqRHz91kpG6Uvyn2tLmnIdJbPE4vYv\\nWLrtXXfFBSSPD4Afn7+3/XUggAlc7oCTizOfbbtOFlYA4g5KcYgS1J2ZAeMQqbUd\\nZseZCcaZZZn65tdqee8UXZlDvx0+NdO0LR+5pFy+juM0wWbu59MvzcmTXbjsi7HY\\n6zd53Yq5K244fwFHRQ8eOB0IWB+4PfM7FeAApZvlfqlKOlLcZL2uyVmzRkyR5yW7\\n2uo9mehX44CiPJ2fse9Y6eQtcfEhMPkmHXI01sN+KwPbpA39+xOsStjhP9N1Y1a2\\ntQAVo+yVgLgV2Hws73Fc0o3wC78qPEA+v2aRs/Be3ZFDgDyghc/1fgU+7C+P6kbq\\nd4poyb6IW8KCJbxfMJvkordNOgOUUxndPHEi/tb/U7uLjLOgPA==\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\nMIIFSzCCA7OgAwIBAgIJANEHdl0yo7CUMA0GCSqGSIb3DQEBCwUAMH4xCzAJBgNV\\nBAYTAlVTMQswCQYDVQQIDAJDQTEUMBIGA1UEBwwLU2FudGEgQ2xhcmExGjAYBgNV\\nBAoMEUludGVsIENvcnBvcmF0aW9uMTAwLgYDVQQDDCdJbnRlbCBTR1ggQXR0ZXN0\\nYXRpb24gUmVwb3J0IFNpZ25pbmcgQ0EwIBcNMTYxMTE0MTUzNzMxWhgPMjA0OTEy\\nMzEyMzU5NTlaMH4xCzAJBgNVBAYTAlVTMQswCQYDVQQIDAJDQTEUMBIGA1UEBwwL\\nU2FudGEgQ2xhcmExGjAYBgNVBAoMEUludGVsIENvcnBvcmF0aW9uMTAwLgYDVQQD\\nDCdJbnRlbCBTR1ggQXR0ZXN0YXRpb24gUmVwb3J0IFNpZ25pbmcgQ0EwggGiMA0G\\nCSqGSIb3DQEBAQUAA4IBjwAwggGKAoIBgQCfPGR+tXc8u1EtJzLA10Feu1Wg+p7e\\nLmSRmeaCHbkQ1TF3Nwl3RmpqXkeGzNLd69QUnWovYyVSndEMyYc3sHecGgfinEeh\\nrgBJSEdsSJ9FpaFdesjsxqzGRa20PYdnnfWcCTvFoulpbFR4VBuXnnVLVzkUvlXT\\nL/TAnd8nIZk0zZkFJ7P5LtePvykkar7LcSQO85wtcQe0R1Raf/sQ6wYKaKmFgCGe\\nNpEJUmg4ktal4qgIAxk+QHUxQE42sxViN5mqglB0QJdUot/o9a/V/mMeH8KvOAiQ\\nbyinkNndn+Bgk5sSV5DFgF0DffVqmVMblt5p3jPtImzBIH0QQrXJq39AT8cRwP5H\\nafuVeLHcDsRp6hol4P+ZFIhu8mmbI1u0hH3W/0C2BuYXB5PC+5izFFh/nP0lc2Lf\\n6rELO9LZdnOhpL1ExFOq9H/B8tPQ84T3Sgb4nAifDabNt/zu6MmCGo5U8lwEFtGM\\nRoOaX4AS+909x00lYnmtwsDVWv9vBiJCXRsCAwEAAaOByTCBxjBgBgNVHR8EWTBX\\nMFWgU6BRhk9odHRwOi8vdHJ1c3RlZHNlcnZpY2VzLmludGVsLmNvbS9jb250ZW50\\nL0NSTC9TR1gvQXR0ZXN0YXRpb25SZXBvcnRTaWduaW5nQ0EuY3JsMB0GA1UdDgQW\\nBBR4Q3t2pn680K9+QjfrNXw7hwFRPDAfBgNVHSMEGDAWgBR4Q3t2pn680K9+Qjfr\\nNXw7hwFRPDAOBgNVHQ8BAf8EBAMCAQYwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkq\\nhkiG9w0BAQsFAAOCAYEAeF8tYMXICvQqeXYQITkV2oLJsp6J4JAqJabHWxYJHGir\\nIEqucRiJSSx+HjIJEUVaj8E0QjEud6Y5lNmXlcjqRXaCPOqK0eGRz6hi+ripMtPZ\\nsFNaBwLQVV905SDjAzDzNIDnrcnXyB4gcDFCvwDFKKgLRjOB/WAqgscDUoGq5ZVi\\nzLUzTqiQPmULAQaB9c6Oti6snEFJiCQ67JLyW/E83/frzCmO5Ru6WjU4tmsmy8Ra\\nUd4APK0wZTGtfPXU7w+IBdG5Ez0kE1qzxGQaL4gINJ1zMyleDnbuS8UicjJijvqA\\n152Sq049ESDz+1rRGc2NVEqh1KaGXmtXvqxXcTB+Ljy5Bw2ke0v8iGngFBPqCTVB\\n3op5KBG3RjbF6RRSzwzuWfL7QErNC8WEy5yDVARzTA5+xmBc388v9Dm21HGfcC8O\\nDD+gT9sSpssq0ascmvH49MOgjt1yoysLtdCtJW/9FZpoOypaHx0R+mJTLwPXVMrv\\nDaVzWh5aiEx+idkSGMnX\\n-----END CERTIFICATE-----\\n\" , \"advisory_url\" : \"https://security-center.intel.com\" , \"advisory_ids\" : \"INTEL-SA-00233,INTEL-SA-00203\" , \"body\" : \"{\\\"id\\\":\\\"104078828093493237715491076693079367680\\\",\\\"timestamp\\\":\\\"2019-07-19T14:31:47.812532\\\",\\\"version\\\":3,\\\"epidPseudonym\\\":\\\"vGG2Lg5xfy2265JaNDYDWx7zPyPl9ECQrmnlI/v+lOPQtwrfCfDK/6RCB0bDZGCTcBbkegtKYurrE1BTdbNX0AZrQWCg41yuuZWJfOqqgmQKsocNtcx0GvAFLjInk0mzGeeesD7WoYbEMMj2XvHiGK+cRpQ70mazM7c4YC4ruvI=\\\",\\\"isvEnclaveQuoteStatus\\\":\\\"GROUP_OUT_OF_DATE\\\",\\\"platformInfoBlob\\\":\\\"1502006504000700000909020401010100000000000000000008000009000000020000000000000AF9E2845415680090212746E93210A997D3FDFB85EA2090AF195255C2F71182C8AD108C5B84187F6DE950FEE178C92E89B07FC54CF7AD8228B0218562844D6823AB\\\",\\\"isvEnclaveQuoteBody\\\":\\\"AgABAPkKAAAHAAYAAAAAAC7FvmTiQrzJ5GcNSeXHCR4AAAAAAAAAAAAAAAAAAAAACAn//wECAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwAAAAAAAAAfAAAAAAAAAHVq793Ho31tmAagb6EQ3HzMXJoz9yKstVHu0MeP0qsDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAARxOFQt2wrFF9/rbbDBFXhBGueb7t1tJxuEzQa2KzFvQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzMh8eWrOC5/EIlYntvTWkNfJSVc2HOpCDlSZkEJbiW3sU0wzzMHHnhQ4b1mMkTbgZa7xMtzpIrEbwci0TkFPN\\\"}\" }, \"additional_data\" : { \"type\" : \"CasAttestationDataV1\" , \"nonce\" : \"12121212121212121212121212121211\" , \"cert_chain\" : \"-----BEGIN CERTIFICATE-----\\r\\nMIIBWjCCAQGgAwIBAgIJALIzkPeLoKKiMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAOMQwwCgYD\\r\\nVQQDDANjYXMwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASyDh3s+6Oauv+Zq0+r\\r\\nP9J0OTGHlLKD0PragTy/6iwr3wF8yDLbIzQGHlLhifSJRs/IuFsqPyEyIKihF/SM\\r\\nF8LGo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQgicw+bI54mO3Jmc4sRYnYRfZFXghI\\r\\nW4JbIGMbckEjUwEwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNHADBEAiApbXda\\r\\nDj3OX//XG4bcj9w4CQ7AUAb9cjDVnKVwHoVIfgIgc0gBka1/+zQHk9IRP7OlaMeH\\r\\nx6kEZDjT7G8Ora2fTTk=\\r\\n-----END CERTIFICATE-----\\r\\n-----BEGIN CERTIFICATE-----\\r\\nMIIBYzCCAQqgAwIBAgIJAL+N9s5W/SUEMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAXMRUwEwYD\\r\\nVQQDDAxjYXMtaW5zdGFuY2UwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQ/BSal\\r\\n8ViDYRB+F+SZCoqmiXefpHc4Eom0m3anOv/4D8r+lu7+INCbE2HK1LZXV5DYSF+G\\r\\nekUveC3Hu/RG/lbpo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQglcUQg4sXlnl6L2Yq\\r\\nVSiIunz6VP7PXoGopkIFf5A7YL8wDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNH\\r\\nADBEAiAgBJfcUx6ch+yjDU60xqHlcrcUG3OI8L9YIui63hn1xgIgE1t3NHBm/MH7\\r\\nFrz2nE/TE99uhYEAnSWwX7kXzl6z/sM=\\r\\n-----END CERTIFICATE-----\\r\\n\" } } Example: CAS Attestation with EPID method Request GET /v1/attest?type=EPID&nonce=12121212121212121212121212121211&spid=12121212121212121212121212121211&linkable=false HTTP / 1.1 Host : localhost:8081 User-Agent : curl/7.58.0 Accept : */* Response HTTP/ 1.1 200 OK content-type: application/json content-length: 2891 date: Fri, 19 Jul 2019 14 : 35 : 12 GMT { \"proof_data\" : { \"type\" : \"EPIDQuote\" , \"quote\" : \"AgAAAPkKAAAHAAYAAAAAABISEhISEhISEhISEhISEhGheRUx3EkAuKScsvlvNdKKCAn//wECAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwAAAAAAAAAfAAAAAAAAAHVq793Ho31tmAagb6EQ3HzMXJoz9yKstVHu0MeP0qsDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAARxOFQt2wrFF9/rbbDBFXhBGueb7t1tJxuEzQa2KzFvQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzMh8eWrOC5/EIlYntvTWkNfJSVc2HOpCDlSZkEJbiW3sU0wzzMHHnhQ4b1mMkTbgZa7xMtzpIrEbwci0TkFPNqAIAAJx7c5hPwJGOtawdvnokuE79Ga2LuvPPB2P5CsfD62VtqJdB5nqk144HQTsQdd51xlHSv/ho5St243b9exSSlVbzv10W17AHfG52JRQoODtCoCWoNjb/pg8ShVeOMqE/VL+rdpuUHssplc8UgdWOA+ld0sdxlj+PMy5t/IgxlqXu+XYYPl91Ss86K5W6KKrteltjBPFlJplgXzHrBlqf2q5LmJWPTtYGXRRm+NSIPEgGHowRRa8HaLX8IjIyPaTSGf3FtknZhcqU3FMpeY8S3zl6pFkGS+9EJ/GPSdAvTayGVNYe6vcaDgqqz5nbjA65JxOqsXaRLGC6cOWNuS1pNl8vIF9cc+SUalcK8+DTSUV6Ak6dHC0pqjJEutLptLEfg7NPdM7nIyOdiLzu02gBAACJBp9/T7IPilddi4HpQ5Rwo/7BmYs0MDuN/h8OhGKdg7uF3WOR6HeYaSefuT9oZngYdIAXMQRXTph+3w8hC6p+5TV7xXZGo1QlkYEs+OUfFDJyKWLFAraQkaD4aLYDlp/a+xYukSCUP9IafjeTJASVGvZl6s+UacP+vcFNRl45byaK08axrBYaCzEMlhQUCfuMbaCoHYak3tGRUnP4ZmCmcyEhANM5sGU4+pdrLV5MvHOCwBR8MvJtJyev/tDcFmjECcIS5EzQQAyw8zKGH2zvxH0D9vKav7aBv2cW695A3uh8U0jtEeNBtmJCDIHaXWq29elt4FfDnfDM66GlSCbzVC6amxXizUIwjpRWIORW4nN0ShBo+oDzdEq0IEn05lXFJmDWtio91m49ujIiwxaEexNMhqrvE1gY11F8eSz/rZn7PVrO0MKwRxThb1K7HusmUKlMbVlY1J9mM2HLzv1q5gqNm8nxlqKqMhkYJrPl4pAMF0jxrppN\" , \"spid\" : \"12121212121212121212121212121211\" , \"linkable\" : false }, \"additional_data\" : { \"type\" : \"CasAttestationDataV1\" , \"nonce\" : \"12121212121212121212121212121211\" , \"cert_chain\" : \"-----BEGIN CERTIFICATE-----\\r\\nMIIBWjCCAQGgAwIBAgIJALIzkPeLoKKiMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAOMQwwCgYD\\r\\nVQQDDANjYXMwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASyDh3s+6Oauv+Zq0+r\\r\\nP9J0OTGHlLKD0PragTy/6iwr3wF8yDLbIzQGHlLhifSJRs/IuFsqPyEyIKihF/SM\\r\\nF8LGo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQgicw+bI54mO3Jmc4sRYnYRfZFXghI\\r\\nW4JbIGMbckEjUwEwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNHADBEAiApbXda\\r\\nDj3OX//XG4bcj9w4CQ7AUAb9cjDVnKVwHoVIfgIgc0gBka1/+zQHk9IRP7OlaMeH\\r\\nx6kEZDjT7G8Ora2fTTk=\\r\\n-----END CERTIFICATE-----\\r\\n-----BEGIN CERTIFICATE-----\\r\\nMIIBYzCCAQqgAwIBAgIJAL+N9s5W/SUEMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAXMRUwEwYD\\r\\nVQQDDAxjYXMtaW5zdGFuY2UwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQ/BSal\\r\\n8ViDYRB+F+SZCoqmiXefpHc4Eom0m3anOv/4D8r+lu7+INCbE2HK1LZXV5DYSF+G\\r\\nekUveC3Hu/RG/lbpo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQglcUQg4sXlnl6L2Yq\\r\\nVSiIunz6VP7PXoGopkIFf5A7YL8wDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNH\\r\\nADBEAiAgBJfcUx6ch+yjDU60xqHlcrcUG3OI8L9YIui63hn1xgIgE1t3NHBm/MH7\\r\\nFrz2nE/TE99uhYEAnSWwX7kXzl6z/sM=\\r\\n-----END CERTIFICATE-----\\r\\n\" } } scone CLI extension We provide a new cas subcommand in scone-cli to simplify attestation and key provisioning. scone cas attest Attest a CAS instance USAGE: scone cas attest [FLAGS] [OPTIONS] --address <address> --mrenclave <mrenclave> FLAGS: -C Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) -d Allow CAS to run in debug mode (only for testing purposes!) -G Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) -h, --help Prints help information -V, --version Prints version information -v, --verbose Print received verification report to stderr OPTIONS: -a, --address <address> CAS address -m, --mrenclave <mrenclave> Expected mrenclave -n, --nonce <nonce> Random number to include in report data This command will contact CAS, request its certificate chain (via /v1/attest ), request IAS report, validate the verification response and finally ensure that the hash of the certificate chain matches the hash in the report data. Upon successful verification, will print certificate chain to stdout, or return an error otherwise. Example: $ scone cas attest -m ad9c03d643d740435a2b1841266ed3a928c42a797c21809fe2332054021af40f -a cas:8081 -G -C Policy Access Control By default, access to a security policy is granted to the client that creates the policy. The creator is identified by a certificate which the client provides when it sets up the TLS connection to Palaemon. For any operation that Palaemon executes, it will first perform access control with the help of the provided client certificate. If the operation is not explicitly permitted, the operation will fail. A policy can be created by any client \u2012 as long as the name of the policy is unique. A policy can define Access Control Lists (ACLs) in section access_control - lists of certificates allowed to perform a particular operation. For each operation for which no ACL is defined, some default lists is defined (see below). For any operation that Palaemon executes, it will first perform access control, i.e., it checks that the provided client certificate is permitted to perform the operation. Note that TLS checks that the client knows the private key of the client certificate. Predefined Names Palaemon also defines a constant for the client certificate creator, i.e., CREATOR of the policy, i.e., this represents the public key of the client that created the policy. We also define constants ANY : To permit access by anybody, define a list with a single member ANY NONE : To emphasize that nobody has access, you can define a ACL with a single member NONE . Note that ANY and NONE can only be part of a list with a single entry. CREATOR can be part of lists with arbitrarily many public keys. PALAEMON Operations on sessions For any RUD operation, as well for reading the policy certificate, the policy can define an ACL: read : permit to read the policy - without the secrets. By default this set to CREATOR . secrets : permit to read the policy including the secrets. This can be used for migrating sessions and must be used with care. By default this is set to NONE . update : permit to update the policy. By default this set to CREATOR . delete : permit to delete the policy. By default this set to CREATOR . In other words, to perform any of these operations, a client has to provide a certificate that matches a certificate in the specified ACL. TLS will ensure that the client knows the private key to the given public key. Example To prohibit any read access of a policy, i.e., even by the creator, define a NONE policy: access_policy : read : - NONE To permit access by anybody, define a list with a single member ANY : access_policy : read : - ANY Notes: Alternatives to specifying certificates are i) public keys, ii) hashes of certificates, or iii) names of certificates. We decided to support certificates instead of public keys or hashes of certificates since they provide some more information and can be verified if they are revoked or expired. Example: Access Control A certificate has to be encoded in PEM format like this: -----BEGIN CERTIFICATE----- MIIGGzCCBAOgAwIBAgIJALrB28lSsN6VMA0GCSqGSIb3DQEBCwUAMGYxCzAJBgNV BAYTAkRFMQ8wDQYDVQQIEwZTYXhvbnkxEDAOBgNVBAcTB0RyZXNkZW4xITAfBgNV HhcNMTgwNDI0MTc1MjU0WhcNMzgwNDE5MTc1MjU0WjBmMQswCQYDVQQGEwJERTEP ... wpV1hfHc8tqJyYovBpFLc55QrGyLZkY5mZX7ug4nn48x2XCenDm1Vn86R23/a2uu ZhOKX5xBi6SZTca8egCCPGY+HuW6kvHSyrBSFygxmPLdq4gYdjVsgFujMvknGw8X r5Ofz+k7EYtRuzS5jO7+NQmeZqy/ItUFFc5oW5hpug== -----END CERTIFICATE----- We can pack this in an access control list. Reading a policy is controlled via a read ACL. This can be defined as part of the policy as follows: access_policy : - read : - |- -----BEGIN CERTIFICATE----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAryQICCl6NZ5gDKrnSztO 3Hy8PEUcuyvg/ikC+VcIo2SFFSf18a3IMYldIugqqqZCs4/4uVW3sbdLs/6PfgdX 7O9D22ZiFWHPYA2k2N744MNiCD1UE+tJyllUhSblK48bn+v1oZHCM0nYQ2NqUkvS ... j+hwUU3RiWl7x3D2s9wSdNt7XUtW05a/FXehsPSiJfKvHJJnGOX0BgTvkLnkAOTd OrUZ/wK69Dzu4IvrN4vs9Nes8vbwPa/ddZEzGR0cQMt0JBkhk9kU/qwqUseP1QRJ 5I1jR4g8aYPL/ke9K35PxZWuDp3U0UPAZ3PjFAh+5T+fc7gzCs9dPzSHloruU+gl FQIDAQAB -----END CERTIFICATE----- - |- -----BEGIN CERTIFICATE----- MIIGGzCCBAOgAwIBAgIJALrB28lSsN6VMA0GCSqGSIb3DQEBCwUAMGYxCzAJBgNV BAYTAkRFMQ8wDQYDVQQIEwZTYXhvbnkxEDAOBgNVBAcTB0RyZXNkZW4xITAfBgNV HhcNMTgwNDI0MTc1MjU0WhcNMzgwNDE5MTc1MjU0WjBmMQswCQYDVQQGEwJERTEP ... wpV1hfHc8tqJyYovBpFLc55QrGyLZkY5mZX7ug4nn48x2XCenDm1Vn86R23/a2uu ZhOKX5xBi6SZTca8egCCPGY+HuW6kvHSyrBSFygxmPLdq4gYdjVsgFujMvknGw8X r5Ofz+k7EYtRuzS5jO7+NQmeZqy/ItUFFc5oW5hpug== -----END CERTIFICATE----- Default ACL When a policy is created, each ACL is set to a default value that can be overwritten by the policy. The default ACLs are defined as follows: access_policy : read : - [ CREATOR ] secrets : - [ NONE ] update : - [ CREATOR ] delete : - [ NONE ] This means that by default, the creator can read the policy (without the secrets), can read the policy certificate and can update the policy, but it cannot delete the policy not read the secrets. Example: Policy Readable by all Clients To ensure that all clients can read the policy, a policy can be defined as follows: access_policy : read : - [ ANY ] Note that the creator can change and delete the policy. Example: Immutable Policy To ensure that the above policy cannot be modified, we prevent that it can be deleted and that it can be updated: access_policy : read : - [ ANY ] secrets : - [ NONE ] update : - [ NONE ] delete : - [ NONE ] Note that in this case not even the creator can update the policy - nor can the creator delete the policy. Note that by preventing that the policy can be deleted, it cannot be replaced by a new policy with the same name - which could be equivalent with updating the policy. Automatic Volume Creation Palaemon can create an empty volume automatically during the first use of this volume. A random key is generated by Palaemon. As soon as the volume is created, Palaemon will track the tag and the key. In other words, the volume is guaranteed to be initialized only once. Palaemon creates an initialized volume only if neither a filesystem key nor a filesystem tag is specified for this volume. In this way, we can ensure that the key for the file system is only visible inside of Palaemon and the application(s) that get access to this volume, i.e., no system administrator will ever seen that key during the creation of the volume. volumes : - name : DB path : /db One could, for example, initialize a volume with one service that pulls the initial content from a trusted repository or that explicitly initializes all files in this volume. PALAEMON Files We can inject the service certificate and the private key of a service into its container as follows. Right now these files must contain PALAEMON variables that refer to secrets and these variables are replaced by the values of the secrets. images : - name : server_image palaemons : # content: $$PALAEMON::server-tls-credentials.chain$$ - \"/certs/chain.pem\" # content: $$PALAEMON::server-tls-credentials.crt$$ - \"/certs/cert.pem\" # content: $$PALAEMON::server-tls-credentials.key$$ - \"/certs/key.pem\" We will extend this mechanism to inject directly the values of secrets without the need to define these files in the first place: this is represented by the key content: In this way, one does not need to place any trust in the original file system of the service. services : - name : myservice palaemons : - file : \"/etc/myservice/sessioncert.crt\" content : '$$PALAEMON:servicecert.crt' - file : \"/etc/myservice/sessioncert.key\" content : '$$PALAEMON:servicecert.key' - file : \"/etc/myservice/sessioncert.chain\" content : '$$PALAEMON:servicecert.chain' keys : - name : servicecert kind : Rsa-CA size : 4096 Service Certificates PALAEMON can create certificates for services in the context of a security policy. These certificates are signed by the session certificate . The session certificate is specific to a given security policy an is itself is signed by the Palaemon instance certificate . A service can access these certificates either via palaemon files (see above), or via environment variables: services : - name : server environment : # Inject into the application environment variables with TLS credentials # for the server (see \"keys\" section for the definition of # server-tls-credentials). # # server-tls-credentials.crt: public certificate signed by the session CA SCONE_CERTIFICATE : $$PALAEMON::server-tls-credentials.crt$$ # server-tls-credentials.key: private key SCONE_PRIVATE_KEY : $$PALAEMON::server-tls-credentials.key$$ # server-tls-credentials.chain: certificate chain for the service # certificate. Will contain the following certificates: CAS CA # certificate -> CAS instance CA certificate -> session CA certificate SCONE_CERTIFICATE_CA_SESSION : $$PALAEMON::server-tls-credentials.chain$$ # Generate TLS credentials for the server. This will produce a certificate, # signed by the session CA certificate, and a corresponding private key keys : - name : server-tls-credentials kind : Rsa-TLS size : 4096 Certificate Chain Certificate chain of a service consists of the following certificates: - subject: CN = cas issuer: CN = cas description: self-signed CAS CA certificate - subject: CN = cas-instance issuer: CN = cas description: CAS Instace CA certificate, signed by CAS CA - subject: CN = cas-session:test issuer: CN = cas-instance description: session CA certificate for session test , signed by CAS Instance CA - subject: CN = server-tls-credentials, L = 91a05b97059623ffd447411b40fabefd167c929fa4951d509c89060fac9c0c7d issuer: CN = cas-session:test description: leaf certificate generated by Palaemon from session key named server-tls-credentials , signed by test session CA certificate. L attribute contains platform id on which the service instance runs.","title":"Outline"},{"location":"ProductionModeCAS/#outline","text":"We describe the following aspects of CAS: CAS Production Version CAS Attestation Policy Access Control PALAEMON Operations on sessions Automatic Volume Creation Palaemon Files Service Certificates","title":"Outline"},{"location":"ProductionModeCAS/#cas-production-version","text":"The SCONE CAS (Configuration and Attestation Service) - a.k.a. Palaemon - manages secrets and hence, must be protected by running inside of a production enclave and encrypting its database which is stored in the filesystem. Encrypting files is transparent to applications running in the context of a CAS: CAS attests the application and then passes its filesystem keys to the application. Actually, it passes the keys to the SCONE runtime of the application and the runtime transparently encrypts files on writes and decrypts files on reads. CAS can run in the context of a parent CAS to manage the filesystem keys of the child CAS. This is the preferred way to run CAS . However, there always exists a CAS that does not have a parent CAS: we call this a root CAS . We describe the principles of operating a root CAS below.","title":"CAS Production Version"},{"location":"ProductionModeCAS/#root-cas","text":"Scontain.com operates its own root CAS . Customers might, however, want to operate their own rootCAS . This root CAS runs inside an enclave and uses an encrypted filesystem. Starting up a root CAS is different from a child CAS: A root CAS uses the SGX sealing key (mrenclave-based) to encrypt its database. This can be enabled by setting sealed to true in the CAS config file, in the database section. If this option is enabled during the first start of CAS, it will create a volume.fspf with an encrypted region for the database file ( palaemon.db ), encrypt the volume.fspf with the sealing key and store the fspf file in the fs, initialize the FS shield with the newly created volume.fspf , and create an empty palaemon.db in this protected volume. Note that the creation is done inside of the enclave, i.e., only the encrypted DB and volume.fspf is visible outside of the enclave. During subsequent runs, CAS will detect that the volume/database already exist, and will simply initialize the FS shield using already existing volume.fspf. Limitations: The root CAS does no have rollback protection: Intel has stopped supporting monotonic counters which are implemented by the Intel Management Engine (ME) when using Linux. We plan to add rollback protection in a later version of CAS using an alternative mechanism.","title":"Root CAS"},{"location":"ProductionModeCAS/#cas-configuration-protection","text":"As we mentioned above, the CAS configuration file will need to set sealed to true . An attacker could try to attack the initial configuration of CAS. Hence, the CAS configuration file needs to be protected too, as it contains security relevant configuration options. For example, it contains hashes of trusted SCONE quoting enclaves. One possibility would be to hardcode the config into CAS itself, however this is not sufficiently flexible since modifications of a config requires CAS recompilation. We use a different approach: we tie the fs tag with the mrenclave by adding the tag into enclave_parms structure. This can be done by setting SCONE_FSPF_TAG during enclave signing to provide fspf tag that shall be added to enclave_parms, e.g. $ SCONE_FSPF_TAG = 84cae7af24a94f52f5de5dd36c1843dc scone-signer sign /myapp Since mrenclave with this approach depends on the fspf tag, a client can make sure that the configuration file has not been manipulated by performing CAS attestation. If an application was signed with SCONE_FSPF_TAG , environment variable SCONE_BOOTSTRAP_FSPF shall be used to provide path to FSPF (the runtime will terminate execution if it detects that it has an expected fspf tag, but no FSPF was provided): SCONE_BOOTSTRAP_FSPF = /fspf.pb /myapp During FS shield initialization, fspf tag from enclave_parms must match the tag of the FSPF provided via SCONE_BOOTSTRAP_FSPF, otherwise execution will be terminated. Note that when SCONE_BOOTSTRAP_FSPF is used, no application arguments or environment variables will be passed to the application, as they cannot be trusted (this would require application-level modifications to check whether args and env are sane). To protect CAS, we create a FSPF covering everything that needs to be integrity protected, and use SCONE_FSPF_TAG to add the tag into mrenclave of CAS. Limitations: - read-only file system - no args/env - no secrets in the initial fs state","title":"CAS Configuration Protection"},{"location":"ProductionModeCAS/#cas-attestation","text":"To establish trust into a root CAS, the client needs to perform a remote attestation of the root CAS. CAS provides a REST endpoint ( /v1/attest ) to request its quote, which can then be validated by a remote party. The quote will contain a hash of the CAS's certificate chain (in the report data), allowing the client to verify whether the hash of the certificates presented by CAS matches the hash in the report data, and use these certificates for future communication with CAS without the need for re-attestation.","title":"CAS Attestation"},{"location":"ProductionModeCAS/#cas-attestation-api","text":"The following types of attestation information are provided by CAS: IAS verification report ( IASReport ) - verification report provided by the Intel Attestation Service (IAS) EPID quote ( EPID ) - unverified quote produced by a quoting enclave. Such quote needs to be verified by the requester via IAS To request CAS attestation information, make a GET request to v1/attest , for example: GET /v1/attest","title":"CAS Attestation API"},{"location":"ProductionModeCAS/#query-parameters","text":"Name Type Description type string Type of requested attestation information. Accepted values: IASReport , EPID . If no type is specified, IASReport is assumed. This is an optional field nonce 16 byte hex string Nonce to include in the request. This is an optional field spid 16 byte hex string Identifier of the IAS user. Valid only for EPID requests. This is a mandatory field (if type is EPID ) linkable boolean Whether linkable or unlinkable signature scheme should be used. Valid only for EPID requests. This is a mandatory field (if type is EPID )","title":"Query parameters"},{"location":"ProductionModeCAS/#response","text":"Succesfull calls will be responded with a JSON map with the following fields: field name description proof_data Attestation result proving the software identity of CAS additional_data Additional data integrity protected via proof","title":"Response"},{"location":"ProductionModeCAS/#proof-data","text":"proof_data is a JSON map. The returned proof data depends on the requested attestation method. Accordingly, \"IASReport\", and \"EPID\" proof data might be returned. The returned type of the proof data is specified in the type field. IASReport The data returned for an IASReport is equivalent to the data obtained from Intel's IAS. Please refer to Intel Attestation Service API documentation, section 3.2 for description of the individual data fields. Data obtained from headers are put into the following JSON fields: - request_id - report_signature - signing_certificate - advisory_url - advisory_ids - body The IAS response's body is found in the body field. See the example below. EPID The proof data returned for an EPID quote is a JSON map with the following fields: field description type Will be \" EPID \" quote Base64 encoded EPID quote spid 16-byte hex encoded IAS SPID specified in the request linkable true or false as specified in the request spid and linkable merely reflect the provided query parameters of the request. For verification, quote has to be sent to Intel's IAS service. IAS has to be queried with the specified spid and linkable settings to obtain verification reports successfully.","title":"Proof Data"},{"location":"ProductionModeCAS/#additional-data","text":"The additional_data value of the response's JSON contains additional data that is integrity protected and sent along with the attestation proof. This data is typically used to, but not limited to, transmit identification information and replay protection that is cryptographically bound to the attestation proof. The additional_data values is a JSON map with the following fields: field name description type Must be \" CasAttestationDataV1 \" nonce random number used during report request, making this data distinguishable from other attestation responses and, thus, protects against replay cert_chain certificate chain used by to secure network communication To protect the additional_data 's integrity it is hashed and its hash value is embedded in the proof field in an attestation method specific manner (see the Proof Data section below). The hash is created by hashing additional_data 's unprettified JSON representation with SHA512 yielding a 64-byte long hash value.","title":"Additional Data"},{"location":"ProductionModeCAS/#response-status-codes","text":"200 ok - no error 400 bad request - invalid argument 500 - internal server error","title":"Response Status Codes"},{"location":"ProductionModeCAS/#example-cas-attestation-with-iasreport-method","text":"Request GET /v1/attest?type=IASReport&nonce=12121212121212121212121212121211 HTTP/1.1 Host: cas:8081 User-Agent: curl/7.58.0 Accept: */* Response HTTP / 1.1 200 OK content-type : application/json content-length : 6713 date : Fri, 19 Jul 2019 14:31:47 GMT { \"proof_data\" : { \"type\" : \"IASReport\" , \"request_id\" : \"04b865cb3b414e5ebfd960b72483993d\" , \"report_signature\" : \"L/JnqRwLET03VNJbw75Bss5bRuJ3p3SEb1CG8ZHwRqyiE87B7zzwY2MWtoV/zhIxIYyhm4Hw/mzvzlSJRDbXizJuhe3Flg8mRulytTpjOxW5nVqnZRBRFSN+jl+vbd9WKxXZgeSuwhwV+rUqaJqnH+eZj1hVTjO+icK0mJ8dODLl3mFkJh4ykMetYJOXNBuJ+GXlMOcUOV01gHcRiY25ecKMax+e9GeZFrur+JgMHhyi15xL+aPRLVMXem35e2NKVB8B2mMJw7WzeDywQTE0p8QOBXKhjGwk9t9yc5I8JJPTJcyixo619xagHNGSxdm+muJEDAyzAcJumU9xs6m54A==\" , \"signing_certificate\" : \"-----BEGIN CERTIFICATE-----\\nMIIEoTCCAwmgAwIBAgIJANEHdl0yo7CWMA0GCSqGSIb3DQEBCwUAMH4xCzAJBgNV\\nBAYTAlVTMQswCQYDVQQIDAJDQTEUMBIGA1UEBwwLU2FudGEgQ2xhcmExGjAYBgNV\\nBAoMEUludGVsIENvcnBvcmF0aW9uMTAwLgYDVQQDDCdJbnRlbCBTR1ggQXR0ZXN0\\nYXRpb24gUmVwb3J0IFNpZ25pbmcgQ0EwHhcNMTYxMTIyMDkzNjU4WhcNMjYxMTIw\\nMDkzNjU4WjB7MQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFDASBgNVBAcMC1Nh\\nbnRhIENsYXJhMRowGAYDVQQKDBFJbnRlbCBDb3Jwb3JhdGlvbjEtMCsGA1UEAwwk\\nSW50ZWwgU0dYIEF0dGVzdGF0aW9uIFJlcG9ydCBTaWduaW5nMIIBIjANBgkqhkiG\\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqXot4OZuphR8nudFrAFiaGxxkgma/Es/BA+t\\nbeCTUR106AL1ENcWA4FX3K+E9BBL0/7X5rj5nIgX/R/1ubhkKWw9gfqPG3KeAtId\\ncv/uTO1yXv50vqaPvE1CRChvzdS/ZEBqQ5oVvLTPZ3VEicQjlytKgN9cLnxbwtuv\\nLUK7eyRPfJW/ksddOzP8VBBniolYnRCD2jrMRZ8nBM2ZWYwnXnwYeOAHV+W9tOhA\\nImwRwKF/95yAsVwd21ryHMJBcGH70qLagZ7Ttyt++qO/6+KAXJuKwZqjRlEtSEz8\\ngZQeFfVYgcwSfo96oSMAzVr7V0L6HSDLRnpb6xxmbPdqNol4tQIDAQABo4GkMIGh\\nMB8GA1UdIwQYMBaAFHhDe3amfrzQr35CN+s1fDuHAVE8MA4GA1UdDwEB/wQEAwIG\\nwDAMBgNVHRMBAf8EAjAAMGAGA1UdHwRZMFcwVaBToFGGT2h0dHA6Ly90cnVzdGVk\\nc2VydmljZXMuaW50ZWwuY29tL2NvbnRlbnQvQ1JML1NHWC9BdHRlc3RhdGlvblJl\\ncG9ydFNpZ25pbmdDQS5jcmwwDQYJKoZIhvcNAQELBQADggGBAGcIthtcK9IVRz4r\\nRq+ZKE+7k50/OxUsmW8aavOzKb0iCx07YQ9rzi5nU73tME2yGRLzhSViFs/LpFa9\\nlpQL6JL1aQwmDR74TxYGBAIi5f4I5TJoCCEqRHz91kpG6Uvyn2tLmnIdJbPE4vYv\\nWLrtXXfFBSSPD4Afn7+3/XUggAlc7oCTizOfbbtOFlYA4g5KcYgS1J2ZAeMQqbUd\\nZseZCcaZZZn65tdqee8UXZlDvx0+NdO0LR+5pFy+juM0wWbu59MvzcmTXbjsi7HY\\n6zd53Yq5K244fwFHRQ8eOB0IWB+4PfM7FeAApZvlfqlKOlLcZL2uyVmzRkyR5yW7\\n2uo9mehX44CiPJ2fse9Y6eQtcfEhMPkmHXI01sN+KwPbpA39+xOsStjhP9N1Y1a2\\ntQAVo+yVgLgV2Hws73Fc0o3wC78qPEA+v2aRs/Be3ZFDgDyghc/1fgU+7C+P6kbq\\nd4poyb6IW8KCJbxfMJvkordNOgOUUxndPHEi/tb/U7uLjLOgPA==\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\nMIIFSzCCA7OgAwIBAgIJANEHdl0yo7CUMA0GCSqGSIb3DQEBCwUAMH4xCzAJBgNV\\nBAYTAlVTMQswCQYDVQQIDAJDQTEUMBIGA1UEBwwLU2FudGEgQ2xhcmExGjAYBgNV\\nBAoMEUludGVsIENvcnBvcmF0aW9uMTAwLgYDVQQDDCdJbnRlbCBTR1ggQXR0ZXN0\\nYXRpb24gUmVwb3J0IFNpZ25pbmcgQ0EwIBcNMTYxMTE0MTUzNzMxWhgPMjA0OTEy\\nMzEyMzU5NTlaMH4xCzAJBgNVBAYTAlVTMQswCQYDVQQIDAJDQTEUMBIGA1UEBwwL\\nU2FudGEgQ2xhcmExGjAYBgNVBAoMEUludGVsIENvcnBvcmF0aW9uMTAwLgYDVQQD\\nDCdJbnRlbCBTR1ggQXR0ZXN0YXRpb24gUmVwb3J0IFNpZ25pbmcgQ0EwggGiMA0G\\nCSqGSIb3DQEBAQUAA4IBjwAwggGKAoIBgQCfPGR+tXc8u1EtJzLA10Feu1Wg+p7e\\nLmSRmeaCHbkQ1TF3Nwl3RmpqXkeGzNLd69QUnWovYyVSndEMyYc3sHecGgfinEeh\\nrgBJSEdsSJ9FpaFdesjsxqzGRa20PYdnnfWcCTvFoulpbFR4VBuXnnVLVzkUvlXT\\nL/TAnd8nIZk0zZkFJ7P5LtePvykkar7LcSQO85wtcQe0R1Raf/sQ6wYKaKmFgCGe\\nNpEJUmg4ktal4qgIAxk+QHUxQE42sxViN5mqglB0QJdUot/o9a/V/mMeH8KvOAiQ\\nbyinkNndn+Bgk5sSV5DFgF0DffVqmVMblt5p3jPtImzBIH0QQrXJq39AT8cRwP5H\\nafuVeLHcDsRp6hol4P+ZFIhu8mmbI1u0hH3W/0C2BuYXB5PC+5izFFh/nP0lc2Lf\\n6rELO9LZdnOhpL1ExFOq9H/B8tPQ84T3Sgb4nAifDabNt/zu6MmCGo5U8lwEFtGM\\nRoOaX4AS+909x00lYnmtwsDVWv9vBiJCXRsCAwEAAaOByTCBxjBgBgNVHR8EWTBX\\nMFWgU6BRhk9odHRwOi8vdHJ1c3RlZHNlcnZpY2VzLmludGVsLmNvbS9jb250ZW50\\nL0NSTC9TR1gvQXR0ZXN0YXRpb25SZXBvcnRTaWduaW5nQ0EuY3JsMB0GA1UdDgQW\\nBBR4Q3t2pn680K9+QjfrNXw7hwFRPDAfBgNVHSMEGDAWgBR4Q3t2pn680K9+Qjfr\\nNXw7hwFRPDAOBgNVHQ8BAf8EBAMCAQYwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkq\\nhkiG9w0BAQsFAAOCAYEAeF8tYMXICvQqeXYQITkV2oLJsp6J4JAqJabHWxYJHGir\\nIEqucRiJSSx+HjIJEUVaj8E0QjEud6Y5lNmXlcjqRXaCPOqK0eGRz6hi+ripMtPZ\\nsFNaBwLQVV905SDjAzDzNIDnrcnXyB4gcDFCvwDFKKgLRjOB/WAqgscDUoGq5ZVi\\nzLUzTqiQPmULAQaB9c6Oti6snEFJiCQ67JLyW/E83/frzCmO5Ru6WjU4tmsmy8Ra\\nUd4APK0wZTGtfPXU7w+IBdG5Ez0kE1qzxGQaL4gINJ1zMyleDnbuS8UicjJijvqA\\n152Sq049ESDz+1rRGc2NVEqh1KaGXmtXvqxXcTB+Ljy5Bw2ke0v8iGngFBPqCTVB\\n3op5KBG3RjbF6RRSzwzuWfL7QErNC8WEy5yDVARzTA5+xmBc388v9Dm21HGfcC8O\\nDD+gT9sSpssq0ascmvH49MOgjt1yoysLtdCtJW/9FZpoOypaHx0R+mJTLwPXVMrv\\nDaVzWh5aiEx+idkSGMnX\\n-----END CERTIFICATE-----\\n\" , \"advisory_url\" : \"https://security-center.intel.com\" , \"advisory_ids\" : \"INTEL-SA-00233,INTEL-SA-00203\" , \"body\" : \"{\\\"id\\\":\\\"104078828093493237715491076693079367680\\\",\\\"timestamp\\\":\\\"2019-07-19T14:31:47.812532\\\",\\\"version\\\":3,\\\"epidPseudonym\\\":\\\"vGG2Lg5xfy2265JaNDYDWx7zPyPl9ECQrmnlI/v+lOPQtwrfCfDK/6RCB0bDZGCTcBbkegtKYurrE1BTdbNX0AZrQWCg41yuuZWJfOqqgmQKsocNtcx0GvAFLjInk0mzGeeesD7WoYbEMMj2XvHiGK+cRpQ70mazM7c4YC4ruvI=\\\",\\\"isvEnclaveQuoteStatus\\\":\\\"GROUP_OUT_OF_DATE\\\",\\\"platformInfoBlob\\\":\\\"1502006504000700000909020401010100000000000000000008000009000000020000000000000AF9E2845415680090212746E93210A997D3FDFB85EA2090AF195255C2F71182C8AD108C5B84187F6DE950FEE178C92E89B07FC54CF7AD8228B0218562844D6823AB\\\",\\\"isvEnclaveQuoteBody\\\":\\\"AgABAPkKAAAHAAYAAAAAAC7FvmTiQrzJ5GcNSeXHCR4AAAAAAAAAAAAAAAAAAAAACAn//wECAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwAAAAAAAAAfAAAAAAAAAHVq793Ho31tmAagb6EQ3HzMXJoz9yKstVHu0MeP0qsDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAARxOFQt2wrFF9/rbbDBFXhBGueb7t1tJxuEzQa2KzFvQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzMh8eWrOC5/EIlYntvTWkNfJSVc2HOpCDlSZkEJbiW3sU0wzzMHHnhQ4b1mMkTbgZa7xMtzpIrEbwci0TkFPN\\\"}\" }, \"additional_data\" : { \"type\" : \"CasAttestationDataV1\" , \"nonce\" : \"12121212121212121212121212121211\" , \"cert_chain\" : \"-----BEGIN CERTIFICATE-----\\r\\nMIIBWjCCAQGgAwIBAgIJALIzkPeLoKKiMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAOMQwwCgYD\\r\\nVQQDDANjYXMwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASyDh3s+6Oauv+Zq0+r\\r\\nP9J0OTGHlLKD0PragTy/6iwr3wF8yDLbIzQGHlLhifSJRs/IuFsqPyEyIKihF/SM\\r\\nF8LGo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQgicw+bI54mO3Jmc4sRYnYRfZFXghI\\r\\nW4JbIGMbckEjUwEwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNHADBEAiApbXda\\r\\nDj3OX//XG4bcj9w4CQ7AUAb9cjDVnKVwHoVIfgIgc0gBka1/+zQHk9IRP7OlaMeH\\r\\nx6kEZDjT7G8Ora2fTTk=\\r\\n-----END CERTIFICATE-----\\r\\n-----BEGIN CERTIFICATE-----\\r\\nMIIBYzCCAQqgAwIBAgIJAL+N9s5W/SUEMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAXMRUwEwYD\\r\\nVQQDDAxjYXMtaW5zdGFuY2UwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQ/BSal\\r\\n8ViDYRB+F+SZCoqmiXefpHc4Eom0m3anOv/4D8r+lu7+INCbE2HK1LZXV5DYSF+G\\r\\nekUveC3Hu/RG/lbpo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQglcUQg4sXlnl6L2Yq\\r\\nVSiIunz6VP7PXoGopkIFf5A7YL8wDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNH\\r\\nADBEAiAgBJfcUx6ch+yjDU60xqHlcrcUG3OI8L9YIui63hn1xgIgE1t3NHBm/MH7\\r\\nFrz2nE/TE99uhYEAnSWwX7kXzl6z/sM=\\r\\n-----END CERTIFICATE-----\\r\\n\" } }","title":"Example: CAS Attestation with IASReport method"},{"location":"ProductionModeCAS/#example-cas-attestation-with-epid-method","text":"Request GET /v1/attest?type=EPID&nonce=12121212121212121212121212121211&spid=12121212121212121212121212121211&linkable=false HTTP / 1.1 Host : localhost:8081 User-Agent : curl/7.58.0 Accept : */* Response HTTP/ 1.1 200 OK content-type: application/json content-length: 2891 date: Fri, 19 Jul 2019 14 : 35 : 12 GMT { \"proof_data\" : { \"type\" : \"EPIDQuote\" , \"quote\" : \"AgAAAPkKAAAHAAYAAAAAABISEhISEhISEhISEhISEhGheRUx3EkAuKScsvlvNdKKCAn//wECAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwAAAAAAAAAfAAAAAAAAAHVq793Ho31tmAagb6EQ3HzMXJoz9yKstVHu0MeP0qsDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAARxOFQt2wrFF9/rbbDBFXhBGueb7t1tJxuEzQa2KzFvQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzMh8eWrOC5/EIlYntvTWkNfJSVc2HOpCDlSZkEJbiW3sU0wzzMHHnhQ4b1mMkTbgZa7xMtzpIrEbwci0TkFPNqAIAAJx7c5hPwJGOtawdvnokuE79Ga2LuvPPB2P5CsfD62VtqJdB5nqk144HQTsQdd51xlHSv/ho5St243b9exSSlVbzv10W17AHfG52JRQoODtCoCWoNjb/pg8ShVeOMqE/VL+rdpuUHssplc8UgdWOA+ld0sdxlj+PMy5t/IgxlqXu+XYYPl91Ss86K5W6KKrteltjBPFlJplgXzHrBlqf2q5LmJWPTtYGXRRm+NSIPEgGHowRRa8HaLX8IjIyPaTSGf3FtknZhcqU3FMpeY8S3zl6pFkGS+9EJ/GPSdAvTayGVNYe6vcaDgqqz5nbjA65JxOqsXaRLGC6cOWNuS1pNl8vIF9cc+SUalcK8+DTSUV6Ak6dHC0pqjJEutLptLEfg7NPdM7nIyOdiLzu02gBAACJBp9/T7IPilddi4HpQ5Rwo/7BmYs0MDuN/h8OhGKdg7uF3WOR6HeYaSefuT9oZngYdIAXMQRXTph+3w8hC6p+5TV7xXZGo1QlkYEs+OUfFDJyKWLFAraQkaD4aLYDlp/a+xYukSCUP9IafjeTJASVGvZl6s+UacP+vcFNRl45byaK08axrBYaCzEMlhQUCfuMbaCoHYak3tGRUnP4ZmCmcyEhANM5sGU4+pdrLV5MvHOCwBR8MvJtJyev/tDcFmjECcIS5EzQQAyw8zKGH2zvxH0D9vKav7aBv2cW695A3uh8U0jtEeNBtmJCDIHaXWq29elt4FfDnfDM66GlSCbzVC6amxXizUIwjpRWIORW4nN0ShBo+oDzdEq0IEn05lXFJmDWtio91m49ujIiwxaEexNMhqrvE1gY11F8eSz/rZn7PVrO0MKwRxThb1K7HusmUKlMbVlY1J9mM2HLzv1q5gqNm8nxlqKqMhkYJrPl4pAMF0jxrppN\" , \"spid\" : \"12121212121212121212121212121211\" , \"linkable\" : false }, \"additional_data\" : { \"type\" : \"CasAttestationDataV1\" , \"nonce\" : \"12121212121212121212121212121211\" , \"cert_chain\" : \"-----BEGIN CERTIFICATE-----\\r\\nMIIBWjCCAQGgAwIBAgIJALIzkPeLoKKiMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAOMQwwCgYD\\r\\nVQQDDANjYXMwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAASyDh3s+6Oauv+Zq0+r\\r\\nP9J0OTGHlLKD0PragTy/6iwr3wF8yDLbIzQGHlLhifSJRs/IuFsqPyEyIKihF/SM\\r\\nF8LGo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQgicw+bI54mO3Jmc4sRYnYRfZFXghI\\r\\nW4JbIGMbckEjUwEwDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNHADBEAiApbXda\\r\\nDj3OX//XG4bcj9w4CQ7AUAb9cjDVnKVwHoVIfgIgc0gBka1/+zQHk9IRP7OlaMeH\\r\\nx6kEZDjT7G8Ora2fTTk=\\r\\n-----END CERTIFICATE-----\\r\\n-----BEGIN CERTIFICATE-----\\r\\nMIIBYzCCAQqgAwIBAgIJAL+N9s5W/SUEMAoGCCqGSM49BAMCMA4xDDAKBgNVBAMM\\r\\nA2NhczAiGA8yMDE5MDcxOTEwMDUzMVoYDzIwMjAwNzE4MTAwNTMxWjAXMRUwEwYD\\r\\nVQQDDAxjYXMtaW5zdGFuY2UwWTATBgcqhkjOPQIBBggqhkjOPQMBBwNCAAQ/BSal\\r\\n8ViDYRB+F+SZCoqmiXefpHc4Eom0m3anOv/4D8r+lu7+INCbE2HK1LZXV5DYSF+G\\r\\nekUveC3Hu/RG/lbpo0QwQjAJBgNVHREEAjAAMCcGA1UdDgQglcUQg4sXlnl6L2Yq\\r\\nVSiIunz6VP7PXoGopkIFf5A7YL8wDAYDVR0TBAUwAwEB/zAKBggqhkjOPQQDAgNH\\r\\nADBEAiAgBJfcUx6ch+yjDU60xqHlcrcUG3OI8L9YIui63hn1xgIgE1t3NHBm/MH7\\r\\nFrz2nE/TE99uhYEAnSWwX7kXzl6z/sM=\\r\\n-----END CERTIFICATE-----\\r\\n\" } }","title":"Example: CAS Attestation with EPID method"},{"location":"ProductionModeCAS/#scone-cli-extension","text":"We provide a new cas subcommand in scone-cli to simplify attestation and key provisioning. scone cas attest Attest a CAS instance USAGE: scone cas attest [FLAGS] [OPTIONS] --address <address> --mrenclave <mrenclave> FLAGS: -C Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) -d Allow CAS to run in debug mode (only for testing purposes!) -G Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) -h, --help Prints help information -V, --version Prints version information -v, --verbose Print received verification report to stderr OPTIONS: -a, --address <address> CAS address -m, --mrenclave <mrenclave> Expected mrenclave -n, --nonce <nonce> Random number to include in report data This command will contact CAS, request its certificate chain (via /v1/attest ), request IAS report, validate the verification response and finally ensure that the hash of the certificate chain matches the hash in the report data. Upon successful verification, will print certificate chain to stdout, or return an error otherwise. Example: $ scone cas attest -m ad9c03d643d740435a2b1841266ed3a928c42a797c21809fe2332054021af40f -a cas:8081 -G -C","title":"scone CLI extension"},{"location":"ProductionModeCAS/#policy-access-control","text":"By default, access to a security policy is granted to the client that creates the policy. The creator is identified by a certificate which the client provides when it sets up the TLS connection to Palaemon. For any operation that Palaemon executes, it will first perform access control with the help of the provided client certificate. If the operation is not explicitly permitted, the operation will fail. A policy can be created by any client \u2012 as long as the name of the policy is unique. A policy can define Access Control Lists (ACLs) in section access_control - lists of certificates allowed to perform a particular operation. For each operation for which no ACL is defined, some default lists is defined (see below). For any operation that Palaemon executes, it will first perform access control, i.e., it checks that the provided client certificate is permitted to perform the operation. Note that TLS checks that the client knows the private key of the client certificate.","title":"Policy Access Control"},{"location":"ProductionModeCAS/#predefined-names","text":"Palaemon also defines a constant for the client certificate creator, i.e., CREATOR of the policy, i.e., this represents the public key of the client that created the policy. We also define constants ANY : To permit access by anybody, define a list with a single member ANY NONE : To emphasize that nobody has access, you can define a ACL with a single member NONE . Note that ANY and NONE can only be part of a list with a single entry. CREATOR can be part of lists with arbitrarily many public keys.","title":"Predefined Names"},{"location":"ProductionModeCAS/#palaemon-operations-on-sessions","text":"For any RUD operation, as well for reading the policy certificate, the policy can define an ACL: read : permit to read the policy - without the secrets. By default this set to CREATOR . secrets : permit to read the policy including the secrets. This can be used for migrating sessions and must be used with care. By default this is set to NONE . update : permit to update the policy. By default this set to CREATOR . delete : permit to delete the policy. By default this set to CREATOR . In other words, to perform any of these operations, a client has to provide a certificate that matches a certificate in the specified ACL. TLS will ensure that the client knows the private key to the given public key.","title":"PALAEMON Operations on sessions"},{"location":"ProductionModeCAS/#example","text":"To prohibit any read access of a policy, i.e., even by the creator, define a NONE policy: access_policy : read : - NONE To permit access by anybody, define a list with a single member ANY : access_policy : read : - ANY Notes: Alternatives to specifying certificates are i) public keys, ii) hashes of certificates, or iii) names of certificates. We decided to support certificates instead of public keys or hashes of certificates since they provide some more information and can be verified if they are revoked or expired.","title":"Example"},{"location":"ProductionModeCAS/#example-access-control","text":"A certificate has to be encoded in PEM format like this: -----BEGIN CERTIFICATE----- MIIGGzCCBAOgAwIBAgIJALrB28lSsN6VMA0GCSqGSIb3DQEBCwUAMGYxCzAJBgNV BAYTAkRFMQ8wDQYDVQQIEwZTYXhvbnkxEDAOBgNVBAcTB0RyZXNkZW4xITAfBgNV HhcNMTgwNDI0MTc1MjU0WhcNMzgwNDE5MTc1MjU0WjBmMQswCQYDVQQGEwJERTEP ... wpV1hfHc8tqJyYovBpFLc55QrGyLZkY5mZX7ug4nn48x2XCenDm1Vn86R23/a2uu ZhOKX5xBi6SZTca8egCCPGY+HuW6kvHSyrBSFygxmPLdq4gYdjVsgFujMvknGw8X r5Ofz+k7EYtRuzS5jO7+NQmeZqy/ItUFFc5oW5hpug== -----END CERTIFICATE----- We can pack this in an access control list. Reading a policy is controlled via a read ACL. This can be defined as part of the policy as follows: access_policy : - read : - |- -----BEGIN CERTIFICATE----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAryQICCl6NZ5gDKrnSztO 3Hy8PEUcuyvg/ikC+VcIo2SFFSf18a3IMYldIugqqqZCs4/4uVW3sbdLs/6PfgdX 7O9D22ZiFWHPYA2k2N744MNiCD1UE+tJyllUhSblK48bn+v1oZHCM0nYQ2NqUkvS ... j+hwUU3RiWl7x3D2s9wSdNt7XUtW05a/FXehsPSiJfKvHJJnGOX0BgTvkLnkAOTd OrUZ/wK69Dzu4IvrN4vs9Nes8vbwPa/ddZEzGR0cQMt0JBkhk9kU/qwqUseP1QRJ 5I1jR4g8aYPL/ke9K35PxZWuDp3U0UPAZ3PjFAh+5T+fc7gzCs9dPzSHloruU+gl FQIDAQAB -----END CERTIFICATE----- - |- -----BEGIN CERTIFICATE----- MIIGGzCCBAOgAwIBAgIJALrB28lSsN6VMA0GCSqGSIb3DQEBCwUAMGYxCzAJBgNV BAYTAkRFMQ8wDQYDVQQIEwZTYXhvbnkxEDAOBgNVBAcTB0RyZXNkZW4xITAfBgNV HhcNMTgwNDI0MTc1MjU0WhcNMzgwNDE5MTc1MjU0WjBmMQswCQYDVQQGEwJERTEP ... wpV1hfHc8tqJyYovBpFLc55QrGyLZkY5mZX7ug4nn48x2XCenDm1Vn86R23/a2uu ZhOKX5xBi6SZTca8egCCPGY+HuW6kvHSyrBSFygxmPLdq4gYdjVsgFujMvknGw8X r5Ofz+k7EYtRuzS5jO7+NQmeZqy/ItUFFc5oW5hpug== -----END CERTIFICATE-----","title":"Example: Access Control"},{"location":"ProductionModeCAS/#default-acl","text":"When a policy is created, each ACL is set to a default value that can be overwritten by the policy. The default ACLs are defined as follows: access_policy : read : - [ CREATOR ] secrets : - [ NONE ] update : - [ CREATOR ] delete : - [ NONE ] This means that by default, the creator can read the policy (without the secrets), can read the policy certificate and can update the policy, but it cannot delete the policy not read the secrets.","title":"Default ACL"},{"location":"ProductionModeCAS/#example-policy-readable-by-all-clients","text":"To ensure that all clients can read the policy, a policy can be defined as follows: access_policy : read : - [ ANY ] Note that the creator can change and delete the policy.","title":"Example: Policy Readable by all Clients"},{"location":"ProductionModeCAS/#example-immutable-policy","text":"To ensure that the above policy cannot be modified, we prevent that it can be deleted and that it can be updated: access_policy : read : - [ ANY ] secrets : - [ NONE ] update : - [ NONE ] delete : - [ NONE ] Note that in this case not even the creator can update the policy - nor can the creator delete the policy. Note that by preventing that the policy can be deleted, it cannot be replaced by a new policy with the same name - which could be equivalent with updating the policy.","title":"Example: Immutable Policy"},{"location":"ProductionModeCAS/#automatic-volume-creation","text":"Palaemon can create an empty volume automatically during the first use of this volume. A random key is generated by Palaemon. As soon as the volume is created, Palaemon will track the tag and the key. In other words, the volume is guaranteed to be initialized only once. Palaemon creates an initialized volume only if neither a filesystem key nor a filesystem tag is specified for this volume. In this way, we can ensure that the key for the file system is only visible inside of Palaemon and the application(s) that get access to this volume, i.e., no system administrator will ever seen that key during the creation of the volume. volumes : - name : DB path : /db One could, for example, initialize a volume with one service that pulls the initial content from a trusted repository or that explicitly initializes all files in this volume.","title":"Automatic Volume Creation"},{"location":"ProductionModeCAS/#palaemon-files","text":"We can inject the service certificate and the private key of a service into its container as follows. Right now these files must contain PALAEMON variables that refer to secrets and these variables are replaced by the values of the secrets. images : - name : server_image palaemons : # content: $$PALAEMON::server-tls-credentials.chain$$ - \"/certs/chain.pem\" # content: $$PALAEMON::server-tls-credentials.crt$$ - \"/certs/cert.pem\" # content: $$PALAEMON::server-tls-credentials.key$$ - \"/certs/key.pem\" We will extend this mechanism to inject directly the values of secrets without the need to define these files in the first place: this is represented by the key content: In this way, one does not need to place any trust in the original file system of the service. services : - name : myservice palaemons : - file : \"/etc/myservice/sessioncert.crt\" content : '$$PALAEMON:servicecert.crt' - file : \"/etc/myservice/sessioncert.key\" content : '$$PALAEMON:servicecert.key' - file : \"/etc/myservice/sessioncert.chain\" content : '$$PALAEMON:servicecert.chain' keys : - name : servicecert kind : Rsa-CA size : 4096","title":"PALAEMON Files"},{"location":"ProductionModeCAS/#service-certificates","text":"PALAEMON can create certificates for services in the context of a security policy. These certificates are signed by the session certificate . The session certificate is specific to a given security policy an is itself is signed by the Palaemon instance certificate . A service can access these certificates either via palaemon files (see above), or via environment variables: services : - name : server environment : # Inject into the application environment variables with TLS credentials # for the server (see \"keys\" section for the definition of # server-tls-credentials). # # server-tls-credentials.crt: public certificate signed by the session CA SCONE_CERTIFICATE : $$PALAEMON::server-tls-credentials.crt$$ # server-tls-credentials.key: private key SCONE_PRIVATE_KEY : $$PALAEMON::server-tls-credentials.key$$ # server-tls-credentials.chain: certificate chain for the service # certificate. Will contain the following certificates: CAS CA # certificate -> CAS instance CA certificate -> session CA certificate SCONE_CERTIFICATE_CA_SESSION : $$PALAEMON::server-tls-credentials.chain$$ # Generate TLS credentials for the server. This will produce a certificate, # signed by the session CA certificate, and a corresponding private key keys : - name : server-tls-credentials kind : Rsa-TLS size : 4096","title":"Service Certificates"},{"location":"ProductionModeCAS/#certificate-chain","text":"Certificate chain of a service consists of the following certificates: - subject: CN = cas issuer: CN = cas description: self-signed CAS CA certificate - subject: CN = cas-instance issuer: CN = cas description: CAS Instace CA certificate, signed by CAS CA - subject: CN = cas-session:test issuer: CN = cas-instance description: session CA certificate for session test , signed by CAS Instance CA - subject: CN = server-tls-credentials, L = 91a05b97059623ffd447411b40fabefd167c929fa4951d509c89060fac9c0c7d issuer: CN = cas-session:test description: leaf certificate generated by Palaemon from session key named server-tls-credentials , signed by test session CA certificate. L attribute contains platform id on which the service instance runs.","title":"Certificate Chain"},{"location":"Python/","text":"Python SCONE supports running Python programs inside of SGX enclaves. We maintain Docker images for various Python versions / Python engines like: Python 3.7.5 : sconecuratedimages/python:3.7-alpine Python 3.5.1 : sconecuratedimages/appspython-3.5-alpine Python 2.7 : sconecuratedimages/apps:python-2.7.13-alpine3.6, and PyPy 2.7 : sconecuratedimages/apps:pypy-2.7.15-alpine3.7 Let us know if you need a specific Python / PyPy version. PyPy for SCONE PyPySCONE's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is PyPy inside an enclave faster than native Python. Workflow SCONE supports the typical Docker workflow to create Docker images that contain the Python engine as well as the Python program. SCONE supports the encryption of the Python programs to ensure both the confidentiality as well as the integrity of the programs. A typical workflow might look like this: We at scontain.com maintain the SCONE-Python image and push this to hub.docker.com. This image can be used by authorized application developers to add encrypted Python programs and encrypted libraries. The SCONE runtime of the Python engine needs to get access to the encryption key to be able to decrypt transparently the Python scripts for the Python engine. To do so, the application developer defines a security policy that ensures that only the Python engine that executes the application of the application provider gets access to this encryption key. When the Python engine starts completely inside of a SGX enclave. The SCONE runtime transparently attests the Python engine as well as the filesystem : Only if both the Python engine has the expected MrEnclave as well as the filesystem state is exactly as expected, the SCONE runtime gets the encryption key from the SCONE CAS (Configuration and Attestation Service). The application developer therefore adds the expected MrEnclave and the initial filesystem state in form of a security policy to SCONE CAS. Complex Workflows SCONE supports more complex workflows in which the user can also specify encrypted volumes for input as well as output data. We explain a more complex example in the context of our blender use case . Image Getting access to Python Images You need access to a private docker hub repository sconecuratedimages/apps to be able to evaluate our Python images. Send us your docker hub ID to get access to this repository. Currently, we provide a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine. You can pull this image as follows: docker pull sconecuratedimages/apps:python-2-alpine3.6 Python Interpreter To run the Python interpreter inside an enclave in interactive mode, first start the container: docker run --rm -it -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/apps:python-2-alpine3.6 sh The execute python inside of the container: SCONE_HEAP = 256M SCONE_VERSION = 1 python Since we set SCONE_VERSION=1 , we get the following outputs 1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 268435456 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab ( Thu Aug 16 16 :45:05 2018 +0200 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /mnt/ssd/franz/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: f129bbd19627367c03e2980c0f04a32809a7aae1d795a75220d9054daf537b30 Python 2 .7.13 ( default, Jun 1 2018 , 13 :20:58 ) [ GCC 7 .3.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. The meaning of protected versus unprotected library is explained in the faq . Running an application Say, you have a Python application called myapp.py in your current directory. To execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. To run Python inside of an enclave, you can set the environment variable SCONE_MODE=HW and SCONE_ALPINE=1 . To issue some debug messages that show that we are running inside an enclave, set SCONE_VERSION=1 In general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of MRENCLAVE , i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set SCONE_ALLOW_DLOPEN=2 . For operations, the environment variables are set by the CAS and you must set SCONE_ALLOW_DLOPEN either to SCONE_ALLOW_DLOPEN=1 to enable loading of dynamic libraries or must not define SCONE_ALLOW_DLOPEN . Python applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable SCONE_HEAP and the stack size via STACK_SIZE at startup. Python program exits Python does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like, SCONE_HEAP=256M to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.** Note that you can set the environment variable of a process - in our case python - running inside a container with docker option -e : docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp -e SCONE_HEAP = 256M -e SCONE_MODE = HW -e SCONE_ALLOW_DLOPEN = 2 -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/apps:python-2-alpine3.6 python myapp.py Will produce an output like export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 ... NumPy Let's see how we can install some extra packages that your python program might need. Let us focus on NumPy first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a Dockerfile . First, we start the SCONE Python image: docker run -it --rm sconecuratedimages/apps:python-2-alpine3.6 sh This is a minimal image and you need to add some packages to be able to install packages that compile external code: apk add --no-cache bats libbsd openssl musl-dev build-base We then install numpy inside of the container with the help of pip : pip install numpy == 1 .14.5 This results in an output like Collecting numpy == 1 .14.5 Downloading https://files.pythonhosted.org/packages/d5/6e/f00492653d0fdf6497a181a1c1d46bbea5a2383e7faf4c8ca6d6f3d2581d/numpy-1.14.5.zip ( 4 .9MB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 .9MB 375kB/s Installing collected packages: numpy Running setup.py install for numpy ... done Successfully installed numpy-1.14.5 Ok, let's try to execute some examples with NumPy . Let's run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages: SCONE_HEAP = 256M SCONE_VERSION = 1 python during startup this issues the following messages export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 10000000000 export SCONE_STACK = 0 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes export SCONE_ALLOW_DLOPEN2 = yes Revision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Python 2 .7.14 ( default, Dec 19 2017 , 22 :29:22 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Now, we can import numpy and execute some commands: >>> import numpy as np >>> a = np.arange ( 15 ) .reshape ( 3 , 5 ) >>> a array ([[ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ]]) >>> a.shape ( 3 , 5 ) >>> a.ndim 2 >>> a.dtype.name 'int64' >>> a.itemsize 8 >>> type ( a ) < type 'numpy.ndarray' > >>> Cairo Let's look at another popular library: the cairo graphics library. cairo is written in C and has Python bindings provided by package pycairo . In this case, we need to install the C-library first: In Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows: apk add --no-cache cairo-dev cairo fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ( 1 /55 ) Installing expat-dev ( 2 .2.5-r0 ) ... ( 55 /55 ) Installing cairo-dev ( 1 .14.10-r0 ) Executing busybox-1.27.2-r6.trigger Executing glib-2.54.2-r0.trigger No schema files found: doing nothing. OK: 297 MiB in 112 packages $ Now we can install the Python bindings of cairo with pip : pip install pycairo Collecting pycairo Downloading pycairo-1.15.4.tar.gz ( 178kB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 184kB 1 .7MB/s Building wheels for collected packages: pycairo Running setup.py bdist_wheel for pycairo ... done Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f Successfully built pycairo Installing collected packages: pycairo Successfully installed pycairo-1.15.4 We can now start Python again via SCONE_HEAP = 256M SCONE_VERSION = 1 python before we execute some cairo examples: >>> import cairo >>> import math >>> WIDTH, HEIGHT = 256 , 256 >>> >>> surface = cairo.ImageSurface ( cairo.FORMAT_ARGB32, WIDTH, HEIGHT ) >>> ctx = cairo.Context ( surface ) >>> ctx.scale ( WIDTH, HEIGHT ) # Normalizing the canvas >>> >>> pat = cairo.LinearGradient ( 0 .0, 0 .0, 0 .0, 1 .0 ) >>> pat.add_color_stop_rgba ( 1 , 0 .7, 0 , 0 , 0 .5 ) # First stop, 50% opacity >>> pat.add_color_stop_rgba ( 0 , 0 .9, 0 .7, 0 .2, 1 ) # Last stop, 100% opacity >>> >>> ctx.rectangle ( 0 , 0 , 1 , 1 ) # Rectangle(x0, y0, x1, y1) >>> ctx.set_source ( pat ) >>> ctx.fill () >>> ctx.translate ( 0 .1, 0 .1 ) # Changing the current transformation matrix >>> >>> ctx.move_to ( 0 , 0 ) >>> # Arc(cx, cy, radius, start_angle, stop_angle) ... ctx.arc ( 0 .2, 0 .1, 0 .1, -math.pi/2, 0 ) >>> ctx.line_to ( 0 .5, 0 .1 ) # Line to (x,y) >>> # Curve(x1, y1, x2, y2, x3, y3) ... ctx.curve_to ( 0 .5, 0 .2, 0 .5, 0 .4, 0 .2, 0 .8 ) >>> ctx.close_path () >>> >>> ctx.set_source_rgb ( 0 .3, 0 .2, 0 .5 ) # Solid color >>> ctx.set_line_width ( 0 .02 ) >>> ctx.stroke () >>> surface.write_to_png ( \"example.png\" ) # Output to PNG >>> exit () This generates a file example.png in the working directory. Example Let's look at another example: We use pip to install a Python chess library. Then we run Python inside of an enclave and import the chess library. We use the Scholar's mate example from https://pypi.python.org/pypi/python-chess Limitation We do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner. Screencast The other environment variables are explained below. Also read Section Environment Variables for further details. \u21a9","title":"Python"},{"location":"Python/#python","text":"SCONE supports running Python programs inside of SGX enclaves. We maintain Docker images for various Python versions / Python engines like: Python 3.7.5 : sconecuratedimages/python:3.7-alpine Python 3.5.1 : sconecuratedimages/appspython-3.5-alpine Python 2.7 : sconecuratedimages/apps:python-2.7.13-alpine3.6, and PyPy 2.7 : sconecuratedimages/apps:pypy-2.7.15-alpine3.7 Let us know if you need a specific Python / PyPy version. PyPy for SCONE PyPySCONE's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is PyPy inside an enclave faster than native Python.","title":"Python"},{"location":"Python/#workflow","text":"SCONE supports the typical Docker workflow to create Docker images that contain the Python engine as well as the Python program. SCONE supports the encryption of the Python programs to ensure both the confidentiality as well as the integrity of the programs. A typical workflow might look like this: We at scontain.com maintain the SCONE-Python image and push this to hub.docker.com. This image can be used by authorized application developers to add encrypted Python programs and encrypted libraries. The SCONE runtime of the Python engine needs to get access to the encryption key to be able to decrypt transparently the Python scripts for the Python engine. To do so, the application developer defines a security policy that ensures that only the Python engine that executes the application of the application provider gets access to this encryption key. When the Python engine starts completely inside of a SGX enclave. The SCONE runtime transparently attests the Python engine as well as the filesystem : Only if both the Python engine has the expected MrEnclave as well as the filesystem state is exactly as expected, the SCONE runtime gets the encryption key from the SCONE CAS (Configuration and Attestation Service). The application developer therefore adds the expected MrEnclave and the initial filesystem state in form of a security policy to SCONE CAS.","title":"Workflow"},{"location":"Python/#complex-workflows","text":"SCONE supports more complex workflows in which the user can also specify encrypted volumes for input as well as output data. We explain a more complex example in the context of our blender use case .","title":"Complex Workflows"},{"location":"Python/#image","text":"Getting access to Python Images You need access to a private docker hub repository sconecuratedimages/apps to be able to evaluate our Python images. Send us your docker hub ID to get access to this repository. Currently, we provide a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine. You can pull this image as follows: docker pull sconecuratedimages/apps:python-2-alpine3.6","title":"Image"},{"location":"Python/#python-interpreter","text":"To run the Python interpreter inside an enclave in interactive mode, first start the container: docker run --rm -it -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/apps:python-2-alpine3.6 sh The execute python inside of the container: SCONE_HEAP = 256M SCONE_VERSION = 1 python Since we set SCONE_VERSION=1 , we get the following outputs 1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 268435456 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab ( Thu Aug 16 16 :45:05 2018 +0200 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /mnt/ssd/franz/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: f129bbd19627367c03e2980c0f04a32809a7aae1d795a75220d9054daf537b30 Python 2 .7.13 ( default, Jun 1 2018 , 13 :20:58 ) [ GCC 7 .3.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. The meaning of protected versus unprotected library is explained in the faq .","title":"Python Interpreter"},{"location":"Python/#running-an-application","text":"Say, you have a Python application called myapp.py in your current directory. To execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. To run Python inside of an enclave, you can set the environment variable SCONE_MODE=HW and SCONE_ALPINE=1 . To issue some debug messages that show that we are running inside an enclave, set SCONE_VERSION=1 In general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of MRENCLAVE , i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set SCONE_ALLOW_DLOPEN=2 . For operations, the environment variables are set by the CAS and you must set SCONE_ALLOW_DLOPEN either to SCONE_ALLOW_DLOPEN=1 to enable loading of dynamic libraries or must not define SCONE_ALLOW_DLOPEN . Python applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable SCONE_HEAP and the stack size via STACK_SIZE at startup. Python program exits Python does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like, SCONE_HEAP=256M to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.** Note that you can set the environment variable of a process - in our case python - running inside a container with docker option -e : docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp -e SCONE_HEAP = 256M -e SCONE_MODE = HW -e SCONE_ALLOW_DLOPEN = 2 -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/apps:python-2-alpine3.6 python myapp.py Will produce an output like export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 ...","title":"Running an application"},{"location":"Python/#numpy","text":"Let's see how we can install some extra packages that your python program might need. Let us focus on NumPy first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a Dockerfile . First, we start the SCONE Python image: docker run -it --rm sconecuratedimages/apps:python-2-alpine3.6 sh This is a minimal image and you need to add some packages to be able to install packages that compile external code: apk add --no-cache bats libbsd openssl musl-dev build-base We then install numpy inside of the container with the help of pip : pip install numpy == 1 .14.5 This results in an output like Collecting numpy == 1 .14.5 Downloading https://files.pythonhosted.org/packages/d5/6e/f00492653d0fdf6497a181a1c1d46bbea5a2383e7faf4c8ca6d6f3d2581d/numpy-1.14.5.zip ( 4 .9MB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 .9MB 375kB/s Installing collected packages: numpy Running setup.py install for numpy ... done Successfully installed numpy-1.14.5 Ok, let's try to execute some examples with NumPy . Let's run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages: SCONE_HEAP = 256M SCONE_VERSION = 1 python during startup this issues the following messages export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 10000000000 export SCONE_STACK = 0 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes export SCONE_ALLOW_DLOPEN2 = yes Revision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Python 2 .7.14 ( default, Dec 19 2017 , 22 :29:22 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Now, we can import numpy and execute some commands: >>> import numpy as np >>> a = np.arange ( 15 ) .reshape ( 3 , 5 ) >>> a array ([[ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ]]) >>> a.shape ( 3 , 5 ) >>> a.ndim 2 >>> a.dtype.name 'int64' >>> a.itemsize 8 >>> type ( a ) < type 'numpy.ndarray' > >>>","title":"NumPy"},{"location":"Python/#cairo","text":"Let's look at another popular library: the cairo graphics library. cairo is written in C and has Python bindings provided by package pycairo . In this case, we need to install the C-library first: In Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows: apk add --no-cache cairo-dev cairo fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ( 1 /55 ) Installing expat-dev ( 2 .2.5-r0 ) ... ( 55 /55 ) Installing cairo-dev ( 1 .14.10-r0 ) Executing busybox-1.27.2-r6.trigger Executing glib-2.54.2-r0.trigger No schema files found: doing nothing. OK: 297 MiB in 112 packages $ Now we can install the Python bindings of cairo with pip : pip install pycairo Collecting pycairo Downloading pycairo-1.15.4.tar.gz ( 178kB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 184kB 1 .7MB/s Building wheels for collected packages: pycairo Running setup.py bdist_wheel for pycairo ... done Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f Successfully built pycairo Installing collected packages: pycairo Successfully installed pycairo-1.15.4 We can now start Python again via SCONE_HEAP = 256M SCONE_VERSION = 1 python before we execute some cairo examples: >>> import cairo >>> import math >>> WIDTH, HEIGHT = 256 , 256 >>> >>> surface = cairo.ImageSurface ( cairo.FORMAT_ARGB32, WIDTH, HEIGHT ) >>> ctx = cairo.Context ( surface ) >>> ctx.scale ( WIDTH, HEIGHT ) # Normalizing the canvas >>> >>> pat = cairo.LinearGradient ( 0 .0, 0 .0, 0 .0, 1 .0 ) >>> pat.add_color_stop_rgba ( 1 , 0 .7, 0 , 0 , 0 .5 ) # First stop, 50% opacity >>> pat.add_color_stop_rgba ( 0 , 0 .9, 0 .7, 0 .2, 1 ) # Last stop, 100% opacity >>> >>> ctx.rectangle ( 0 , 0 , 1 , 1 ) # Rectangle(x0, y0, x1, y1) >>> ctx.set_source ( pat ) >>> ctx.fill () >>> ctx.translate ( 0 .1, 0 .1 ) # Changing the current transformation matrix >>> >>> ctx.move_to ( 0 , 0 ) >>> # Arc(cx, cy, radius, start_angle, stop_angle) ... ctx.arc ( 0 .2, 0 .1, 0 .1, -math.pi/2, 0 ) >>> ctx.line_to ( 0 .5, 0 .1 ) # Line to (x,y) >>> # Curve(x1, y1, x2, y2, x3, y3) ... ctx.curve_to ( 0 .5, 0 .2, 0 .5, 0 .4, 0 .2, 0 .8 ) >>> ctx.close_path () >>> >>> ctx.set_source_rgb ( 0 .3, 0 .2, 0 .5 ) # Solid color >>> ctx.set_line_width ( 0 .02 ) >>> ctx.stroke () >>> surface.write_to_png ( \"example.png\" ) # Output to PNG >>> exit () This generates a file example.png in the working directory.","title":"Cairo"},{"location":"Python/#example","text":"Let's look at another example: We use pip to install a Python chess library. Then we run Python inside of an enclave and import the chess library. We use the Scholar's mate example from https://pypi.python.org/pypi/python-chess","title":"Example"},{"location":"Python/#limitation","text":"We do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner.","title":"Limitation"},{"location":"Python/#screencast","text":"The other environment variables are explained below. Also read Section Environment Variables for further details. \u21a9","title":"Screencast"},{"location":"R/","text":"R We just added experimental support for R. For now, Rscript only support a single script as argument: no expressions or other arguments supported. Image Currently, we provide experimental support for R 3.5.0. You can pull this image as follows: docker pull sconecuratedimages/apps:R Running R To start R, just execute: docker run -it --rm sconecuratedimages/apps:R This will output something like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=4194304 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab (Thu Aug 16 16:45:05 2018 +0200) Branch: new-docker-images-cf (dirty) Configure options: --enable-shared --enable-debug --prefix=/home/christof/GIT/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 71e730d77fcae6fd37b80cd8669f2d75b8e58dbba80afa48929ae817bf263bb0 Warning message: failed to set alternate signal stack R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) Example Execute some first R program (taken from www.rexamples.com ): a <- 42 A <- a * 2 # R is case sensitive print ( a ) cat ( A , \"\\n\" ) # \"84\" is concatenated with \"\\n\" if ( A > a ) # true, 84 > 42 { cat ( A , \">\" , a , \"\\n\" ) } This will result in an output: 84 > 42 Example 2 A somewhat more complex example from www.rexamples.com : #utility functions readinteger <- function () { n <- readline ( prompt = \"Enter an integer: \" ) if ( ! grepl ( \"^[0-9]+$\" , n )) { return ( readinteger ()) } return ( as.integer ( n )) } # real program start here num <- round ( runif ( 1 ) * 100 , digits = 0 ) guess <- -1 cat ( \"Guess a number between 0 and 100.\\n\" ) while ( guess != num ) { guess <- readinteger () if ( guess == num ) { cat ( \"Congratulations,\" , num , \"is right.\\n\" ) } else if ( guess < num ) { cat ( \"It's bigger!\\n\" ) } else if ( guess > num ) { cat ( \"It's smaller!\\n\" ) } } This will result in an otherput as follows: Enter an integer: 50 It's bigger! Enter an integer: 75 It's bigger! Enter an integer: 87 It's smaller! Enter an integer: 82 It's smaller! Enter an integer: 78 It's bigger! Enter an integer: 80 It's bigger! Enter an integer: 81 Congratulations, 81 is right. Screencast","title":"R"},{"location":"R/#r","text":"We just added experimental support for R. For now, Rscript only support a single script as argument: no expressions or other arguments supported.","title":"R"},{"location":"R/#image","text":"Currently, we provide experimental support for R 3.5.0. You can pull this image as follows: docker pull sconecuratedimages/apps:R","title":"Image"},{"location":"R/#running-r","text":"To start R, just execute: docker run -it --rm sconecuratedimages/apps:R This will output something like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=4194304 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab (Thu Aug 16 16:45:05 2018 +0200) Branch: new-docker-images-cf (dirty) Configure options: --enable-shared --enable-debug --prefix=/home/christof/GIT/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 71e730d77fcae6fd37b80cd8669f2d75b8e58dbba80afa48929ae817bf263bb0 Warning message: failed to set alternate signal stack R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit)","title":"Running R"},{"location":"R/#example","text":"Execute some first R program (taken from www.rexamples.com ): a <- 42 A <- a * 2 # R is case sensitive print ( a ) cat ( A , \"\\n\" ) # \"84\" is concatenated with \"\\n\" if ( A > a ) # true, 84 > 42 { cat ( A , \">\" , a , \"\\n\" ) } This will result in an output: 84 > 42","title":"Example"},{"location":"R/#example-2","text":"A somewhat more complex example from www.rexamples.com : #utility functions readinteger <- function () { n <- readline ( prompt = \"Enter an integer: \" ) if ( ! grepl ( \"^[0-9]+$\" , n )) { return ( readinteger ()) } return ( as.integer ( n )) } # real program start here num <- round ( runif ( 1 ) * 100 , digits = 0 ) guess <- -1 cat ( \"Guess a number between 0 and 100.\\n\" ) while ( guess != num ) { guess <- readinteger () if ( guess == num ) { cat ( \"Congratulations,\" , num , \"is right.\\n\" ) } else if ( guess < num ) { cat ( \"It's bigger!\\n\" ) } else if ( guess > num ) { cat ( \"It's smaller!\\n\" ) } } This will result in an otherput as follows: Enter an integer: 50 It's bigger! Enter an integer: 75 It's bigger! Enter an integer: 87 It's smaller! Enter an integer: 82 It's smaller! Enter an integer: 78 It's bigger! Enter an integer: 80 It's bigger! Enter an integer: 81 Congratulations, 81 is right.","title":"Example 2"},{"location":"R/#screencast","text":"","title":"Screencast"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/","text":"Running Java Applications in Scone with CAS-Policy Requirements The following steps assume that you have a running CAS container and a running LAS server and know its addresses. Some of the latest containers for CAS and LAS are available at docker pull sconecuratedimages/services:cas docker pull sconecuratedimages/kubernetes:las but the service repository requires a commercial subscription. You can use the public CAS server scone-cas.cf instead. Make sure the sgx driver and docker is installed and running correctly by doing the java hello world sample: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-8-alpine or, in case the container does not start the bash command automatically: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-8-alpine /bin/sh For Java 11 or Java 15 instead of Java 8: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-11-alpine /bin/sh docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-15-alpine /bin/sh We provide a quick hello-world sample in Java cat > HelloWorld.java << EOF import java.util.Map; public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); Map<String, String> env = System.getenv(); for (String envName : env.keySet()) { System.out.format(\"%s=%s%n\", envName, env.get(envName)); } } } EOF Compile it, by executing: javac HelloWorld.java Expected output: # javac HelloWorld.java Picked up JAVA_TOOL_OPTIONS: -Xmx256m # You need to set some environment variables: export SCONE_CAS_ADDR = scone-cas.cf # we use the public CAS service export SCONE_LAS_ADDR = 127 .0.0.1 # we must run a local LAS service export SCONE_VERSION = 1 # show the SCONE version export SCONE_LOG = 7 # maximum LOG level java HelloWorld Expected output after 2-3 minutes with a NUC and 8GB memory. Note that if you have a CPU that supports EDMM (Enclave dynamic memory management), the startup times will be much quicker. / # export SCONE_VERSION = 1 / # export SCONE_LOG = 7 / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG =/ etc / sgx - musl . conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version : 1.1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 : 29 : 26 2020 + 0000 ) Branch: 6 ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash : 7954791612 f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . Picked up JAVA_TOOL_OPTIONS : - Xmx256m [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / mountinfo is not supported [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / maps is not supported OpenJDK 64 - Bit Server VM warning : Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . Hello World PATH =/ usr / local / sbin :/ usr / local / bin :/ usr / sbin :/ usr / bin :/ sbin :/ bin :/ usr / lib / jvm / java - 1.8 - openjdk / jre / bin :/ usr / lib / jvm / java - 1.8 - openjdk / bin SCONE_VERSION = 1 SCONE_HEAP = 4 G SCONE_LOG = 7 SCONE_CAS_ADDR = scone - cas . cf JAVA_HOME =/ usr / lib / jvm / java - 1.8 - openjdk TERM = xterm LANG = C . UTF - 8 SCONE_ALPINE = 1 SCONE_ALLOW_DLOPEN = 2 HOSTNAME = 86 fd2f848e41 SCONE_MPROTECT = 1 SCONE_LAS_ADDR = 127.0.0.1 LD_LIBRARY_PATH =/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 / server :/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 :/ usr / lib / jvm / java - 1.8 - openjdk / jre /../ lib / amd64 JAVA_TOOL_OPTIONS =- Xmx256m PWD =/ HOME =/ root SHLVL = 2 / # Posting a SCONE-Policy to configure the execution inside of Enclave Using the CAS container to provision an execution inside of an enclave with environment variables etc. which were previously set. Therefore, the SCONE-Policy for Java executions must include the following parameters LD_LIBRARY_PATH and if necessary JAVA_TOOL_OPTIONS and the CLASSPATH of the required Java libraries: First, similar to the posting sessions tutorial , we create a yaml file for java 1.8 openjdk cat > sessionJavaHelloWorld.yml <<EOF name: java services: - name: hello mrenclaves: [7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb] # MRENCLAVE = Enclave hash: from execution with SCONE_VERSION=1 command: \"java HelloWorld\" pwd: / environment: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64\" JAVA_TOOL_OPTIONS: \"-Xmx256m\" CLASSPATH: \"/\" # path to directory contained your code TMP_SECRET_VAR: \"This is a protected secret distributed by Scone CAS!\" EOF We create client certificate and key material to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @sessionJavaHelloWorld.yml -X POST https:// $SCONE_CAS_ADDR :8081/session Expected output (please ignore the 'not-supported' fields): Created Session [ id = b11ac009c564f6e0c8c2b61d7b4774e89f896e83ec09ef3f919bd420aa3ab252, name = 'not-supported' , status = 'not-supported' ] If the output is not giving out an idea the session was not created. One possible reasons can be formatting errors like not using quotation marks (\"java Helloworld\") or single quotation marks ('java HelloWorld') If we have set up the SCONE_CAS_ADDR and SCONE_LAS_ADDR, we export the environment variable according to the policy's name \"java\" and the service \"hello\" export SCONE_CONFIG_ID = \"java/hello\" export SCONE_CAS_ADDR = $YOUR_CAS_ADDR export SCONE_LAS_ADDR = $YOUR_LAS_ADDR export SCONE_VERSION = 1 export SCONE_LOG = 7 Now if we start the command \"java HelloWorld\" again, we see the following output. / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version: 1 .1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 :29:26 2020 +0000 ) Branch: 6ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash: 7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | INFO ] src/shielding/crypto.c:222:crypto_get_identity () : Generated enclave certificate [ SCONE | INFO ] src/shielding/eai_attestor.c:174:eai_attestor_init () : Created TLS context to communicate with CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:288:eai_attestor_attest () : Sending Attestation Request to LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:311:eai_attestor_attest () : Got Quote from LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:324:eai_attestor_attest () : Sending enclave hello message to CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:349:eai_attestor_attest () : Successfully attested enclave via CAS [ SCONE | DEBUG ] src/process/init.c:639:__scone_prepare_secure_config () : Sending configuration request to CAS! [ SCONE | DEBUG ] src/process/init.c:666:__scone_prepare_secure_config () : Received configuration from CAS! [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. Picked up JAVA_TOOL_OPTIONS: -Xmx256m [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/mountinfo is not supported [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/maps is not supported OpenJDK 64 -Bit Server VM warning: Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World TMP_SECRET_VAR = This is a protected secret distributed by Scone CAS! CLASSPATH = / JAVA_TOOL_OPTIONS = -Xmx256m LD_LIBRARY_PATH = /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64 / # Note that the content of TMP_SECRET_VAR came from the Scone policy which was posted earlier to the CAS. And we see the following logfiles in the containers of CAS (in case you are running a local copy): ubuntu@kmaster:~$ docker logs cas_cas_1 [ 2020 -05-08T11:32:02Z DEBUG rustls::server::hs ] decided upon suite SupportedCipherSuite { suite: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, kx: ECDHE, bulk: AES_256_GCM, hash: SHA384, sign: ECDSA, enc_key_len: 32 , fixed_iv_len: 4 , explicit_nonce_len: 8 , hkdf_algorithm: Algorithm ( Algorithm ( SHA384 )) } [ 2020 -05-08T11:32:02Z DEBUG rustls::server::tls12 ] Session saved [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 518 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::EnclaveHello from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Fresh got message: EnclaveMessage::EnclaveHello [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Processing enclave hello message [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is valid, it has not been tampared with. [ 2020 -05-08T11:32:02Z WARN cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : CAS is running in debug mode, simulation mode or w/o SCONE runtime: It will accept quotes from SCONE quoting enclaves running in debug mode [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Hash of channel certificate: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976, SGX reportdata: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976 [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Secure channel terminates in enclave: Hash of channel certifacte matches SGX reportdata [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is signed by known and trusted quoting enclave: true [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Trust established into enclave integrity. Sending attestation complete message [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Complete to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 16 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::ConfigRequest from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Attested got message: EnclaveMessage::ConfigRequest [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Received configuration request for config id javax2/hello [ 2020 -05-08T11:32:02Z DEBUG cas_backend::backend ] Got request for service: hello in session: javax2. Attificate is: SCONE ( SCONEAttificate { identifier: \"791f84651890514eefe32ecfe54a75d95475636f61180a6ca1765cbbee0253dd\" , certificate: \"-----BEGIN CERTIFICATE-----\\nMIIBZjCB6qADAgECAhEA9vVqCQoOsEgtQ7Lo09QqNjAMBggqhkjOPQQDAwUAMA0x\\nCzAJBgNVBAYTAkRFMCAXDTE4MDkxNzE5MzAwMFoYDzIwNTAwMTAxMDEwMDAwWjAA\\nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAEoavE7JGhE5YqrBV979f8NTxUzGURrDE2\\naU3vuZukWkioO8g+3Ec+YaVMu0N3CWy0hnwKD4nyCr7criLlXhvRaxQdJdp6d+4b\\nMJXGtrgvcF4I1ZAb17FJOHoUD9Q5IVKSoxcwFTATBgNVHREEDDAKgghlbmNsYXZl\\nADAMBggqhkjOPQQDAwUAA2kAMGYCMQCusCCDg8EKrrP75FL5kUShjoG28dWsAMVN\\nDfv2InlX5ZCATrZ3YwSl8+MfV1Key0sCMQDMaA+jVF+bqr82C9iIYZhr4vXCWUhI\\njYVnKAo+/GP+pwwvNE7XZwPTXbEPdRoyrbw=\\n-----END CERTIFICATE-----\\n\" , report: SgxReportBody { cpusvn: \"02020000000000000000000000000000\" , miscselect: \"00000000\" , attributes: SgxAttributes { flags: 7 , xfrm: 27 , } , mrenclave: \"7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb\" , mrsigner: \"11c4e150b76c2b145f7fadb6c30455e1046b9e6fbb75b49c6e13341ad8acc5bd\" , isvprodid: 0 , isvsvn: 0 , reportdata: \"ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976\" , } , scone_qe_pub_key: a470cda5245af18ef5d8d3e1b30982a3bc74848376ca4fd03237c0f342e97239, } , ) [ 2020 -05-08T11:32:02Z INFO cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Provisioned configuration javax2/hello [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Config to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z WARN cas::api::enclave ] Failure while handling enclave connection: DisplayChain ( Error ( Msg ( \"Connection closed\" ) , State { next_error: None, backtrace: InternalBacktrace { backtrace: None } })) And we see the following logfiles in the containers of LAS: ubuntu@kmaster:~$ docker logs las_las_1 jhi [ 8 ] : --> jhi start jhi [ 8 ] : <-- jhi start aesm_service [ 11 ] : [ ADMIN ] White List update requested aesm_service [ 11 ] : The server sock is 0x55c64a935e30 aesm_service [ 11 ] : [ ADMIN ] White list update request successful for Version: 73 [ 10000 :INFO@04.05.2020/11:01:42 ] APP: Creating LAS target information message. aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning initiated aesm_service [ 11 ] : The Request ID is f6131707e3714d359f33dca65dc72308 aesm_service [ 11 ] : The Request ID is 23c495fac0b94c8984546fbddc0a75f3 aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning successful [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has MRENCLAVE D9A05D04E07CD75F4251D41A4D7F0FCA63BF392A19BEDE5C69A5BB60A94E376F [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has public key A470CDA5245AF18EF5D8D3E1B30982A3BC74848376CA4FD03237C0F342E97239 [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: LAS is listening on 0 .0.0.0:18766 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 2 , size: 1116 [ 01000 :WARNING@04.05.2020/11:40:47 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@04.05.2020/11:40:47 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection! ...multiple log entries [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 01000 :WARNING@08.05.2020/11:38:27 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection! Common errors Missing LD_LIBRARY_PATH If the system is started without the default LD_LIBRARY_PATH and the SCONE-Policy is missing its environment variable the output will be: [SCONE|ERROR] src/syscall/execve.c:108:syscall_SYS_execve(): execve is only supported after a vfork (vfork is not active!) No error information Error: trying to exec /usr/lib/jvm/java-1.8-openjdk/jre/bin/java. Check if file exists and permissions are set correctly. The solution is settings the LD_LIBRARY_PATH according to the posting session example given above: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/. Author: Hendrik","title":"Java with remote attestation"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#running-java-applications-in-scone-with-cas-policy","text":"","title":"Running Java Applications in Scone with CAS-Policy"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#requirements","text":"The following steps assume that you have a running CAS container and a running LAS server and know its addresses. Some of the latest containers for CAS and LAS are available at docker pull sconecuratedimages/services:cas docker pull sconecuratedimages/kubernetes:las but the service repository requires a commercial subscription. You can use the public CAS server scone-cas.cf instead. Make sure the sgx driver and docker is installed and running correctly by doing the java hello world sample: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-8-alpine or, in case the container does not start the bash command automatically: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-8-alpine /bin/sh For Java 11 or Java 15 instead of Java 8: docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-11-alpine /bin/sh docker run --privileged --device = /dev/isgx -it sconecuratedimages/apps:openjdk-15-alpine /bin/sh We provide a quick hello-world sample in Java cat > HelloWorld.java << EOF import java.util.Map; public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); Map<String, String> env = System.getenv(); for (String envName : env.keySet()) { System.out.format(\"%s=%s%n\", envName, env.get(envName)); } } } EOF Compile it, by executing: javac HelloWorld.java Expected output: # javac HelloWorld.java Picked up JAVA_TOOL_OPTIONS: -Xmx256m # You need to set some environment variables: export SCONE_CAS_ADDR = scone-cas.cf # we use the public CAS service export SCONE_LAS_ADDR = 127 .0.0.1 # we must run a local LAS service export SCONE_VERSION = 1 # show the SCONE version export SCONE_LOG = 7 # maximum LOG level java HelloWorld Expected output after 2-3 minutes with a NUC and 8GB memory. Note that if you have a CPU that supports EDMM (Enclave dynamic memory management), the startup times will be much quicker. / # export SCONE_VERSION = 1 / # export SCONE_LOG = 7 / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG =/ etc / sgx - musl . conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version : 1.1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 : 29 : 26 2020 + 0000 ) Branch: 6 ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash : 7954791612 f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . Picked up JAVA_TOOL_OPTIONS : - Xmx256m [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / mountinfo is not supported [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / maps is not supported OpenJDK 64 - Bit Server VM warning : Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . Hello World PATH =/ usr / local / sbin :/ usr / local / bin :/ usr / sbin :/ usr / bin :/ sbin :/ bin :/ usr / lib / jvm / java - 1.8 - openjdk / jre / bin :/ usr / lib / jvm / java - 1.8 - openjdk / bin SCONE_VERSION = 1 SCONE_HEAP = 4 G SCONE_LOG = 7 SCONE_CAS_ADDR = scone - cas . cf JAVA_HOME =/ usr / lib / jvm / java - 1.8 - openjdk TERM = xterm LANG = C . UTF - 8 SCONE_ALPINE = 1 SCONE_ALLOW_DLOPEN = 2 HOSTNAME = 86 fd2f848e41 SCONE_MPROTECT = 1 SCONE_LAS_ADDR = 127.0.0.1 LD_LIBRARY_PATH =/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 / server :/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 :/ usr / lib / jvm / java - 1.8 - openjdk / jre /../ lib / amd64 JAVA_TOOL_OPTIONS =- Xmx256m PWD =/ HOME =/ root SHLVL = 2 / #","title":"Requirements"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#posting-a-scone-policy-to-configure-the-execution-inside-of-enclave","text":"Using the CAS container to provision an execution inside of an enclave with environment variables etc. which were previously set. Therefore, the SCONE-Policy for Java executions must include the following parameters LD_LIBRARY_PATH and if necessary JAVA_TOOL_OPTIONS and the CLASSPATH of the required Java libraries: First, similar to the posting sessions tutorial , we create a yaml file for java 1.8 openjdk cat > sessionJavaHelloWorld.yml <<EOF name: java services: - name: hello mrenclaves: [7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb] # MRENCLAVE = Enclave hash: from execution with SCONE_VERSION=1 command: \"java HelloWorld\" pwd: / environment: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64\" JAVA_TOOL_OPTIONS: \"-Xmx256m\" CLASSPATH: \"/\" # path to directory contained your code TMP_SECRET_VAR: \"This is a protected secret distributed by Scone CAS!\" EOF We create client certificate and key material to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @sessionJavaHelloWorld.yml -X POST https:// $SCONE_CAS_ADDR :8081/session Expected output (please ignore the 'not-supported' fields): Created Session [ id = b11ac009c564f6e0c8c2b61d7b4774e89f896e83ec09ef3f919bd420aa3ab252, name = 'not-supported' , status = 'not-supported' ] If the output is not giving out an idea the session was not created. One possible reasons can be formatting errors like not using quotation marks (\"java Helloworld\") or single quotation marks ('java HelloWorld') If we have set up the SCONE_CAS_ADDR and SCONE_LAS_ADDR, we export the environment variable according to the policy's name \"java\" and the service \"hello\" export SCONE_CONFIG_ID = \"java/hello\" export SCONE_CAS_ADDR = $YOUR_CAS_ADDR export SCONE_LAS_ADDR = $YOUR_LAS_ADDR export SCONE_VERSION = 1 export SCONE_LOG = 7 Now if we start the command \"java HelloWorld\" again, we see the following output. / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version: 1 .1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 :29:26 2020 +0000 ) Branch: 6ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash: 7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | INFO ] src/shielding/crypto.c:222:crypto_get_identity () : Generated enclave certificate [ SCONE | INFO ] src/shielding/eai_attestor.c:174:eai_attestor_init () : Created TLS context to communicate with CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:288:eai_attestor_attest () : Sending Attestation Request to LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:311:eai_attestor_attest () : Got Quote from LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:324:eai_attestor_attest () : Sending enclave hello message to CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:349:eai_attestor_attest () : Successfully attested enclave via CAS [ SCONE | DEBUG ] src/process/init.c:639:__scone_prepare_secure_config () : Sending configuration request to CAS! [ SCONE | DEBUG ] src/process/init.c:666:__scone_prepare_secure_config () : Received configuration from CAS! [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. Picked up JAVA_TOOL_OPTIONS: -Xmx256m [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/mountinfo is not supported [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/maps is not supported OpenJDK 64 -Bit Server VM warning: Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World TMP_SECRET_VAR = This is a protected secret distributed by Scone CAS! CLASSPATH = / JAVA_TOOL_OPTIONS = -Xmx256m LD_LIBRARY_PATH = /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64 / # Note that the content of TMP_SECRET_VAR came from the Scone policy which was posted earlier to the CAS. And we see the following logfiles in the containers of CAS (in case you are running a local copy): ubuntu@kmaster:~$ docker logs cas_cas_1 [ 2020 -05-08T11:32:02Z DEBUG rustls::server::hs ] decided upon suite SupportedCipherSuite { suite: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, kx: ECDHE, bulk: AES_256_GCM, hash: SHA384, sign: ECDSA, enc_key_len: 32 , fixed_iv_len: 4 , explicit_nonce_len: 8 , hkdf_algorithm: Algorithm ( Algorithm ( SHA384 )) } [ 2020 -05-08T11:32:02Z DEBUG rustls::server::tls12 ] Session saved [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 518 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::EnclaveHello from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Fresh got message: EnclaveMessage::EnclaveHello [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Processing enclave hello message [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is valid, it has not been tampared with. [ 2020 -05-08T11:32:02Z WARN cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : CAS is running in debug mode, simulation mode or w/o SCONE runtime: It will accept quotes from SCONE quoting enclaves running in debug mode [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Hash of channel certificate: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976, SGX reportdata: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976 [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Secure channel terminates in enclave: Hash of channel certifacte matches SGX reportdata [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is signed by known and trusted quoting enclave: true [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Trust established into enclave integrity. Sending attestation complete message [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Complete to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 16 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::ConfigRequest from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Attested got message: EnclaveMessage::ConfigRequest [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Received configuration request for config id javax2/hello [ 2020 -05-08T11:32:02Z DEBUG cas_backend::backend ] Got request for service: hello in session: javax2. Attificate is: SCONE ( SCONEAttificate { identifier: \"791f84651890514eefe32ecfe54a75d95475636f61180a6ca1765cbbee0253dd\" , certificate: \"-----BEGIN CERTIFICATE-----\\nMIIBZjCB6qADAgECAhEA9vVqCQoOsEgtQ7Lo09QqNjAMBggqhkjOPQQDAwUAMA0x\\nCzAJBgNVBAYTAkRFMCAXDTE4MDkxNzE5MzAwMFoYDzIwNTAwMTAxMDEwMDAwWjAA\\nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAEoavE7JGhE5YqrBV979f8NTxUzGURrDE2\\naU3vuZukWkioO8g+3Ec+YaVMu0N3CWy0hnwKD4nyCr7criLlXhvRaxQdJdp6d+4b\\nMJXGtrgvcF4I1ZAb17FJOHoUD9Q5IVKSoxcwFTATBgNVHREEDDAKgghlbmNsYXZl\\nADAMBggqhkjOPQQDAwUAA2kAMGYCMQCusCCDg8EKrrP75FL5kUShjoG28dWsAMVN\\nDfv2InlX5ZCATrZ3YwSl8+MfV1Key0sCMQDMaA+jVF+bqr82C9iIYZhr4vXCWUhI\\njYVnKAo+/GP+pwwvNE7XZwPTXbEPdRoyrbw=\\n-----END CERTIFICATE-----\\n\" , report: SgxReportBody { cpusvn: \"02020000000000000000000000000000\" , miscselect: \"00000000\" , attributes: SgxAttributes { flags: 7 , xfrm: 27 , } , mrenclave: \"7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb\" , mrsigner: \"11c4e150b76c2b145f7fadb6c30455e1046b9e6fbb75b49c6e13341ad8acc5bd\" , isvprodid: 0 , isvsvn: 0 , reportdata: \"ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976\" , } , scone_qe_pub_key: a470cda5245af18ef5d8d3e1b30982a3bc74848376ca4fd03237c0f342e97239, } , ) [ 2020 -05-08T11:32:02Z INFO cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Provisioned configuration javax2/hello [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Config to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z WARN cas::api::enclave ] Failure while handling enclave connection: DisplayChain ( Error ( Msg ( \"Connection closed\" ) , State { next_error: None, backtrace: InternalBacktrace { backtrace: None } })) And we see the following logfiles in the containers of LAS: ubuntu@kmaster:~$ docker logs las_las_1 jhi [ 8 ] : --> jhi start jhi [ 8 ] : <-- jhi start aesm_service [ 11 ] : [ ADMIN ] White List update requested aesm_service [ 11 ] : The server sock is 0x55c64a935e30 aesm_service [ 11 ] : [ ADMIN ] White list update request successful for Version: 73 [ 10000 :INFO@04.05.2020/11:01:42 ] APP: Creating LAS target information message. aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning initiated aesm_service [ 11 ] : The Request ID is f6131707e3714d359f33dca65dc72308 aesm_service [ 11 ] : The Request ID is 23c495fac0b94c8984546fbddc0a75f3 aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning successful [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has MRENCLAVE D9A05D04E07CD75F4251D41A4D7F0FCA63BF392A19BEDE5C69A5BB60A94E376F [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has public key A470CDA5245AF18EF5D8D3E1B30982A3BC74848376CA4FD03237C0F342E97239 [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: LAS is listening on 0 .0.0.0:18766 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 2 , size: 1116 [ 01000 :WARNING@04.05.2020/11:40:47 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@04.05.2020/11:40:47 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection! ...multiple log entries [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 01000 :WARNING@08.05.2020/11:38:27 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection!","title":"Posting a SCONE-Policy to configure the execution inside of Enclave"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#common-errors","text":"","title":"Common errors"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#missing-ld_library_path","text":"If the system is started without the default LD_LIBRARY_PATH and the SCONE-Policy is missing its environment variable the output will be: [SCONE|ERROR] src/syscall/execve.c:108:syscall_SYS_execve(): execve is only supported after a vfork (vfork is not active!) No error information Error: trying to exec /usr/lib/jvm/java-1.8-openjdk/jre/bin/java. Check if file exists and permissions are set correctly. The solution is settings the LD_LIBRARY_PATH according to the posting session example given above: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/. Author: Hendrik","title":"Missing LD_LIBRARY_PATH"},{"location":"Rust/","text":"Rust SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves. To build Rust applications, we provide variants of the rustc and cargo command line utilities as part of image sconecuratedimages/crosscompilers:ubuntu : scone-rustc / scone rustc You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container sconecuratedimages/crosscompilers:ubuntu ): > docker run -it sconecuratedimages/crosscompilers:ubuntu $ scone rustc --version rustc 1 .38.0 ( 625451e37 2019 -09-23 ) Let's try a simple hello world program. $ mkdir ~/projects $ cd ~/projects $ mkdir hello_world $ cd hello_world Let's try our rust program: $ cat > main.rs << EOF fn main() { println!(\"Hello, world!\"); } EOF Let's compile the program for running inside of enclaves: $ scone rustc main.rs --target = x86_64-scone-linux-musl $ ls main main.rs Let's run main inside an enclave and print some debug information: $ SCONE_MODE = HW SCONE_VERSION = 1 ./main export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = no export SCONE_ALLOW_DLOPEN2 = no Hello, world! Screencast scone-cargo and scone cargo : You can build projects with scone cargo : $ scone cargo build --target = x86_64-scone-linux-musl Alternatively, you can use scone-cargo if, for example, you need a command without a space. scone cargo , as well as, scone rustc has access to the SCONE-compiled rust standard library and the target file. --target=x86_64-scone-linux-musl instructs it to use our target file - essentially triggering a cross-compiler build. Due to the cross-compilation, crates that depend on compiled C libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate. The following is an example of how an executable with openssl can be compiled: $ OPENSSL_LIB_DIR = /libressl-2.4.5 OPENSSL_INCLUDE_DIR = /libressl-2.4.5/include/ OPENSSL_STATIC = 1 PKG_CONFIG_ALLOW_CROSS = 1 scone-cargo build --target = scone In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.","title":"Rust"},{"location":"Rust/#rust","text":"SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves. To build Rust applications, we provide variants of the rustc and cargo command line utilities as part of image sconecuratedimages/crosscompilers:ubuntu :","title":"Rust"},{"location":"Rust/#scone-rustc-scone-rustc","text":"You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container sconecuratedimages/crosscompilers:ubuntu ): > docker run -it sconecuratedimages/crosscompilers:ubuntu $ scone rustc --version rustc 1 .38.0 ( 625451e37 2019 -09-23 ) Let's try a simple hello world program. $ mkdir ~/projects $ cd ~/projects $ mkdir hello_world $ cd hello_world Let's try our rust program: $ cat > main.rs << EOF fn main() { println!(\"Hello, world!\"); } EOF Let's compile the program for running inside of enclaves: $ scone rustc main.rs --target = x86_64-scone-linux-musl $ ls main main.rs Let's run main inside an enclave and print some debug information: $ SCONE_MODE = HW SCONE_VERSION = 1 ./main export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = no export SCONE_ALLOW_DLOPEN2 = no Hello, world!","title":"scone-rustc /  scone rustc"},{"location":"Rust/#screencast","text":"","title":"Screencast"},{"location":"Rust/#scone-cargo-and-scone-cargo","text":"You can build projects with scone cargo : $ scone cargo build --target = x86_64-scone-linux-musl Alternatively, you can use scone-cargo if, for example, you need a command without a space. scone cargo , as well as, scone rustc has access to the SCONE-compiled rust standard library and the target file. --target=x86_64-scone-linux-musl instructs it to use our target file - essentially triggering a cross-compiler build. Due to the cross-compilation, crates that depend on compiled C libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate. The following is an example of how an executable with openssl can be compiled: $ OPENSSL_LIB_DIR = /libressl-2.4.5 OPENSSL_INCLUDE_DIR = /libressl-2.4.5/include/ OPENSSL_STATIC = 1 PKG_CONFIG_ALLOW_CROSS = 1 scone-cargo build --target = scone In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.","title":"scone-cargo and scone cargo:"},{"location":"SCONE_CAS/","text":"SCONE CAS SCONE CAS ( Configuration and Attestation Service ) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with their command line arguments their environment variables after the service was attested by the CAS. To attest a service in a swarm, the CAS requires the help of a local attestation service (LAS). As part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure. To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS. CAS Development Environment To set up a development environment with CAS, you can perform the following steps. We assume that you run the following commands inside of a container with the scone CLI like sconecuratedimages/sconecli . Also, we assume that you have access to a Docker swarm managed by some node called faye . First, check that the swarm is properly installed: $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE faye Ready Active Leader If scone swarm ls issues some warnings or there are no SGX-capable machines listed, you might want to run scone swarm check to update the labels of the swarm nodes. To run LAS on all nodes of a cluster managed by node faye , pull the newest LAS image and then execute on all nodes of a SWARM: $ scone service pull sconecuratedimages/services:las $ scone service create --detach = true --mode global --publish mode = host,target = 18766 ,published = 18766 --name = las localhost:5000/services:las Check that the service las is indeed running, check with command ps : $ scone service ps las ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS xvnsruar64qx las.q26sp44pp12uf81zlyhb5pnxf localhost:5000/services:las edna Running Running 3 minutes ago *:18766->18766/tcp zfgx4t292ew6 las.cr3bxhy0nqmg77goxaih5sw8d localhost:5000/services:las dorothy Running Running 3 minutes ago *:18766->18766/tcp n441o9qqmnwa las.os7ihjrel2jb4bplcn81h7f0i localhost:5000/services:las faye Running Running 3 minutes ago *:18766->18766/tcp Now, we can start the CAS service as follows. We first pull the newest CAS image and run it on one node of the swarm. $ scone service pull sconecuratedimages/services:cas $ scone service create --name cas --detach = true --publish 8081 :8081 --publish 18765 :18765 localhost:5000/services:cas Let's check that CAS is indeed running: $ scone service ps cas ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS e6b0skuph9ve cas.1 localhost:5000/services:cas edna Running Running 3 hours ago","title":"SCONE CAS"},{"location":"SCONE_CAS/#scone-cas","text":"SCONE CAS ( Configuration and Attestation Service ) helps to securely configure secure services. A CAS helps to provide services running inside of enclaves with their command line arguments their environment variables after the service was attested by the CAS. To attest a service in a swarm, the CAS requires the help of a local attestation service (LAS). As part of the SCONE Enterprise Version, you can run your own CAS and LAS infrastructure. To do so, we provide you with container images to simplify the execution of CAS and LAS. For other SCONE versions, we can provide you with access to a global CAS.","title":"SCONE CAS"},{"location":"SCONE_CAS/#cas-development-environment","text":"To set up a development environment with CAS, you can perform the following steps. We assume that you run the following commands inside of a container with the scone CLI like sconecuratedimages/sconecli . Also, we assume that you have access to a Docker swarm managed by some node called faye . First, check that the swarm is properly installed: $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE faye Ready Active Leader If scone swarm ls issues some warnings or there are no SGX-capable machines listed, you might want to run scone swarm check to update the labels of the swarm nodes. To run LAS on all nodes of a cluster managed by node faye , pull the newest LAS image and then execute on all nodes of a SWARM: $ scone service pull sconecuratedimages/services:las $ scone service create --detach = true --mode global --publish mode = host,target = 18766 ,published = 18766 --name = las localhost:5000/services:las Check that the service las is indeed running, check with command ps : $ scone service ps las ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS xvnsruar64qx las.q26sp44pp12uf81zlyhb5pnxf localhost:5000/services:las edna Running Running 3 minutes ago *:18766->18766/tcp zfgx4t292ew6 las.cr3bxhy0nqmg77goxaih5sw8d localhost:5000/services:las dorothy Running Running 3 minutes ago *:18766->18766/tcp n441o9qqmnwa las.os7ihjrel2jb4bplcn81h7f0i localhost:5000/services:las faye Running Running 3 minutes ago *:18766->18766/tcp Now, we can start the CAS service as follows. We first pull the newest CAS image and run it on one node of the swarm. $ scone service pull sconecuratedimages/services:cas $ scone service create --name cas --detach = true --publish 8081 :8081 --publish 18765 :18765 localhost:5000/services:cas Let's check that CAS is indeed running: $ scone service ps cas ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS e6b0skuph9ve cas.1 localhost:5000/services:cas edna Running Running 3 hours ago","title":"CAS Development Environment"},{"location":"SCONE_CLI/","text":"SCONE CLI We maintain a single unified command line interface (CLI) scone that helps to to start and stop secure containers as well as secure applications. scone also provides functionality to install and monitor SCONE hosts. Use of docker CLI In many cases you can just use the docker CLI to run SCONE containers. This is particularly useful if you want to run some SCONE containers on your local machine. This removes, for example, the need to setup ssh . The scone command is structured similar as the docker CLI or the infinit CLI: One needs to specify an object (like host ) and a command (like install ) and some options. For some commands, some of the options are actually not optional but mandatory. You need to have access to SCONE container images You require permissions to be able to run the examples given in this section: please send an email with your free Docker ID to info@scontain.com . To use the scone CLI , you need to start it in a container. Assuming that you have a docker engine installed, you try the following examples by running the following container: > docker run -it sconecuratedimages/sconecli Help scone has a built in help. To get a list of all objects , just type: $ scone --help To get a list of all commands for a given object (like host), execute: $ scone host --help To get a list of all options for a given object and command (e.g., host install) and some examples, just execute: $ scone host install --help bash auto-completion If you are using bash as your shell, scone supports auto-completion. This means that instead you can use the TAB key to see the options. For example, $ scone <TAB> will show all available objects. If you have already specified an object, auto-completion helps you to list all commands: $ scone host <TAB> If you also specified an command, it will provide you with a list of options (that you have not specified yet): $ scone host install <TAB> Of course, it also supports auto-completion: $ scone host install -n<TAB> will result in $ scone host install -name","title":"SCONE CLI"},{"location":"SCONE_CLI/#scone-cli","text":"We maintain a single unified command line interface (CLI) scone that helps to to start and stop secure containers as well as secure applications. scone also provides functionality to install and monitor SCONE hosts. Use of docker CLI In many cases you can just use the docker CLI to run SCONE containers. This is particularly useful if you want to run some SCONE containers on your local machine. This removes, for example, the need to setup ssh . The scone command is structured similar as the docker CLI or the infinit CLI: One needs to specify an object (like host ) and a command (like install ) and some options. For some commands, some of the options are actually not optional but mandatory. You need to have access to SCONE container images You require permissions to be able to run the examples given in this section: please send an email with your free Docker ID to info@scontain.com . To use the scone CLI , you need to start it in a container. Assuming that you have a docker engine installed, you try the following examples by running the following container: > docker run -it sconecuratedimages/sconecli","title":"SCONE CLI"},{"location":"SCONE_CLI/#help","text":"scone has a built in help. To get a list of all objects , just type: $ scone --help To get a list of all commands for a given object (like host), execute: $ scone host --help To get a list of all options for a given object and command (e.g., host install) and some examples, just execute: $ scone host install --help","title":"Help"},{"location":"SCONE_CLI/#bash-auto-completion","text":"If you are using bash as your shell, scone supports auto-completion. This means that instead you can use the TAB key to see the options. For example, $ scone <TAB> will show all available objects. If you have already specified an object, auto-completion helps you to list all commands: $ scone host <TAB> If you also specified an command, it will provide you with a list of options (that you have not specified yet): $ scone host install <TAB> Of course, it also supports auto-completion: $ scone host install -n<TAB> will result in $ scone host install -name","title":"bash auto-completion"},{"location":"SCONE_Compose/","text":"Stack Files SCONE supports to run secure applications consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker stack file. Such an extended stack file defines for each process that runs inside of an enclave, a unique hash value ( MRENCLAVE ). During startup, SCONE performs an attestation for all these secure processes to ensure that the hash of the started program is as expected, i.e., is equal to MRENCLAVE . Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process. By default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field not_scone: \"true\" . In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE). Let's consider an example, that consists of a haproxy that runs in native mode and is directly started with Docker Swarm (indicated by line not_scone: \"true\" ) Moreover, we start myapp (defined in container image myapp-image ) and specify MRENCLAVE . Only after ensuring that the program started in the enclave has the expected MRENCLAVE the arguments and the environment variables are passed to myapp . version : \"3.1.scone\" services : primary-service : image : myapp-image:latest command : /myapp arg1 arg2 arg3 $my_password mrenclave : 5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335 environment : - SECURE_ENV=value - MY_PIN=$my_pin - MY_PASSWORD=$my_password working_dir : / proxy : image : haproxy command : haproxy --read_config_from_environ not_scone : \"true\" environment : - \"HAPROXY_CONFIG=a=b,c=d,3=4\" secrets : my_pin : kind : numeric length : \"4\" my_password : kind : ascii length : \"8\" The extended stack file is split by SCONE into a standard stack file and a configuration information that is stored in the SCONE Configruation and Attestation Service ( CAS ): In the current version of scone, we assume that the stack file is stored on a trusted host. The split functionality is part of the scone CLI , i.e., you can split a stack file via $ scone cas split <STACK_FILE> --stack <STACK_ID> The stack ID is a randomly chose unique ID. This creates a stack file that can be used to start a set of services in your Swarm. Read nginx example for some more details how to do this. To be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address $IP . If you have set up the CAS in the way specified in CAS Setup , set environment variable IP to the external IP address of the swarm manager node. You can login into CAS via: $ scone cas login -c CA.PEM YOURID CASALIAS --host $IP :8081:18765 Replace YOURID by your desired user id and CASALIAS by a name that you want to use to refer to this cas (see example ). See nginx example for some more details and read CAS Setup to learn how to run a CAS service for development. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Stack Files"},{"location":"SCONE_Compose/#stack-files","text":"SCONE supports to run secure applications consisting of multiple secure containers. To do so, SCONE introduces slightly extended Docker stack file. Such an extended stack file defines for each process that runs inside of an enclave, a unique hash value ( MRENCLAVE ). During startup, SCONE performs an attestation for all these secure processes to ensure that the hash of the started program is as expected, i.e., is equal to MRENCLAVE . Only if it is equal, the arguments and the environment variables are passed to the process. In this way, we can pass secrets as arguments or environment variables in a secure fashion to a secure process. By default, all containers are started with SCONE and the arguments are passed in a secure fashion to the started processes. However, you might not want to run all processes inside of enclaves. For containers that should be directly started with Docker Swarm, you need to set field not_scone: \"true\" . In this case, all arguments and the environment variables are directly passed to the container with Docker Swarm (instead of SCONE). Let's consider an example, that consists of a haproxy that runs in native mode and is directly started with Docker Swarm (indicated by line not_scone: \"true\" ) Moreover, we start myapp (defined in container image myapp-image ) and specify MRENCLAVE . Only after ensuring that the program started in the enclave has the expected MRENCLAVE the arguments and the environment variables are passed to myapp . version : \"3.1.scone\" services : primary-service : image : myapp-image:latest command : /myapp arg1 arg2 arg3 $my_password mrenclave : 5764436f08dd4cdb526f082be1a07a3422f79ef2b01a5e24f78f9034a838c335 environment : - SECURE_ENV=value - MY_PIN=$my_pin - MY_PASSWORD=$my_password working_dir : / proxy : image : haproxy command : haproxy --read_config_from_environ not_scone : \"true\" environment : - \"HAPROXY_CONFIG=a=b,c=d,3=4\" secrets : my_pin : kind : numeric length : \"4\" my_password : kind : ascii length : \"8\" The extended stack file is split by SCONE into a standard stack file and a configuration information that is stored in the SCONE Configruation and Attestation Service ( CAS ): In the current version of scone, we assume that the stack file is stored on a trusted host. The split functionality is part of the scone CLI , i.e., you can split a stack file via $ scone cas split <STACK_FILE> --stack <STACK_ID> The stack ID is a randomly chose unique ID. This creates a stack file that can be used to start a set of services in your Swarm. Read nginx example for some more details how to do this. To be able to use a CAS, you are required to login to a CAS. We assume that your CAS is available at IP address $IP . If you have set up the CAS in the way specified in CAS Setup , set environment variable IP to the external IP address of the swarm manager node. You can login into CAS via: $ scone cas login -c CA.PEM YOURID CASALIAS --host $IP :8081:18765 Replace YOURID by your desired user id and CASALIAS by a name that you want to use to refer to this cas (see example ). See nginx example for some more details and read CAS Setup to learn how to run a CAS service for development. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Stack Files"},{"location":"SCONE_Curated_Images/","text":"SCONE Curated Images We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub: Private images: 1 Image Name Description sconecuratedimages/crosscompilers a container image with all the SCONE crosscompilers. sconecuratedimages/crosscompilers:runtime a container image that can run dynamically linked applications inside of an enclave. sconecuratedimages/apps:python-3.7.3-alpine3.10 a container image including a python interpreter running inside of an enclave. sconecuratedimages/apps:python-2.7-alpine3.6 a container image including a python interpreter running inside of an enclave. sconecuratedimages/apps:mongodb-alpine MongoDB container image. sconecuratedimages/apps:scone-vault-latest Vault 0.8.1 container image. sconecuratedimages/apps:memcached-alpine Memcached container image. sconecuratedimages/apps:node-8.9-alpine a container image for node running inside an enclave. sconecuratedimages/apps:nginx-1.13-alpine a container image for nginx running inside an enclave. sconecuratedimages/apps:8-jdk-alpine a container image for Java applications running inside an enclave. Please send us an email if you need a curated image of another application or a different/newer version of an application. Most of the time, we will be able to provide you an image on short notice. Login in Access to some SCONE images is restricted. First, create a new docker hub ID (- in case you do not yet have one). Second, get access to the private images for evaluation by sending email to scontain.com with your docker hub id and short statement what images you want to evaluate and what you plan to do with the images. Second, log into to docker hub via: > docker login before you will be able to pull any of the private curated images. Scone Compilers To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer. Dynamically-Linked Binaries Even if you have no SGX CPU extension / no SGX driver installed on your computer, you can use a standard gcc compiler - as long as the requirements mentioned in SGX ToolChain are satisfied. docker pull sconecuratedimages/muslgcc Note that the binaries generated with the above image are just native binaries, i.e., they run outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library. To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image: docker pull sconecuratedimages/crosscompilers:runtime Statically-Linked Binaries To generate statically-linked secure binaries you need a cross compiler. You can pull this image from Docker hub (you need to be granted access rights for that): docker pull sconecuratedimages/crosscompilers Scone Hello World You can pull the following (private) image. This image only runs in hardware mode: docker pull sconecuratedimages/helloworld If you installed the patched Docker engine , run the helloworld program inside of an enclave via > docker run sconecuratedimages/helloworld Hello World This command will fail in case you have the standard Docker engine installed: docker run sconecuratedimages/helloworld error opening sgx device: No such file or directory You can run on the standard Docker engine - if you have the SGX driver installed: > docker run --device = /dev/isgx sconecuratedimages/helloworld Hello World If you do not have the SGX driver installed, you get an error message: > docker run --device = /dev/isgx sconecuratedimages/helloworld docker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device \"/dev/isgx\" : no such file or directory. In this case, install the SGX driver . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled. Screencast \u00a9 scontain.com , 2020. Questions or Suggestions? send email to info@scontain.com to get access. \u21a9","title":"SCONE Curated Images"},{"location":"SCONE_Curated_Images/#scone-curated-images","text":"We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub: Private images: 1 Image Name Description sconecuratedimages/crosscompilers a container image with all the SCONE crosscompilers. sconecuratedimages/crosscompilers:runtime a container image that can run dynamically linked applications inside of an enclave. sconecuratedimages/apps:python-3.7.3-alpine3.10 a container image including a python interpreter running inside of an enclave. sconecuratedimages/apps:python-2.7-alpine3.6 a container image including a python interpreter running inside of an enclave. sconecuratedimages/apps:mongodb-alpine MongoDB container image. sconecuratedimages/apps:scone-vault-latest Vault 0.8.1 container image. sconecuratedimages/apps:memcached-alpine Memcached container image. sconecuratedimages/apps:node-8.9-alpine a container image for node running inside an enclave. sconecuratedimages/apps:nginx-1.13-alpine a container image for nginx running inside an enclave. sconecuratedimages/apps:8-jdk-alpine a container image for Java applications running inside an enclave. Please send us an email if you need a curated image of another application or a different/newer version of an application. Most of the time, we will be able to provide you an image on short notice.","title":"SCONE Curated Images"},{"location":"SCONE_Curated_Images/#login-in","text":"Access to some SCONE images is restricted. First, create a new docker hub ID (- in case you do not yet have one). Second, get access to the private images for evaluation by sending email to scontain.com with your docker hub id and short statement what images you want to evaluate and what you plan to do with the images. Second, log into to docker hub via: > docker login before you will be able to pull any of the private curated images.","title":"Login in"},{"location":"SCONE_Curated_Images/#scone-compilers","text":"To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer.","title":"Scone Compilers"},{"location":"SCONE_Curated_Images/#dynamically-linked-binaries","text":"Even if you have no SGX CPU extension / no SGX driver installed on your computer, you can use a standard gcc compiler - as long as the requirements mentioned in SGX ToolChain are satisfied. docker pull sconecuratedimages/muslgcc Note that the binaries generated with the above image are just native binaries, i.e., they run outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library. To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image: docker pull sconecuratedimages/crosscompilers:runtime","title":"Dynamically-Linked Binaries"},{"location":"SCONE_Curated_Images/#statically-linked-binaries","text":"To generate statically-linked secure binaries you need a cross compiler. You can pull this image from Docker hub (you need to be granted access rights for that): docker pull sconecuratedimages/crosscompilers","title":"Statically-Linked Binaries"},{"location":"SCONE_Curated_Images/#scone-hello-world","text":"You can pull the following (private) image. This image only runs in hardware mode: docker pull sconecuratedimages/helloworld If you installed the patched Docker engine , run the helloworld program inside of an enclave via > docker run sconecuratedimages/helloworld Hello World This command will fail in case you have the standard Docker engine installed: docker run sconecuratedimages/helloworld error opening sgx device: No such file or directory You can run on the standard Docker engine - if you have the SGX driver installed: > docker run --device = /dev/isgx sconecuratedimages/helloworld Hello World If you do not have the SGX driver installed, you get an error message: > docker run --device = /dev/isgx sconecuratedimages/helloworld docker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device \"/dev/isgx\" : no such file or directory. In this case, install the SGX driver . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled.","title":"Scone Hello World"},{"location":"SCONE_Curated_Images/#screencast","text":"\u00a9 scontain.com , 2020. Questions or Suggestions? send email to info@scontain.com to get access. \u21a9","title":"Screencast"},{"location":"SCONE_Dockerfile/","text":"Dockerfile We show how to generate a first secure container image with the help of a Dockerfile. Prerequisites Ensure that the sgx driver is installed > ls /dev/isgx /dev/isgx If the driver is not installed, read Section Installation of SGX driver to learn how to install the SGX driver. Install the tutorial Clone the tutorial: > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git Access to SCONE Curated Images Right now, access to the curated images is still restricted. Please, send email to scontain.ceo@gmail.com to request access. Generate HelloAgain image (dynamically-linked) We first generate a hello again container image with a dynamically-linked secure program: > cd SCONE_TUTORIAL/DLDockerFile The Dockerfile to generate the new image looks like this: FROM sconecuratedimages/crosscompilers:runtime RUN mkdir /hello COPY dyn_hello_again /hello/ CMD SCONE_MODE = HW SCONE_ALPINE = 1 SCONE_VERSION = 1 /hello/dyn_hello_again This assumes that we already generated the dynamically linked binary with an appropriately configured gcc. We generate this with the provided gcc image: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_again.c -o dyn_hello_again We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials): > ./generate.sh You can run this program inside of enclave (with the output of debug messages): > docker run -it sconecuratedimages/helloworld:dynamic export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello Again This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment. Running on a docker engine without access to SGX, we get an error message: > docker run -it sconecuratedimages/helloworld:dynamic [ Error ] Could not create enclave: Error opening SGX device Screencast Generate HelloAgain image (statically-linked) We generate a hello again container image. > cd SCONE_TUTORIAL/DockerFile The Dockerfile is quite straight forward: FROM sconecuratedimages/crosscompilers MAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\" RUN mkdir /hello COPY hello_again.c /hello/ RUN cd /hello && scone-gcc hello_again.c -o again CMD [ \"/hello/again\" ] You can either execute all step manually (see below) or you can just execute > docker login ./generate.sh and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub. We define the image name and tag that we want to generate: export TAG = \"again\" export FULLTAG = \"sconecuratedimages/helloworld: $TAG \" We build the image: > docker build --pull -t $FULLTAG . > docker run -it $FULLTAG We push it to docker hub (will fail unless you have the right to push $FULLTAG ): > docker push $FULLTAG Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"myrepository/helloAgain\" Screencast \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"SCONE Dockerfile"},{"location":"SCONE_Dockerfile/#dockerfile","text":"We show how to generate a first secure container image with the help of a Dockerfile.","title":"Dockerfile"},{"location":"SCONE_Dockerfile/#prerequisites","text":"","title":"Prerequisites"},{"location":"SCONE_Dockerfile/#ensure-that-the-sgx-driver-is-installed","text":"> ls /dev/isgx /dev/isgx If the driver is not installed, read Section Installation of SGX driver to learn how to install the SGX driver.","title":"Ensure that the sgx driver is installed"},{"location":"SCONE_Dockerfile/#install-the-tutorial","text":"Clone the tutorial: > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git","title":"Install the tutorial"},{"location":"SCONE_Dockerfile/#access-to-scone-curated-images","text":"Right now, access to the curated images is still restricted. Please, send email to scontain.ceo@gmail.com to request access.","title":"Access to SCONE Curated Images"},{"location":"SCONE_Dockerfile/#generate-helloagain-image-dynamically-linked","text":"We first generate a hello again container image with a dynamically-linked secure program: > cd SCONE_TUTORIAL/DLDockerFile The Dockerfile to generate the new image looks like this: FROM sconecuratedimages/crosscompilers:runtime RUN mkdir /hello COPY dyn_hello_again /hello/ CMD SCONE_MODE = HW SCONE_ALPINE = 1 SCONE_VERSION = 1 /hello/dyn_hello_again This assumes that we already generated the dynamically linked binary with an appropriately configured gcc. We generate this with the provided gcc image: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_again.c -o dyn_hello_again We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials): > ./generate.sh You can run this program inside of enclave (with the output of debug messages): > docker run -it sconecuratedimages/helloworld:dynamic export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello Again This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment. Running on a docker engine without access to SGX, we get an error message: > docker run -it sconecuratedimages/helloworld:dynamic [ Error ] Could not create enclave: Error opening SGX device","title":"Generate HelloAgain image (dynamically-linked)"},{"location":"SCONE_Dockerfile/#screencast","text":"","title":"Screencast"},{"location":"SCONE_Dockerfile/#generate-helloagain-image-statically-linked","text":"We generate a hello again container image. > cd SCONE_TUTORIAL/DockerFile The Dockerfile is quite straight forward: FROM sconecuratedimages/crosscompilers MAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\" RUN mkdir /hello COPY hello_again.c /hello/ RUN cd /hello && scone-gcc hello_again.c -o again CMD [ \"/hello/again\" ] You can either execute all step manually (see below) or you can just execute > docker login ./generate.sh and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub. We define the image name and tag that we want to generate: export TAG = \"again\" export FULLTAG = \"sconecuratedimages/helloworld: $TAG \" We build the image: > docker build --pull -t $FULLTAG . > docker run -it $FULLTAG We push it to docker hub (will fail unless you have the right to push $FULLTAG ): > docker push $FULLTAG Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"myrepository/helloAgain\"","title":"Generate HelloAgain image (statically-linked)"},{"location":"SCONE_Dockerfile/#screencast_1","text":"\u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_EE2EE/","text":"Example: End-to-End Encryption We show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted. In what follows, we only interact with one swarm. Hence, we set environment variable SCONE_MANAGER to point to the manager node of the swarm. Say, in our case the manage is called faye : $ export SCONE_MANAGER = faye We have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files from the file system and decrypting these files inside the enclave. Let's assume that we have the newest image in our local swarm registry: $ scone service pull sconecuratedimages/sconetainer:fss ... fss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size: 1783 new tag: localhost:5000/sconetainer:fss We will start this service as a stack. Hence, we define a simple stack/compose file: $ cat > compose.yml << EOF version: \"3.1.scone\" services: nginx: image: 127.0.0.1:5000/sconetainer:fss command: nginx -p /nginx -c nginx.conf mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb fspf_path: /nginx/fspf.pb fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag: 5930bffbd9ea2f1317e6872b032334db working_dir: / ports: - 8190:8080 - 8192:8082 EOF We explain in section MrEnclave how to determine mrenclave. For now, this file contains some metadata related to the encrypted files: fspf_path the path of the file system protection file fspf_key the key used to encrypt the file system protection file fspf_tag the tag (i.e., MAC) of the file system protection file We provide a low level for encrypting files with the help of scone fspf . (We will soon release a higher level support that will simplify the encryption of files). In this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps us to to pass some secrets like the fspf_key to an enclave and protecting both the confidentiality as well as the integrity of this secret. Clients can identify the CAS via its certificate. In this example, we explicitly specify the certificate of the CAS: $ cat > ca.pem << EOF -----BEGIN CERTIFICATE----- MIICFzCCAb2gAwIBAgIJAOawWIYrvd1oMAoGCCqGSM49BAMCMGgxCzAJBgNVBAYT AkRFMQ8wDQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoM C3Njb250YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTAe Fw0xODA0MDYxMDIyMzdaFw0yODA0MDMxMDIyMzdaMGgxCzAJBgNVBAYTAkRFMQ8w DQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoMC3Njb250 YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTBZMBMGByqG SM49AgEGCCqGSM49AwEHA0IABD9u85bK+nFdbnQVgZuR/rA9BmNmow3v4v5srS3M YGpVmRqpNbb/QYQ9iJN854N42L9T6mGyI402tKPYyfwz3k+jUDBOMB0GA1UdDgQW BBRhGJGv9MWohiZD3AySHz/otlxbyDAfBgNVHSMEGDAWgBRhGJGv9MWohiZD3AyS Hz/otlxbyDAMBgNVHRMEBTADAQH/MAoGCCqGSM49BAMCA0gAMEUCIQCWkKHgeDgn 4PrHfmfjYYerxyFyGmWOKjO5UcijrPqI9wIgUyhZ2OwuyzsjTEC6ofR4fzlnrUQJ XlkFOKY2/HqOPVE= -----END CERTIFICATE----- EOF and now log into the CAS: $ export IP = x.y.z.u $ mkdir -p ~/.config $ rm -f ~/.config/scone_cmd.conf $ scone cas login -c ca.pem christof cas --host $IP :8081:18765 We split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS: $ scone cas split compose.yml --stack 283299 For now, the stack ID is a randomly chose unique ID. If you want to modify this stack later, you need to use the same stack ID. Show public part that is sent to docker stack (which is the original stack file name appended with \"docker.yml\"): $ cat compose.yml.docker.yml --- services: nginx: command: nginx environment: SCONE_CAS_ADDR: \"x.y.z.u:18765\" SCONE_CONFIG_ID: christof/283299/nginx SCONE_LAS_ADDR: \"172.17.0.1:18766\" image: \"127.0.0.1:5000/sconetainer:fss\" ports: - \"8190:8080\" - \"8192:8082\" version: \"3.1\" Note: in the above example we assume that the host on which nginx is running is available at address 172.17.0.1 . Hence, the LAS is available at address \"172.17.0.1:18766 . In case you run a non-standard configuration, you might need to change SCONE_LAS_ADDR to the ip address and port where the LAS is available. The LAS must run on the same host for the attestation to work. We can now deploy the service with the help of docker/scone stack: $ scone stack deploy --compose-file compose.yml nginx Creating network nginx_default Creating service nginx_nginx Show running stack: $ scone stack ls NAME SERVICES nginx 1 We can get some more information about this stack: $ scone stack ps nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS h08r5hmeu9bt nginx_nginx.1 127 .0.0.1:5000/sconetainer:fss dorothy Running Starting 19 seconds ago File Encryption Let's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node dorothy and execute the following commands: > sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b01ab65e1579 127 .0.0.1:5000/sconetainer:fss \"nginx\" 6 minutes ago Up 6 minutes 8080 /tcp, 8082 /tcp nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix ... Let's look inside the container: > sudo docker exec -it b01ab65e1579 sh $ ls bin media proc srv dev mnt root sys etc nginx run tmp home nginx-etc sbin usr lib opt scone-test.key var The nginx configuration file as well as the html files should be encrypted. Let's verify this: $ cd nginx $ ls certificate.pem key.pem nginx.conf www_root fspf.pb mime.types nginx.pid $ head -c 80 nginx.conf ] P?\u025f \"4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t'?2?L????????y??\u02f2?\u07cb Ok, the nginx configuration file seems to be encrypted. Now, look at the html files too: $ cd www_root $ ls 4K SCONE_TUTORIAL GO aboutScone Python appsecurity ... $ head -c 80 index.html ????V?\u036a?rhk? \"k???? $? .???k?\u042c?y<??\u0660n?+????P2?G;o_'.i?);?I??xFg[???[f? \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Example: End-to-End Encryption"},{"location":"SCONE_EE2EE/#example-end-to-end-encryption","text":"We show how to encrypt an application end-to-end, i.e., all the data in the file system is encrypted, all data in main memory is encrypted and all data on the wire is encrypted. In what follows, we only interact with one swarm. Hence, we set environment variable SCONE_MANAGER to point to the manager node of the swarm. Say, in our case the manage is called faye : $ export SCONE_MANAGER = faye We have prepared an nginx image that contains this website as encrypted files and nginx is running inside of an enclave reading the encrypted files from the file system and decrypting these files inside the enclave. Let's assume that we have the newest image in our local swarm registry: $ scone service pull sconecuratedimages/sconetainer:fss ... fss: digest: sha256:2e76b4a0b090cc75c2e56594e20c7ec36bc1abd13e36e2f00d36e2f67b13c1d5 size: 1783 new tag: localhost:5000/sconetainer:fss We will start this service as a stack. Hence, we define a simple stack/compose file: $ cat > compose.yml << EOF version: \"3.1.scone\" services: nginx: image: 127.0.0.1:5000/sconetainer:fss command: nginx -p /nginx -c nginx.conf mrenclave: 1516e3d41590cf3842282c6f037535af64336138a6b5eff7e8754e97b4c64ecb fspf_path: /nginx/fspf.pb fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag: 5930bffbd9ea2f1317e6872b032334db working_dir: / ports: - 8190:8080 - 8192:8082 EOF We explain in section MrEnclave how to determine mrenclave. For now, this file contains some metadata related to the encrypted files: fspf_path the path of the file system protection file fspf_key the key used to encrypt the file system protection file fspf_tag the tag (i.e., MAC) of the file system protection file We provide a low level for encrypting files with the help of scone fspf . (We will soon release a higher level support that will simplify the encryption of files). In this swarm, we have a CAS (Configuration and Attestation Service running): the CAS helps us to to pass some secrets like the fspf_key to an enclave and protecting both the confidentiality as well as the integrity of this secret. Clients can identify the CAS via its certificate. In this example, we explicitly specify the certificate of the CAS: $ cat > ca.pem << EOF -----BEGIN CERTIFICATE----- MIICFzCCAb2gAwIBAgIJAOawWIYrvd1oMAoGCCqGSM49BAMCMGgxCzAJBgNVBAYT AkRFMQ8wDQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoM C3Njb250YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTAe Fw0xODA0MDYxMDIyMzdaFw0yODA0MDMxMDIyMzdaMGgxCzAJBgNVBAYTAkRFMQ8w DQYDVQQIDAZTYXhvbnkxEDAOBgNVBAcMB0RyZXNkZW4xFDASBgNVBAoMC3Njb250 YWluIFVHMSAwHgYJKoZIhvcNAQkBFhFpbmZvQHNjb250YWluLmNvbTBZMBMGByqG SM49AgEGCCqGSM49AwEHA0IABD9u85bK+nFdbnQVgZuR/rA9BmNmow3v4v5srS3M YGpVmRqpNbb/QYQ9iJN854N42L9T6mGyI402tKPYyfwz3k+jUDBOMB0GA1UdDgQW BBRhGJGv9MWohiZD3AySHz/otlxbyDAfBgNVHSMEGDAWgBRhGJGv9MWohiZD3AyS Hz/otlxbyDAMBgNVHRMEBTADAQH/MAoGCCqGSM49BAMCA0gAMEUCIQCWkKHgeDgn 4PrHfmfjYYerxyFyGmWOKjO5UcijrPqI9wIgUyhZ2OwuyzsjTEC6ofR4fzlnrUQJ XlkFOKY2/HqOPVE= -----END CERTIFICATE----- EOF and now log into the CAS: $ export IP = x.y.z.u $ mkdir -p ~/.config $ rm -f ~/.config/scone_cmd.conf $ scone cas login -c ca.pem christof cas --host $IP :8081:18765 We split our stack file into two parts: public part sent to the docker engine and a secret part that is directly sent to the CAS: $ scone cas split compose.yml --stack 283299 For now, the stack ID is a randomly chose unique ID. If you want to modify this stack later, you need to use the same stack ID. Show public part that is sent to docker stack (which is the original stack file name appended with \"docker.yml\"): $ cat compose.yml.docker.yml --- services: nginx: command: nginx environment: SCONE_CAS_ADDR: \"x.y.z.u:18765\" SCONE_CONFIG_ID: christof/283299/nginx SCONE_LAS_ADDR: \"172.17.0.1:18766\" image: \"127.0.0.1:5000/sconetainer:fss\" ports: - \"8190:8080\" - \"8192:8082\" version: \"3.1\" Note: in the above example we assume that the host on which nginx is running is available at address 172.17.0.1 . Hence, the LAS is available at address \"172.17.0.1:18766 . In case you run a non-standard configuration, you might need to change SCONE_LAS_ADDR to the ip address and port where the LAS is available. The LAS must run on the same host for the attestation to work. We can now deploy the service with the help of docker/scone stack: $ scone stack deploy --compose-file compose.yml nginx Creating network nginx_default Creating service nginx_nginx Show running stack: $ scone stack ls NAME SERVICES nginx 1 We can get some more information about this stack: $ scone stack ps nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS h08r5hmeu9bt nginx_nginx.1 127 .0.0.1:5000/sconetainer:fss dorothy Running Starting 19 seconds ago","title":"Example: End-to-End Encryption"},{"location":"SCONE_EE2EE/#file-encryption","text":"Let's check that the files inside of nginx container are indeed encrypted. To do so, we ssh to node dorothy and execute the following commands: > sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b01ab65e1579 127 .0.0.1:5000/sconetainer:fss \"nginx\" 6 minutes ago Up 6 minutes 8080 /tcp, 8082 /tcp nginx_nginx.1.0wh80346mpmm5f3l1rc8dipix ... Let's look inside the container: > sudo docker exec -it b01ab65e1579 sh $ ls bin media proc srv dev mnt root sys etc nginx run tmp home nginx-etc sbin usr lib opt scone-test.key var The nginx configuration file as well as the html files should be encrypted. Let's verify this: $ cd nginx $ ls certificate.pem key.pem nginx.conf www_root fspf.pb mime.types nginx.pid $ head -c 80 nginx.conf ] P?\u025f \"4???zS????\u01b5?kj\u0198?Ec?!S^!??????8j-?e;?t'?2?L????????y??\u02f2?\u07cb Ok, the nginx configuration file seems to be encrypted. Now, look at the html files too: $ cd www_root $ ls 4K SCONE_TUTORIAL GO aboutScone Python appsecurity ... $ head -c 80 index.html ????V?\u036a?rhk? \"k???? $? .???k?\u042c?y<??\u0660n?+????P2?G;o_'.i?);?I??xFg[???[f? \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"File Encryption"},{"location":"SCONE_ENV/","text":"SCONE Environment Variables To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service. Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters. SCONE Configuration File The location of the SCONE configuration file can be controlled with an environment variable: SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened, the default configuration file located in /etc/sgx-musl.conf is read instead. If the default configuration file cannot be read either, the program exits with an error message. Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves. The configuration file can define most of the behaviors that one can define via environment variables. However, the SCONE_... environment variables have higher priority than the settings from the config file. Format of SCONE Configuration File The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: Command Option(s) Description Q n defines the number of queues used to execute system calls n [number of queues] sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the SCONE_QUEUES environment variable H s defines the heap size in bytes s [heap size in bytes] sets the size of heap, equivalent to setting SCONE_HEAP environment variable P N determines the backoff behavior of the queues N [spin number] equivalent to setting SCONE_SSPINS environment variable L S determines the backoff behavior of the queues S [sleep time] equivalent to setting SCONE_SSLEEP environment variable; s C Q R sthread serving system calls outside of an enclave C [core-no] if non-negative number: pin this sthread to core C ; if a negative number, do not pin this thread Q [queue-no] in [0..n] ; this sthreads serves this queue R [realtime] always set this to 0 e C Q R ethread running inside of enclave and executes application threads (which we call lthreads) C [core-no] non-negative number: pin to this core; negative number: no pinning ot this thread Q [queue-no] in [0..n]: this sthreads serves this queue R [realtime] always set this to 0 M mode determine mode: HW is hardware more, SIM is simulation mode, and AUTO is automatic mode K s stack size in bytes L l log level between 0 and 7 D l 0: do not permit to load libraries after startup, 1: protected libraries only, 2: any library The number of sthreads is automatically increased if more sthreads are needed to process system calls. An sthread will block if it does not have any work left to do. Hence, we recommend to start exactly one sthread per ethread . ethreads will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one ethread per hyper-thread. Unless you want to limit the the number of CPU resources an enclave should use, the following is a good generic configuration file: $ sudo tee /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF Run Mode SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode. Value Description SCONE_MODE=HW run program inside of enclave. If program fails to create an enclave, it will fail. SCONE_MODE=SIM run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode. SCONE_MODE=AUTO run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode Memory-Related Environment Variables SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave. The default heap size is 64MB and is used if SCONE_HEAP is not set. Value Description SCONE_HEAP=s the requested heap size in bytes SCONE_HEAP=sK the requested heap size in KiloBytes SCONE_HEAP=sM the requested heap size in MegaBytes SCONE_HEAP=sG the requested heap size in GigaBytes SCONE_STACK : size of the stack allocated to threads spawned in the enclave. The default stack size is 64 KBytes. Please increase this if your expect your threads to require more than 64KBytes. For programs like MariaDB, MongoDB, Python, or node, we increase the stack size to 4 MBytes . Value Description SCONE_STACK=s the requested stack size in bytes SCONE_STACK=sK the requested stack size in KiloBytes SCONE_STACK=sM the requested stack size in MegaBytes Debug SCONE_VERSION if defined, SCONE will print the values of some of the SCONE environment variables during startup. SCONE_LOG set to value between 0 (no) and 7 (debug) to see more or less messages on stderr from the SCONE platform. Messages could be warnings if certain functions are not (yet) implemented inside of an enclave. Example output for SCONE_VERSION=1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_LOG = 6 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes Revision: e37dda73a6db435973f2e00347bd4cf462e4e027 ( Sat Aug 25 22 :59:30 2018 +0200 ) Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Dynamic library support: SCONE_ALLOW_DLOPEN=\"1\" : if defined, permit to load protected libraries after startup: These libraries must be authenticated or encrypted to be able to load these. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of MRENCLAVE . The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup. Note that SCONE_ALLOW_DLOPEN=\"1\" must be set in the policy of an attested application to have an effect. SCONE_ALLOW_DLOPEN=\"2\" : must be used for debugging only : this value enables loading of unprotected dynamic libraries after startup, i.e., libraries that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should either set SCONE_ALLOW_DLOPEN=\"1\" or you should not define SCONE_ALLOW_DLOPEN . Never use SCONE_ALLOW_DLOPEN=2 in production mode . Performance tuning variables SCONE_QUEUES : number of systemcall queues to be used. SCONE_SLOTS : systemcalls queue length: must be larger than the maximum number of lthreads . SCONE_SIGPIPE : if set to 1 , SIGPIPE signals are delivered to the application SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. SCONE_SSLEEP : determines how fast we increase the backoff. Safety SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker. Dynamic link loader The dynamic link loader is part of image sconecuratedimages/crosscompilers:runtime ( see tutorial ). SCONE_LD_DEBUG : print the dynamically loaded libraries and their SHA-256 hashes LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries. LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself. SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave.","title":"environment variables"},{"location":"SCONE_ENV/#scone-environment-variables","text":"To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service. Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters.","title":"SCONE Environment Variables"},{"location":"SCONE_ENV/#scone-configuration-file","text":"The location of the SCONE configuration file can be controlled with an environment variable: SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened, the default configuration file located in /etc/sgx-musl.conf is read instead. If the default configuration file cannot be read either, the program exits with an error message. Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves. The configuration file can define most of the behaviors that one can define via environment variables. However, the SCONE_... environment variables have higher priority than the settings from the config file.","title":"SCONE Configuration File"},{"location":"SCONE_ENV/#format-of-scone-configuration-file","text":"The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: Command Option(s) Description Q n defines the number of queues used to execute system calls n [number of queues] sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the SCONE_QUEUES environment variable H s defines the heap size in bytes s [heap size in bytes] sets the size of heap, equivalent to setting SCONE_HEAP environment variable P N determines the backoff behavior of the queues N [spin number] equivalent to setting SCONE_SSPINS environment variable L S determines the backoff behavior of the queues S [sleep time] equivalent to setting SCONE_SSLEEP environment variable; s C Q R sthread serving system calls outside of an enclave C [core-no] if non-negative number: pin this sthread to core C ; if a negative number, do not pin this thread Q [queue-no] in [0..n] ; this sthreads serves this queue R [realtime] always set this to 0 e C Q R ethread running inside of enclave and executes application threads (which we call lthreads) C [core-no] non-negative number: pin to this core; negative number: no pinning ot this thread Q [queue-no] in [0..n]: this sthreads serves this queue R [realtime] always set this to 0 M mode determine mode: HW is hardware more, SIM is simulation mode, and AUTO is automatic mode K s stack size in bytes L l log level between 0 and 7 D l 0: do not permit to load libraries after startup, 1: protected libraries only, 2: any library The number of sthreads is automatically increased if more sthreads are needed to process system calls. An sthread will block if it does not have any work left to do. Hence, we recommend to start exactly one sthread per ethread . ethreads will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one ethread per hyper-thread. Unless you want to limit the the number of CPU resources an enclave should use, the following is a good generic configuration file: $ sudo tee /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF","title":"Format of SCONE Configuration File"},{"location":"SCONE_ENV/#run-mode","text":"SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode. Value Description SCONE_MODE=HW run program inside of enclave. If program fails to create an enclave, it will fail. SCONE_MODE=SIM run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode. SCONE_MODE=AUTO run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode","title":"Run Mode"},{"location":"SCONE_ENV/#memory-related-environment-variables","text":"SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave. The default heap size is 64MB and is used if SCONE_HEAP is not set. Value Description SCONE_HEAP=s the requested heap size in bytes SCONE_HEAP=sK the requested heap size in KiloBytes SCONE_HEAP=sM the requested heap size in MegaBytes SCONE_HEAP=sG the requested heap size in GigaBytes SCONE_STACK : size of the stack allocated to threads spawned in the enclave. The default stack size is 64 KBytes. Please increase this if your expect your threads to require more than 64KBytes. For programs like MariaDB, MongoDB, Python, or node, we increase the stack size to 4 MBytes . Value Description SCONE_STACK=s the requested stack size in bytes SCONE_STACK=sK the requested stack size in KiloBytes SCONE_STACK=sM the requested stack size in MegaBytes","title":"Memory-Related Environment Variables"},{"location":"SCONE_ENV/#debug","text":"SCONE_VERSION if defined, SCONE will print the values of some of the SCONE environment variables during startup. SCONE_LOG set to value between 0 (no) and 7 (debug) to see more or less messages on stderr from the SCONE platform. Messages could be warnings if certain functions are not (yet) implemented inside of an enclave. Example output for SCONE_VERSION=1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_LOG = 6 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes Revision: e37dda73a6db435973f2e00347bd4cf462e4e027 ( Sat Aug 25 22 :59:30 2018 +0200 ) Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl","title":"Debug"},{"location":"SCONE_ENV/#dynamic-library-support","text":"SCONE_ALLOW_DLOPEN=\"1\" : if defined, permit to load protected libraries after startup: These libraries must be authenticated or encrypted to be able to load these. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of MRENCLAVE . The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup. Note that SCONE_ALLOW_DLOPEN=\"1\" must be set in the policy of an attested application to have an effect. SCONE_ALLOW_DLOPEN=\"2\" : must be used for debugging only : this value enables loading of unprotected dynamic libraries after startup, i.e., libraries that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should either set SCONE_ALLOW_DLOPEN=\"1\" or you should not define SCONE_ALLOW_DLOPEN . Never use SCONE_ALLOW_DLOPEN=2 in production mode .","title":"Dynamic library support:"},{"location":"SCONE_ENV/#performance-tuning-variables","text":"SCONE_QUEUES : number of systemcall queues to be used. SCONE_SLOTS : systemcalls queue length: must be larger than the maximum number of lthreads . SCONE_SIGPIPE : if set to 1 , SIGPIPE signals are delivered to the application SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. SCONE_SSLEEP : determines how fast we increase the backoff.","title":"Performance tuning variables"},{"location":"SCONE_ENV/#safety","text":"SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker.","title":"Safety"},{"location":"SCONE_ENV/#dynamic-link-loader","text":"The dynamic link loader is part of image sconecuratedimages/crosscompilers:runtime ( see tutorial ). SCONE_LD_DEBUG : print the dynamically loaded libraries and their SHA-256 hashes LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries. LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself. SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave.","title":"Dynamic link loader"},{"location":"SCONE_Fileshield/","text":"SCONE File Protection SCONE supports the transparent encryption and/or authentication of files. By transparent , we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection: a low-level interface intended to be used at the developer site. We assume that the developer machine is sufficiently trust worthy. This is made available via command scone fspf and described in this document. a high-level interface simplifies the use of the file protection and it does and in particular, takes care of key management. ( The high-level interface is not yet available ). Concepts The underlying idea of SCONE file protection is that a user specifies that each file is either : authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity, encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or not-protected , i.e. SCONE reads and write the files without any extra protection mechanisms. For example, you might use not-protected if your application already encrypts its files or if you need direct access to devices. Marking all files individually as either authenticated , encrypted , or not-protected would not be very practical. Hence, we support to partition the filesystem into regions : regions do not overlap and each file belongs to exactly one region. A region is defined by a path. For example, region / is the root region and you could, for example, specify that all files in region / must be authenticated. You can define a second region, for example, region /data/db and that this region is encrypted. Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file /etc/db.conf would belong, in this case, to region / and file /data/db/table.db would belong to region /data/db . SCONE supports ephemeral regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted. When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. All files in an ephemeral region are lost when the application exits. All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a kernel region. For each region, you need to specify if the region is either ephemeral or kernel . Each region belongs to one of the following six classes: { ephemeral | kernel } X { not-protected | authenticated | encrypted } Example Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted. Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory volume ) and we will later mount this in the container as /data . The original (non-encrypted) files are stored in directory data-original . > mkdir -p volume > mkdir -p data-original Let's write some files in the data-original directory: cat > data-original/hello.txt << EOF Hello World EOF cat > data-original/world.py << EOF f = open('/data/hello.txt', 'r') print str(f.read()) EOF Let's check that volume is empty and we print the hash values of the two files in data-original : > ls volume > shasum data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c data-original/world.py Now, we start the SCONE crosscompiler in a container to create the encrypted volume: > docker run -it -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" sconecuratedimages/crosscompilers File System Protection File All the metadata required for checking the consistency of the files is stored in a file system protection file , or, short fspf . SCONE supports multiple *fspf*s. Let's start with a simple example with a single fspf . The fspf file is created via command scone fspf create and let us name this file fspf.pb . We execute the following commands inside the container (as indicated by the $ prompt): $ cd /data $ scone fspf create fspf.pb Created empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423 We can now split the file system in regions , a region is a subtree. You can add regions to a fspf with the help of command scone fspf addr . Each region has exactly one of the following properties: authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option --authenticated . encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option --encrypted . not-protected : files are neither authenticated nor encrypted. Specify command line option --not-protected . File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead. Say for now, that by default we do not protect files and we want to read files and write back changed files to the file system. To do so, we define that the root tree is --kernel as well as --not-protected : $ scone fspf addr fspf.pb / --kernel / --not-protected Added region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84 Let us add another region /data that should be encrypted and persisted. To encrypt the files, we specify option --encrypted . We specify option --kernel followed by a path (here, also /data ) to request that files in this region are written to the kernel file system into directory /data . $ scone fspf addr fspf.pb /data --encrypted --kernel /data Added region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7 The encryption key for a file is chosen at random and stored in fspf.pb . We use the Intel random number generator RdRand to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option --key-length 16 and --key-length 24 when creating a region with command scone fspf addr . Now, that we defined the regions, i.e., / and /data , we can add files to region /data . Let's just add all files in /data-original , encrypt these and write the encrypted files to /data . Note, the first /data argument specifies the protection region that determines the protection policy. The second, specifies where the encrypted files will be stored. That is, the command iterates over and reads the existing files in /data-original and encrypts them. The encrypted file content is written into the directory /data while the protection metadata of the individual files is added to the fsfp.pb file. $ scone fspf addf fspf.pb /data /data-original /data Added files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63 Note that if the /data region would have been only authenticated and not encrypted, the tool does not need to write out any (encrypted) files. It will only add the file names and the checksums (tags) of the files located in /data-original to the fspf.pb file. Thus, you could drop the last argument in this case. Coming back to the above example, we can now compare the hash values of the original files and the encrypted files: $ shasum /data/* 87fd97468024e3d2864516ff5840e15d9615340d /data/fspf.pb 31732914910f4a08b9832c442074b0932915476c /data/hello.txt 8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c /data/world.py $ shasum /data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a /data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c /data-original/world.py The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb $ scone fspf encrypt fspf.pb > /data-original/keytag We store the random encryption key as well as the tag of file fspf.pb in file /data-original/keytag . We introduce a very simple program that reads the two files: $ cat > example.c << EOF #include <stdio.h> #include <stdlib.h> void printfile(const char* fn) { FILE *fp = fopen(fn, \"r\"); char c; while((c=fgetc(fp))!=EOF ){ printf ( \"%c\" ,c ) ; } fclose ( fp ) ; } int main () { printfile ( \"/data/hello.txt\" ) ; printfile ( \"/data/world.py\" ) ; } EOF Let's crosscompile this program: scone gcc example.c -o example Executing this program results in an output like this: $./example R??C? q?z??E?? | \u042e? } \u00fc ?o $? ?!rga??\u0387* ` ?????????Gw? We need to activate the file system shield via environment variables by setting the location of the file system protection file (in SCONE_FSPF ), the encryption key of the file (in SCONE_FSPF_KEY ) and the tag of the fspf (in SCONE_FSPF_TAG ). We can extract the encryption key as well as the tag of fspf.pb from file /data-original/keytag : $ export SCONE_FSPF_KEY = $( cat /data-original/keytag | awk '{print $11}' ) $ export SCONE_FSPF_TAG = $( cat /data-original/keytag | awk '{print $9}' ) $ export SCONE_FSPF = /data/fspf.pb We can now execute this program again: $ ./example Hello World f = open ( '/data/hello.txt' , 'r' ) print str ( f.read ()) Variables SCONE_FSPF_KEY , SCONE_FSPF_TAG and SCONE_FSPF should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please use CAS . Python Let's try a similar approach for Python. In the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file: docker run -it -v \" $PWD /volume:/data\" sconecuratedimages/apps:python-2.7-alpine3.6 bash The files /data/world.py and /data/hello.txt are encrypted: $ cat /data/world.py ? = ??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y??>f? Let's activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /data/fspf.pb We can now run the encrypted world.py program with the the Python interpreter: SCONE_HEAP = 100000000 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python /data/world.py export SCONE_QUEUES = 1 ... Hello World Protecting the Root Region Note that in the above example, Python will not be permitted to load dynamic libraries outside of the protected directory /data : a dynamic library must reside in either an authenticated or an encrypted region. To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries. Let us show how to authenticate all files in region / $ scone fspf addr fspf.pb / --kernel / --authenticated We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region / . Adding files from an existing container image We show how to add a subset of the files of container image sconecuratedimages/apps:python-2.7-alpine3.6 to our root region. To do so, we ensure that we have the newest images: > docker pull sconecuratedimages/apps:python-2.7-alpine3.6 > docker pull sconecuratedimages/crosscompilers How can we add all files in a container to the fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to /var/run/docker.sock : > docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" sconecuratedimages/crosscompilers Let us ensure that Docker is installed in this container: apt-get update apt-get install -y docker.io Now, we want to add all files of some target container. In our example, this is an instance of image sconecuratedimages/apps:python-2.7-alpine3.6 . We ensure that we pulled the latest image before we start the container: CONTAINER_ID = ` docker run -d sconecuratedimages/apps:python-2.7-alpine3.6 printf OK ` We can now copy all files from this container into a new directory rootvol : $ cd $ mkdir -p rootvol $ docker cp $CONTAINER_ID :/ ./rootvol Now that we have a copy of the files, we should not forget to garbage collect this container: docker rm $CONTAINER_ID Let's remove some directories that we do not want our program to access, like for example, /dev : $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++ Now, we create a root fspf : $ scone fspf create fspf.pb $ scone fspf addr fspf.pb / --kernel / --authenticated $ scone fspf addf fspf.pb / ./rootvol / $ scone fspf encrypt fspf.pb > keytag We can now create a new container image with this file system protection file using this Dockerfile $ cat > Dockerfile << EOF FROM sconecuratedimages/apps:python-2.7-alpine3.6 COPY fspf.pb / EOF $ docker build -t sconecuratedimages/apps:python-2.7-alpine3.6-authenticated . We can run a container as follows: $ docker run -it sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh Let us activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb Let's run python with authenticated file system: SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python Checking the File System Shield Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in side of a python container: > docker run -i sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh $ cat > helloworld-manual.py << EOF print \"Hello World\" EOF When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file. $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld-manual.py ( fails ) We can, however, add a new file via programs that have access to the key of the fspf . We can, for example, write a python program to add a new python program to the file system. By default, we disable that the root fspf is updated. We can enable updates by setting environment variable SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable SCONE_FSPF_MUTABLE=1 ). $ SCONE_HEAP = 1000000000 SCONE_FSPF_MUTABLE = 1 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python << PYTHON f = open('helloworld.py', 'w') f.write('print \"Hello World\"\\n') f.close() PYTHON ``` The tag of the file system protection file is now changed. We can determine the new TAG with the help of command scone fspf show : $ export SCONE_FSPF_TAG = $( scone fspf show --tag /fspf.pb ) Now, we can run the new helloworld.py : $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld.py ... Hello World Extended Example To learn how to use multiple file system protection files, please have a look at the following screencast. Below is the script that is executed in the screencast: > docker run -it -v $PWD :/mnt sconecuratedimages/crosscompilers $ mkdir -p /example $ mkdir -p /mnt/authenticated/ $ mkdir -p /mnt/encrypted/ $ cd /example $ mkdir -p .original $ scone fspf create fspf.pb $ scone fspf create authenticated.pb $ scone fspf create encrypted.pb # add protection regions $ scone fspf addr fspf.pb / -e --ephemeral $ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated $ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted # add files # enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated $ scone fspf addf authenticated.pb /mnt/authenticated ./original # enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted # the client will write the encrypted files to ./mnt/encrypted $ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted encrypted_key = ` scone fspf encrypt encrypted.pb | awk '{print $11}' ` $ echo \"encrypted.pb key: ${ encrypted_key } \" $ scone fspf addfspf fspf.pb authenticated.pb $ scone fspf addfspf fspf.pb encrypted.pb ${ encrypted_key } $ cat > example.c << EOF #include <stdio.h> int main() { FILE *fp = fopen(\"/mnt/authenticated/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); fp = fopen(\"/mnt/encrypted/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); } EOF $ scone gcc example.c -o sgxex $ cat > /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF $ SCONE_FSPF = fspf.pb ./sgxex $ cat /mnt/authenticated/hello $ cat /mnt/encrypted/hello $ cat > cat.c << EOF #include <stdio.h> int main() { char buf[80]; FILE *fp = fopen(\"/mnt/authenticated/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); fp = fopen(\"/mnt/encrypted/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); } EOF $ scone gcc cat.c -o native_cat $ ./native_cat $ scone gcc cat.c -o sgxcat $ SCONE_FSPF = fspf.pb ./sgxcat Notes The SCONE File Protection documentation is not yet completed and more information will be provided soon. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"scone fspf"},{"location":"SCONE_Fileshield/#scone-file-protection","text":"SCONE supports the transparent encryption and/or authentication of files. By transparent , we mean that there are no application code changes needed to support this. We support two ways to use the SCONE file protection: a low-level interface intended to be used at the developer site. We assume that the developer machine is sufficiently trust worthy. This is made available via command scone fspf and described in this document. a high-level interface simplifies the use of the file protection and it does and in particular, takes care of key management. ( The high-level interface is not yet available ).","title":"SCONE File Protection"},{"location":"SCONE_Fileshield/#concepts","text":"The underlying idea of SCONE file protection is that a user specifies that each file is either : authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity, encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or not-protected , i.e. SCONE reads and write the files without any extra protection mechanisms. For example, you might use not-protected if your application already encrypts its files or if you need direct access to devices. Marking all files individually as either authenticated , encrypted , or not-protected would not be very practical. Hence, we support to partition the filesystem into regions : regions do not overlap and each file belongs to exactly one region. A region is defined by a path. For example, region / is the root region and you could, for example, specify that all files in region / must be authenticated. You can define a second region, for example, region /data/db and that this region is encrypted. Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file /etc/db.conf would belong, in this case, to region / and file /data/db/table.db would belong to region /data/db . SCONE supports ephemeral regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted. When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. All files in an ephemeral region are lost when the application exits. All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a kernel region. For each region, you need to specify if the region is either ephemeral or kernel . Each region belongs to one of the following six classes: { ephemeral | kernel } X { not-protected | authenticated | encrypted }","title":"Concepts"},{"location":"SCONE_Fileshield/#example","text":"Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted. Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory volume ) and we will later mount this in the container as /data . The original (non-encrypted) files are stored in directory data-original . > mkdir -p volume > mkdir -p data-original Let's write some files in the data-original directory: cat > data-original/hello.txt << EOF Hello World EOF cat > data-original/world.py << EOF f = open('/data/hello.txt', 'r') print str(f.read()) EOF Let's check that volume is empty and we print the hash values of the two files in data-original : > ls volume > shasum data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c data-original/world.py Now, we start the SCONE crosscompiler in a container to create the encrypted volume: > docker run -it -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" sconecuratedimages/crosscompilers","title":"Example"},{"location":"SCONE_Fileshield/#file-system-protection-file","text":"All the metadata required for checking the consistency of the files is stored in a file system protection file , or, short fspf . SCONE supports multiple *fspf*s. Let's start with a simple example with a single fspf . The fspf file is created via command scone fspf create and let us name this file fspf.pb . We execute the following commands inside the container (as indicated by the $ prompt): $ cd /data $ scone fspf create fspf.pb Created empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423 We can now split the file system in regions , a region is a subtree. You can add regions to a fspf with the help of command scone fspf addr . Each region has exactly one of the following properties: authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option --authenticated . encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option --encrypted . not-protected : files are neither authenticated nor encrypted. Specify command line option --not-protected . File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead. Say for now, that by default we do not protect files and we want to read files and write back changed files to the file system. To do so, we define that the root tree is --kernel as well as --not-protected : $ scone fspf addr fspf.pb / --kernel / --not-protected Added region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84 Let us add another region /data that should be encrypted and persisted. To encrypt the files, we specify option --encrypted . We specify option --kernel followed by a path (here, also /data ) to request that files in this region are written to the kernel file system into directory /data . $ scone fspf addr fspf.pb /data --encrypted --kernel /data Added region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7 The encryption key for a file is chosen at random and stored in fspf.pb . We use the Intel random number generator RdRand to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option --key-length 16 and --key-length 24 when creating a region with command scone fspf addr . Now, that we defined the regions, i.e., / and /data , we can add files to region /data . Let's just add all files in /data-original , encrypt these and write the encrypted files to /data . Note, the first /data argument specifies the protection region that determines the protection policy. The second, specifies where the encrypted files will be stored. That is, the command iterates over and reads the existing files in /data-original and encrypts them. The encrypted file content is written into the directory /data while the protection metadata of the individual files is added to the fsfp.pb file. $ scone fspf addf fspf.pb /data /data-original /data Added files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63 Note that if the /data region would have been only authenticated and not encrypted, the tool does not need to write out any (encrypted) files. It will only add the file names and the checksums (tags) of the files located in /data-original to the fspf.pb file. Thus, you could drop the last argument in this case. Coming back to the above example, we can now compare the hash values of the original files and the encrypted files: $ shasum /data/* 87fd97468024e3d2864516ff5840e15d9615340d /data/fspf.pb 31732914910f4a08b9832c442074b0932915476c /data/hello.txt 8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c /data/world.py $ shasum /data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a /data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c /data-original/world.py The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb $ scone fspf encrypt fspf.pb > /data-original/keytag We store the random encryption key as well as the tag of file fspf.pb in file /data-original/keytag . We introduce a very simple program that reads the two files: $ cat > example.c << EOF #include <stdio.h> #include <stdlib.h> void printfile(const char* fn) { FILE *fp = fopen(fn, \"r\"); char c; while((c=fgetc(fp))!=EOF ){ printf ( \"%c\" ,c ) ; } fclose ( fp ) ; } int main () { printfile ( \"/data/hello.txt\" ) ; printfile ( \"/data/world.py\" ) ; } EOF Let's crosscompile this program: scone gcc example.c -o example Executing this program results in an output like this: $./example R??C? q?z??E?? | \u042e? } \u00fc ?o $? ?!rga??\u0387* ` ?????????Gw? We need to activate the file system shield via environment variables by setting the location of the file system protection file (in SCONE_FSPF ), the encryption key of the file (in SCONE_FSPF_KEY ) and the tag of the fspf (in SCONE_FSPF_TAG ). We can extract the encryption key as well as the tag of fspf.pb from file /data-original/keytag : $ export SCONE_FSPF_KEY = $( cat /data-original/keytag | awk '{print $11}' ) $ export SCONE_FSPF_TAG = $( cat /data-original/keytag | awk '{print $9}' ) $ export SCONE_FSPF = /data/fspf.pb We can now execute this program again: $ ./example Hello World f = open ( '/data/hello.txt' , 'r' ) print str ( f.read ()) Variables SCONE_FSPF_KEY , SCONE_FSPF_TAG and SCONE_FSPF should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please use CAS .","title":"File System Protection File"},{"location":"SCONE_Fileshield/#python","text":"Let's try a similar approach for Python. In the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file: docker run -it -v \" $PWD /volume:/data\" sconecuratedimages/apps:python-2.7-alpine3.6 bash The files /data/world.py and /data/hello.txt are encrypted: $ cat /data/world.py ? = ??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y??>f? Let's activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /data/fspf.pb We can now run the encrypted world.py program with the the Python interpreter: SCONE_HEAP = 100000000 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python /data/world.py export SCONE_QUEUES = 1 ... Hello World","title":"Python"},{"location":"SCONE_Fileshield/#protecting-the-root-region","text":"Note that in the above example, Python will not be permitted to load dynamic libraries outside of the protected directory /data : a dynamic library must reside in either an authenticated or an encrypted region. To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries. Let us show how to authenticate all files in region / $ scone fspf addr fspf.pb / --kernel / --authenticated We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region / .","title":"Protecting the Root Region"},{"location":"SCONE_Fileshield/#adding-files-from-an-existing-container-image","text":"We show how to add a subset of the files of container image sconecuratedimages/apps:python-2.7-alpine3.6 to our root region. To do so, we ensure that we have the newest images: > docker pull sconecuratedimages/apps:python-2.7-alpine3.6 > docker pull sconecuratedimages/crosscompilers How can we add all files in a container to the fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to /var/run/docker.sock : > docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" sconecuratedimages/crosscompilers Let us ensure that Docker is installed in this container: apt-get update apt-get install -y docker.io Now, we want to add all files of some target container. In our example, this is an instance of image sconecuratedimages/apps:python-2.7-alpine3.6 . We ensure that we pulled the latest image before we start the container: CONTAINER_ID = ` docker run -d sconecuratedimages/apps:python-2.7-alpine3.6 printf OK ` We can now copy all files from this container into a new directory rootvol : $ cd $ mkdir -p rootvol $ docker cp $CONTAINER_ID :/ ./rootvol Now that we have a copy of the files, we should not forget to garbage collect this container: docker rm $CONTAINER_ID Let's remove some directories that we do not want our program to access, like for example, /dev : $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++ Now, we create a root fspf : $ scone fspf create fspf.pb $ scone fspf addr fspf.pb / --kernel / --authenticated $ scone fspf addf fspf.pb / ./rootvol / $ scone fspf encrypt fspf.pb > keytag We can now create a new container image with this file system protection file using this Dockerfile $ cat > Dockerfile << EOF FROM sconecuratedimages/apps:python-2.7-alpine3.6 COPY fspf.pb / EOF $ docker build -t sconecuratedimages/apps:python-2.7-alpine3.6-authenticated . We can run a container as follows: $ docker run -it sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh Let us activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb Let's run python with authenticated file system: SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python","title":"Adding files from an existing container image"},{"location":"SCONE_Fileshield/#checking-the-file-system-shield","text":"Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in side of a python container: > docker run -i sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh $ cat > helloworld-manual.py << EOF print \"Hello World\" EOF When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file. $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld-manual.py ( fails ) We can, however, add a new file via programs that have access to the key of the fspf . We can, for example, write a python program to add a new python program to the file system. By default, we disable that the root fspf is updated. We can enable updates by setting environment variable SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable SCONE_FSPF_MUTABLE=1 ). $ SCONE_HEAP = 1000000000 SCONE_FSPF_MUTABLE = 1 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python << PYTHON f = open('helloworld.py', 'w') f.write('print \"Hello World\"\\n') f.close() PYTHON ``` The tag of the file system protection file is now changed. We can determine the new TAG with the help of command scone fspf show : $ export SCONE_FSPF_TAG = $( scone fspf show --tag /fspf.pb ) Now, we can run the new helloworld.py : $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld.py ... Hello World","title":"Checking the File System Shield"},{"location":"SCONE_Fileshield/#extended-example","text":"To learn how to use multiple file system protection files, please have a look at the following screencast. Below is the script that is executed in the screencast: > docker run -it -v $PWD :/mnt sconecuratedimages/crosscompilers $ mkdir -p /example $ mkdir -p /mnt/authenticated/ $ mkdir -p /mnt/encrypted/ $ cd /example $ mkdir -p .original $ scone fspf create fspf.pb $ scone fspf create authenticated.pb $ scone fspf create encrypted.pb # add protection regions $ scone fspf addr fspf.pb / -e --ephemeral $ scone fspf addr authenticated.pb /mnt/authenticated -a --kernel /mnt/authenticated $ scone fspf addr encrypted.pb /mnt/encrypted -e --kernel /mnt/encrypted # add files # enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated $ scone fspf addf authenticated.pb /mnt/authenticated ./original # enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted # the client will write the encrypted files to ./mnt/encrypted $ scone fspf addf encrypted.pb /mnt/encrypted ./original ./mnt/encrypted encrypted_key = ` scone fspf encrypt encrypted.pb | awk '{print $11}' ` $ echo \"encrypted.pb key: ${ encrypted_key } \" $ scone fspf addfspf fspf.pb authenticated.pb $ scone fspf addfspf fspf.pb encrypted.pb ${ encrypted_key } $ cat > example.c << EOF #include <stdio.h> int main() { FILE *fp = fopen(\"/mnt/authenticated/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); fp = fopen(\"/mnt/encrypted/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); } EOF $ scone gcc example.c -o sgxex $ cat > /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF $ SCONE_FSPF = fspf.pb ./sgxex $ cat /mnt/authenticated/hello $ cat /mnt/encrypted/hello $ cat > cat.c << EOF #include <stdio.h> int main() { char buf[80]; FILE *fp = fopen(\"/mnt/authenticated/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); fp = fopen(\"/mnt/encrypted/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); } EOF $ scone gcc cat.c -o native_cat $ ./native_cat $ scone gcc cat.c -o sgxcat $ SCONE_FSPF = fspf.pb ./sgxcat","title":"Extended Example"},{"location":"SCONE_Fileshield/#notes","text":"The SCONE File Protection documentation is not yet completed and more information will be provided soon. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Notes"},{"location":"SCONE_GENERATE_IMAGE/","text":"Generating Container Image with SCONE We show how to generate a Docker image that contains our hello world running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section. Prerequisites Check that all prerequisites from SCONE Tutorial are satisfied. Clone the SCONE_TUTORIAL before you start creating a hello world image. Generate HelloWorld image We generate a hello world container image. > cd SCONE_TUTORIAL/CreateImage You can either execute all step manually by copy&pasting all instructions or you can just execute > docker login > sudo ./Dockerfile.sh and watch the outputs. Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"sconecuratedimages/helloworld\" We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler: > CONTAINER_ID = ` docker run -d -it --device = /dev/isgx -v $( pwd ) :/mnt sconecuratedimages/crosscompilers bash -c \" set -e printf 'Q 1\\ne 0 0 0\\ns 1 0 0\\n' > /etc/sgx-musl.conf sgxmusl-hw-async-gcc /mnt/hello_world.c -o /usr/local/bin/sgx_hello_world \" ` Note that above will fail if you do not have access to the SGX device /dev/isgx . Turn the container into an image: > IMAGE_ID = $( docker commit -p -c 'CMD sgx_hello_world' $CONTAINER_ID $IMAGE_NAME : $TAG ) You can run this image by executing: > sudo docker run --device = /dev/isgx $IMAGE_NAME : $TAG You can push this image to Docker. However, ensure that you first login to docker: > sudo docker login before you push the image to docker hub: > sudo docker push $IMAGE_NAME : $TAG Note: this will fail in case you do not have the permission to push to this repository. Screencast \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"SCONE Create Image"},{"location":"SCONE_GENERATE_IMAGE/#generating-container-image-with-scone","text":"We show how to generate a Docker image that contains our hello world running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section.","title":"Generating Container Image with SCONE"},{"location":"SCONE_GENERATE_IMAGE/#prerequisites","text":"Check that all prerequisites from SCONE Tutorial are satisfied. Clone the SCONE_TUTORIAL before you start creating a hello world image.","title":"Prerequisites"},{"location":"SCONE_GENERATE_IMAGE/#generate-helloworld-image","text":"We generate a hello world container image. > cd SCONE_TUTORIAL/CreateImage You can either execute all step manually by copy&pasting all instructions or you can just execute > docker login > sudo ./Dockerfile.sh and watch the outputs. Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"sconecuratedimages/helloworld\" We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler: > CONTAINER_ID = ` docker run -d -it --device = /dev/isgx -v $( pwd ) :/mnt sconecuratedimages/crosscompilers bash -c \" set -e printf 'Q 1\\ne 0 0 0\\ns 1 0 0\\n' > /etc/sgx-musl.conf sgxmusl-hw-async-gcc /mnt/hello_world.c -o /usr/local/bin/sgx_hello_world \" ` Note that above will fail if you do not have access to the SGX device /dev/isgx . Turn the container into an image: > IMAGE_ID = $( docker commit -p -c 'CMD sgx_hello_world' $CONTAINER_ID $IMAGE_NAME : $TAG ) You can run this image by executing: > sudo docker run --device = /dev/isgx $IMAGE_NAME : $TAG You can push this image to Docker. However, ensure that you first login to docker: > sudo docker login before you push the image to docker hub: > sudo docker push $IMAGE_NAME : $TAG Note: this will fail in case you do not have the permission to push to this repository.","title":"Generate HelloWorld image"},{"location":"SCONE_GENERATE_IMAGE/#screencast","text":"\u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_HOST/","text":"scone host This page describes the CLI scone host in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section SCONE Host Setup . Read section SCONE Command Line Interface to see how to install command scone . Commands scone host supports the following commands: check : checks that host is properly installed (patched docker engine and patched sgx driver) install : installs the patched SGX driver and patched docker engine reboot : reboots a host uninstall : uninstalls SGX driver and patched docker engine swarm : join a new or another swarm scone host install Command install installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm. NOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care. Options You must always specify option --name HOST , where, HOST is the name of the host that you want to install. You need to have ssh access to this host - without the need to type in a password. Example: To install host dorothy without joining any swarm, just execute $ scone host install --name dorothy If you want to install a host and make this host a manager of a new or an existing swarm, you need to add option --as-manager . Example: To install host faye and create a new swarm with faye as a manager, just execute $ scone host install --name faye --as-manager You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a worker , you have to specify option --join MANAGER Example: To install host edna and then join the swarm managed by manager faye , execute: $ scone host install --name edna --join faye You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye ` NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active 2 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a manager , you have to specify options --join MANAGER --as-manager Example: To install host edna and then join the swarm managed by manager faye as a manager, execute: $ scone host install --name edna --join faye --as-manager scone host swarm In the above example, we installed host dorothy without joining any swarm. Sometimes one wants to revise this decision via command swarm . You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options --join MANAGER and --as-manager as described for scone host install . Example: To add host dorothy to the swarm managed by host faye , just execute: $ scone host swarm --name dorothy --join faye Example: To add host dorothy to the swarm managed by host faye as a manager, just execute: $ scone host swarm --name dorothy --join faye --as-manager scone host check To check if a host is properly installed, you can use command check . Example: To check if host faye is properly installed, just execute: $ scone host check --name faye If the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host. scone host reboot Sometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility lsof : $ sudo lsof | grep dev/isgx Sometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with scone host reboot . You must specify options --name HOST to indicate which host to reboot. Moreover, scone host reboot will exit with an error unless you specify option --force . Example: To reboot host dorothy, just execute the following: $ scone host reboot --name dorothy --force If you want to wait until the host is again available, you can specify option --wait . Example: To reboot host dorothy and only return after the host has indeed rebooted, just execute the following: $ scone host reboot --name dorothy --force --wait scone host uninstall With the help of scone host uninstall you can force an host to leave any Docker swarm it might be part of uninstall the patched Docker engine uninstall the patched SGX driver NOTE: all containers that might run on this host, will be destroyed. Options You must define the name of the host to be uninstalled via option --name HOST . You must always give the --force option, otherwise, an error is issued. If a node is part of a swarm, you must explicitly specify the option --manager MANAGERHOST . If the node is not part of a swarm, you must specify the option --noswarm . Note: While other objects / commands support the use of environment variable SCONE_MANAGER as an implicit definition of --manager $SCONE_MANAGER , we decided that users must explicitly specify the manager of the swarm for command uninstall . Example: To uninstall a host edna that is part of a swarm managed by host faye , execute: $ scone host uninstall --name edna --force --manager faye Example: Let's assume that host edna is not part of a swarm. To uninstall host edna , execute: $ scone host uninstall --name edna --force --noswarm Example: To uninstall a host faye which is also manager can be tricky. First, make sure that all other nodes are removed from the swarm. To check the status, execute: $ scone host check --name faye $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If the output matches the above execute the following to uninstall host faye (both parameters --manager MANAGERHOST and --noswarm must be added): $ scone host uninstall --name faye --noswarm --manager faye --force General options --help (or, -h ): issue help message for object host . If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"scone host"},{"location":"SCONE_HOST/#scone-host","text":"This page describes the CLI scone host in more details. For an more general introduction on how to install a host and how to set up ssh to this host, please read section SCONE Host Setup . Read section SCONE Command Line Interface to see how to install command scone .","title":"scone host"},{"location":"SCONE_HOST/#commands","text":"scone host supports the following commands: check : checks that host is properly installed (patched docker engine and patched sgx driver) install : installs the patched SGX driver and patched docker engine reboot : reboots a host uninstall : uninstalls SGX driver and patched docker engine swarm : join a new or another swarm","title":"Commands"},{"location":"SCONE_HOST/#scone-host-install","text":"Command install installs a patched docker engine and a patched Intel SGX driver. It also supports that the installed host joins an existing Docker swarm or it becomes the manager of a newly created Docker swarm. NOTE: if a docker engine or an Intel SGX driver is already installed, the installed software is uninstalled and replaced by a patched versions. In this process, all containers that run on this machine and all enclaves that execute on this host are removed. Use command install with care.","title":"scone host install"},{"location":"SCONE_HOST/#options","text":"You must always specify option --name HOST , where, HOST is the name of the host that you want to install. You need to have ssh access to this host - without the need to type in a password. Example: To install host dorothy without joining any swarm, just execute $ scone host install --name dorothy If you want to install a host and make this host a manager of a new or an existing swarm, you need to add option --as-manager . Example: To install host faye and create a new swarm with faye as a manager, just execute $ scone host install --name faye --as-manager You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a worker , you have to specify option --join MANAGER Example: To install host edna and then join the swarm managed by manager faye , execute: $ scone host install --name edna --join faye You can now check your swarm with scone swarm to see the members of your new swarm: $ scone swarm ls --manager faye ` NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE edna Ready Active 2 1 SCONE SCONE faye Ready Active Leader If you want to join an existing swarm as a manager , you have to specify options --join MANAGER --as-manager Example: To install host edna and then join the swarm managed by manager faye as a manager, execute: $ scone host install --name edna --join faye --as-manager","title":"Options"},{"location":"SCONE_HOST/#scone-host-swarm","text":"In the above example, we installed host dorothy without joining any swarm. Sometimes one wants to revise this decision via command swarm . You can add an existing node to another swarm or you can as a node to become a manager of an existing or a new node. You need to set the options --join MANAGER and --as-manager as described for scone host install . Example: To add host dorothy to the swarm managed by host faye , just execute: $ scone host swarm --name dorothy --join faye Example: To add host dorothy to the swarm managed by host faye as a manager, just execute: $ scone host swarm --name dorothy --join faye --as-manager","title":"scone host swarm"},{"location":"SCONE_HOST/#scone-host-check","text":"To check if a host is properly installed, you can use command check . Example: To check if host faye is properly installed, just execute: $ scone host check --name faye If the host is not properly installed, warnings or errors are issued. Typically, you can fix errors and warnings be reinstalling the host.","title":"scone host check"},{"location":"SCONE_HOST/#scone-host-reboot","text":"Sometimes, the installation of an host fails because the existing SGX driver cannot be removed. Most of the time, the issue is an enclave that currently uses the SGX driver. You can find these processes, for example, with Linux utility lsof : $ sudo lsof | grep dev/isgx Sometimes removing an existing Intel SGX driver fails, despite the fact that no process seems to use the driver. In case this happens, one last resort is to reboot the machine. You issue the reboot manually or you can perform this with scone host reboot . You must specify options --name HOST to indicate which host to reboot. Moreover, scone host reboot will exit with an error unless you specify option --force . Example: To reboot host dorothy, just execute the following: $ scone host reboot --name dorothy --force If you want to wait until the host is again available, you can specify option --wait . Example: To reboot host dorothy and only return after the host has indeed rebooted, just execute the following: $ scone host reboot --name dorothy --force --wait","title":"scone host reboot"},{"location":"SCONE_HOST/#scone-host-uninstall","text":"With the help of scone host uninstall you can force an host to leave any Docker swarm it might be part of uninstall the patched Docker engine uninstall the patched SGX driver NOTE: all containers that might run on this host, will be destroyed.","title":"scone host uninstall"},{"location":"SCONE_HOST/#options_1","text":"You must define the name of the host to be uninstalled via option --name HOST . You must always give the --force option, otherwise, an error is issued. If a node is part of a swarm, you must explicitly specify the option --manager MANAGERHOST . If the node is not part of a swarm, you must specify the option --noswarm . Note: While other objects / commands support the use of environment variable SCONE_MANAGER as an implicit definition of --manager $SCONE_MANAGER , we decided that users must explicitly specify the manager of the swarm for command uninstall . Example: To uninstall a host edna that is part of a swarm managed by host faye , execute: $ scone host uninstall --name edna --force --manager faye Example: Let's assume that host edna is not part of a swarm. To uninstall host edna , execute: $ scone host uninstall --name edna --force --noswarm Example: To uninstall a host faye which is also manager can be tricky. First, make sure that all other nodes are removed from the swarm. To check the status, execute: $ scone host check --name faye $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE faye Ready Active Leader If the output matches the above execute the following to uninstall host faye (both parameters --manager MANAGERHOST and --noswarm must be added): $ scone host uninstall --name faye --noswarm --manager faye --force","title":"Options"},{"location":"SCONE_HOST/#general-options","text":"--help (or, -h ): issue help message for object host . If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case a command fails. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"General options"},{"location":"SCONE_HOST_SETUP/","text":"SCONE: Host Installation Guide (Deprecated) Do not use scone host CLI We will soon remove the host installation functionality. Please follow this installation guide to set up hosts. Patched SGX Driver and Patched Docker Engine You only need the patched driver and patched docker engine in case you want to run with iExec. If you have already installed a SGX driver and a docker engine on your machine, you should be all set to go, i.e., no need to install the patched drivers. SCONE AUTO Mode If you just want to take SCONE for a test drive, you can run applications without installing an SGX driver. Note, however, that you need a reasonably new CPU to run the containers since we compile applications assuming that have access to most modern CPU extensions. This page describes how to set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and we remind you of how to set up your ssh configuration to be able to use your scone CLI. During this setup, we install a patched Intel SGX driver - required for better monitoring support, install a patched docker engine - to ensure that all containers have access to SGX, and start or join a docker swarm - if requested by command line options. Prerequisite : We assume that you have set up the scone command line interface . The scone CLI can run on your developer machine, a virtual machine or inside a container. The easiest way to get started is to run scone in a Docker container. No matter where scone is running, it requires that ssh be properly installed. We recommend for now to install Ubuntu 16.04 LTS on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. Also, Intel SGX driver still recommends Ubuntu 16.04 LTS and not Ubuntu 18.04 LTS. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size). The **scone CLI* uses ssh to log into remote hosts. We assume that ssh is setup in such a way that you do not need passwords to log into these hosts. Please read section ssh setup to learn how to ensure this. Installation of a single host After you set up the scone CLI and your passwordless ssh works, you can install the scone related software on a new host, say, alice as follows: $ scone host install --name alice To verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute: $ scone host check --name alice This command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed. Installation of a swarm For a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that alice and bob should be managers but caroline a non-manager, execute the following: $ scone host install --name alice --as-manager $ scone host install --name bob --as-manager --join alice $ scone host install --name caroline --join alice Note that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects that they will be in the same local area network. Checking your Installation To test the installation, one can run a simple hello-world container: > sudo docker run hello-world Background Information Patched Docker Engine (Moby) For an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves. Some docker commands (like docker run ) support an option --device (i.e., --device /dev/isgx ) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like docker build ) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx). With the help of this patched engine, we can use Dockerfiles to generate container images (see this Tutorial ). Right now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017). Patched SGX Driver We also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of available and free EPC (Extended Page Cache) pages. Right now, we provide a patched version of the latest Intel SGX driver (November 11, 2017). Note We have been updating SCONE such that SCONE does not need access to /dev/isgx during cross-compilation - which could, for example, be executed during docker build . Hence, we will soon switch to an unpatched Docker engine and SGX driver. Screencast This screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since scone host depends on systemd to start the swarm. In case systemd is not available, scone host will still be able to install the patched SGX driver and the patched Docker engine. Another issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by scone host . The reason for that is that typically that some process is still using the /dev/isgx device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"SCONE: Host Installation Guide  (Deprecated)"},{"location":"SCONE_HOST_SETUP/#scone-host-installation-guide-deprecated","text":"Do not use scone host CLI We will soon remove the host installation functionality. Please follow this installation guide to set up hosts. Patched SGX Driver and Patched Docker Engine You only need the patched driver and patched docker engine in case you want to run with iExec. If you have already installed a SGX driver and a docker engine on your machine, you should be all set to go, i.e., no need to install the patched drivers. SCONE AUTO Mode If you just want to take SCONE for a test drive, you can run applications without installing an SGX driver. Note, however, that you need a reasonably new CPU to run the containers since we compile applications assuming that have access to most modern CPU extensions. This page describes how to set up a host such that it can run SCONE secure containers, i.e., containers in which processes run inside of SGX enclaves, and we remind you of how to set up your ssh configuration to be able to use your scone CLI. During this setup, we install a patched Intel SGX driver - required for better monitoring support, install a patched docker engine - to ensure that all containers have access to SGX, and start or join a docker swarm - if requested by command line options. Prerequisite : We assume that you have set up the scone command line interface . The scone CLI can run on your developer machine, a virtual machine or inside a container. The easiest way to get started is to run scone in a Docker container. No matter where scone is running, it requires that ssh be properly installed. We recommend for now to install Ubuntu 16.04 LTS on the Swarm machines. For older versions of Ubuntu, some minor manual fixing might be needed during installation of the SGX drivers and the Docker engine. Also, Intel SGX driver still recommends Ubuntu 16.04 LTS and not Ubuntu 18.04 LTS. The screencast below shows some of the issues one faces on older Ubuntu versions. The secure containers are mostly based on Ubuntu or Alpine Linux (smaller image size). The **scone CLI* uses ssh to log into remote hosts. We assume that ssh is setup in such a way that you do not need passwords to log into these hosts. Please read section ssh setup to learn how to ensure this.","title":"SCONE: Host Installation Guide  (Deprecated)"},{"location":"SCONE_HOST_SETUP/#installation-of-a-single-host","text":"After you set up the scone CLI and your passwordless ssh works, you can install the scone related software on a new host, say, alice as follows: $ scone host install --name alice To verify that a host is properly installed for SCONE and contains the newest patched Docker engine and SGX driver, just execute: $ scone host check --name alice This command will issue an error unless the newest versions of the patched Docker engine and the patched SGX driver is installed.","title":"Installation of a single host"},{"location":"SCONE_HOST_SETUP/#installation-of-a-swarm","text":"For a set of hosts to form a (Docker) swarm, you need to decide which hosts should be managers and which should be just members of the swarm. Say, you decided that alice and bob should be managers but caroline a non-manager, execute the following: $ scone host install --name alice --as-manager $ scone host install --name bob --as-manager --join alice $ scone host install --name caroline --join alice Note that the hosts must be able to communicate with each other (i.e., not partitioned through firewalls). Docker recommends/expects that they will be in the same local area network.","title":"Installation of a swarm"},{"location":"SCONE_HOST_SETUP/#checking-your-installation","text":"To test the installation, one can run a simple hello-world container: > sudo docker run hello-world","title":"Checking your Installation"},{"location":"SCONE_HOST_SETUP/#background-information","text":"","title":"Background Information"},{"location":"SCONE_HOST_SETUP/#patched-docker-engine-moby","text":"For an container to be able to use SGX, it has to have access to a device (/dev/isgx). This device permits the container to talk to the SGX driver. This driver is needed, in particular, to create SGX enclaves. Some docker commands (like docker run ) support an option --device (i.e., --device /dev/isgx ) which allows us to give a container access to the SGX device. We need to point out that some docker commands (like docker build ) do, however, not yet support the device option. Therefore, we maintain and install a slightly patched docker engine (i.e., a variant of moby): this engine ensures that each container has access to the SGX device (/dev/isgx). With the help of this patched engine, we can use Dockerfiles to generate container images (see this Tutorial ). Right now we provide a patched version of the currently active branch of Moby (a.k.a., the Docker engine): 17.05.0-ce, build 89658be (November 11, 2017).","title":"Patched Docker Engine (Moby)"},{"location":"SCONE_HOST_SETUP/#patched-sgx-driver","text":"We also maintain a patched version of the SGX driver. This version adds some additional monitoring like the number of available and free EPC (Extended Page Cache) pages. Right now, we provide a patched version of the latest Intel SGX driver (November 11, 2017).","title":"Patched SGX Driver"},{"location":"SCONE_HOST_SETUP/#note","text":"We have been updating SCONE such that SCONE does not need access to /dev/isgx during cross-compilation - which could, for example, be executed during docker build . Hence, we will soon switch to an unpatched Docker engine and SGX driver.","title":"Note"},{"location":"SCONE_HOST_SETUP/#screencast","text":"This screencast shows the installation of three machines SGX-capable hosts. In this screencast, we show the installation on machines that run older versions of Ubuntu (sgx2 = 14.04, sgx3 = 14.04 with custom kernel, and sgx4 = 16.04). In this case, we will see some warnings since scone host depends on systemd to start the swarm. In case systemd is not available, scone host will still be able to install the patched SGX driver and the patched Docker engine. Another issue that one sometimes faces is that an older SGX drivers is already installed but cannot be offloaded and replaced by the patched driver by scone host . The reason for that is that typically that some process is still using the /dev/isgx device. This needs to be manually fixed by stopping the process or by rebooting the machine. Alternatively, one can use the existing SGX driver. However, the monitoring of the used EPC pages will not be provided in this case. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_OpenStack/","text":"","title":"SCONE OpenStack"},{"location":"SCONE_Publications/","text":"SCONE Related Publications SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016 This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system call interface. Authors : Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer Media : pdf , slides , audio , url Abstract : In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers. We describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput. Bibtex @inproceedings { 199364 , author = {Sergei Arnautov and Bohdan Trach and Franz Gregor and Thomas Knauth and Andre Martin and Christian Priebe and Joshua Lind and Divya Muthukumaran and Dan O{\\textquoteright}Keeffe and Mark L. Stillwell and David Goltzsche and Dave Eyers and R{\\\"u}diger Kapitza and Peter Pietzuch and Christof Fetzer} , title = {{SCONE}: Secure Linux Containers with Intel {SGX}} , booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)} , year = {2016} , isbn = {978-1-931971-33-1} , address = {Savannah, GA} , pages = {689--703} , url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/arnautov} , publisher = {{USENIX} Association} , } Building Critical Applications Using Microservices, IEEE Security & Privacy, Volume: 14 Issue: 6, December 2016 Author : Christof Fetzer Media : pdf , html Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves. Bibtex @ARTICLE { 7782696 , author = {C. Fetzer} , journal = {IEEE Security Privacy} , title = {Building Critical Applications Using Microservices} , year = {2016} , volume = {14} , number = {6} , pages = {86-89} , keywords = {trusted computing;critical applications;critical software correctness;legacy hardware;microservice confidentiality;microservice integrity;microservice-based system;secure enclaves;software components;trustworthy systems;Buildings;Containers;Kernel;Linux;Security;Linux;hardware;microservices;secure enclaves;security;software} , doi = {10.1109/MSP.2016.129} , ISSN = {1540-7993} , month = {Nov} , } SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017 To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding the overheads, please see this paper. This won the best paper award of EuroSys 2017. Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer Media : pdf , html Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx. Bibtex @inproceedings { Kuvaiskii:2017:SMS:3064176.3064192 , author = {Kuvaiskii, Dmitrii and Oleksenko, Oleksii and Arnautov, Sergei and Trach, Bohdan and Bhatotia, Pramod and Felber, Pascal and Fetzer, Christof} , title = {SGXBOUNDS: Memory Safety for Shielded Execution} , booktitle = {Proceedings of the Twelfth European Conference on Computer Systems} , series = {EuroSys '17} , year = {2017} , isbn = {978-1-4503-4938-3} , location = {Belgrade, Serbia} , pages = {205--221} , numpages = {17} , url = {http://doi.acm.org/10.1145/3064176.3064192} , doi = {10.1145/3064176.3064192} , acmid = {3064192} , publisher = {ACM} , address = {New York, NY, USA} , } FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017 This paper describes our new lock-free queue for our asynchronous system calls. Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach Media : pdf , html Abstract : With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability. Bibtex @INPROCEEDINGS { 7967181 , author = {S. Arnautov and P. Felber and C. Fetzer and B. Trach} , booktitle = {2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)} , title = {FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue} , year = {2017} , volume = {} , number = {} , pages = {907-916} , keywords = {microprocessor chips;multi-threading;multiprocessing systems;operating systems (computers);queueing theory;FFQ;FIFO queue;consumer threads;fast single producer-multiple consumer concurrent FIFO queue;hardware threads;lock-free consumers;multicore architectures;multiple producer;operating systems;secure application;single producer variant;synchronization operations;Algorithm design and analysis;Context;Instruction sets;Message systems;Scalability;Synchronization;Throughput;FIFO queue;SPMC queue;concurrent FIFO queue;concurrent queue;lock-free algorithm;wait-free algorithm} , doi = {10.1109/IPDPS.2017.41} , ISSN = {} , month = {May} , } PESOS: Policy Enhanced Secure Object Store, EuroSys 2018 Authors : Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden) Media : pdf , html Abstract : Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language. Pesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small. Bibtex @inproceedings { Krahn:2018:PEP:3190508.3190518 , author = {Krahn, Robert and Trach, Bohdan and Vahldiek-Oberwagner, Anjo and Knauth, Thomas and Bhatotia, Pramod and Fetzer, Christof} , title = {Pesos: Policy Enhanced Secure Object Store} , booktitle = {Proceedings of the Thirteenth EuroSys Conference} , series = {EuroSys '18} , year = {2018} , isbn = {978-1-4503-5584-1} , location = {Porto, Portugal} , pages = {25:1--25:17} , articleno = {25} , numpages = {17} , url = {http://doi.acm.org/10.1145/3190508.3190518} , doi = {10.1145/3190508.3190518} , acmid = {3190518} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {intel SGX, kinetic disks, policy language, storage security} , } ShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018 Authors : Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer Media : pdf , html , video Abstract : Middleboxes that process confidential data cannot be securely deployed in untrusted cloud environments. To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic. Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic. This motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers. ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate. Bibtex @inproceedings { Trach:2018:SSM:3185467.3185469 , author = {Trach, Bohdan and Krohmer, Alfred and Gregor, Franz and Arnautov, Sergei and Bhatotia, Pramod and Fetzer, Christof} , title = {ShieldBox: Secure Middleboxes Using Shielded Execution} , booktitle = {Proceedings of the Symposium on SDN Research} , series = {SOSR '18} , year = {2018} , isbn = {978-1-4503-5664-0} , location = {Los Angeles, CA, USA} , pages = {2:1--2:14} , articleno = {2} , numpages = {14} , url = {http://doi.acm.org/10.1145/3185467.3185469} , doi = {10.1145/3185467.3185469} , acmid = {3185469} , publisher = {ACM} , address = {New York, NY, USA} , } Varys: Protecting SGX enclaves from practical side-channel attacks, USENIX ATC 2018 Authors : Oleksii Oleksenko, Bohdan Trach, Robert Krahn, and Andr\u00e9 Martin, TU Dresden; Mark Silberstein, Technion; Christof Fetzer, TU Dresden Media : pdf , slides , audio , url Abstract : Numerous recent works have experimentally shown that Intel Software Guard Extensions (SGX) are vulnerable to cache timing and page table side-channel attacks which could be used to circumvent the data confidentiality guarantees provided by SGX. Existing mechanisms that protect against these attacks either incur high execution costs, are ineffective against certain attack variants, or require significant code modifications. We present Varys, a system that protects unmodified programs running in SGX enclaves from cache timing and page table side-channel attacks. Varys takes a pragmatic approach of strict reservation of physical cores to security-sensitive threads, thereby preventing the attacker from accessing shared CPU resources during enclave execution. The key challenge that we are addressing is that of maintaining the core reservation in the presence of an untrusted OS. Varys fully protects against all L1/L2 cache timing attacks and significantly raises the bar for page table side-channel attacks - all with only 15% overhead on average for Phoenix and PARSEC benchmarks. Additionally, we propose a set of minor hardware extensions that hold the potential to extend Varys' security guarantees to L3 cache and further improve its performance. Bibtex @inproceedings { 216033 , author = {Oleksii Oleksenko and Bohdan Trach and Robert Krahn and Mark Silberstein and Christof Fetzer} , title = {Varys: Protecting {SGX} Enclaves from Practical Side-Channel Attacks} , booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)} , year = {2018} , isbn = {978-1-931971-44-7} , address = {Boston, MA} , pages = {227--240} , url = {https://www.usenix.org/conference/atc18/presentation/oleksenko} , publisher = {{USENIX} Association} , } SGX-PySpark: Secure Distributed Data Analytics. Authors : Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. Media : https://doi.org/10.1145/3308558.3314129 Abstract : Data analytics is central to modern online services, particularly those data-driven. Often this entails the processing of large-scale datasets which may contain private, personal and sensitive information relating to individuals and organisations. Particular challenges arise where cloud is used to store and process the sensitive data. In such settings, security and privacy concerns become paramount, as the cloud provider is trusted to guarantee the security of the services they offer, including data confidentiality. Therefore, the issue this work tackles is \u201cHow to securely perform data analytics in a public cloud?\u201d To assist this question, we design and implement SGX-PySpark- a secure distributed data analytics system which relies on a trusted execution environment (TEE) such as Intel SGX to provide strong security guarantees. To build SGX-PySpark, we integrate PySpark - a widely used framework for data analytics in industry to support a wide range of queries, with SCONE - a shielded execution framework using Intel SGX. Bibtex @inproceedings { LeQuoc:2019:SSD:3308558.3314129 , author = {Le Quoc, Do and Gregor, Franz and Singh, Jatinder and Fetzer, Christof} , title = {SGX-PySpark: Secure Distributed Data Analytics} , booktitle = {The World Wide Web Conference} , series = {WWW '19} , year = {2019} , isbn = {978-1-4503-6674-8} , location = {San Francisco, CA, USA} , pages = {3564--3563} , numpages = {0} , url = {http://doi.acm.org/10.1145/3308558.3314129} , doi = {10.1145/3308558.3314129} , acmid = {3314129} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {Confidential computing, data analytics, distributed system, security} , } \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Publications"},{"location":"SCONE_Publications/#scone-related-publications","text":"","title":"SCONE Related Publications"},{"location":"SCONE_Publications/#scone-secure-linux-containers-with-intel-sgx-usenix-osdi-2016","text":"This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system call interface. Authors : Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer Media : pdf , slides , audio , url Abstract : In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers. We describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput. Bibtex @inproceedings { 199364 , author = {Sergei Arnautov and Bohdan Trach and Franz Gregor and Thomas Knauth and Andre Martin and Christian Priebe and Joshua Lind and Divya Muthukumaran and Dan O{\\textquoteright}Keeffe and Mark L. Stillwell and David Goltzsche and Dave Eyers and R{\\\"u}diger Kapitza and Peter Pietzuch and Christof Fetzer} , title = {{SCONE}: Secure Linux Containers with Intel {SGX}} , booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)} , year = {2016} , isbn = {978-1-931971-33-1} , address = {Savannah, GA} , pages = {689--703} , url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/arnautov} , publisher = {{USENIX} Association} , }","title":"SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016"},{"location":"SCONE_Publications/#building-critical-applications-using-microservices-ieee-security-privacy-volume-14-issue-6-december-2016","text":"Author : Christof Fetzer Media : pdf , html Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves. Bibtex @ARTICLE { 7782696 , author = {C. Fetzer} , journal = {IEEE Security Privacy} , title = {Building Critical Applications Using Microservices} , year = {2016} , volume = {14} , number = {6} , pages = {86-89} , keywords = {trusted computing;critical applications;critical software correctness;legacy hardware;microservice confidentiality;microservice integrity;microservice-based system;secure enclaves;software components;trustworthy systems;Buildings;Containers;Kernel;Linux;Security;Linux;hardware;microservices;secure enclaves;security;software} , doi = {10.1109/MSP.2016.129} , ISSN = {1540-7993} , month = {Nov} , }","title":"Building Critical Applications Using Microservices, IEEE Security &amp; Privacy, Volume: 14 Issue: 6, December 2016"},{"location":"SCONE_Publications/#sgxbounds-memory-safety-for-shielded-execution-eurosys-2017","text":"To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding the overheads, please see this paper. This won the best paper award of EuroSys 2017. Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer Media : pdf , html Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx. Bibtex @inproceedings { Kuvaiskii:2017:SMS:3064176.3064192 , author = {Kuvaiskii, Dmitrii and Oleksenko, Oleksii and Arnautov, Sergei and Trach, Bohdan and Bhatotia, Pramod and Felber, Pascal and Fetzer, Christof} , title = {SGXBOUNDS: Memory Safety for Shielded Execution} , booktitle = {Proceedings of the Twelfth European Conference on Computer Systems} , series = {EuroSys '17} , year = {2017} , isbn = {978-1-4503-4938-3} , location = {Belgrade, Serbia} , pages = {205--221} , numpages = {17} , url = {http://doi.acm.org/10.1145/3064176.3064192} , doi = {10.1145/3064176.3064192} , acmid = {3064192} , publisher = {ACM} , address = {New York, NY, USA} , }","title":"SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017"},{"location":"SCONE_Publications/#ffq-a-fast-single-producermultiple-consumer-concurrent-fifo-queue-ipdps-2017","text":"This paper describes our new lock-free queue for our asynchronous system calls. Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach Media : pdf , html Abstract : With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability. Bibtex @INPROCEEDINGS { 7967181 , author = {S. Arnautov and P. Felber and C. Fetzer and B. Trach} , booktitle = {2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)} , title = {FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue} , year = {2017} , volume = {} , number = {} , pages = {907-916} , keywords = {microprocessor chips;multi-threading;multiprocessing systems;operating systems (computers);queueing theory;FFQ;FIFO queue;consumer threads;fast single producer-multiple consumer concurrent FIFO queue;hardware threads;lock-free consumers;multicore architectures;multiple producer;operating systems;secure application;single producer variant;synchronization operations;Algorithm design and analysis;Context;Instruction sets;Message systems;Scalability;Synchronization;Throughput;FIFO queue;SPMC queue;concurrent FIFO queue;concurrent queue;lock-free algorithm;wait-free algorithm} , doi = {10.1109/IPDPS.2017.41} , ISSN = {} , month = {May} , }","title":"FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017"},{"location":"SCONE_Publications/#pesos-policy-enhanced-secure-object-store-eurosys-2018","text":"Authors : Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden) Media : pdf , html Abstract : Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language. Pesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small. Bibtex @inproceedings { Krahn:2018:PEP:3190508.3190518 , author = {Krahn, Robert and Trach, Bohdan and Vahldiek-Oberwagner, Anjo and Knauth, Thomas and Bhatotia, Pramod and Fetzer, Christof} , title = {Pesos: Policy Enhanced Secure Object Store} , booktitle = {Proceedings of the Thirteenth EuroSys Conference} , series = {EuroSys '18} , year = {2018} , isbn = {978-1-4503-5584-1} , location = {Porto, Portugal} , pages = {25:1--25:17} , articleno = {25} , numpages = {17} , url = {http://doi.acm.org/10.1145/3190508.3190518} , doi = {10.1145/3190508.3190518} , acmid = {3190518} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {intel SGX, kinetic disks, policy language, storage security} , }","title":"PESOS: Policy Enhanced Secure Object Store, EuroSys 2018"},{"location":"SCONE_Publications/#shieldbox-secure-middleboxes-using-shielded-execution-sigcomm-sosr-2018","text":"Authors : Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer Media : pdf , html , video Abstract : Middleboxes that process confidential data cannot be securely deployed in untrusted cloud environments. To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic. Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic. This motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers. ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate. Bibtex @inproceedings { Trach:2018:SSM:3185467.3185469 , author = {Trach, Bohdan and Krohmer, Alfred and Gregor, Franz and Arnautov, Sergei and Bhatotia, Pramod and Fetzer, Christof} , title = {ShieldBox: Secure Middleboxes Using Shielded Execution} , booktitle = {Proceedings of the Symposium on SDN Research} , series = {SOSR '18} , year = {2018} , isbn = {978-1-4503-5664-0} , location = {Los Angeles, CA, USA} , pages = {2:1--2:14} , articleno = {2} , numpages = {14} , url = {http://doi.acm.org/10.1145/3185467.3185469} , doi = {10.1145/3185467.3185469} , acmid = {3185469} , publisher = {ACM} , address = {New York, NY, USA} , }","title":"ShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018"},{"location":"SCONE_Publications/#varys-protecting-sgx-enclaves-from-practical-side-channel-attacks-usenix-atc-2018","text":"Authors : Oleksii Oleksenko, Bohdan Trach, Robert Krahn, and Andr\u00e9 Martin, TU Dresden; Mark Silberstein, Technion; Christof Fetzer, TU Dresden Media : pdf , slides , audio , url Abstract : Numerous recent works have experimentally shown that Intel Software Guard Extensions (SGX) are vulnerable to cache timing and page table side-channel attacks which could be used to circumvent the data confidentiality guarantees provided by SGX. Existing mechanisms that protect against these attacks either incur high execution costs, are ineffective against certain attack variants, or require significant code modifications. We present Varys, a system that protects unmodified programs running in SGX enclaves from cache timing and page table side-channel attacks. Varys takes a pragmatic approach of strict reservation of physical cores to security-sensitive threads, thereby preventing the attacker from accessing shared CPU resources during enclave execution. The key challenge that we are addressing is that of maintaining the core reservation in the presence of an untrusted OS. Varys fully protects against all L1/L2 cache timing attacks and significantly raises the bar for page table side-channel attacks - all with only 15% overhead on average for Phoenix and PARSEC benchmarks. Additionally, we propose a set of minor hardware extensions that hold the potential to extend Varys' security guarantees to L3 cache and further improve its performance. Bibtex @inproceedings { 216033 , author = {Oleksii Oleksenko and Bohdan Trach and Robert Krahn and Mark Silberstein and Christof Fetzer} , title = {Varys: Protecting {SGX} Enclaves from Practical Side-Channel Attacks} , booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)} , year = {2018} , isbn = {978-1-931971-44-7} , address = {Boston, MA} , pages = {227--240} , url = {https://www.usenix.org/conference/atc18/presentation/oleksenko} , publisher = {{USENIX} Association} , }","title":"Varys: Protecting SGX enclaves from practical side-channel attacks, USENIX ATC 2018"},{"location":"SCONE_Publications/#sgx-pyspark-secure-distributed-data-analytics","text":"Authors : Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. Media : https://doi.org/10.1145/3308558.3314129 Abstract : Data analytics is central to modern online services, particularly those data-driven. Often this entails the processing of large-scale datasets which may contain private, personal and sensitive information relating to individuals and organisations. Particular challenges arise where cloud is used to store and process the sensitive data. In such settings, security and privacy concerns become paramount, as the cloud provider is trusted to guarantee the security of the services they offer, including data confidentiality. Therefore, the issue this work tackles is \u201cHow to securely perform data analytics in a public cloud?\u201d To assist this question, we design and implement SGX-PySpark- a secure distributed data analytics system which relies on a trusted execution environment (TEE) such as Intel SGX to provide strong security guarantees. To build SGX-PySpark, we integrate PySpark - a widely used framework for data analytics in industry to support a wide range of queries, with SCONE - a shielded execution framework using Intel SGX. Bibtex @inproceedings { LeQuoc:2019:SSD:3308558.3314129 , author = {Le Quoc, Do and Gregor, Franz and Singh, Jatinder and Fetzer, Christof} , title = {SGX-PySpark: Secure Distributed Data Analytics} , booktitle = {The World Wide Web Conference} , series = {WWW '19} , year = {2019} , isbn = {978-1-4503-6674-8} , location = {San Francisco, CA, USA} , pages = {3564--3563} , numpages = {0} , url = {http://doi.acm.org/10.1145/3308558.3314129} , doi = {10.1145/3308558.3314129} , acmid = {3314129} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {Confidential computing, data analytics, distributed system, security} , } \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"SGX-PySpark: Secure Distributed Data Analytics."},{"location":"SCONE_SERVICE/","text":"scone service scone service manages remote docker services. scone service is mainly a thin wrapper around docker service . However, instead of executing commands locally, it forwards the commands to the manager of a remote swarm. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER : this variable defines the default swarm to be used by scone service in case option --manager is not given. scone service supports all commands and all the options of docker service . The semantics of command create is, however, slightly modified: scone service create only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label SGX VERSION greater than 0. In addition to the docker defined commands, scone service supports two new commands: scone service Description registry checks if a registry service is running in the swarm and if it is not, it starts a new registry service. pull pulls an image from a repository and stores it in the registry of the swarm. scone service commands The following commands of scone service are implemented by docker service : scone service Description create Creates a new service on a SGX-enabled machine . inspect Display detailed information on one or more services logs Fetch the logs of a service or task ls List services ps List the tasks of one or more services rm Remove one or more services rollback Revert changes to a service's configuration scale Scale one or multiple replicated services update Update a service The options of the above commands are the same as for docker service with the exception that all commands support the new option --manager MANAGER . scone service registry [OPTIONS] Starts a local registry service in the swarm managed by node MANAGER . The identity of MANAGER is given via option --manager=MANAGER or via environment variable SCONE_MANAGER=MANAGER . Options Description --manager MANAGER manager of swarm (required unless SCONE_MANAGER is defined) --verbose print verbose messages --debug print debug messages --help print usage of this command scone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG] Pulls an image from a repository (typically, docker hub) and stores the image in the local registry. First, create this local repository with $ scone service registry --manager MANAGER It is expected that the name of the image to be pulled has the following format: REPOSITORY/IMAGE [ :TAG ] After pulling the image, it is locally available in the swarm. The name of the image id now: localhost:5000/IMAGE [ :TAG ] The identity of MANAGER is given via option --manager or via environment variable SCONE_MANAGER. Options Description --manager MANAGER manager of swarm (required) --verbose print verbose messages --debug print debug messages --help print usage of this command Example To ensure that a registry is running on the swarm managed by node faye , execute: $ export SCONE_MANAGER = faye $ scone service registry To pull image sconecuratedimages/sconetainer:shielded from docker hub and store it in the local registry, execute $ scone service pull sconecuratedimages/sconetainer:shielded You can then start this image as a service on the swarm managed by faye as follows: $ scone service create --name nginx-shielded --detach = true --publish 8090 :8080 --publish 8092 :8082 localhost:5000/sconetainer:shielded \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"scone service"},{"location":"SCONE_SERVICE/#scone-service","text":"scone service manages remote docker services. scone service is mainly a thin wrapper around docker service . However, instead of executing commands locally, it forwards the commands to the manager of a remote swarm. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER : this variable defines the default swarm to be used by scone service in case option --manager is not given. scone service supports all commands and all the options of docker service . The semantics of command create is, however, slightly modified: scone service create only creates services on SGX-enabled nodes. More precisely, it limits the nodes that can run a service to those that have a label SGX VERSION greater than 0. In addition to the docker defined commands, scone service supports two new commands: scone service Description registry checks if a registry service is running in the swarm and if it is not, it starts a new registry service. pull pulls an image from a repository and stores it in the registry of the swarm.","title":"scone service"},{"location":"SCONE_SERVICE/#scone-service-commands","text":"The following commands of scone service are implemented by docker service : scone service Description create Creates a new service on a SGX-enabled machine . inspect Display detailed information on one or more services logs Fetch the logs of a service or task ls List services ps List the tasks of one or more services rm Remove one or more services rollback Revert changes to a service's configuration scale Scale one or multiple replicated services update Update a service The options of the above commands are the same as for docker service with the exception that all commands support the new option --manager MANAGER .","title":"scone service commands"},{"location":"SCONE_SERVICE/#scone-service-registry-options","text":"Starts a local registry service in the swarm managed by node MANAGER . The identity of MANAGER is given via option --manager=MANAGER or via environment variable SCONE_MANAGER=MANAGER . Options Description --manager MANAGER manager of swarm (required unless SCONE_MANAGER is defined) --verbose print verbose messages --debug print debug messages --help print usage of this command","title":"scone service registry [OPTIONS]"},{"location":"SCONE_SERVICE/#scone-service-pull-options-repositoryimagetag","text":"Pulls an image from a repository (typically, docker hub) and stores the image in the local registry. First, create this local repository with $ scone service registry --manager MANAGER It is expected that the name of the image to be pulled has the following format: REPOSITORY/IMAGE [ :TAG ] After pulling the image, it is locally available in the swarm. The name of the image id now: localhost:5000/IMAGE [ :TAG ] The identity of MANAGER is given via option --manager or via environment variable SCONE_MANAGER. Options Description --manager MANAGER manager of swarm (required) --verbose print verbose messages --debug print debug messages --help print usage of this command","title":"scone service pull [OPTIONS] REPOSITORY/IMAGE[:TAG]"},{"location":"SCONE_SERVICE/#example","text":"To ensure that a registry is running on the swarm managed by node faye , execute: $ export SCONE_MANAGER = faye $ scone service registry To pull image sconecuratedimages/sconetainer:shielded from docker hub and store it in the local registry, execute $ scone service pull sconecuratedimages/sconetainer:shielded You can then start this image as a service on the swarm managed by faye as follows: $ scone service create --name nginx-shielded --detach = true --publish 8090 :8080 --publish 8092 :8082 localhost:5000/sconetainer:shielded \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Example"},{"location":"SCONE_STACK/","text":"scone stack scone stack manages remote docker stacks. scone stack is mainly a thin wrapper around docker stack that forwards the commands to a remote swarm - instead of executing the commands locally. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given. scone stack commands scone stack Description deploy deploy a new stack or update an existing stack ls list stacks ps list the tasks in the stack rm remove one or more stacks services list the services in the stack scone stack deploy The command deploy is used to create a set of services specified by a stack file . scone stack deploy expects the stack file specified via option -compose-file or -c . We expect this file to reside on your local machine/container (i.e., where you start scone stack deploy ). scone stack deploy copies the stack file to the remote swarm manager before executing docker stack deploy on the swarm manager. Example To deploy a stack that is called nginx on a remote swarm managed by node faye , execute the following: $ scone stack deploy --compose-file compose.yml --manager faye nginx In case you set the environment variable SCONE_MANAGER to faye , you can drop option --manager : $ export SCONE_MANAGER = faye $ scone stack deploy --compose-file compose.yml nginx \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"scone stack"},{"location":"SCONE_STACK/#scone-stack","text":"scone stack manages remote docker stacks. scone stack is mainly a thin wrapper around docker stack that forwards the commands to a remote swarm - instead of executing the commands locally. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given.","title":"scone stack"},{"location":"SCONE_STACK/#scone-stack-commands","text":"scone stack Description deploy deploy a new stack or update an existing stack ls list stacks ps list the tasks in the stack rm remove one or more stacks services list the services in the stack","title":"scone stack commands"},{"location":"SCONE_STACK/#scone-stack-deploy","text":"The command deploy is used to create a set of services specified by a stack file . scone stack deploy expects the stack file specified via option -compose-file or -c . We expect this file to reside on your local machine/container (i.e., where you start scone stack deploy ). scone stack deploy copies the stack file to the remote swarm manager before executing docker stack deploy on the swarm manager.","title":"scone stack deploy"},{"location":"SCONE_STACK/#example","text":"To deploy a stack that is called nginx on a remote swarm managed by node faye , execute the following: $ scone stack deploy --compose-file compose.yml --manager faye nginx In case you set the environment variable SCONE_MANAGER to faye , you can drop option --manager : $ export SCONE_MANAGER = faye $ scone stack deploy --compose-file compose.yml nginx \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Example"},{"location":"SCONE_SWARM/","text":"scone swarm Command scone swarm ls lists the nodes of a docker swarm. This is an extension of docker node ls in the sense that one can list the nodes of a swarm managed by some remote host, and several SCONE-related attributes of the nodes of a swarm are also printed. The SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands. Commands scone swarm supports the following commands: check : checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct. ls : lists all nodes of a swarm and their SCONE-related labels. Attributes As we mentioned above, scone introduces multiple new attributes: NODENO : each host in the swarm has a unique number in the range [1, number of swarm nodes ]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO. SGX VERSION : denotes the SGX version of the CPU of a host: 0 : the host does not support SGX (or, does not have a SGX driver installed) 1 : the host supports SGX version 1 2 : the host supports SGX version 2 (CPUs are not yet available) DOCKER-ENGINE : shows the version of the Docker engine that is installed. It will show SCONE if the latest patched Docker engine is installed. SGX-DRIVER : shows the version of the SGX driver. It will show SCONE if the latest patched Intel driver is installed. scone swarm ls You can list all nodes of a swarm and their attributes with the help of command scone swarm ls . This command requires you to specify a manager of the swarm with the help of option --manager MANAGER Example: To list all nodes of a swarm managed by host faye , execute $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader To list the nodes of the swarm managed by host alice , execute $ scone swarm ls --manager alice NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE alice Ready Active Leader 2 1 SCONE SCONE beatrix Ready Active 3 1 SCONE SCONE caroline Ready Active Environment Variable In case you mainly work with one swarm, you can set environment variable SCONE_MANAGER . If option --manager is not specified and SCONE_MANAGER is defined, the value stored in SCONE_MANAGER is used as the name of the manager. Example: To list all nodes of a swarm managed by host faye : $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader scone swarm check scone stores the attributes of a node using Docker: the attributes of a node are stored as node labels . For example, the attributes if a node supports SGX ( SGX VERSION ), if the patched docker engine ( DOCKER-ENGINE ) and the patched Intel driver ( SGX-DRIVER ) is installed are all stored as labels. Attributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued. To check and update the labels of the swarm nodes, you can execute command scone swarm check . You must specify the manager of the swarm by defining option --manager MANAGER or by defining environment variable SCONE_MANAGER . Example: To check the labels of the swarm managed by node faye , execute: $ scone swarm check --manager faye --verbose General options --help (or, -h ): issue help message for object **host*. If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes Screencast \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"scone swarm"},{"location":"SCONE_SWARM/#scone-swarm","text":"Command scone swarm ls lists the nodes of a docker swarm. This is an extension of docker node ls in the sense that one can list the nodes of a swarm managed by some remote host, and several SCONE-related attributes of the nodes of a swarm are also printed. The SCONE-related attributes need to be updated when nodes leave or join a swarm. While the scone commands try to update the labels whenever they might cause a label change, users might add nodes to a swarm with Docker commands.","title":"scone swarm"},{"location":"SCONE_SWARM/#commands","text":"scone swarm supports the following commands: check : checks that the SCONE-related labels of all nodes of a swarm and corrects these if not correct. ls : lists all nodes of a swarm and their SCONE-related labels.","title":"Commands"},{"location":"SCONE_SWARM/#attributes","text":"As we mentioned above, scone introduces multiple new attributes: NODENO : each host in the swarm has a unique number in the range [1, number of swarm nodes ]. The hosts are alphabetically sorted and the node with the smallest hostname gets assigned NODENO 1 and the host with the largest name, gets assigned the largest NODENO. SGX VERSION : denotes the SGX version of the CPU of a host: 0 : the host does not support SGX (or, does not have a SGX driver installed) 1 : the host supports SGX version 1 2 : the host supports SGX version 2 (CPUs are not yet available) DOCKER-ENGINE : shows the version of the Docker engine that is installed. It will show SCONE if the latest patched Docker engine is installed. SGX-DRIVER : shows the version of the SGX driver. It will show SCONE if the latest patched Intel driver is installed.","title":"Attributes"},{"location":"SCONE_SWARM/#scone-swarm-ls","text":"You can list all nodes of a swarm and their attributes with the help of command scone swarm ls . This command requires you to specify a manager of the swarm with the help of option --manager MANAGER Example: To list all nodes of a swarm managed by host faye , execute $ scone swarm ls --manager faye NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader To list the nodes of the swarm managed by host alice , execute $ scone swarm ls --manager alice NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE alice Ready Active Leader 2 1 SCONE SCONE beatrix Ready Active 3 1 SCONE SCONE caroline Ready Active","title":"scone swarm ls"},{"location":"SCONE_SWARM/#environment-variable","text":"In case you mainly work with one swarm, you can set environment variable SCONE_MANAGER . If option --manager is not specified and SCONE_MANAGER is defined, the value stored in SCONE_MANAGER is used as the name of the manager. Example: To list all nodes of a swarm managed by host faye : $ export SCONE_MANAGER = faye $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 1 1 SCONE SCONE dorothy Ready Active 2 1 SCONE SCONE edna Ready Active 3 1 SCONE SCONE faye Ready Active Leader","title":"Environment Variable"},{"location":"SCONE_SWARM/#scone-swarm-check","text":"scone stores the attributes of a node using Docker: the attributes of a node are stored as node labels . For example, the attributes if a node supports SGX ( SGX VERSION ), if the patched docker engine ( DOCKER-ENGINE ) and the patched Intel driver ( SGX-DRIVER ) is installed are all stored as labels. Attributes might change over time. After a node departs from a swarm or when a node joins a swarm, we need to update the labels. Otherwise, the Docker scheduler might not properly schedule containers on the nodes of a swarm. Also, when nodes of a swarm are listed, warnings might be issued. To check and update the labels of the swarm nodes, you can execute command scone swarm check . You must specify the manager of the swarm by defining option --manager MANAGER or by defining environment variable SCONE_MANAGER . Example: To check the labels of the swarm managed by node faye , execute: $ scone swarm check --manager faye --verbose","title":"scone swarm check"},{"location":"SCONE_SWARM/#general-options","text":"--help (or, -h ): issue help message for object **host*. If a command is specified, it issues a help message specific to this command. --debug (or, -x ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the output of the failing command with --debug set. --verbose (or, -v ): display all commands that are executed by scone host . This can be helpful in case commands fail. When you submit a support request regarding a failed command, please send a copy of the log of the output that includes","title":"General options"},{"location":"SCONE_SWARM/#screencast","text":"\u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Screencast"},{"location":"SCONE_Swarm_Example/","text":"Starting a SCONE Application on a Swarm We show how to run a secure nginx version, i.e., one that runs inside an enclave in a docker swarm with automatic restarts. To simplify the running of services, we provide a simple wrapper around the docker service command: the scone service executes docker service commands on the manager of a swarm. The manager is either specified via an option --manager or via environment variable SCONE_MANAGER . This is done in the same way as for command scone swarm . In what follows, we assume that SCONE_MANAGER is set to the leader of the swarm. The scone commands are typically executed in a container running at the developer site . Prerequisites Registry support For running an application in a Docker Swarm, you need to set up a local registry to ensure that all nodes get access to the same container image. The scone CLI expects the registry to be available at localhost:5000 . You can start a default registry with the help of scone : $ scone service registry --verbose Registry is already running in swarm beatrix To simplify pushing images to the local registry, the scone CLI includes a scone service pull command to pull an image from docker hub and then to push this image to the local registry. For example, to pull image sconecuratedimages/sconetainer:noshielding and store it as localhost:5000/sconetainer:noshielding , just execute: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding SGX Support Services are automatically restarted. In case, there is a persistent failure in some service ha , we would see repeated restarts like: $ scone service ps ha ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS f65id6ow5n6w ha.1 sconecuratedimages/nginx beatrix Ready Ready 1 second ago jt6wj5e3lso4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 3 seconds ago \"task: non-zero exit (1)\" sspou3mcis8m \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 9 seconds ago \"task: non-zero exit (1)\" p3bw780pu63b \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 15 seconds ago \"task: non-zero exit (1)\" 75zjsesil5k4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 22 seconds ago \"task: non-zero exit (1)\" Reasons for such failures might be that that the containers might not have access to the sgx device. There are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute scone swarm check : $ scone swarm check warning: 'sgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0)' ( Line numer: '198' ) warning: '--device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0)' ( Line numer: '199' ) To get a summary view of a swarm after you performed a check, just executed: $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 2 1 SCONE SCONE caroline Ready Active Reachable 3 1 SCONE SCONE dorothy Ready Active 4 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE beatrix Ready Active Leader Starting a Service After pulling an image into the local registry (see above), we can start a service in the swarm via scone service create . Docker swarm will start the image and it also takes care of failures by restarting failed services. For the next steps, make sure that all nodes have access to image sconecuratedimages/sconetainer:noshielding and pull this image via: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding We start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes: $ scone service create --name sconeweb --detach = true --publish 80 :80 --publish 443 :443 --replicas = 2 localhost:5000/sconetainer:noshielding In case the service starts up correctly, you will see a status like this: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ba3odjkz6mx2 sconeweb.1 localhost:5000/nginx:noshielding alice Running Running 8 minutes ago x2xq1c3aede7 sconeweb.2 localhost:5000/nginx:noshielding beatrix Running Running 8 minutes ago If, for example, an image is not available on all nodes, you might see the following status: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o79714pw2fpn sconeweb.1 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago t0byepte0fzj \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" mg4xdq868syq \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" ry1pqen9jgan \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" q05ti7gkxc7r \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" zxj74inh2zdf sconeweb.2 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago Stopping the service Stop the service via: $ scone service rm sconeweb Updating the image of a service Say, there is a new version of the sconetainer image available. We can update this image in our local registry as follows: $ scone pull sconecuratedimages/sconetainer:noshielding We can now update the service as follows: $ scone service update --image localhost:5000/sconetainer sconeweb Draining a node To be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node: > sudo docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 91a1vvex4dgozfrzy1y136gmg * alice Ready Active Leader jhrayos9ylu02egwvkxpqtbwb beatrix Ready Active You can now take node alice out of service by executing: > sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg To put the node back in service by executing: > sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Starting a SCONE Application on a Swarm"},{"location":"SCONE_Swarm_Example/#starting-a-scone-application-on-a-swarm","text":"We show how to run a secure nginx version, i.e., one that runs inside an enclave in a docker swarm with automatic restarts. To simplify the running of services, we provide a simple wrapper around the docker service command: the scone service executes docker service commands on the manager of a swarm. The manager is either specified via an option --manager or via environment variable SCONE_MANAGER . This is done in the same way as for command scone swarm . In what follows, we assume that SCONE_MANAGER is set to the leader of the swarm. The scone commands are typically executed in a container running at the developer site .","title":"Starting a SCONE Application on a Swarm"},{"location":"SCONE_Swarm_Example/#prerequisites","text":"","title":"Prerequisites"},{"location":"SCONE_Swarm_Example/#registry-support","text":"For running an application in a Docker Swarm, you need to set up a local registry to ensure that all nodes get access to the same container image. The scone CLI expects the registry to be available at localhost:5000 . You can start a default registry with the help of scone : $ scone service registry --verbose Registry is already running in swarm beatrix To simplify pushing images to the local registry, the scone CLI includes a scone service pull command to pull an image from docker hub and then to push this image to the local registry. For example, to pull image sconecuratedimages/sconetainer:noshielding and store it as localhost:5000/sconetainer:noshielding , just execute: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding","title":"Registry support"},{"location":"SCONE_Swarm_Example/#sgx-support","text":"Services are automatically restarted. In case, there is a persistent failure in some service ha , we would see repeated restarts like: $ scone service ps ha ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS f65id6ow5n6w ha.1 sconecuratedimages/nginx beatrix Ready Ready 1 second ago jt6wj5e3lso4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 3 seconds ago \"task: non-zero exit (1)\" sspou3mcis8m \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 9 seconds ago \"task: non-zero exit (1)\" p3bw780pu63b \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 15 seconds ago \"task: non-zero exit (1)\" 75zjsesil5k4 \\_ ha.1 sconecuratedimages/nginx beatrix Shutdown Failed 22 seconds ago \"task: non-zero exit (1)\" Reasons for such failures might be that that the containers might not have access to the sgx device. There are multiple reasons why the driver might not be accessible inside of a container: Did you indeed install the patched docker version? Did you indeed label the nodes correctly? To automatically diagnoses and in some cases, to perform some automatic corrections, just execute scone swarm check : $ scone swarm check warning: 'sgx device is not automatically mapped inside of container on host beatrix (stack=198 434 0)' ( Line numer: '198' ) warning: '--device=/dev/isgx: device mapper does not work inside of container on host beatrix (stack=199 434 0)' ( Line numer: '199' ) To get a summary view of a swarm after you performed a check, just executed: $ scone swarm ls NODENO SGX VERSION DOCKER-ENGINE SGX-DRIVER HOST STATUS AVAILABILITY MANAGER 2 1 SCONE SCONE caroline Ready Active Reachable 3 1 SCONE SCONE dorothy Ready Active 4 1 SCONE SCONE edna Ready Active Reachable 1 1 SCONE SCONE beatrix Ready Active Leader","title":"SGX Support"},{"location":"SCONE_Swarm_Example/#starting-a-service","text":"After pulling an image into the local registry (see above), we can start a service in the swarm via scone service create . Docker swarm will start the image and it also takes care of failures by restarting failed services. For the next steps, make sure that all nodes have access to image sconecuratedimages/sconetainer:noshielding and pull this image via: $ scone service pull sconecuratedimages/sconetainer:noshielding new tag: localhost:5000/sconetainer:noshielding We start a nginx service including a version of the scontain.com website. We start two replicas running inside separate enclaves - most likely on two different nodes: $ scone service create --name sconeweb --detach = true --publish 80 :80 --publish 443 :443 --replicas = 2 localhost:5000/sconetainer:noshielding In case the service starts up correctly, you will see a status like this: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ba3odjkz6mx2 sconeweb.1 localhost:5000/nginx:noshielding alice Running Running 8 minutes ago x2xq1c3aede7 sconeweb.2 localhost:5000/nginx:noshielding beatrix Running Running 8 minutes ago If, for example, an image is not available on all nodes, you might see the following status: $ scone service ps sconeweb ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o79714pw2fpn sconeweb.1 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago t0byepte0fzj \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" mg4xdq868syq \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" ry1pqen9jgan \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" q05ti7gkxc7r \\_ sconeweb.1 sconecuratedimages/sconetainer:noshielding beatrix Shutdown Rejected 4 hours ago \"No such image: sconecuratedim\u2026\" zxj74inh2zdf sconeweb.2 sconecuratedimages/sconetainer:noshielding alice Running Running 4 hours ago","title":"Starting a Service"},{"location":"SCONE_Swarm_Example/#stopping-the-service","text":"Stop the service via: $ scone service rm sconeweb","title":"Stopping the service"},{"location":"SCONE_Swarm_Example/#updating-the-image-of-a-service","text":"Say, there is a new version of the sconetainer image available. We can update this image in our local registry as follows: $ scone pull sconecuratedimages/sconetainer:noshielding We can now update the service as follows: $ scone service update --image localhost:5000/sconetainer sconeweb","title":"Updating the image of a service"},{"location":"SCONE_Swarm_Example/#draining-a-node","text":"To be able to drain all containers from a node, we need to figure out the node's id. We can do this manually by executing the following command on the leader node: > sudo docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 91a1vvex4dgozfrzy1y136gmg * alice Ready Active Leader jhrayos9ylu02egwvkxpqtbwb beatrix Ready Active You can now take node alice out of service by executing: > sudo docker node update --availability drain 91a1vvex4dgozfrzy1y136gmg To put the node back in service by executing: > sudo docker node update --availability active 91a1vvex4dgozfrzy1y136gmg \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Draining a node"},{"location":"SCONE_TUTORIAL/","text":"SCONE Hello World Install sgxmusl cross compiler image Ensure that you installed the various sconecuratedimages/crosscompilers container image: > docker image ls sconecuratedimages/* REPOSITORY TAG IMAGE ID CREATED SIZE sconecuratedimages/crosscompilers latest dff7975b7f32 7 hours ago 1 .57GB sconecuratedimages/crosscompilers scone dff7975b7f32 7 hours ago 1 .57GB If the cross compiler image is not yet installed, read Section SCONE Curated Container Images to learn how to install the SCONE cross compiler image. If the docker command fails, please ensure that docker is indeed installed ( docker installation ). Also, on some systems you might need to use sudo to run docker commands 1 . Install the tutorial Clone the tutorial: > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git Native Hello World Ensure that hello world runs natively on your machine: > cd SCONE_TUTORIAL/HelloWorld/ > gcc hello_world.c -o native_hello_world > ./native_hello_world Hello World Note that the generated executable, i.e., sim_hello_world , will only run on Linux. Statically-Linked Hello World The default cross compiler variant that runs hello world inside of an enclave is scone gcc and you can find this in container sconecuratedimages/crosscompilers . This variant requires access to the SGX device. In Linux, the SGX device is made available as /dev/isgx and we can give the cross compiler inside of an container access via option --device=/dev/isgx : > docker run --rm --device = /dev/isgx -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers scone-gcc hello_world.c -o sgx_hello_world This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically linked binary since it is wrapped in a dynamically linked loader program: > ldd ./sgx_hello_world linux-vdso.so.1 = > ( 0x00007ffcf73ad000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007f7c2a0e9000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007f7c29d1f000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007f7c2a306000 ) Ensure that file /etc/sgx-musl.conf exists. If not, store some default file like: > printf \"Q 1\\ne 0 0 0\\ns 1 0 0\\n\" | sudo tee /etc/sgx-musl.conf To run sgx_hello_world , in an enclave, just execute: > ./sgx_hello_world Hello World To see some more debug messages, set environment variable SCONE_VERSION=1 : > SCONE_VERSION = 1 ./sgx_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_ALLOW_DLOPEN = no Revision: 9b355b99170ad434010353bb9f4dca24e532b1b7 Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Hello World The debug outputs SCONE_MODE=hw shows that sgx_hello_world runs in hardware mode, i.e., inside an SGX enclave. Note. The compilation as well as the hello world program will fail in case you do not have an SGX driver installed. Dynamically-Linked Hello World > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_world.c -o dyn_hello_world To run this natively, just execute the following: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable SCONE_VERSION=1 : Hardware Mode vs Simulation Mode For debugging, we support three different modes for execution: hardware, simulation, and automatic : hardware : by setting environment variable to SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave. simulation : by setting environment variable to SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable. automatic : by setting environment variable to SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode) NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave. Execution on a SGX-capable machine > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello World Execution on a non-SGX machine If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world [ Error ] Could not create enclave: Error opening SGX device You could run this in simulation mode as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = SIM -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 Hello World Alternatively, you could run this program in automatic mode: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = AUTO -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 HelloWorld Run STRACE Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows: > docker run --cap-add SYS_PTRACE -it --rm --device = /dev/isgx -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace -f /usr/src/myapp/sgx_hello_world > strace.log Hello World head strace.log execve ( \"/usr/src/myapp/sgx_hello_world\" , [ \"/usr/src/myapp/sgx_hello_world\" ] , [ /* 10 vars */ ]) = 0 brk ( NULL ) = 0x10e8000 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory ) mmap ( NULL, 8192 , PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 ) = 0x7f17f07f1000 access ( \"/etc/ld.so.preload\" , R_OK ) = -1 ENOENT ( No such file or directory ) open ( \"/etc/ld.so.cache\" , O_RDONLY | O_CLOEXEC ) = 3 fstat ( 3 , { st_mode = S_IFREG | 0644 , st_size = 18506 , ... }) = 0 mmap ( NULL, 18506 , PROT_READ, MAP_PRIVATE, 3 , 0 ) = 0x7f17f07ec000 close ( 3 ) = 0 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory ) Screencast \u00a9 scontain.com , January 2020. Questions or Suggestions? Follow the steps described in https://docs.docker.com/install/linux/linux-postinstall/ on how to avoid using sudo to run docker . \u21a9","title":"SCONE Static vs Dynamic"},{"location":"SCONE_TUTORIAL/#scone-hello-world","text":"","title":"SCONE Hello World"},{"location":"SCONE_TUTORIAL/#install-sgxmusl-cross-compiler-image","text":"Ensure that you installed the various sconecuratedimages/crosscompilers container image: > docker image ls sconecuratedimages/* REPOSITORY TAG IMAGE ID CREATED SIZE sconecuratedimages/crosscompilers latest dff7975b7f32 7 hours ago 1 .57GB sconecuratedimages/crosscompilers scone dff7975b7f32 7 hours ago 1 .57GB If the cross compiler image is not yet installed, read Section SCONE Curated Container Images to learn how to install the SCONE cross compiler image. If the docker command fails, please ensure that docker is indeed installed ( docker installation ). Also, on some systems you might need to use sudo to run docker commands 1 .","title":"Install sgxmusl cross compiler image"},{"location":"SCONE_TUTORIAL/#install-the-tutorial","text":"Clone the tutorial: > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git","title":"Install the tutorial"},{"location":"SCONE_TUTORIAL/#native-hello-world","text":"Ensure that hello world runs natively on your machine: > cd SCONE_TUTORIAL/HelloWorld/ > gcc hello_world.c -o native_hello_world > ./native_hello_world Hello World Note that the generated executable, i.e., sim_hello_world , will only run on Linux.","title":"Native Hello World"},{"location":"SCONE_TUTORIAL/#statically-linked-hello-world","text":"The default cross compiler variant that runs hello world inside of an enclave is scone gcc and you can find this in container sconecuratedimages/crosscompilers . This variant requires access to the SGX device. In Linux, the SGX device is made available as /dev/isgx and we can give the cross compiler inside of an container access via option --device=/dev/isgx : > docker run --rm --device = /dev/isgx -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers scone-gcc hello_world.c -o sgx_hello_world This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically linked binary since it is wrapped in a dynamically linked loader program: > ldd ./sgx_hello_world linux-vdso.so.1 = > ( 0x00007ffcf73ad000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007f7c2a0e9000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007f7c29d1f000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007f7c2a306000 ) Ensure that file /etc/sgx-musl.conf exists. If not, store some default file like: > printf \"Q 1\\ne 0 0 0\\ns 1 0 0\\n\" | sudo tee /etc/sgx-musl.conf To run sgx_hello_world , in an enclave, just execute: > ./sgx_hello_world Hello World To see some more debug messages, set environment variable SCONE_VERSION=1 : > SCONE_VERSION = 1 ./sgx_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_ALLOW_DLOPEN = no Revision: 9b355b99170ad434010353bb9f4dca24e532b1b7 Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Hello World The debug outputs SCONE_MODE=hw shows that sgx_hello_world runs in hardware mode, i.e., inside an SGX enclave. Note. The compilation as well as the hello world program will fail in case you do not have an SGX driver installed.","title":"Statically-Linked Hello World"},{"location":"SCONE_TUTORIAL/#dynamically-linked-hello-world","text":"> docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc hello_world.c -o dyn_hello_world To run this natively, just execute the following: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./dyn_hello_world To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable SCONE_VERSION=1 :","title":"Dynamically-Linked Hello World"},{"location":"SCONE_TUTORIAL/#hardware-mode-vs-simulation-mode","text":"For debugging, we support three different modes for execution: hardware, simulation, and automatic : hardware : by setting environment variable to SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave. simulation : by setting environment variable to SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable. automatic : by setting environment variable to SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode) NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave.","title":"Hardware Mode vs Simulation Mode"},{"location":"SCONE_TUTORIAL/#execution-on-a-sgx-capable-machine","text":"> docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello World","title":"Execution on a SGX-capable machine"},{"location":"SCONE_TUTORIAL/#execution-on-a-non-sgx-machine","text":"If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world [ Error ] Could not create enclave: Error opening SGX device You could run this in simulation mode as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = SIM -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 Hello World Alternatively, you could run this program in automatic mode: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = AUTO -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 sconecuratedimages/crosscompilers:runtime export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 HelloWorld","title":"Execution on a non-SGX machine"},{"location":"SCONE_TUTORIAL/#run-strace","text":"Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows: > docker run --cap-add SYS_PTRACE -it --rm --device = /dev/isgx -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/crosscompilers strace -f /usr/src/myapp/sgx_hello_world > strace.log Hello World head strace.log execve ( \"/usr/src/myapp/sgx_hello_world\" , [ \"/usr/src/myapp/sgx_hello_world\" ] , [ /* 10 vars */ ]) = 0 brk ( NULL ) = 0x10e8000 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory ) mmap ( NULL, 8192 , PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 ) = 0x7f17f07f1000 access ( \"/etc/ld.so.preload\" , R_OK ) = -1 ENOENT ( No such file or directory ) open ( \"/etc/ld.so.cache\" , O_RDONLY | O_CLOEXEC ) = 3 fstat ( 3 , { st_mode = S_IFREG | 0644 , st_size = 18506 , ... }) = 0 mmap ( NULL, 18506 , PROT_READ, MAP_PRIVATE, 3 , 0 ) = 0x7f17f07ec000 close ( 3 ) = 0 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory )","title":"Run STRACE"},{"location":"SCONE_TUTORIAL/#screencast","text":"\u00a9 scontain.com , January 2020. Questions or Suggestions? Follow the steps described in https://docs.docker.com/install/linux/linux-postinstall/ on how to avoid using sudo to run docker . \u21a9","title":"Screencast"},{"location":"SCONE_VOLUME/","text":"scone volume scone volume provides functionality to create volumes that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to volumes. The underlying technology of scone volume is infinit - a software created by a Docker Inc subsidiary. Please read the infinit documentation to understand the infinit concepts of user , network , and silo . NOTE: infinit is labeled as alpha software: use this only for development or testing . The performance of infinit needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given. scone volume commands scone volume Description install install infinit on all nodes of a swarm. create create new volume and install infinit storage platform if required check check that all volumes are available on all hosts delete delete a volume scone volume install [OPTIONS] scone volume install ensures that infinit is installed on all nodes of a swarm. Options Description --manager MANAGER manager of swarm (required) --as user run as user (default random id) --network name name of network to use/create (default scone-network) --silo name name of silo to use/create (default scone-silo) --capacity sz size of silo in GB (default 10) --help show this help message Notes: If you do not specify a default user via option --user USER , a random default user is created. If you do not specify a network name via option --network name , the default network name is set to scone-network . You can specify a silo name via potion --silo name . The default silo name is \"scone-silo\" . The capacity of the silo (in GB) is given via --capacity name . The default capacity is 10GB. **Examples: ** To install infinit with a storage capacity of 15 GB on each node, execute the following: $ scone volume install --verbose --manager alice --capacity 15 If you want to keep explicit control over user and network names, execute the following: $ scone volume install --verbose --manager alice --as scone --capacity 5 --silo my-silo --network scone-networkg If you want to see the user and network names for a given swarm, execute: $ scone volume install --manager alice --help scone volume create scone volume create creates a new infinit volume for a given user (option --as USER ) and with a given volume name (via option --name VOLUME ). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the --fast option to avoid checks and reinstallation of infinit. This volume will be available on all hosts of a swarm at location: /mnt/infinit/USER/VOLUME Options Description --manager HOST manager of swarm (required) --name volume name of volume to create (required) --help show this help message --as user run as user (default ) --network name name of network to use/create (default ) --silo name name of silo to use/create (default ) --capacity sz size of silo in GB (default ) --fast create volume without reinstalling all software (default=false) --verbose print location of created volume **Example: ** In case you already installed infinit on your swarm and you just want to create a new volume, execute the following: $ scone volume create --verbose --fast --name new-volume --verbose In case you do not know if infinit is already installed when you create a volume and your want to keep control over the names used by infinit, execute the following: $ scone volume create --verbose --manager alice --as scone --name my-volume --capacity 5 --silo my-silo --network scone-network --export config scone volume check Checks if all created volumes are available on all swarm nodes. Note: the current implementation of **scone volume check uses the metadata stored in the container/environment in which scone volume check executes. In case you created some volumes in a different container, the checks will complain.** Will plan to fix this issue in a future version of scone volume check . Options Description --manager HOST manager of swarm (required) Example: $ scone volume check --verbose --manager alice scone volume delete scone volume delete deletes a volume. This removes the volume from all nodes of a swarm. Options Description --manager HOST manager of swarm (required) --name volume name of volume to be deleted (required) Example: $ scone volume delete --verbose --manager alice --volume new-volume Example To install infinit on all nodes of a swarm managed by node faye and reserve a capacity of 15GB per node, execute the following: $ scone volume install --verbose --manager faye --capacity 15 To create a volume named test_scone_volume for some arbitrary infinit user test_scone_user , execute the following: $ scone volume create --verbose --manager faye --as test_scone_user --name test_scone_volume Note that the infinit user test_scone_user is automatically created if it does not yet exists. After completion of this command, you will have access to the newly created volume at location /mnt/infinit/test_scone_user/test_scone_volume on all nodes of the swarm. You can give a service access to this volume with the help of its stack file. Say, the service needs to access this mapped at /mnt/vol as follows: myservice: volumes: - /mnt/infinit/test_scone_user/test_scone_volume:/mnt/vol Note that scone volume create will check that infinit is properly installed and will reinstall parts that are missing. If you are sure that infinit is properly installed, you can add option --fast to omit the checks that infinit is properly installed. To create a volume test_scone2_volume on a swarm managed by faye when you know that infinit is installed, just executed $ scone volume create --verbose --fast --manager faye --as test_scone_user --name test_scone2_volume To check that the volumes on the swarm are properly installed, execute command check : $ scone volume check --verbose --manager faye To delete a volume, we provide command delete . For example, to delete volume test_scone2_volume , execute: $ scone volume delete --verbose --manager faye --as test_scone_user --name test_scone2_volume \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"scone volume"},{"location":"SCONE_VOLUME/#scone-volume","text":"scone volume provides functionality to create volumes that are available on all nodes of a swarm. This is handy in case you want to be able to schedule a service on any node of a swarm and still be able to give it access to volumes. The underlying technology of scone volume is infinit - a software created by a Docker Inc subsidiary. Please read the infinit documentation to understand the infinit concepts of user , network , and silo . NOTE: infinit is labeled as alpha software: use this only for development or testing . The performance of infinit needs to be improved. Hence, for production, one would most likely install a more mature distributed file system on the swarm nodes. The remote swarm is specified via command line option --manager MANAGER . Alternatively, one can export an environment variable SCONE_MANAGER - which defines the default swarm to be used by scone swarm , in case option --manager is not given.","title":"scone volume"},{"location":"SCONE_VOLUME/#scone-volume-commands","text":"scone volume Description install install infinit on all nodes of a swarm. create create new volume and install infinit storage platform if required check check that all volumes are available on all hosts delete delete a volume","title":"scone volume commands"},{"location":"SCONE_VOLUME/#scone-volume-install-options","text":"scone volume install ensures that infinit is installed on all nodes of a swarm. Options Description --manager MANAGER manager of swarm (required) --as user run as user (default random id) --network name name of network to use/create (default scone-network) --silo name name of silo to use/create (default scone-silo) --capacity sz size of silo in GB (default 10) --help show this help message Notes: If you do not specify a default user via option --user USER , a random default user is created. If you do not specify a network name via option --network name , the default network name is set to scone-network . You can specify a silo name via potion --silo name . The default silo name is \"scone-silo\" . The capacity of the silo (in GB) is given via --capacity name . The default capacity is 10GB. **Examples: ** To install infinit with a storage capacity of 15 GB on each node, execute the following: $ scone volume install --verbose --manager alice --capacity 15 If you want to keep explicit control over user and network names, execute the following: $ scone volume install --verbose --manager alice --as scone --capacity 5 --silo my-silo --network scone-networkg If you want to see the user and network names for a given swarm, execute: $ scone volume install --manager alice --help","title":"scone volume install [OPTIONS]"},{"location":"SCONE_VOLUME/#scone-volume-create","text":"scone volume create creates a new infinit volume for a given user (option --as USER ) and with a given volume name (via option --name VOLUME ). If required, it (re-)installs the infinit storage platform. If you are sure that the infinit is already properly installed, pass the --fast option to avoid checks and reinstallation of infinit. This volume will be available on all hosts of a swarm at location: /mnt/infinit/USER/VOLUME Options Description --manager HOST manager of swarm (required) --name volume name of volume to create (required) --help show this help message --as user run as user (default ) --network name name of network to use/create (default ) --silo name name of silo to use/create (default ) --capacity sz size of silo in GB (default ) --fast create volume without reinstalling all software (default=false) --verbose print location of created volume **Example: ** In case you already installed infinit on your swarm and you just want to create a new volume, execute the following: $ scone volume create --verbose --fast --name new-volume --verbose In case you do not know if infinit is already installed when you create a volume and your want to keep control over the names used by infinit, execute the following: $ scone volume create --verbose --manager alice --as scone --name my-volume --capacity 5 --silo my-silo --network scone-network --export config","title":"scone volume create"},{"location":"SCONE_VOLUME/#scone-volume-check","text":"Checks if all created volumes are available on all swarm nodes. Note: the current implementation of **scone volume check uses the metadata stored in the container/environment in which scone volume check executes. In case you created some volumes in a different container, the checks will complain.** Will plan to fix this issue in a future version of scone volume check . Options Description --manager HOST manager of swarm (required) Example: $ scone volume check --verbose --manager alice","title":"scone volume check"},{"location":"SCONE_VOLUME/#scone-volume-delete","text":"scone volume delete deletes a volume. This removes the volume from all nodes of a swarm. Options Description --manager HOST manager of swarm (required) --name volume name of volume to be deleted (required) Example: $ scone volume delete --verbose --manager alice --volume new-volume","title":"scone volume delete"},{"location":"SCONE_VOLUME/#example","text":"To install infinit on all nodes of a swarm managed by node faye and reserve a capacity of 15GB per node, execute the following: $ scone volume install --verbose --manager faye --capacity 15 To create a volume named test_scone_volume for some arbitrary infinit user test_scone_user , execute the following: $ scone volume create --verbose --manager faye --as test_scone_user --name test_scone_volume Note that the infinit user test_scone_user is automatically created if it does not yet exists. After completion of this command, you will have access to the newly created volume at location /mnt/infinit/test_scone_user/test_scone_volume on all nodes of the swarm. You can give a service access to this volume with the help of its stack file. Say, the service needs to access this mapped at /mnt/vol as follows: myservice: volumes: - /mnt/infinit/test_scone_user/test_scone_volume:/mnt/vol Note that scone volume create will check that infinit is properly installed and will reinstall parts that are missing. If you are sure that infinit is properly installed, you can add option --fast to omit the checks that infinit is properly installed. To create a volume test_scone2_volume on a swarm managed by faye when you know that infinit is installed, just executed $ scone volume create --verbose --fast --manager faye --as test_scone_user --name test_scone2_volume To check that the volumes on the swarm are properly installed, execute command check : $ scone volume check --verbose --manager faye To delete a volume, we provide command delete . For example, to delete volume test_scone2_volume , execute: $ scone volume delete --verbose --manager faye --as test_scone_user --name test_scone2_volume \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Example"},{"location":"SCONE_toolchain/","text":"SCONE SGX Toolchain SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran. The objective of these (cross-)compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves. To simplify the use of these (cross-)compilers, SCONE maintains curated container image that includes these cross-compilers. Compiler variants Depending on if you want to generate a dynamically-linked or a statically-linked binary, you can use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. Portability of SCONE programs Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code base, i.e., it includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we recommend both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs one does not change the build process when moving to SCONE. Loading Shared Libraries after startup SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java and Python. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2). For production enclaves, you will need to protect the integrity of the shared libraries with the help of the SCONE file shield . Dynamically-Linked Binaries The easiest to get started is to compile your programs such that the generated code is position independent ( -fPIC ), the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ), your binary is dynamically linked (i.e., do not use -static ), and link against musl as your libc (i.e., not glibc or any other libc). When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the file system shield . To simplify the compiling of your programs for scone, we make available a docker image sconecuratedimages/muslgcc which includes gcc and g++ support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options. Statically-Linked Binaries For statically linked binaries, we make available a (private) docker image sconecuratedimages/crosscompilers which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves. Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program web-srv-go , you will still see dynamic dependencies: $ ldd web-srv-go linux-vdso.so.1 = > ( 0x00007ffe423fd000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007effa344f000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007effa3085000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007effa366c000 ) The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked loader program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked. Using the cross compiler container How to use the compiler: use this as a base image and build your programs inside of a container we a Dockerfile ), or map volumes such that the compiler can compile files living outside the container (see SCONE Tutorial ). For an example how to use the crosscompilers, see how to compile a programs written in GO . Example Note on some systems you will need to run docker with root permissions, i.e., in this case you should prefix a > docker ... command with sudo , i.e., you execute > sudo docker ... One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is in file myapp.c in your current directory ( $PWD ). You can compile this code as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c This call will generate a binary a.out in your working directory. This binary is dynamically linked against musl: > ldd a.out /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) libc.musl-x86_64.so.1 = > /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our hello world in Section SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of Dockerfiles . In most cases, you might just set to use one of our crosscompilers in your configure script or Makefile . A simple way is to use the Docker image sconecuratedimages/crosscompilers as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of cargo which is scone-cargo . Debugger support We also support gdb to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image sconecuratedimages/crosscompilers as scone-gdb . For example on how to use the debugger, see how to debug a program written in GO . Also, we support an IDE to debug a program running in an enclave in debug mode. We describe how to integrate with Eclipse . \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"SCONE SGX toolchain"},{"location":"SCONE_toolchain/#scone-sgx-toolchain","text":"SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran. The objective of these (cross-)compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves. To simplify the use of these (cross-)compilers, SCONE maintains curated container image that includes these cross-compilers.","title":"SCONE SGX Toolchain"},{"location":"SCONE_toolchain/#compiler-variants","text":"Depending on if you want to generate a dynamically-linked or a statically-linked binary, you can use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. Portability of SCONE programs Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code base, i.e., it includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we recommend both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs one does not change the build process when moving to SCONE. Loading Shared Libraries after startup SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java and Python. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2). For production enclaves, you will need to protect the integrity of the shared libraries with the help of the SCONE file shield .","title":"Compiler variants"},{"location":"SCONE_toolchain/#dynamically-linked-binaries","text":"The easiest to get started is to compile your programs such that the generated code is position independent ( -fPIC ), the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ), your binary is dynamically linked (i.e., do not use -static ), and link against musl as your libc (i.e., not glibc or any other libc). When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the file system shield . To simplify the compiling of your programs for scone, we make available a docker image sconecuratedimages/muslgcc which includes gcc and g++ support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options.","title":"Dynamically-Linked Binaries"},{"location":"SCONE_toolchain/#statically-linked-binaries","text":"For statically linked binaries, we make available a (private) docker image sconecuratedimages/crosscompilers which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves. Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program web-srv-go , you will still see dynamic dependencies: $ ldd web-srv-go linux-vdso.so.1 = > ( 0x00007ffe423fd000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007effa344f000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007effa3085000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007effa366c000 ) The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked loader program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked.","title":"Statically-Linked Binaries"},{"location":"SCONE_toolchain/#using-the-cross-compiler-container","text":"How to use the compiler: use this as a base image and build your programs inside of a container we a Dockerfile ), or map volumes such that the compiler can compile files living outside the container (see SCONE Tutorial ). For an example how to use the crosscompilers, see how to compile a programs written in GO .","title":"Using the cross compiler container"},{"location":"SCONE_toolchain/#example","text":"Note on some systems you will need to run docker with root permissions, i.e., in this case you should prefix a > docker ... command with sudo , i.e., you execute > sudo docker ... One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is in file myapp.c in your current directory ( $PWD ). You can compile this code as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc gcc myapp.c This call will generate a binary a.out in your working directory. This binary is dynamically linked against musl: > ldd a.out /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) libc.musl-x86_64.so.1 = > /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp sconecuratedimages/muslgcc ./a.out To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our hello world in Section SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of Dockerfiles . In most cases, you might just set to use one of our crosscompilers in your configure script or Makefile . A simple way is to use the Docker image sconecuratedimages/crosscompilers as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of cargo which is scone-cargo .","title":"Example"},{"location":"SCONE_toolchain/#debugger-support","text":"We also support gdb to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image sconecuratedimages/crosscompilers as scone-gdb . For example on how to use the debugger, see how to debug a program written in GO . Also, we support an IDE to debug a program running in an enclave in debug mode. We describe how to integrate with Eclipse . \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Debugger support"},{"location":"aboutScone/","text":"About SCONE The SCONE platform is commercially supported by scontain.com . The SCONE platform has been developed at the Systems Engineering group at TU Dresden in the context of the following EU H2020 projects : Sereca which investigates how to use Intel SGX enclave in the context of reactive programs written in Vert.x . Secure Cloud which focuses on the processing of big data in untrusted clouds. The first paper about SCONE has been published in OSDI 2016 with our colleagues from Imperial College London, Technische Universit\u00e4t Braunschweig and University of Otago. More papers about SCONE can be found here . We investigate use cases and extensions of SCONE in the context of the following EU H2020 projects : SELIS : we investigate how to secure data processing within a Shared European Logistics Intelligent Information Space with the help of SCONE. ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform. LEGATO : a new EU project in which we address high integrity computations inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves. Computing Resources scontain.com can provide access to SGX-capable machines. Consulting Services scontain.com provides consulting services as well as helping you to port your applications to SGX. Contact If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at info@scontain.com .","title":"About"},{"location":"aboutScone/#about-scone","text":"The SCONE platform is commercially supported by scontain.com . The SCONE platform has been developed at the Systems Engineering group at TU Dresden in the context of the following EU H2020 projects : Sereca which investigates how to use Intel SGX enclave in the context of reactive programs written in Vert.x . Secure Cloud which focuses on the processing of big data in untrusted clouds. The first paper about SCONE has been published in OSDI 2016 with our colleagues from Imperial College London, Technische Universit\u00e4t Braunschweig and University of Otago. More papers about SCONE can be found here . We investigate use cases and extensions of SCONE in the context of the following EU H2020 projects : SELIS : we investigate how to secure data processing within a Shared European Logistics Intelligent Information Space with the help of SCONE. ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform. LEGATO : a new EU project in which we address high integrity computations inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves.","title":"About SCONE"},{"location":"aboutScone/#computing-resources","text":"scontain.com can provide access to SGX-capable machines.","title":"Computing Resources"},{"location":"aboutScone/#consulting-services","text":"scontain.com provides consulting services as well as helping you to port your applications to SGX.","title":"Consulting Services"},{"location":"aboutScone/#contact","text":"If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at info@scontain.com .","title":"Contact"},{"location":"advantages/","text":"Advantages of SCONE SCONE simplifies building and operating confidential applications . You can rebuild existing applications to become confidential applications running on top of vanilla Kubernetes clusters (as long as the containers are permitted to use SGX). SCONE supports confidential multi-party computations . Modern software services are quite complex - involving multiple stakeholders to get services running. Not all of these stakeholder trust each other and we need to expect that some attackers have root access on the hosts on which the services execute. SCONE helps multiple stakeholders by supporting the composition of security policies of multiple stakeholders (see overview) . SCONE transparently attests applications . This ensures that application run indeed inside of an enclave. Only after a successful attestation, the application gets its keys to unlock the file system, its arguments and its environment variables - which all might contain secrets that need to be protected. SCONE has an integrated secrets and configuration management - simplifying the distribution of secrets without application changes by performing a transparent attestation of applications. The integrated key management is required to ensure that a client can ensure that its data is protected from accesses by other clients and attackers ( see an example in the contexted of a trusted DApps ). SCONE scales better than competing solutions since it uses an advanced thread management and a very efficient way how to perform asynchronous system calls : when an enclave performs a system call, SCONE switches to another application thread while the system call is performed by threads running outside the enclave. This minimizes the need for the threads running inside the enclave to exit the enclave. Minimizing the enclave exits is particularly important looking at recent CPU microcode updates in the context of L1TF : The CPU needs to flush the L1 cache - which is an expensive operation. Single threaded applications can be tuned for low-latency system call processing. SCONE has smaller executables . SCONE is based on a modified C library instead of running a complete library OS inside of an enclave. This does not only reduces the size of the enclaves and hence, the number of software bugs inside the enclave. To see how large typical code sizes are and the defender's dilemma, have a look at our background section . Large code size does not only mean more bugs (expect about 0.61 bugs per 1000 lines) but also negatively impacts performance: SGX CPUs have limited EPC (extended page cache) and larger memory footprints result in general in better performance. SCONE comes with a toolchain . While SCONE supports binaries compiled for Alpine Linux, we recommend to recompile binaries to minimize code size and to ensure better performance and security of the applications. Also, the crosscompiler ensures that the correct model for thread local variables is used (- i.e., no use of os-controlled segmentation registers). SCONE comes with curated images . Since compiling and configuring applications is an effort, we provide common applications like Vault , nginx , MariaDB , Apache , etc. SCONE support binary compatibility . We support binary compatibility for Alpine Linux , i.e., we can run native Alpine applications without modifications / recompilation inside of SGX enclaves. SCONE protects the OS interface . SCONE provides shields to protect the interaction with the operating system interface. For example, it provides the transparent encryption of files ( example ). While the OS interface has more calls than the VMM interface used by a library OS (like Haven), we decided in SCONE to protect the OS interface instead since it provides us with more specific semantics which in turn simplifies the shielding. SCONE ensures better Linux compatibility . By providing a native OS interface, SCONE reduces compatibility issues of the application. A library OS will most likely not be 100% compatible with the latest Linux kernel. SCONE is hardware independent . The design of SCONE is such that we can support other TEEs (trusted execution environments) when they become available. In this way, one does not have to port applications to different TEEs. SCONE supports various package managers . While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux. In particular, you can deploy SCONE-based application with your favorite package manager.","title":"Advantages of SCONE"},{"location":"advantages/#advantages-of-scone","text":"SCONE simplifies building and operating confidential applications . You can rebuild existing applications to become confidential applications running on top of vanilla Kubernetes clusters (as long as the containers are permitted to use SGX). SCONE supports confidential multi-party computations . Modern software services are quite complex - involving multiple stakeholders to get services running. Not all of these stakeholder trust each other and we need to expect that some attackers have root access on the hosts on which the services execute. SCONE helps multiple stakeholders by supporting the composition of security policies of multiple stakeholders (see overview) . SCONE transparently attests applications . This ensures that application run indeed inside of an enclave. Only after a successful attestation, the application gets its keys to unlock the file system, its arguments and its environment variables - which all might contain secrets that need to be protected. SCONE has an integrated secrets and configuration management - simplifying the distribution of secrets without application changes by performing a transparent attestation of applications. The integrated key management is required to ensure that a client can ensure that its data is protected from accesses by other clients and attackers ( see an example in the contexted of a trusted DApps ). SCONE scales better than competing solutions since it uses an advanced thread management and a very efficient way how to perform asynchronous system calls : when an enclave performs a system call, SCONE switches to another application thread while the system call is performed by threads running outside the enclave. This minimizes the need for the threads running inside the enclave to exit the enclave. Minimizing the enclave exits is particularly important looking at recent CPU microcode updates in the context of L1TF : The CPU needs to flush the L1 cache - which is an expensive operation. Single threaded applications can be tuned for low-latency system call processing. SCONE has smaller executables . SCONE is based on a modified C library instead of running a complete library OS inside of an enclave. This does not only reduces the size of the enclaves and hence, the number of software bugs inside the enclave. To see how large typical code sizes are and the defender's dilemma, have a look at our background section . Large code size does not only mean more bugs (expect about 0.61 bugs per 1000 lines) but also negatively impacts performance: SGX CPUs have limited EPC (extended page cache) and larger memory footprints result in general in better performance. SCONE comes with a toolchain . While SCONE supports binaries compiled for Alpine Linux, we recommend to recompile binaries to minimize code size and to ensure better performance and security of the applications. Also, the crosscompiler ensures that the correct model for thread local variables is used (- i.e., no use of os-controlled segmentation registers). SCONE comes with curated images . Since compiling and configuring applications is an effort, we provide common applications like Vault , nginx , MariaDB , Apache , etc. SCONE support binary compatibility . We support binary compatibility for Alpine Linux , i.e., we can run native Alpine applications without modifications / recompilation inside of SGX enclaves. SCONE protects the OS interface . SCONE provides shields to protect the interaction with the operating system interface. For example, it provides the transparent encryption of files ( example ). While the OS interface has more calls than the VMM interface used by a library OS (like Haven), we decided in SCONE to protect the OS interface instead since it provides us with more specific semantics which in turn simplifies the shielding. SCONE ensures better Linux compatibility . By providing a native OS interface, SCONE reduces compatibility issues of the application. A library OS will most likely not be 100% compatible with the latest Linux kernel. SCONE is hardware independent . The design of SCONE is such that we can support other TEEs (trusted execution environments) when they become available. In this way, one does not have to port applications to different TEEs. SCONE supports various package managers . While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux. In particular, you can deploy SCONE-based application with your favorite package manager.","title":"Advantages of SCONE"},{"location":"appsecurity/","text":"Application-Oriented Security SCONE supports developers and service providers , to protect the confidentiality and integrity of their applications - even when running in environments that cannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like microservice-based applications as well as cloud-native applications . However, SCONE can protect most programs running on top of Linux. SCONE supports developers and service providers to ensure end-to-end encryption in the sense that data is always encrypted , i.e., while being transmitted, while being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect. For example, a service that operates only on encrypted data might not need to be protected with SGX. Keep it simple Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.* Ease of Use SCONE supports strong application-oriented security with a workflow like Docker, in particular, SCONE supports Dockerfiles . This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container. The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Kubernetes can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Kubernetes, failed containers will automatically be restarted on an appropriate host.","title":"Application-Oriented Security"},{"location":"appsecurity/#application-oriented-security","text":"SCONE supports developers and service providers , to protect the confidentiality and integrity of their applications - even when running in environments that cannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like microservice-based applications as well as cloud-native applications . However, SCONE can protect most programs running on top of Linux. SCONE supports developers and service providers to ensure end-to-end encryption in the sense that data is always encrypted , i.e., while being transmitted, while being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect. For example, a service that operates only on encrypted data might not need to be protected with SGX. Keep it simple Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.*","title":"Application-Oriented Security"},{"location":"appsecurity/#ease-of-use","text":"SCONE supports strong application-oriented security with a workflow like Docker, in particular, SCONE supports Dockerfiles . This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container. The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Kubernetes can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Kubernetes, failed containers will automatically be restarted on an appropriate host.","title":"Ease of Use"},{"location":"background/","text":"SCONE Background Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data. Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack isolated hosts or on OpenStack baremetal clouds . Problem: Defender's Dilemma Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems. A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities. For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected. To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect. SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release: Linux Lines of Code (StefanPohl, CC0, original } OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines): OpenStack Lines of Code (OpenHub original ) To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code: Docker Lines of Code (OpenHub original ) Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too. SCONE Approach The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team. A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves. Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices. Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.","title":"SCONE Background"},{"location":"background/#scone-background","text":"Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data. Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack isolated hosts or on OpenStack baremetal clouds .","title":"SCONE Background"},{"location":"background/#problem-defenders-dilemma","text":"Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems. A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities. For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected. To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect. SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release: Linux Lines of Code (StefanPohl, CC0, original } OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines): OpenStack Lines of Code (OpenHub original ) To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code: Docker Lines of Code (OpenHub original ) Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too.","title":"Problem: Defender's Dilemma"},{"location":"background/#scone-approach","text":"The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team. A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves. Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to enclaved microservices. Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.","title":"SCONE Approach"},{"location":"blender/","text":"Blender Use Case Blender is an open-source software 3D creation suite that in particular, supports rendering. We show how to render images with the help of blender on a remote, untrusted host. In this use case, we show how to execute blender using simple ssh to run blender remotely: we encrypt the input file(s), ship the input files to the remote host, execute blender inside an enclave, and ship the results back to the local host. This Blender image also runs as a trusted DApp for the iExec platform . Assuming the Python scripts are located in the INPUTS directory, we can compute the output image on host faye as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender_python --host faye The output of this script is stored in file simple_sphere.png directory OUTPUTS . Opening this file - which was computed inside an enclave with all inputs, outputs, the scripts, and the processing being always encrypted , results in the following output. For some more examples, scroll down. Technical Background Blender is a relative large binary that is almost 60MB large. It has multiple extensions / addons implemented in Python. These extensions are dynamically loaded when they are needed. The source files of these extensions are located at different places in the filesystem: As we show below, a user can add more Python extensions that help to render images and animations. These python extensions might contain intellectual properties of the user. The confidentiality as well as the integrity of these extensions mus be protected. The blender image contains all the above files but not the user extensions. We need to ensure the integrity of the blender application as well as the extensions , i.e., an attacker cannot modify the blender image without being detected. We recommend that these container images are signed using Docker content trust . Note that content trust is, however, not sufficient to ensure that blender will actually read the original, unmodified files. For example, an attacker with root access could change this image on the host on which blender is executed. We split the file system of the blender image in the following file regions: the root file system itself is not protected. In this unprotected part is the blender binary as well as a script to start the blender image. the /usr file region is authenticated , i.e., the integrity of all files in this region is verified by the SCONE runtime before being passed to blender . This file region is generated by the image creator - in this case, scontain.com. the /encryptedInputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. The files in this region are provided by the user: with the help of a CLI (see below), the user encrypts the files and pushes these to the host on which blender is executed. the /encryptedOutputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. This region is initially empty but is still generated by the user with the help of a CLI (see below), the client encrypts the files and pushes these to the host on which blender is executed. Secrets Management We can summarize the responsibilities of the image creator and the user as follows. The image creator ensures the integrity of blender and all extensions. The user ensures that both the confidentiality as well as the integrity of the user extensions: We should be explicit about the different roles in the context of the blender use case: the image creator encrypts the file system, i.e., defines the /usr authenticated file region and permits the user to map protected regions /encryptedInputs and /encryptedOutputs into the blender image. The image creator controls the key and tag of the root file system but must not be able to access the user-controlled protected regions. The image creator defines the expected tag of the file system as well as the expected MrEnclave . the user controls the protected regions /encryptedInputs and /encryptedOutputs . However, the user must not be able to modify the authenticated files in the root file systems. Otherwise, an attacker might be able to use this right to modify the images of other users. Moreover, the user must not be able to access the protected file regions of other users. an attacker must not be able to read the protected regions of any user nor must an attacker be able to modify the authenticated files in the root filesystem. Based on these different roles, we need to manage the keys when executing the blender image. The key and tag of the blender image must be controlled by the image creator. In particular, neither a user nor an attacker must be able to get access to this key. Otherwise, this entity might be able to modify the image - which we would actually detect with the help of the tag. However, in case of protected images to protect the intellectual property of the image creator, this entity could read the content of the filesystem. The key and tag of the protected volumes of a user must only be visible by the user itself. Note, however, that all keys and tags must be accessible by the SCONE runtime of the blender image itself. In other words, the blender image must be able to get acceas to the keys while we need to prevent the user as well as the image creator to see all keys. The SCONE CAS (Configuration and Attestation System) can manage the keys in such a way that an image can gain access to the keys of multiple entities. The main mechanism to support this is the opaque export and import of keys. The creator of the blender image can export the key of root filesystem but can restrict the export to blender that is identified by a MrEnclave as well as tag of the file system. Sessions A session is a security policy that defines secrets and who can access these secrets. The session of the image will define the key and tag and also defines if and what other sessions can access this information. In this case, we export to blender services. CAS ensures that only blender images that have the expected image tag (i.e., fspf_tag ) and enclaves with the expected MrEnclave (i.e., mrenclaves ) can access the key (i.e., fspf_key ). The session will also determine the arguments of blender. In this case, the base image defines that an python program /encryptedInputs/application.py is executed. The python program itself is defined in a volume that is provided by the user, i.e., it is not under the control of the image creator. name: scone:blender digest: create services: - name: blender image_name: sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] pwd: / command: blender -b -P /encryptedInputs/application.py fspf_path: /usr/fspf.pb fspf_key: $FSPF_KEY fspf_tag: $FSPF_TAG environment: SCONE_NO_MMAP_ACCESS: 1 SCONE_ALLOW_DLOPEN: 1 images: - name: sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs exports: - namespace: service name: blender export: \"*\" - namespace: image name: sconecuratedimages/iexecsgx:blender_python export: \"*\" In this case, the volumes themselves are defined by the user. To simplify this process of defining the session, creating encrypted volumes and pushing these to the site at which the image is executed, we provide a simple CLI (Command Line Interface). When a user executes blender, this blender container will be started in the context of a CAS session. The session defines the actually keys and tags of the volumes that are mapped into the blender container. The actually session is derived by the CLI from a session template that looks like this: name: $SESSION digest: create services: - name: blender import: scone:blender image_name: sconecuratedimages/iexecsgx:blender_python command: blender -b -P /encryptedInputs/application.py pwd: / volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: sconecuratedimages/iexecsgx:blender_python import: scone:blender CLI Most users, they probably do not want to learn about sessions and the details how to protect their intellectual property. They expect instead that their IP is properly protected, and it is easy to use. We provide a simple CLI to execute applications remotely. For now, the CLI expects that the application images have a certain input directory (i.e., /encryptedInputs ) and a certain output directory (i.e., /encryptedOutputs ). The CLI expects that the unencrypted input directory is mapped into the CLI container at /inputs , the unencrypted output directory is mapped into the CLI container at /decryptedOutputs 1 : , and a directory that contains some configuration data (which are automatically generated) at /conf . To remotely start a container with the help of ssh , we need to give the CLI container access via command add-host . After giving ssh access, we can start the blend applications. To do so, we can give it docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender_python --host faye Blender Scripting Let us show an example with an encrypted Python program that is executed on the remote site along with blender to render an image. We take an example from github by Nikolai Janakiev : This Python program is unmodified an will be shipped and run in an encrypted fashion on the remote host that executes the blender image. While this is probably not necessary for open source code to protect its confidentiality , it is important to protect its integrity : if an attacker could change the code, it could get access to all data of the image and could modify the results. In general, we want to protect both the integrity as well as the confidentiality of the Python code. import bpy from math import pi from mathutils import Euler tau = 2 * pi # Check if script is opened in Blender program import os , sys if ( bpy . context . space_data == None ): cwd = os . path . dirname ( os . path . abspath ( __file__ )) else : cwd = os . path . dirname ( bpy . context . space_data . text . filepath ) # Get folder of script and add current working directory to path sys . path . append ( cwd ) import utils def createSphere ( origin = ( 0 , 0 , 0 )): # Create icosphere bpy . ops . mesh . primitive_ico_sphere_add ( location = origin ) obj = bpy . context . object return obj if __name__ == '__main__' : # Remove all elements utils . removeAll () # Create camera bpy . ops . object . add ( type = 'CAMERA' , location = ( 0 , - 3.5 , 0 )) cam = bpy . context . object cam . rotation_euler = Euler (( pi / 2 , 0 , 0 ), 'XYZ' ) # Make this the current camera bpy . context . scene . camera = cam # Create lamps utils . rainbowLights () # Create object and its material sphere = createSphere () utils . setSmooth ( sphere , 3 ) # Specify folder to save rendering render_folder = os . path . join ( cwd , '../encryptedOutputs' ) if ( not os . path . exists ( render_folder )): os . mkdir ( render_folder ) # Render image rnd = bpy . data . scenes [ 'Scene' ] . render rnd . resolution_x = 500 rnd . resolution_y = 500 rnd . resolution_percentage = 100 rnd . filepath = os . path . join ( render_folder , 'simple_sphere.png' ) bpy . ops . render . render ( write_still = True ) We have to copy this scripts - together with some the utils module - in folder INPUTS . This folder will be encrypted and pushed to remote host which is given by argument --host ALIAS . We can execute this Python script on host faye as shown above. More Examples We rendered a few more examples by Nikolai Janakiev in an always encrypted fashion: This will eventually be changed to /outputs . \u21a9","title":"Blender"},{"location":"blender/#blender-use-case","text":"Blender is an open-source software 3D creation suite that in particular, supports rendering. We show how to render images with the help of blender on a remote, untrusted host. In this use case, we show how to execute blender using simple ssh to run blender remotely: we encrypt the input file(s), ship the input files to the remote host, execute blender inside an enclave, and ship the results back to the local host. This Blender image also runs as a trusted DApp for the iExec platform . Assuming the Python scripts are located in the INPUTS directory, we can compute the output image on host faye as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender_python --host faye The output of this script is stored in file simple_sphere.png directory OUTPUTS . Opening this file - which was computed inside an enclave with all inputs, outputs, the scripts, and the processing being always encrypted , results in the following output. For some more examples, scroll down.","title":"Blender Use Case"},{"location":"blender/#technical-background","text":"Blender is a relative large binary that is almost 60MB large. It has multiple extensions / addons implemented in Python. These extensions are dynamically loaded when they are needed. The source files of these extensions are located at different places in the filesystem: As we show below, a user can add more Python extensions that help to render images and animations. These python extensions might contain intellectual properties of the user. The confidentiality as well as the integrity of these extensions mus be protected. The blender image contains all the above files but not the user extensions. We need to ensure the integrity of the blender application as well as the extensions , i.e., an attacker cannot modify the blender image without being detected. We recommend that these container images are signed using Docker content trust . Note that content trust is, however, not sufficient to ensure that blender will actually read the original, unmodified files. For example, an attacker with root access could change this image on the host on which blender is executed. We split the file system of the blender image in the following file regions: the root file system itself is not protected. In this unprotected part is the blender binary as well as a script to start the blender image. the /usr file region is authenticated , i.e., the integrity of all files in this region is verified by the SCONE runtime before being passed to blender . This file region is generated by the image creator - in this case, scontain.com. the /encryptedInputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. The files in this region are provided by the user: with the help of a CLI (see below), the user encrypts the files and pushes these to the host on which blender is executed. the /encryptedOutputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. This region is initially empty but is still generated by the user with the help of a CLI (see below), the client encrypts the files and pushes these to the host on which blender is executed.","title":"Technical Background"},{"location":"blender/#secrets-management","text":"We can summarize the responsibilities of the image creator and the user as follows. The image creator ensures the integrity of blender and all extensions. The user ensures that both the confidentiality as well as the integrity of the user extensions: We should be explicit about the different roles in the context of the blender use case: the image creator encrypts the file system, i.e., defines the /usr authenticated file region and permits the user to map protected regions /encryptedInputs and /encryptedOutputs into the blender image. The image creator controls the key and tag of the root file system but must not be able to access the user-controlled protected regions. The image creator defines the expected tag of the file system as well as the expected MrEnclave . the user controls the protected regions /encryptedInputs and /encryptedOutputs . However, the user must not be able to modify the authenticated files in the root file systems. Otherwise, an attacker might be able to use this right to modify the images of other users. Moreover, the user must not be able to access the protected file regions of other users. an attacker must not be able to read the protected regions of any user nor must an attacker be able to modify the authenticated files in the root filesystem. Based on these different roles, we need to manage the keys when executing the blender image. The key and tag of the blender image must be controlled by the image creator. In particular, neither a user nor an attacker must be able to get access to this key. Otherwise, this entity might be able to modify the image - which we would actually detect with the help of the tag. However, in case of protected images to protect the intellectual property of the image creator, this entity could read the content of the filesystem. The key and tag of the protected volumes of a user must only be visible by the user itself. Note, however, that all keys and tags must be accessible by the SCONE runtime of the blender image itself. In other words, the blender image must be able to get acceas to the keys while we need to prevent the user as well as the image creator to see all keys. The SCONE CAS (Configuration and Attestation System) can manage the keys in such a way that an image can gain access to the keys of multiple entities. The main mechanism to support this is the opaque export and import of keys. The creator of the blender image can export the key of root filesystem but can restrict the export to blender that is identified by a MrEnclave as well as tag of the file system.","title":"Secrets Management"},{"location":"blender/#sessions","text":"A session is a security policy that defines secrets and who can access these secrets. The session of the image will define the key and tag and also defines if and what other sessions can access this information. In this case, we export to blender services. CAS ensures that only blender images that have the expected image tag (i.e., fspf_tag ) and enclaves with the expected MrEnclave (i.e., mrenclaves ) can access the key (i.e., fspf_key ). The session will also determine the arguments of blender. In this case, the base image defines that an python program /encryptedInputs/application.py is executed. The python program itself is defined in a volume that is provided by the user, i.e., it is not under the control of the image creator. name: scone:blender digest: create services: - name: blender image_name: sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] pwd: / command: blender -b -P /encryptedInputs/application.py fspf_path: /usr/fspf.pb fspf_key: $FSPF_KEY fspf_tag: $FSPF_TAG environment: SCONE_NO_MMAP_ACCESS: 1 SCONE_ALLOW_DLOPEN: 1 images: - name: sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs exports: - namespace: service name: blender export: \"*\" - namespace: image name: sconecuratedimages/iexecsgx:blender_python export: \"*\" In this case, the volumes themselves are defined by the user. To simplify this process of defining the session, creating encrypted volumes and pushing these to the site at which the image is executed, we provide a simple CLI (Command Line Interface). When a user executes blender, this blender container will be started in the context of a CAS session. The session defines the actually keys and tags of the volumes that are mapped into the blender container. The actually session is derived by the CLI from a session template that looks like this: name: $SESSION digest: create services: - name: blender import: scone:blender image_name: sconecuratedimages/iexecsgx:blender_python command: blender -b -P /encryptedInputs/application.py pwd: / volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: sconecuratedimages/iexecsgx:blender_python import: scone:blender","title":"Sessions"},{"location":"blender/#cli","text":"Most users, they probably do not want to learn about sessions and the details how to protect their intellectual property. They expect instead that their IP is properly protected, and it is easy to use. We provide a simple CLI to execute applications remotely. For now, the CLI expects that the application images have a certain input directory (i.e., /encryptedInputs ) and a certain output directory (i.e., /encryptedOutputs ). The CLI expects that the unencrypted input directory is mapped into the CLI container at /inputs , the unencrypted output directory is mapped into the CLI container at /decryptedOutputs 1 : , and a directory that contains some configuration data (which are automatically generated) at /conf . To remotely start a container with the help of ssh , we need to give the CLI container access via command add-host . After giving ssh access, we can start the blend applications. To do so, we can give it docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender_python --host faye","title":"CLI"},{"location":"blender/#blender-scripting","text":"Let us show an example with an encrypted Python program that is executed on the remote site along with blender to render an image. We take an example from github by Nikolai Janakiev : This Python program is unmodified an will be shipped and run in an encrypted fashion on the remote host that executes the blender image. While this is probably not necessary for open source code to protect its confidentiality , it is important to protect its integrity : if an attacker could change the code, it could get access to all data of the image and could modify the results. In general, we want to protect both the integrity as well as the confidentiality of the Python code. import bpy from math import pi from mathutils import Euler tau = 2 * pi # Check if script is opened in Blender program import os , sys if ( bpy . context . space_data == None ): cwd = os . path . dirname ( os . path . abspath ( __file__ )) else : cwd = os . path . dirname ( bpy . context . space_data . text . filepath ) # Get folder of script and add current working directory to path sys . path . append ( cwd ) import utils def createSphere ( origin = ( 0 , 0 , 0 )): # Create icosphere bpy . ops . mesh . primitive_ico_sphere_add ( location = origin ) obj = bpy . context . object return obj if __name__ == '__main__' : # Remove all elements utils . removeAll () # Create camera bpy . ops . object . add ( type = 'CAMERA' , location = ( 0 , - 3.5 , 0 )) cam = bpy . context . object cam . rotation_euler = Euler (( pi / 2 , 0 , 0 ), 'XYZ' ) # Make this the current camera bpy . context . scene . camera = cam # Create lamps utils . rainbowLights () # Create object and its material sphere = createSphere () utils . setSmooth ( sphere , 3 ) # Specify folder to save rendering render_folder = os . path . join ( cwd , '../encryptedOutputs' ) if ( not os . path . exists ( render_folder )): os . mkdir ( render_folder ) # Render image rnd = bpy . data . scenes [ 'Scene' ] . render rnd . resolution_x = 500 rnd . resolution_y = 500 rnd . resolution_percentage = 100 rnd . filepath = os . path . join ( render_folder , 'simple_sphere.png' ) bpy . ops . render . render ( write_still = True ) We have to copy this scripts - together with some the utils module - in folder INPUTS . This folder will be encrypted and pushed to remote host which is given by argument --host ALIAS . We can execute this Python script on host faye as shown above.","title":"Blender Scripting"},{"location":"blender/#more-examples","text":"We rendered a few more examples by Nikolai Janakiev in an always encrypted fashion: This will eventually be changed to /outputs . \u21a9","title":"More Examples"},{"location":"buildingApps/","text":"Building SCONE-based applications SCONE supports running applications written in common programming languages inside of Intel SGX enclaves without source code changes . These languages include compiled languages like C, Rust, C++, GO, and Fortran and interpreted / just-in-time languages like Python and Java . For compiled languages, our recommend approach to run an application with SCONE is as follows: Use of precompiled binary: For many common applications like nginx and memcached , we already support a curated image image on Docker hub. Ask us for help : if a standard application is not yet a curated image on docker hub, send us an email to see if we can help you with this Cross-compile : you can cross-compile your application with the help of the SCONE cross-compilers, for example, have a look how to compile C programs , and No Cross-Compilation: , you can run native Alpine-Linux applications inside of enclaves without recompilation . Use Cross-Compilation instead of native compilation While SCONE supports executing programs without recompilations for Alpine Linux, we recommend to always cross-compile : The interface to the operating system needs to be replaced, i.e., libc . Hence, one needs not only to provide the same version of libc but one must ensure that all bits are represented in the same way as in the native libc. This is difficult to achieve and better left to the compiler. For stability , we therefore recommend cross-compilation since the compiler checks that all the dependencies have the matching versions, all data types are bit compatible and includes the correct libraries statically in the binary. In this way, an application will have a unique and known MrEnclave .","title":"(No) Cross-Compilation"},{"location":"buildingApps/#building-scone-based-applications","text":"SCONE supports running applications written in common programming languages inside of Intel SGX enclaves without source code changes . These languages include compiled languages like C, Rust, C++, GO, and Fortran and interpreted / just-in-time languages like Python and Java . For compiled languages, our recommend approach to run an application with SCONE is as follows: Use of precompiled binary: For many common applications like nginx and memcached , we already support a curated image image on Docker hub. Ask us for help : if a standard application is not yet a curated image on docker hub, send us an email to see if we can help you with this Cross-compile : you can cross-compile your application with the help of the SCONE cross-compilers, for example, have a look how to compile C programs , and No Cross-Compilation: , you can run native Alpine-Linux applications inside of enclaves without recompilation . Use Cross-Compilation instead of native compilation While SCONE supports executing programs without recompilations for Alpine Linux, we recommend to always cross-compile : The interface to the operating system needs to be replaced, i.e., libc . Hence, one needs not only to provide the same version of libc but one must ensure that all bits are represented in the same way as in the native libc. This is difficult to achieve and better left to the compiler. For stability , we therefore recommend cross-compilation since the compiler checks that all the dependencies have the matching versions, all data types are bit compatible and includes the correct libraries statically in the binary. In this way, an application will have a unique and known MrEnclave .","title":"Building SCONE-based applications"},{"location":"cas_blender_example/","text":"Posting a Session We show how to interact with CAS s with the help of curl - this might be helpful during development since it simplifies quick tests. We provide a scone command line interface that can be executed inside of an enclave itself. It can perform an attestation of CAS as well as creating and verifying policies. Hence, we recommend to use the scone CLI . We assume that you already started a CAS instance and a LAS instance on your local host. Alternatively, you can use our public CAS instance at scone-cas.cf . Hence, we set the address of CAS as follows: export SCONE_CAS_ADDR = 127 .0.0.1 If you use our public CAS instance, set it as follows: export SCONE_CAS_ADDR = scone-cas.cf Client Certificate To interact with CAS, we need to create a client certificate. When we create a session, it is associated with the client certificate of the creator. Any access to this session requires that the client knows the private key of the client certificate. Let's create a client certificate without a password. Note that you would typically add a password! mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Hello World Session Let's create a minimal session: cat > session.yml <<EOF name: blender digest: create services: - name: application image_name: sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: / environment: SCONE_MODE: hw images: - name: sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] tags: [demo] EOF We can now upload the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar like this: Created Session[id=00ed7ade-bba6-4d43-9135-51d0ca2da9ba, name=blender, status=Pending] Session already exists If the session with name \"blender\" already exists - which will be the case when you use scone-cas.cf - the following error message is issued: Could not create successor session. Invalid previous session digest: ... In case the session with name blender already exists, you must chose a different session name. We can read the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key https:// $SCONE_CAS_ADDR :8081/session/blender This will result in an output like this: --- name: blender digest: 313c6c3b824f0a560c445c8ef0cf69781345aae753bdbeaedbfff15c5a348099 board_members: [] board_policy: minimum: 0 timeout: 30 images: - name: \"sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 tags: - demo services: - name: application image_name: \"sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 environment: SCONE_MODE: hw command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: /","title":"Posting Sessions"},{"location":"cas_blender_example/#posting-a-session","text":"We show how to interact with CAS s with the help of curl - this might be helpful during development since it simplifies quick tests. We provide a scone command line interface that can be executed inside of an enclave itself. It can perform an attestation of CAS as well as creating and verifying policies. Hence, we recommend to use the scone CLI . We assume that you already started a CAS instance and a LAS instance on your local host. Alternatively, you can use our public CAS instance at scone-cas.cf . Hence, we set the address of CAS as follows: export SCONE_CAS_ADDR = 127 .0.0.1 If you use our public CAS instance, set it as follows: export SCONE_CAS_ADDR = scone-cas.cf","title":"Posting a Session"},{"location":"cas_blender_example/#client-certificate","text":"To interact with CAS, we need to create a client certificate. When we create a session, it is associated with the client certificate of the creator. Any access to this session requires that the client knows the private key of the client certificate. Let's create a client certificate without a password. Note that you would typically add a password! mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi","title":"Client Certificate"},{"location":"cas_blender_example/#hello-world-session","text":"Let's create a minimal session: cat > session.yml <<EOF name: blender digest: create services: - name: application image_name: sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: / environment: SCONE_MODE: hw images: - name: sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] tags: [demo] EOF We can now upload the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar like this: Created Session[id=00ed7ade-bba6-4d43-9135-51d0ca2da9ba, name=blender, status=Pending] Session already exists If the session with name \"blender\" already exists - which will be the case when you use scone-cas.cf - the following error message is issued: Could not create successor session. Invalid previous session digest: ... In case the session with name blender already exists, you must chose a different session name. We can read the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key https:// $SCONE_CAS_ADDR :8081/session/blender This will result in an output like this: --- name: blender digest: 313c6c3b824f0a560c445c8ef0cf69781345aae753bdbeaedbfff15c5a348099 board_members: [] board_policy: minimum: 0 timeout: 30 images: - name: \"sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 tags: - demo services: - name: application image_name: \"sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 environment: SCONE_MODE: hw command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: /","title":"Hello World Session"},{"location":"cas_intro/","text":"SCONE CAS Concepts Objectives SCONE CAS (Configuration and Attestation Service) helps to address the following questions: How to provide services with secrets that do not yet have any secrets to authenticate themselves? (see how to provision secret GREETING in our Kubernetes Hello World! Tutorial ) How to inject secrets inside of files and services without any human ever being able to see these secrets? (see how to inject file /app/key.pem in our Kubernetes Hello World! Tutorial ) How to attest and executed encrypted code? (see how to run and attest encrypted Python code in our Kubernetes Hello World! Tutorial ) How to share secrets across several SCONE-based services? How to manage secrets in case one cannot trust any individuals, i.e., how to protect against insider attacks? How to import external secrets into a system without any human ever being able to see these secrets? Secrets Secrets are managed by a security policy . A security policy can generate secrets and can define which programs can access a given secret. Since a security policy can change over time, we use the term session to denote such a sequence of instances of a security policies. CAS distinguishes between the following kinds of secrets: Internal: An internal secret is generated by CAS on behalf of a CAS security policy . These secrets are created by CAS and the access to these is controlled by the security policy. A security policy could, for example, enforce that a secret is never seen by a human. Imported: A client can import secrets into a session from other local sessions or from remote sessions, i.e., sessions belonging to a remote CAS service. Such secrets are called imported secrets . The security policy associated with an internal secret will still be enforced after the secret was imported. External: External secrets are secrets that have been created by external entities, and that CAS maintains. The external secrets can be securely imported into CAS without them ever being visible by humans. CAS Features Protect all secrets from being visible by humans: CAS can help to ensure that secrets are only visible inside trusted execution environments ( TEEs ), in particular Intel SGX. This is not only true for internal and imported secrets, but CAS also supports secure import of external secrets , such that the external secrets never are visible to humans, neither during nor after the import. CAS does not require application source code changes: CAS permits injecting secrets into an application, e.g., into its configuration files, into its source files of scripting languages like Python and JavaScript, or into its compiled binaries. Due to this injection of secrets, no changes need to be made to any of the source code of the application. To ensure that the secrets are not exposed during the injection, the secrets are only injected inside of the TEE where the application is executed. The injection is completely transparent to the application. CAS can be a managed service: One can delegate the operations of CAS to an untrusted cloud provider and still ensure the confidentiality , integrity and freshness of the secrets - despite having a comprehensive threat model . Comprehensive threat model: CAS does not need to trust root users, it actually does not trust any individual user. Hence, it also cannot trust the operating system or any input or output. However, a session can trust that there exists sufficiently many users with trusted devices. While we do not know in general who to trust, a session might define that it trusts that out of a group of F+1 users, at least one is honest and uses an uncompromised device. We refer to this group as the policy board. Each session can define its policy board and a value of F such that at least F+1 members of the policy board must approve any changes of the security policy of this session. Advanced Governance: Policy creations and changes must be approved by a policy board . The policy board can consist of humans as well as programs. We refer to these programs as automated policy checkers . Secure Policy Checking: Policy checkers can be used to, for example, approve simple policy changes automatically or to perform careful checking of a policy change and reject policy mistakes that a human board member (who might have a very limited time budget) might miss. The policy checkers - as well as all other components of CAS - run inside of individual trusted execution environments . If one assumes that the likelihood of wrong executions of such a policy checker is negligible, one can assign these policy checkers the power to approve or alternatively, to reject a policy change single-handedly. Alternatively, one can require that a minimum number of approvals are necessary. Secure External Extensions: Key store services (e.g., Barbican) provide plugins to support extensions. The issue with plugins is that a plugin might modify the behavior of a key store. Also, plugins would change the code signature of CAS (if they were loaded during startup). Instead of integrating extensions into CAS, we permit external additions, e.g., to import external secrets in a secure way into CAS.","title":"Concepts"},{"location":"cas_intro/#scone-cas-concepts","text":"","title":"SCONE CAS Concepts"},{"location":"cas_intro/#objectives","text":"SCONE CAS (Configuration and Attestation Service) helps to address the following questions: How to provide services with secrets that do not yet have any secrets to authenticate themselves? (see how to provision secret GREETING in our Kubernetes Hello World! Tutorial ) How to inject secrets inside of files and services without any human ever being able to see these secrets? (see how to inject file /app/key.pem in our Kubernetes Hello World! Tutorial ) How to attest and executed encrypted code? (see how to run and attest encrypted Python code in our Kubernetes Hello World! Tutorial ) How to share secrets across several SCONE-based services? How to manage secrets in case one cannot trust any individuals, i.e., how to protect against insider attacks? How to import external secrets into a system without any human ever being able to see these secrets?","title":"Objectives"},{"location":"cas_intro/#secrets","text":"Secrets are managed by a security policy . A security policy can generate secrets and can define which programs can access a given secret. Since a security policy can change over time, we use the term session to denote such a sequence of instances of a security policies. CAS distinguishes between the following kinds of secrets: Internal: An internal secret is generated by CAS on behalf of a CAS security policy . These secrets are created by CAS and the access to these is controlled by the security policy. A security policy could, for example, enforce that a secret is never seen by a human. Imported: A client can import secrets into a session from other local sessions or from remote sessions, i.e., sessions belonging to a remote CAS service. Such secrets are called imported secrets . The security policy associated with an internal secret will still be enforced after the secret was imported. External: External secrets are secrets that have been created by external entities, and that CAS maintains. The external secrets can be securely imported into CAS without them ever being visible by humans.","title":"Secrets"},{"location":"cas_intro/#cas-features","text":"Protect all secrets from being visible by humans: CAS can help to ensure that secrets are only visible inside trusted execution environments ( TEEs ), in particular Intel SGX. This is not only true for internal and imported secrets, but CAS also supports secure import of external secrets , such that the external secrets never are visible to humans, neither during nor after the import. CAS does not require application source code changes: CAS permits injecting secrets into an application, e.g., into its configuration files, into its source files of scripting languages like Python and JavaScript, or into its compiled binaries. Due to this injection of secrets, no changes need to be made to any of the source code of the application. To ensure that the secrets are not exposed during the injection, the secrets are only injected inside of the TEE where the application is executed. The injection is completely transparent to the application. CAS can be a managed service: One can delegate the operations of CAS to an untrusted cloud provider and still ensure the confidentiality , integrity and freshness of the secrets - despite having a comprehensive threat model . Comprehensive threat model: CAS does not need to trust root users, it actually does not trust any individual user. Hence, it also cannot trust the operating system or any input or output. However, a session can trust that there exists sufficiently many users with trusted devices. While we do not know in general who to trust, a session might define that it trusts that out of a group of F+1 users, at least one is honest and uses an uncompromised device. We refer to this group as the policy board. Each session can define its policy board and a value of F such that at least F+1 members of the policy board must approve any changes of the security policy of this session. Advanced Governance: Policy creations and changes must be approved by a policy board . The policy board can consist of humans as well as programs. We refer to these programs as automated policy checkers . Secure Policy Checking: Policy checkers can be used to, for example, approve simple policy changes automatically or to perform careful checking of a policy change and reject policy mistakes that a human board member (who might have a very limited time budget) might miss. The policy checkers - as well as all other components of CAS - run inside of individual trusted execution environments . If one assumes that the likelihood of wrong executions of such a policy checker is negligible, one can assign these policy checkers the power to approve or alternatively, to reject a policy change single-handedly. Alternatively, one can require that a minimum number of approvals are necessary. Secure External Extensions: Key store services (e.g., Barbican) provide plugins to support extensions. The issue with plugins is that a plugin might modify the behavior of a key store. Also, plugins would change the code signature of CAS (if they were loaded during startup). Instead of integrating extensions into CAS, we permit external additions, e.g., to import external secrets in a secure way into CAS.","title":"CAS Features"},{"location":"community_version/","text":"SCONE Community Version The SCONE community version provides you access to most features of the SCONE platform. The community versions can help you to see if the SCONE Confidential Computing Platform is the appropriate choice for your application. You can also develop with the help of the SCONE community edition. The community is not intended for production and is provided as is . The commercial SCONE versions provide you not only support but also more features to improve the security of SCONE-based applications, and the ease of use to build and install applications. For example, tuning SCONE-based applications requires skills since there are multiple tuning parameters. We provide an application tuner as part of the SCONE commercial offering. intended for production and development . In the future, we plan to describe also the commercial features on this website. Getting access to the Community Version Send us your docker hub ID to get access to the SCONE community edition repositories.","title":"Community Version"},{"location":"community_version/#scone-community-version","text":"The SCONE community version provides you access to most features of the SCONE platform. The community versions can help you to see if the SCONE Confidential Computing Platform is the appropriate choice for your application. You can also develop with the help of the SCONE community edition. The community is not intended for production and is provided as is . The commercial SCONE versions provide you not only support but also more features to improve the security of SCONE-based applications, and the ease of use to build and install applications. For example, tuning SCONE-based applications requires skills since there are multiple tuning parameters. We provide an application tuner as part of the SCONE commercial offering. intended for production and development . In the future, we plan to describe also the commercial features on this website. Getting access to the Community Version Send us your docker hub ID to get access to the SCONE community edition repositories.","title":"SCONE Community Version"},{"location":"configuration/","text":"SCONE Configuration File SCONE permits users to tune certain internal parameters. These parameters affect performance but do not - at least directly - impact confidentiality nor integrity. These tuning parameters are defined by default in file /etc/sgx-musl.conf . When executing a program, one can change this default via environment variable SCONE_CONFIG . SCONE runs four different types of threads: ETHREADS : this are operating systems threads that run inside an enclave STHREADS : these are operating systems threads that run outside an enclave. SCONE_ETHREADS, SCONE_STHREADS, and SCONE_ESPINS, SCONE_ESLEEP variables are gone. As a replacement for SCONE_ETHREADS and SCONE_STHREADS variable, there is now SCONE_QUEUES variable, and a configurations file (in which you can set most of environment variables). The path to default config is /etc/sgx-musl.conf, you can also specify it via SCONE_CONFIG option. A reasonable default configuration file is already installed on sgx1-sgx3 machines. The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: - Q - sets the number of syscall-return queue pairs to allocate, equivalent to setting SCONE_QUEUES env variable; - H - sets the size of heap, equivalent to setting SCONE_HEAP env variable; - P - equivalent to setting SCONE_SSPINS env variable; - L - equivalent to setting SCONE_SSLEEP env variable; - s - creates an sthread pinned to core , serving queue pair with index , if is non-zero - sets SCHED_FIFO scheduler for the thread. - e - same as before, but creates an ethread. SCONE_* env variables have higher priority than settings from the config file. application interface SCONE is developed for legacy compatible code. Sometimes though it is wanted to adapt legacy code to benefit from SGX specific functionalities. SCONE exposes a set of these functionalities to the application in the scone.h header file. The source code can make sure that it is acutally executed with SCONE by testing for the __SCONE_LIBC__ macro defined in features.h .","title":"SCONE Configuration File"},{"location":"configuration/#scone-configuration-file","text":"SCONE permits users to tune certain internal parameters. These parameters affect performance but do not - at least directly - impact confidentiality nor integrity. These tuning parameters are defined by default in file /etc/sgx-musl.conf . When executing a program, one can change this default via environment variable SCONE_CONFIG . SCONE runs four different types of threads: ETHREADS : this are operating systems threads that run inside an enclave STHREADS : these are operating systems threads that run outside an enclave. SCONE_ETHREADS, SCONE_STHREADS, and SCONE_ESPINS, SCONE_ESLEEP variables are gone. As a replacement for SCONE_ETHREADS and SCONE_STHREADS variable, there is now SCONE_QUEUES variable, and a configurations file (in which you can set most of environment variables). The path to default config is /etc/sgx-musl.conf, you can also specify it via SCONE_CONFIG option. A reasonable default configuration file is already installed on sgx1-sgx3 machines. The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: - Q - sets the number of syscall-return queue pairs to allocate, equivalent to setting SCONE_QUEUES env variable; - H - sets the size of heap, equivalent to setting SCONE_HEAP env variable; - P - equivalent to setting SCONE_SSPINS env variable; - L - equivalent to setting SCONE_SSLEEP env variable; - s - creates an sthread pinned to core , serving queue pair with index , if is non-zero - sets SCHED_FIFO scheduler for the thread. - e - same as before, but creates an ethread. SCONE_* env variables have higher priority than settings from the config file.","title":"SCONE Configuration File"},{"location":"configuration/#application-interface","text":"SCONE is developed for legacy compatible code. Sometimes though it is wanted to adapt legacy code to benefit from SGX specific functionalities. SCONE exposes a set of these functionalities to the application in the scone.h header file. The source code can make sure that it is acutally executed with SCONE by testing for the __SCONE_LIBC__ macro defined in features.h .","title":"application interface"},{"location":"curated_images/","text":"SCONE Curated Images For simplicity, we maintain a set of curated images of popular applications: Application Name barbican sconecuratedimages/apps:barbican-alpine jdk sconecuratedimages/apps:openjdk-8-alpine jdk sconecuratedimages/apps:openjdk-11-alpine jdk sconecuratedimages/apps:openjdk-15-alpine lua sconecuratedimages/apps:lua-alpine mariadb sconecuratedimages/apps:mariadb-alpine maxscale sconecuratedimages/apps:maxscale-alpine memcached sconecuratedimages/apps:memcached-alpine mongodb sconecuratedimages/apps:mongodb-alpine nginx sconecuratedimages/apps:nginx-alpine node sconecuratedimages/apps:node-8-alpine node sconecuratedimages/apps:node-10-alpine pypy sconecuratedimages/apps:pypy3-6.0.0-alpine3.7 python sconecuratedimages/apps:python-3.7.3-alpine3.10 R sconecuratedimages/apps:R redis sconecuratedimages/apps:redis-6-alpine vault sconecuratedimages/apps:vault-alpine zookeeper sconecuratedimages/apps:zookeeper-alpine We support various versions of these images. Please read about Semantic Versioning to learn about the image names that you want to use. If any open-source application is not yet listed as a curated image, send us an email and we might be able to help you.","title":"Curated Images"},{"location":"curated_images/#scone-curated-images","text":"For simplicity, we maintain a set of curated images of popular applications: Application Name barbican sconecuratedimages/apps:barbican-alpine jdk sconecuratedimages/apps:openjdk-8-alpine jdk sconecuratedimages/apps:openjdk-11-alpine jdk sconecuratedimages/apps:openjdk-15-alpine lua sconecuratedimages/apps:lua-alpine mariadb sconecuratedimages/apps:mariadb-alpine maxscale sconecuratedimages/apps:maxscale-alpine memcached sconecuratedimages/apps:memcached-alpine mongodb sconecuratedimages/apps:mongodb-alpine nginx sconecuratedimages/apps:nginx-alpine node sconecuratedimages/apps:node-8-alpine node sconecuratedimages/apps:node-10-alpine pypy sconecuratedimages/apps:pypy3-6.0.0-alpine3.7 python sconecuratedimages/apps:python-3.7.3-alpine3.10 R sconecuratedimages/apps:R redis sconecuratedimages/apps:redis-6-alpine vault sconecuratedimages/apps:vault-alpine zookeeper sconecuratedimages/apps:zookeeper-alpine We support various versions of these images. Please read about Semantic Versioning to learn about the image names that you want to use. If any open-source application is not yet listed as a curated image, send us an email and we might be able to help you.","title":"SCONE Curated Images"},{"location":"dapps/","text":"Trusted DApps with SCONE and iExec This tutorial shows how to build a trusted DApp for the iExec platform . A DApp is a decentralized application that can be executed on hosts of a decentralized network. To learn about basic terminology and concepts of SCONE, we recommend that you read the preceding tutorial pages first. Note that SCONE supports most common programming languages: please have a look at our examples for C , C++ , Fortran , GO , Rust , Python , Java , and JavaScript/Node.js . In this tutorial, we show how to build a trusted DApp using C. The transfer service used by this use case is not operational anymore To try this example, you would need to replace the the \"transfer service\" by a more reliable way to transfer files. Eventually, we will rework this example to use a reliable service to transfer files. For now, we leave it online since it demonstrate some of the features of SCONE. Also, maybe somebody will send us a pull request for using github instead of tansfer.sh ? CLI Trusted DApp SCONE helps developers to protect the integrity as well as the confidentiality not only of data at rest (i.e., on disk) and data being transmitted but also all data stored in main memory . Note that any root user can easily dump the main memory of any application and in this way might gain secrets like the keys used to encrypt the data at rest. By trusted DApp , we refer to a DApp that executes inside a trusted execution environment (TEE) in such a way that not even an attacker with root rights can access the data of the application. SCONE simplifies to run applications inside of TEEs as well as to transparently encrypt and decrypt files. SCONE is designed to be independent of the actual TEE implementation. Right now -- since there is a lack of appropriate hardware alternatives -- SCONE supports only Intel SGX . SCONE Platform The SCONE platform helps to run DApps inside of TEEs without source code modifications. In this tutorial, we describe how to build a simple application to run with SCONE inside of an enclave , i.e., inside an Intel SGX TEE, show how to compile the simple trusted DApp with the help of an Alpine container, how to deploy and execute inside of an Alpine container, introduce the workflow of where and how data is encrypted, pushed to the application and then decrypted explain how keys are managed and passed to a trusted DApp, and show how to test this application on your own infrastructure. Trusted copy DApp We show how to implement a simple copy command as a trusted DApp. This is of course not a realistic DApp but it helps us to show all steps necessary to build and run trusted DApps. The copy DApp copies files located in an input directory /INPUTS to an output directory /OUTPUTS of the client the machine. With the help of SCONE, this copy DApp actually copies encrypted files such that the SCONE transparently decrypts and checks the integrity of files that are read by the application. This protects the confidentiality and integrity of the files without any need to change the application itself. Also, SCONE encrypts and integrity protects the files that are written by the application. Each file is encrypted with a random key generated and managed by SCONE inside the enclave. With the help of a CLI (command line interface deployed in a container, see below), a client can encrypt and push files stored on its client machine to the copy DApp. The copy DApp copies the files. The CLI can decrypt the files after the execution of the DApp. We introduce the commands to perform this example below. Encrypted Volumes Containers support volumes as a way to persist data. Trusted DApps for the iExec platform have always two encrypted volumes mapped into their file system: an encrypted volume for input files mapped in the container in directory /encryptedInputs , and an encrypted volume for output files /encryptedOutputs . We show below how the input files are encrypted with the help of the CLI and pushed to the container. Moreover, we show how to decrypt the files with the help of the CLI. To access an encrypted volume, one needs a key and a tag . The key is used to encrypt the individual keys of the files of the volume, i.e., it is used to protect the confidentiality of the files. The tag describes the current state of the volume. Any change of any file in the volume will result in a new tag . The tag is used to protect the integrity and freshness of the files: any unauthorized modification of the volume will be detected since the correct tag can only be computed by parties knowing the key of the volume ( integrity protection ). any rollback to older version of the volume will be correctly encrypted but will have an old tag. Hence, rollbacks to older versions will be detected ( freshness protection ). Key and Tag Management The copy DApp requires access to keys and tags to read the encrypted input files (i.e., the input volume) and to write the encrypted output files (i.e., the output volume). Passing the keys and tags to the copy DApp is non-trivial since we need to ensure that no other DApp nor any attacker can read or modify the keys or the tags. To pass the keys and tags to a trusted DApp, SCONE provides a configuration and attestation service (CAS) . The CAS can ensure that only the copy DApp can access the keys and tags by verifying that it talks to an unmodified copy DApp . This process is called attestation and is performed with the help of the Intel SGX CPU extension. A trusted DApp is deployed with the help of a container image hosted on docker hub or some similar repository. We recommend that these container images are signed using Docker content trust . Note that content trust is not sufficient to ensure that we talk to an unmodified copy DApp since, for example, an attacker with root access could change this image on the host on which it is deployed. To establish trust between two communication partners, they typically authenticate each other with the help of TLS . An entity needs to know a private key to authenticate itself. When a trusted DApp like the copy DApp starts up it does not know such a private key yet. Note that any key that might be stored in plain text in the image, could be read by any entity that is permitted to access that image. One could encrypt the private key in the image but then the trust DApps would need to know the key to decrypt the private key. Note that the code that runs inside of an enclave, is in general not encrypted. Hence, one cannot store the private key inside of the code either. Attestation - unlike authentication - does not need a private key. It ensures that the correct code is executing. To distinguish between different instances of a trusted DApp, on startup, the SCONE runtime generates a random private/public key pair inside of the enclave. This means that the private key can be kept secret inside of the enclave. This public key is used to identify this instance and one can perform TLS authentication to ensure that one talks to a specific instance. To attest a trusted DApp, we need to describe the state of this DApp. In general, this consists of a hash value that describes the trusted DApp (called MrEnclave ) and a tag that describes the file systems state. In the case of the copy DApp, the file system (mounted at /) does not need to be attested - only the encrypted input volume (mounted at /encryptedInputs) and encrypted output volume (mounted at /encryptedOutputs) needs to be attested. When a trusted DApp starts up, it connects to the CAS to perform an attestation. This attestation ensures that MrEnclave is the expected value and that the file system and the mounted volumes are in the correct state, i.e., the tags have the expected values. Note that MrEnclave will depend on the heap and stack size, i.e., changing the values will require an update of the expected MrEnclave . A trusted DApp will typically write to the output volume. This will change the tag of the output volume. To ensure that a client can read the current state of the volumes written by a trusted DApp, the tags will be pushed to the CAS when a trusted DApp exits. Attesting CAS CAS is written in Rust , a safe and efficient programming language. The type-safety ensures that simple programming bugs can be exploited by attackers to hijack services. A client can attest a valid CAS in two ways: via the Intel attestations service, and/or via the certificates provided by CAS. Right now, the CLI verifies that it talks to a correct CAS with the help of TLS. The client only uses TLS and verifies that the CAS has the correct certificate. Only a CAS will be able to get this certificate. In a later version, we will additionally ensure that the CAS provides a correct report by the Intel attestations service. CAS is trusted, managed service : it can be operated by a provider but the provider is not able to access the sessions and in particular, the secrets of the clients. Right now, we run a public CAS service for debugging and testing. This CAS runs in debug mode , i.e., you should not use this CAS to pass secrets. For clients that need to run a private CAS services, we will enable clients to run their own CAS. Please contact us if you want to run your own CAS or if you need access to CAS running in production mode. To run a trusted DApp in production mode, you need either to get a MrSigner from Intel or you need access to Intel SGX CPU that supports flexible launch control. Contact us, if you want to run trusted DApps in production mode. Sessions We need to specify the value of MrEnclave and the state (i.e., the tags) of the of the volumes and in the general case, also the file system. To do so, CAS supports a security policies. We call such a security policy a session . A session specifies what volumes are mounted and where they are mounted, the keys and tags of these volumes and MrEnclave of the trusted DApp. One problem that we need to address is that each client of a trusted DApp cannot only protect its data from an attacker but also from other clients of the trusted DApp, the entity that generated the image and the entities that manage the CAS or the host on which the trusted DApp is executed. To address this issue, we provide a way to have individually encrypted volumes for each session. In other words, each client of a trusted DApp can customize the image by providing input and output volumes and only the instance of this client can access these volumes. No other client nor the image generator can access the keys and hence, these volumes . A client generates these keys (in the CLI). A session specifies these keys. A session is typically derived from a session template . The session template of the copy DApp looks like this: name: $SESSION digest: create services: - name: application image_name: sconecuratedimages/iexecsgx:copy_demo mrenclaves: [e0f45a9a862c83f383730cc2aef144081025e3f925615da5b35c4d980faaa3c9] tags: [demo] command: /application pwd: / environment: SCONE_MODE: hw volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: sconecuratedimages/iexecsgx:copy_demo mrenclaves: [e0f45a9a862c83f383730cc2aef144081025e3f925615da5b35c4d980faaa3c9] tags: [demo] volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs Note that in most cases you can start from a generic session template and you do not need to worry about the syntax and all the details of session descriptions. The session templates of trusted DApps look very similar to the session template above. The three differences are that the image name will be updated, the list of mrenclaves will be different, and one might want to specify arguments passed to the trusted DApp ( key command ). Building an DApp The source code of this application , you can retrieve from git clone https://github.com/scontain/copy_dapp.git cd copy_dapp The source code of the trusted copy DApp is file copy_files.c . We build this program with the help of the standard gcc that is part of Alpine Linux (see the Dockerfile below). For convenience, we provide an Alpine image (i.e., sconecuratedimages/muslgcc) that has gcc preinstalled. FROM sconecuratedimages/muslgcc COPY copy_files.c / # compile with vanilla gcc RUN gcc -Wall copy_files.c -o /copy_files FROM iexechub/sgx-scone:runtime COPY --from = 0 /copy_files /application RUN apk add curl bash unzip zip ENTRYPOINT [ \"/application.sh\" ] To run the application inside of an enclave, we need the SCONE runtime environment. This runtime is part of image iexechub/sgx-scone:runtime . Moreover, this image contains a script /application.sh that pulls the encrypted input and output volumes from some transfer service, executes the application inside of an enclave. For testing, the script can push the encrypted output volume to a transfer service (see below). The image sconecuratedimages/iexecsgx:copy_demo is built by the script build-image.sh . This script also determines MrEnclave of the application and writes a session template copy_dapp.yml . Testing We now show how to test the trusted copy DApp. After building the image by executing ./build-image.sh We can execute this image on a SGX-capable host. This host has to have the SCONE local attestation service and Docker installed. We describe in the installation section on how to install these prerequisites. In what follows, we assume that the name of the host is stored in environment variable host . SETUP To execute the trusted copy DApp, we need to create some files to copy. We create an INPUTS directory and store some files there. We also create some OUTPUTS directory and some directory to hold some generated keys (KEYS) and to hold some temporary files (ZIP). We also copy the session template generated when building the container image. mkdir -p EXAMPLE cd EXAMPLE mkdir -p KEYS INPUTS OUTPUTS ZIP echo \"Hello world\" > INPUTS/f1.txt echo \"Hello together\" > INPUTS/f2.txt cp ../copy_dapp.yml KEYS/ Step 1 We encrypt these files and push the zipped up files for now to filepush.co . Let's execute the first step of encrypting the files and pushing these to the transfer service: CMD = $( docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /INPUTS:/inputs sconecuratedimages/iexecsgx:scone.cli encryptedpush --application sconecuratedimages/iexecsgx:copy_demo -t /conf/copy_dapp.yml ) Let's look at the output: echo $CMD should result in an output like: \"cmdline\" : --sessionID 180713033312847980809017601 /application --secretManagementService 87 .190.236.136 --url https://filepush.co/XPHT/scone-upload.zip You can look in your browser at the URL to see the uploaded files! Step 2 Before running this application on the iExec platform, we might want to test it on some SGX-capable host. Ensure that the newest image is loaded on host . This step is only needed if it is expected that the image has changed since docker run does not pull/update the image. ssh $host docker pull sconecuratedimages/iexecsgx:copy_demo Ensure that you are executing a bash shell (execute bash ). Now, execute the command on host and we ask it to be pushed to filepush.co : URL = $( ssh $host docker run -t --device = /dev/isgx --rm sconecuratedimages/iexecsgx:copy_demo ${ CMD //[ $'\\t\\r\\n' ] } --push filepush.co/upload ) Let's look at the output: echo $URL this results in an output like: https://filepush.co/qSMf/scone-upload.zip You can now download this (if running a bash shell) curl --output ZIP/encryptedOutputFiles.zip ${ URL //[ $'\\t\\r\\n' ] } Step 3 docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /ZIP:/encryptedOutputs -v $PWD /OUTPUTS:/decryptedOutputs sconecuratedimages/iexecsgx:scone.cli decrypt We can now look at the files in OUTPUTS directory: cat OUTPUTS/f1.txt results in Hello world and cat OUTPUTS/f2.txt results in Hello together .","title":"Trusted DApps"},{"location":"dapps/#trusted-dapps-with-scone-and-iexec","text":"This tutorial shows how to build a trusted DApp for the iExec platform . A DApp is a decentralized application that can be executed on hosts of a decentralized network. To learn about basic terminology and concepts of SCONE, we recommend that you read the preceding tutorial pages first. Note that SCONE supports most common programming languages: please have a look at our examples for C , C++ , Fortran , GO , Rust , Python , Java , and JavaScript/Node.js . In this tutorial, we show how to build a trusted DApp using C. The transfer service used by this use case is not operational anymore To try this example, you would need to replace the the \"transfer service\" by a more reliable way to transfer files. Eventually, we will rework this example to use a reliable service to transfer files. For now, we leave it online since it demonstrate some of the features of SCONE. Also, maybe somebody will send us a pull request for using github instead of tansfer.sh ?","title":"Trusted DApps with SCONE and iExec"},{"location":"dapps/#cli","text":"","title":"CLI"},{"location":"dapps/#trusted-dapp","text":"SCONE helps developers to protect the integrity as well as the confidentiality not only of data at rest (i.e., on disk) and data being transmitted but also all data stored in main memory . Note that any root user can easily dump the main memory of any application and in this way might gain secrets like the keys used to encrypt the data at rest. By trusted DApp , we refer to a DApp that executes inside a trusted execution environment (TEE) in such a way that not even an attacker with root rights can access the data of the application. SCONE simplifies to run applications inside of TEEs as well as to transparently encrypt and decrypt files. SCONE is designed to be independent of the actual TEE implementation. Right now -- since there is a lack of appropriate hardware alternatives -- SCONE supports only Intel SGX .","title":"Trusted DApp"},{"location":"dapps/#scone-platform","text":"The SCONE platform helps to run DApps inside of TEEs without source code modifications. In this tutorial, we describe how to build a simple application to run with SCONE inside of an enclave , i.e., inside an Intel SGX TEE, show how to compile the simple trusted DApp with the help of an Alpine container, how to deploy and execute inside of an Alpine container, introduce the workflow of where and how data is encrypted, pushed to the application and then decrypted explain how keys are managed and passed to a trusted DApp, and show how to test this application on your own infrastructure.","title":"SCONE Platform"},{"location":"dapps/#trusted-copy-dapp","text":"We show how to implement a simple copy command as a trusted DApp. This is of course not a realistic DApp but it helps us to show all steps necessary to build and run trusted DApps. The copy DApp copies files located in an input directory /INPUTS to an output directory /OUTPUTS of the client the machine. With the help of SCONE, this copy DApp actually copies encrypted files such that the SCONE transparently decrypts and checks the integrity of files that are read by the application. This protects the confidentiality and integrity of the files without any need to change the application itself. Also, SCONE encrypts and integrity protects the files that are written by the application. Each file is encrypted with a random key generated and managed by SCONE inside the enclave. With the help of a CLI (command line interface deployed in a container, see below), a client can encrypt and push files stored on its client machine to the copy DApp. The copy DApp copies the files. The CLI can decrypt the files after the execution of the DApp. We introduce the commands to perform this example below.","title":"Trusted copy DApp"},{"location":"dapps/#encrypted-volumes","text":"Containers support volumes as a way to persist data. Trusted DApps for the iExec platform have always two encrypted volumes mapped into their file system: an encrypted volume for input files mapped in the container in directory /encryptedInputs , and an encrypted volume for output files /encryptedOutputs . We show below how the input files are encrypted with the help of the CLI and pushed to the container. Moreover, we show how to decrypt the files with the help of the CLI. To access an encrypted volume, one needs a key and a tag . The key is used to encrypt the individual keys of the files of the volume, i.e., it is used to protect the confidentiality of the files. The tag describes the current state of the volume. Any change of any file in the volume will result in a new tag . The tag is used to protect the integrity and freshness of the files: any unauthorized modification of the volume will be detected since the correct tag can only be computed by parties knowing the key of the volume ( integrity protection ). any rollback to older version of the volume will be correctly encrypted but will have an old tag. Hence, rollbacks to older versions will be detected ( freshness protection ).","title":"Encrypted Volumes"},{"location":"dapps/#key-and-tag-management","text":"The copy DApp requires access to keys and tags to read the encrypted input files (i.e., the input volume) and to write the encrypted output files (i.e., the output volume). Passing the keys and tags to the copy DApp is non-trivial since we need to ensure that no other DApp nor any attacker can read or modify the keys or the tags. To pass the keys and tags to a trusted DApp, SCONE provides a configuration and attestation service (CAS) . The CAS can ensure that only the copy DApp can access the keys and tags by verifying that it talks to an unmodified copy DApp . This process is called attestation and is performed with the help of the Intel SGX CPU extension. A trusted DApp is deployed with the help of a container image hosted on docker hub or some similar repository. We recommend that these container images are signed using Docker content trust . Note that content trust is not sufficient to ensure that we talk to an unmodified copy DApp since, for example, an attacker with root access could change this image on the host on which it is deployed. To establish trust between two communication partners, they typically authenticate each other with the help of TLS . An entity needs to know a private key to authenticate itself. When a trusted DApp like the copy DApp starts up it does not know such a private key yet. Note that any key that might be stored in plain text in the image, could be read by any entity that is permitted to access that image. One could encrypt the private key in the image but then the trust DApps would need to know the key to decrypt the private key. Note that the code that runs inside of an enclave, is in general not encrypted. Hence, one cannot store the private key inside of the code either. Attestation - unlike authentication - does not need a private key. It ensures that the correct code is executing. To distinguish between different instances of a trusted DApp, on startup, the SCONE runtime generates a random private/public key pair inside of the enclave. This means that the private key can be kept secret inside of the enclave. This public key is used to identify this instance and one can perform TLS authentication to ensure that one talks to a specific instance. To attest a trusted DApp, we need to describe the state of this DApp. In general, this consists of a hash value that describes the trusted DApp (called MrEnclave ) and a tag that describes the file systems state. In the case of the copy DApp, the file system (mounted at /) does not need to be attested - only the encrypted input volume (mounted at /encryptedInputs) and encrypted output volume (mounted at /encryptedOutputs) needs to be attested. When a trusted DApp starts up, it connects to the CAS to perform an attestation. This attestation ensures that MrEnclave is the expected value and that the file system and the mounted volumes are in the correct state, i.e., the tags have the expected values. Note that MrEnclave will depend on the heap and stack size, i.e., changing the values will require an update of the expected MrEnclave . A trusted DApp will typically write to the output volume. This will change the tag of the output volume. To ensure that a client can read the current state of the volumes written by a trusted DApp, the tags will be pushed to the CAS when a trusted DApp exits.","title":"Key and Tag Management"},{"location":"dapps/#attesting-cas","text":"CAS is written in Rust , a safe and efficient programming language. The type-safety ensures that simple programming bugs can be exploited by attackers to hijack services. A client can attest a valid CAS in two ways: via the Intel attestations service, and/or via the certificates provided by CAS. Right now, the CLI verifies that it talks to a correct CAS with the help of TLS. The client only uses TLS and verifies that the CAS has the correct certificate. Only a CAS will be able to get this certificate. In a later version, we will additionally ensure that the CAS provides a correct report by the Intel attestations service. CAS is trusted, managed service : it can be operated by a provider but the provider is not able to access the sessions and in particular, the secrets of the clients. Right now, we run a public CAS service for debugging and testing. This CAS runs in debug mode , i.e., you should not use this CAS to pass secrets. For clients that need to run a private CAS services, we will enable clients to run their own CAS. Please contact us if you want to run your own CAS or if you need access to CAS running in production mode. To run a trusted DApp in production mode, you need either to get a MrSigner from Intel or you need access to Intel SGX CPU that supports flexible launch control. Contact us, if you want to run trusted DApps in production mode.","title":"Attesting CAS"},{"location":"dapps/#sessions","text":"We need to specify the value of MrEnclave and the state (i.e., the tags) of the of the volumes and in the general case, also the file system. To do so, CAS supports a security policies. We call such a security policy a session . A session specifies what volumes are mounted and where they are mounted, the keys and tags of these volumes and MrEnclave of the trusted DApp. One problem that we need to address is that each client of a trusted DApp cannot only protect its data from an attacker but also from other clients of the trusted DApp, the entity that generated the image and the entities that manage the CAS or the host on which the trusted DApp is executed. To address this issue, we provide a way to have individually encrypted volumes for each session. In other words, each client of a trusted DApp can customize the image by providing input and output volumes and only the instance of this client can access these volumes. No other client nor the image generator can access the keys and hence, these volumes . A client generates these keys (in the CLI). A session specifies these keys. A session is typically derived from a session template . The session template of the copy DApp looks like this: name: $SESSION digest: create services: - name: application image_name: sconecuratedimages/iexecsgx:copy_demo mrenclaves: [e0f45a9a862c83f383730cc2aef144081025e3f925615da5b35c4d980faaa3c9] tags: [demo] command: /application pwd: / environment: SCONE_MODE: hw volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: sconecuratedimages/iexecsgx:copy_demo mrenclaves: [e0f45a9a862c83f383730cc2aef144081025e3f925615da5b35c4d980faaa3c9] tags: [demo] volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs Note that in most cases you can start from a generic session template and you do not need to worry about the syntax and all the details of session descriptions. The session templates of trusted DApps look very similar to the session template above. The three differences are that the image name will be updated, the list of mrenclaves will be different, and one might want to specify arguments passed to the trusted DApp ( key command ).","title":"Sessions"},{"location":"dapps/#building-an-dapp","text":"The source code of this application , you can retrieve from git clone https://github.com/scontain/copy_dapp.git cd copy_dapp The source code of the trusted copy DApp is file copy_files.c . We build this program with the help of the standard gcc that is part of Alpine Linux (see the Dockerfile below). For convenience, we provide an Alpine image (i.e., sconecuratedimages/muslgcc) that has gcc preinstalled. FROM sconecuratedimages/muslgcc COPY copy_files.c / # compile with vanilla gcc RUN gcc -Wall copy_files.c -o /copy_files FROM iexechub/sgx-scone:runtime COPY --from = 0 /copy_files /application RUN apk add curl bash unzip zip ENTRYPOINT [ \"/application.sh\" ] To run the application inside of an enclave, we need the SCONE runtime environment. This runtime is part of image iexechub/sgx-scone:runtime . Moreover, this image contains a script /application.sh that pulls the encrypted input and output volumes from some transfer service, executes the application inside of an enclave. For testing, the script can push the encrypted output volume to a transfer service (see below). The image sconecuratedimages/iexecsgx:copy_demo is built by the script build-image.sh . This script also determines MrEnclave of the application and writes a session template copy_dapp.yml .","title":"Building an DApp"},{"location":"dapps/#testing","text":"We now show how to test the trusted copy DApp. After building the image by executing ./build-image.sh We can execute this image on a SGX-capable host. This host has to have the SCONE local attestation service and Docker installed. We describe in the installation section on how to install these prerequisites. In what follows, we assume that the name of the host is stored in environment variable host .","title":"Testing"},{"location":"dapps/#setup","text":"To execute the trusted copy DApp, we need to create some files to copy. We create an INPUTS directory and store some files there. We also create some OUTPUTS directory and some directory to hold some generated keys (KEYS) and to hold some temporary files (ZIP). We also copy the session template generated when building the container image. mkdir -p EXAMPLE cd EXAMPLE mkdir -p KEYS INPUTS OUTPUTS ZIP echo \"Hello world\" > INPUTS/f1.txt echo \"Hello together\" > INPUTS/f2.txt cp ../copy_dapp.yml KEYS/","title":"SETUP"},{"location":"dapps/#step-1","text":"We encrypt these files and push the zipped up files for now to filepush.co . Let's execute the first step of encrypting the files and pushing these to the transfer service: CMD = $( docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /INPUTS:/inputs sconecuratedimages/iexecsgx:scone.cli encryptedpush --application sconecuratedimages/iexecsgx:copy_demo -t /conf/copy_dapp.yml ) Let's look at the output: echo $CMD should result in an output like: \"cmdline\" : --sessionID 180713033312847980809017601 /application --secretManagementService 87 .190.236.136 --url https://filepush.co/XPHT/scone-upload.zip You can look in your browser at the URL to see the uploaded files!","title":"Step 1"},{"location":"dapps/#step-2","text":"Before running this application on the iExec platform, we might want to test it on some SGX-capable host. Ensure that the newest image is loaded on host . This step is only needed if it is expected that the image has changed since docker run does not pull/update the image. ssh $host docker pull sconecuratedimages/iexecsgx:copy_demo Ensure that you are executing a bash shell (execute bash ). Now, execute the command on host and we ask it to be pushed to filepush.co : URL = $( ssh $host docker run -t --device = /dev/isgx --rm sconecuratedimages/iexecsgx:copy_demo ${ CMD //[ $'\\t\\r\\n' ] } --push filepush.co/upload ) Let's look at the output: echo $URL this results in an output like: https://filepush.co/qSMf/scone-upload.zip You can now download this (if running a bash shell) curl --output ZIP/encryptedOutputFiles.zip ${ URL //[ $'\\t\\r\\n' ] }","title":"Step 2"},{"location":"dapps/#step-3","text":"docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /ZIP:/encryptedOutputs -v $PWD /OUTPUTS:/decryptedOutputs sconecuratedimages/iexecsgx:scone.cli decrypt We can now look at the files in OUTPUTS directory: cat OUTPUTS/f1.txt results in Hello world and cat OUTPUTS/f2.txt results in Hello together .","title":"Step 3"},{"location":"dockerfileexample/","text":"Dockerfile Example We show now how to create a container image that contains a very simple hello world program running inside an enclave. The program is given in C but could be any other compiled language that we support like Rust , C++ and Fortran . Getting access You need access to a private docker hub repository sconecuratedimages/crosscompilers to execute this example. Send us your docker hub ID to get access to this repository. Building images without the SCONE tool chain This example builds an image that contains the complete SCONE platform. Build your container images with a multi-stage build such that they only contain your binaries when you push your images to a public repository. Here is the dockerfile: cat > Dockerfile << EOF FROM sconecuratedimages/crosscompilers RUN echo \"#include <stdio.h>\" > helloworld.c \\ && echo \"int main() {\" >> helloworld.c \\ && echo \"printf(\\\"Hello World!\\n\\\"); }\" >> helloworld.c RUN gcc -o helloworld helloworld.c CMD bash -c \"SCONE_VERSION=1 /helloworld\" EOF Let's generate an image ( helloworld ) with this Dockerfile: docker build --pull -t helloworld . Let's run the image as follows: docker run --device = /dev/isgx --rm helloworld The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 597cdef086651d46652cab78a89386b790ed058427ce1a5feacc3da7bc731902 Hello World! Note In case you do not have a SGX driver installed, the run will fail. Run the program in simulation mode by executing docker run --rm helloworld Screencast","title":"Dockerfile Example"},{"location":"dockerfileexample/#dockerfile-example","text":"We show now how to create a container image that contains a very simple hello world program running inside an enclave. The program is given in C but could be any other compiled language that we support like Rust , C++ and Fortran . Getting access You need access to a private docker hub repository sconecuratedimages/crosscompilers to execute this example. Send us your docker hub ID to get access to this repository. Building images without the SCONE tool chain This example builds an image that contains the complete SCONE platform. Build your container images with a multi-stage build such that they only contain your binaries when you push your images to a public repository. Here is the dockerfile: cat > Dockerfile << EOF FROM sconecuratedimages/crosscompilers RUN echo \"#include <stdio.h>\" > helloworld.c \\ && echo \"int main() {\" >> helloworld.c \\ && echo \"printf(\\\"Hello World!\\n\\\"); }\" >> helloworld.c RUN gcc -o helloworld helloworld.c CMD bash -c \"SCONE_VERSION=1 /helloworld\" EOF Let's generate an image ( helloworld ) with this Dockerfile: docker build --pull -t helloworld . Let's run the image as follows: docker run --device = /dev/isgx --rm helloworld The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 597cdef086651d46652cab78a89386b790ed058427ce1a5feacc3da7bc731902 Hello World! Note In case you do not have a SGX driver installed, the run will fail. Run the program in simulation mode by executing docker run --rm helloworld","title":"Dockerfile Example"},{"location":"dockerfileexample/#screencast","text":"","title":"Screencast"},{"location":"dockerinstall/","text":"Installation of Docker Engine To install the docker engine, you can follow the official description . To simplify the installation of Docker, on Ubuntu you can install the docker engine by executing the following in a terminal. Note that this script will also add your user to the group docker in case you cannot execute docker without sudo . curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_docker.sh | bash","title":"Installing docker"},{"location":"dockerinstall/#installation-of-docker-engine","text":"To install the docker engine, you can follow the official description . To simplify the installation of Docker, on Ubuntu you can install the docker engine by executing the following in a terminal. Note that this script will also add your user to the group docker in case you cannot execute docker without sudo . curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_docker.sh | bash","title":"Installation of Docker Engine"},{"location":"eclipse/","text":"Eclipse Integration In this section, we explain how to set up the Eclipse IDE such that you can develop and in particular, code that executed inside of an enclave. The instructions have been tested on Ubuntu 18.04. Installing Eclipse Download the Eclipse IDE for C/C++ Developers for Linux 64-bit from https://www.eclipse.org/downloads/packages/ Extract it to a suitable location, e.g. tar -xzf \"eclipse-cpp-2019-03-R-linux-gtk-x86_64.tar.gz\" -C ~/ (optional) If you want to have a desktop entry for eclipse, open or create ~/.local/share/applications/.desktop and add the following content: [Desktop Entry] Comment = Eclipse Terminal = false Name = Eclipse Exec = /home/myusername/eclipse/eclipse Type = Application Icon = /home/myusername/eclipse/icon.xpm (Make sure to adapt the paths accordingly.) Creating a workspace Create a new folder as your workspace, e.g.: mkdir ~/eclipse-workspace Clone or copy the code you want to work with into the workspace, e.g.: cd ~/eclipse-workspace && git clone ... Start Eclipse! (Use the desktop entry or run ~/eclipse/eclipse ) When prompted, select the created workspace folder and Launch Click on Workbench at the top right Configure Eclipse for developing applications using SCONE We show the steps to integrate SCONE with Eclipse. We assume that you have already installed scone-gcc and scone-gdb one the machine your are now installing Eclipse: In Eclipse, select File > Import... > C/C++ > Existing Code as Makefile Project > Next Existing Code Location : Select <your-path-to-workspace>/<path_to_project>/<your_project> . Select Linux GCC as Toolchain Click Finish Right-click the <your_project> project, select Properties > C/C++ Build > Environment Add the following entries: CC=scone-gcc , CFLAGS=-g -Og , PATH=<your-build-directory>/built/bin (Tick Add to all configurations for PATH . Your regular PATH will be added automatically.) Apply and Close Press Ctrl+B to build. Both SCONE (if configured) and the test applications will be built. While the <your_project> is selected, Select Run > Debug configurations... Double-click C/C++ Application Change Name to <your_app> , C/C++ Application to target/<your_app> Switch to tab Environment and add all environment variables used by SCONE, example: Switch to tab Debugger and change GDB debugger to scone-gdb If the build-oot.sh script is used to build outside the source directory, it is advised to re-map the sources back to their original location for Debugging purposes: Switch to tab Source and Add... a Path Mapping with Name SCONE mapping , Add Compilation path <your-scone-build-directory> and Local file system path <your-scone-source-directory> (Selecting the local path is tricky, as a button seems to not be rendered correctly. Click on the empty space highlighted in the screenshot below) OK > Apply > Debug When asked, Switch view The debugger is launched and the application is halted in main (can be disabled in the Debugger settings of the launch configuration). You can show local variables, Ctr+Click to follow references, etc. as usual Continue with F8","title":"Eclipse"},{"location":"eclipse/#eclipse-integration","text":"In this section, we explain how to set up the Eclipse IDE such that you can develop and in particular, code that executed inside of an enclave. The instructions have been tested on Ubuntu 18.04.","title":"Eclipse Integration"},{"location":"eclipse/#installing-eclipse","text":"Download the Eclipse IDE for C/C++ Developers for Linux 64-bit from https://www.eclipse.org/downloads/packages/ Extract it to a suitable location, e.g. tar -xzf \"eclipse-cpp-2019-03-R-linux-gtk-x86_64.tar.gz\" -C ~/ (optional) If you want to have a desktop entry for eclipse, open or create ~/.local/share/applications/.desktop and add the following content: [Desktop Entry] Comment = Eclipse Terminal = false Name = Eclipse Exec = /home/myusername/eclipse/eclipse Type = Application Icon = /home/myusername/eclipse/icon.xpm (Make sure to adapt the paths accordingly.)","title":"Installing Eclipse"},{"location":"eclipse/#creating-a-workspace","text":"Create a new folder as your workspace, e.g.: mkdir ~/eclipse-workspace Clone or copy the code you want to work with into the workspace, e.g.: cd ~/eclipse-workspace && git clone ... Start Eclipse! (Use the desktop entry or run ~/eclipse/eclipse ) When prompted, select the created workspace folder and Launch Click on Workbench at the top right","title":"Creating a workspace"},{"location":"eclipse/#configure-eclipse-for-developing-applications-using-scone","text":"We show the steps to integrate SCONE with Eclipse. We assume that you have already installed scone-gcc and scone-gdb one the machine your are now installing Eclipse: In Eclipse, select File > Import... > C/C++ > Existing Code as Makefile Project > Next Existing Code Location : Select <your-path-to-workspace>/<path_to_project>/<your_project> . Select Linux GCC as Toolchain Click Finish Right-click the <your_project> project, select Properties > C/C++ Build > Environment Add the following entries: CC=scone-gcc , CFLAGS=-g -Og , PATH=<your-build-directory>/built/bin (Tick Add to all configurations for PATH . Your regular PATH will be added automatically.) Apply and Close Press Ctrl+B to build. Both SCONE (if configured) and the test applications will be built. While the <your_project> is selected, Select Run > Debug configurations... Double-click C/C++ Application Change Name to <your_app> , C/C++ Application to target/<your_app> Switch to tab Environment and add all environment variables used by SCONE, example: Switch to tab Debugger and change GDB debugger to scone-gdb If the build-oot.sh script is used to build outside the source directory, it is advised to re-map the sources back to their original location for Debugging purposes: Switch to tab Source and Add... a Path Mapping with Name SCONE mapping , Add Compilation path <your-scone-build-directory> and Local file system path <your-scone-source-directory> (Selecting the local path is tricky, as a button seems to not be rendered correctly. Click on the empty space highlighted in the screenshot below) OK > Apply > Debug When asked, Switch view The debugger is launched and the application is halted in main (can be disabled in the Debugger settings of the launch configuration). You can show local variables, Ctr+Click to follow references, etc. as usual Continue with F8","title":"Configure Eclipse for developing applications using SCONE"},{"location":"encrypted_code/","text":"Encrypted Code SCONE supports encrypted code - in addition to encrypted main memory, encrypted files and encrypted communication. We show how to encrypt code in the context of SCONE.","title":"Encrypted Code"},{"location":"encrypted_code/#encrypted-code","text":"SCONE supports encrypted code - in addition to encrypted main memory, encrypted files and encrypted communication. We show how to encrypt code in the context of SCONE.","title":"Encrypted Code"},{"location":"faq/","text":"Frequently Asked Questions Attestation How can I verify the authenticity and integrity of my running running in enclave, if I access it remotely One approach is to store the encrypted certificate in an encrypted file region and SCONE CAS gives the service access to the file key only after a successful attestation. If the service can authenticate itself with the correct certificate, this means that it passed the attestation. How do I bind an interpreter (Python, JavaScript, Java) with its scripts/programs in a secure container? The problem is that the scripts/programs are not measured during the attestation of the enclave, i.e., it is not included in MrEnclave . This code must be protected with the SCONE file shield, i.e., this must be encrypted and/or authenticated. The current state of the file system is checked during attestation and this ensures the integrity of the code (i.e., no modification), the freshness (i.e., no old version is used), and the confidentiality (i.e., an adversary cannot see the code - in case the code is stored an encrypted file region). In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. How can one restrict the initial script that the protected interpreter executes? The arguments of the code executed inside of an enclave must be passed via SCONE CAS. In other words, the initial script is protected by CAS, i.e., by passing the arguments to Python only after attestation via a secure channel directly into the enclave. In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. This means that only code that is already include in an encrypted / authenticated file region can be executed. How can I pass secrets to my enclave? You could pass these secrets as environment variables via CAS or you can store these in encrypted files. SCONE will pass the environment variables and the file encryption key only after a successful attestation to the application via TLS. Simulation Mode Program crashes/stops in simulation mode Simulation mode assumes a modern CPU with instructions like AESNI and SSE and AVX extensions. If they are not available, your program will exit with an error message. Has my CPU sufficient features for simulation mode? Just run the following program to check if your CPU has sufficient features for simulation mode: docker run --rm sconecuratedimages/apps:check_cpuid Attestation of programs in simulation mode While we permit to run applications in simulation mode, i.e., to run on machines without an SGX-capable CPU, we do not support the attestation of programs in simulation mode. In case you want to use attestation, you need to run on an SGX-capable machine. Memory related issues My process/enclave is getting killed without any error message Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. This means that the enclave might request so much memory that the Linux Out-Of-Memory-Killer might kill the process in which this enclave runs. Ensure that you have enough free main memory such that your enclave can start. My machine has lots of main memory but still processes are getting killed Try to figure out - using tools like top - which processes are using up your memory. Sometimes, you might have too many active docker containers running: check this with docker ps -a . Try to run docker containers with docker run --rm to enforce automatic cleanup after a container exits. My program does not start up Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. We cannot estimate how much memory your application needs. Hence, for SGX version 1, we provide environment variables SCONE_HEAP and SCONE_STACK . For Java and GO programs, set SCONE_HEAP to at least 1G . The default heap size is 64MB. The default stack size is 64KBytes. This is too little for some applications like Python and MariaDB. A good value for applications that use lots of stack seem to be 4MBytes, i.e., set export SCONE_STACK=4M . My program has a very large VIRT memory footprint SCONE reserves 64GB of the virtual address space for each enclave using the SGX driver. Hence, when monitoring a process, e.g., with top , VIRT is reported as at least 64g . Note that the important measure is the physical memory used: top reports this in column RES (given in KiB)). Crash Failures My program crashes / gets killed SGX version 1: check that your program has a sufficiently large heap by setting environment variable SCONE_HEAP ( see above ) SGX version 1: check that your program has sufficiently large stacks by setting environment variable SCONE_STACK ( see above ). SGX version 1: check that you machine has sufficient main memory see above ) Run your program with scone-gdb to determine where your program crashes Running inside of enclaves How do I know that I run inside of an enclave? Set environment variable SCONE_VERSION=1 : you will see in what mode your program is running. How can I experimentally show that I run inside of an enclave? Please check out our memory dump tutorial on this. How can I enforce/verify that a service/program runs inside of an enclave? You need to attest that your program runs inside of an enclave. SCONE supports transparent attestation with the help of CAS . On the application level, one often does not want to perform an explicit attestation but an implicit attestation with the help of TLS to reduce/eliminate the amount of reengineering that is needed. The idea is that a service can only provide the correct TLS certificate if it runs inside an enclave. To do so, one would give the enclave an encrypted TLS private key in the file system (can be generate by Scone CAS if this is requested) and the enclave gets only access to the encryption key after a successful attestation. The decryption of the TLS private key is done transparently by SCONE. Does SCONE support enclaves in production mode? Note that by default SCONE runs in debug or pre-release mode. For production enclaves, you still need to get an enclave signer key from Intel. This will change when Intel makes flexible launch control available. Shielding Does SCONE ensure the security of incoming and outgoing TCP connections to/from a service running in an enclave? Please use TLS in your service. If your service does not support TLS out of the box, use our TCP shield. How do I encrypt stdin/stdout/stderr? Send us an email on how to use the terminal shield How do I encrypt pipes? Send us an email regarding the encryption of pipes. How do I encrypt TCP connections? Context: Services like memcached do not support TLS out of the box. Send us an email on how to use the TCP shield. How do I transparently encrypt/decrypt files? Enable the file shield. Are all files in a container encrypted? No, only those in an encrypted file region. What do I need to do to protect the files my service needs, e.g. HTTPS encryption key and certificate You need to encrypt them when you build you container image. Dynamic Libraries What are protected dynamic libraries? The dynamic libraries loaded during start up of program are included in the hash of the enclave, i.e., any modification of any of these libraries will change MrEnclave . In that sense, the dynamic libraries are integrity protected since any modifications will result in a failed attestation. One can determine the dynamic libraries which are loaded during startup with command ldd for native binaries. Note that depending on how the binary is build (static linking, dynamic linking, for Alpine Linux or for Ubuntu), ldd might only print the dynamic libraries used to start the enclave. To enable the loading of protected dynamic libraries , i.e., encrypted or authenticated shared libraries, set environment variable SCONE_ALLOW_DLOPEN=1 in your session policy. If you want to disallow this, do not define SCONE_ALLOW_DLOPEN . What are unprotected dynamic libraries? An unprotected shared library , i.e., a shared library that is neither encrypted nor authenticated by the filesystem shield). It is loaded after the program has started by the application with a call to function dlopen (or similar). These libraries are not integrity protected by MrEnclave since they are loaded after MrEnclave was computed. To ensure the integrity of these shared library they have to be located in an authenticated or an encrypted file region. To enable the loading of unprotected dynamic libraries after startup, set environment variable SCONE_ALLOW_DLOPEN=2 . This will also enable loading libraries during startup. Never use SCONE_ALLOW_DLOPEN=2 in production mode . Encrypted Code and Libraries One can encrypt code by pushing your main code in a shared library that is stored in an encrypted file region. In this way, you can protect the integrity as well as the confidentiality of your code. Library Path If you experience undefined symbols like __cxa_init_primary_exception , ensure that your library path ( LD_LIBRARY_PATH ) is set such that if finds the SCONE libraries. For example, inside our SCONE crosscompiler container, you might set export LD_LIBRARY_PATH = /opt/scone/cross-compiler/x86_64-linux-musl/lib/ Resource Usage CPU utilization / number of threads is higher than in native mode By default, we use multiple threads to serve inside the enclave and to serve system calls outside the enclave. If theses threads have no work to do, they go to sleep. You can reduce the number of threads / CPU utilization by specifying a SCONE configuration file which uses less threads. For example, you could use one thread inside the enclave and one outside the enclave with this configuration file: cat > /etc/sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF The CPU utilization is still higher than in native mode In our experience, then newest version of SCONE needs less than 1-2% CPU utilization when a service is idling. If the CPU utilization of a service is higher than the native version during idle periods, you could try to tune the the backoff behavior of the queues by setting parameters L and P appropriately. Note that the standard values should in most cases do not need any tuning. The memory usage is higher than in native mode The issue is that SGX v1 enclaves must allocate all memory at startup since enclaves are fixed (- this will change with SGXv2). You can reduce memory consumption by setting SCONE_HEAP and SCONE_STACK . In SGX v2 we will allocate memory on demand, i.e., we will be more memory efficient. Note that in SGX v2, we will be able to dynamically adjust the size of the heap and the stack sizes automatically, i.e., one does not need to allocate all memory in the beginning. This will also reduce the startup times. High startup times Since in SGX v1 one needs to allocate all memory at the start of an enclave, startup times can be very large. In SGX v2, we will be able to dynamically scale the size of an enclave during runtime. Since we will allocate less memory during startup time, this will reduce the startup times. Side-Channel Protection The newest microcode of new Intel CPUs protects against L1TF side channels when hyperthreading is disabled. Please ensure that your CPU microcode is up-to-date: You can follow the following instruction to update the microcode of your CPU . Unimplemented Functions Some function returns ENOSYS Sometimes it is difficult to diagnose why a function fails. In most cases, the issue is that we do not yet support fork . You can check which functions might not be supported by running your application with environment variable SCONE_LOG=7 . Driver Issues No Access to SGX device Ensure that you installed the Intel SGX driver on your machine and that the SGX driver is accessible in your container .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#attestation","text":"","title":"Attestation"},{"location":"faq/#how-can-i-verify-the-authenticity-and-integrity-of-my-running-running-in-enclave-if-i-access-it-remotely","text":"One approach is to store the encrypted certificate in an encrypted file region and SCONE CAS gives the service access to the file key only after a successful attestation. If the service can authenticate itself with the correct certificate, this means that it passed the attestation.","title":"How can I verify the authenticity and integrity of my running running in enclave, if I access it remotely"},{"location":"faq/#how-do-i-bind-an-interpreter-python-javascript-java-with-its-scriptsprograms-in-a-secure-container","text":"The problem is that the scripts/programs are not measured during the attestation of the enclave, i.e., it is not included in MrEnclave . This code must be protected with the SCONE file shield, i.e., this must be encrypted and/or authenticated. The current state of the file system is checked during attestation and this ensures the integrity of the code (i.e., no modification), the freshness (i.e., no old version is used), and the confidentiality (i.e., an adversary cannot see the code - in case the code is stored an encrypted file region). In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail.","title":"How do I bind an interpreter (Python, JavaScript, Java) with its scripts/programs in a secure container?"},{"location":"faq/#how-can-one-restrict-the-initial-script-that-the-protected-interpreter-executes","text":"The arguments of the code executed inside of an enclave must be passed via SCONE CAS. In other words, the initial script is protected by CAS, i.e., by passing the arguments to Python only after attestation via a secure channel directly into the enclave. In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. This means that only code that is already include in an encrypted / authenticated file region can be executed.","title":"How can one restrict the initial script that the protected interpreter executes?"},{"location":"faq/#how-can-i-pass-secrets-to-my-enclave","text":"You could pass these secrets as environment variables via CAS or you can store these in encrypted files. SCONE will pass the environment variables and the file encryption key only after a successful attestation to the application via TLS.","title":"How can I pass secrets to my enclave?"},{"location":"faq/#simulation-mode","text":"","title":"Simulation Mode"},{"location":"faq/#program-crashesstops-in-simulation-mode","text":"Simulation mode assumes a modern CPU with instructions like AESNI and SSE and AVX extensions. If they are not available, your program will exit with an error message.","title":"Program crashes/stops in simulation mode"},{"location":"faq/#has-my-cpu-sufficient-features-for-simulation-mode","text":"Just run the following program to check if your CPU has sufficient features for simulation mode: docker run --rm sconecuratedimages/apps:check_cpuid","title":"Has my CPU sufficient features for simulation mode?"},{"location":"faq/#attestation-of-programs-in-simulation-mode","text":"While we permit to run applications in simulation mode, i.e., to run on machines without an SGX-capable CPU, we do not support the attestation of programs in simulation mode. In case you want to use attestation, you need to run on an SGX-capable machine.","title":"Attestation of programs in simulation mode"},{"location":"faq/#memory-related-issues","text":"","title":"Memory related issues"},{"location":"faq/#my-processenclave-is-getting-killed-without-any-error-message","text":"Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. This means that the enclave might request so much memory that the Linux Out-Of-Memory-Killer might kill the process in which this enclave runs. Ensure that you have enough free main memory such that your enclave can start.","title":"My process/enclave is getting killed without any error message"},{"location":"faq/#my-machine-has-lots-of-main-memory-but-still-processes-are-getting-killed","text":"Try to figure out - using tools like top - which processes are using up your memory. Sometimes, you might have too many active docker containers running: check this with docker ps -a . Try to run docker containers with docker run --rm to enforce automatic cleanup after a container exits.","title":"My machine has lots of main memory but still processes are getting killed"},{"location":"faq/#my-program-does-not-start-up","text":"Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. We cannot estimate how much memory your application needs. Hence, for SGX version 1, we provide environment variables SCONE_HEAP and SCONE_STACK . For Java and GO programs, set SCONE_HEAP to at least 1G . The default heap size is 64MB. The default stack size is 64KBytes. This is too little for some applications like Python and MariaDB. A good value for applications that use lots of stack seem to be 4MBytes, i.e., set export SCONE_STACK=4M .","title":"My program does not start up"},{"location":"faq/#my-program-has-a-very-large-virt-memory-footprint","text":"SCONE reserves 64GB of the virtual address space for each enclave using the SGX driver. Hence, when monitoring a process, e.g., with top , VIRT is reported as at least 64g . Note that the important measure is the physical memory used: top reports this in column RES (given in KiB)).","title":"My program has a very large VIRT memory footprint"},{"location":"faq/#crash-failures","text":"","title":"Crash Failures"},{"location":"faq/#my-program-crashes-gets-killed","text":"SGX version 1: check that your program has a sufficiently large heap by setting environment variable SCONE_HEAP ( see above ) SGX version 1: check that your program has sufficiently large stacks by setting environment variable SCONE_STACK ( see above ). SGX version 1: check that you machine has sufficient main memory see above ) Run your program with scone-gdb to determine where your program crashes","title":"My program crashes / gets killed"},{"location":"faq/#running-inside-of-enclaves","text":"","title":"Running inside of enclaves"},{"location":"faq/#how-do-i-know-that-i-run-inside-of-an-enclave","text":"Set environment variable SCONE_VERSION=1 : you will see in what mode your program is running.","title":"How do I know that I run inside of an enclave?"},{"location":"faq/#how-can-i-experimentally-show-that-i-run-inside-of-an-enclave","text":"Please check out our memory dump tutorial on this.","title":"How can I experimentally show that I run inside of an enclave?"},{"location":"faq/#how-can-i-enforceverify-that-a-serviceprogram-runs-inside-of-an-enclave","text":"You need to attest that your program runs inside of an enclave. SCONE supports transparent attestation with the help of CAS . On the application level, one often does not want to perform an explicit attestation but an implicit attestation with the help of TLS to reduce/eliminate the amount of reengineering that is needed. The idea is that a service can only provide the correct TLS certificate if it runs inside an enclave. To do so, one would give the enclave an encrypted TLS private key in the file system (can be generate by Scone CAS if this is requested) and the enclave gets only access to the encryption key after a successful attestation. The decryption of the TLS private key is done transparently by SCONE.","title":"How can I enforce/verify that a service/program runs inside of an enclave?"},{"location":"faq/#does-scone-support-enclaves-in-production-mode","text":"Note that by default SCONE runs in debug or pre-release mode. For production enclaves, you still need to get an enclave signer key from Intel. This will change when Intel makes flexible launch control available.","title":"Does SCONE support enclaves in production mode?"},{"location":"faq/#shielding","text":"","title":"Shielding"},{"location":"faq/#does-scone-ensure-the-security-of-incoming-and-outgoing-tcp-connections-tofrom-a-service-running-in-an-enclave","text":"Please use TLS in your service. If your service does not support TLS out of the box, use our TCP shield.","title":"Does SCONE ensure the security of incoming and outgoing TCP connections to/from a service running in an enclave?"},{"location":"faq/#how-do-i-encrypt-stdinstdoutstderr","text":"Send us an email on how to use the terminal shield","title":"How do I encrypt stdin/stdout/stderr?"},{"location":"faq/#how-do-i-encrypt-pipes","text":"Send us an email regarding the encryption of pipes.","title":"How do I encrypt pipes?"},{"location":"faq/#how-do-i-encrypt-tcp-connections","text":"Context: Services like memcached do not support TLS out of the box. Send us an email on how to use the TCP shield.","title":"How do I encrypt TCP connections?"},{"location":"faq/#how-do-i-transparently-encryptdecrypt-files","text":"Enable the file shield.","title":"How do I transparently encrypt/decrypt files?"},{"location":"faq/#are-all-files-in-a-container-encrypted","text":"No, only those in an encrypted file region.","title":"Are all files in a container encrypted?"},{"location":"faq/#what-do-i-need-to-do-to-protect-the-files-my-service-needs-eg-https-encryption-key-and-certificate","text":"You need to encrypt them when you build you container image.","title":"What do I need to do to protect the files my service needs, e.g. HTTPS encryption key and certificate"},{"location":"faq/#dynamic-libraries","text":"","title":"Dynamic Libraries"},{"location":"faq/#what-are-protected-dynamic-libraries","text":"The dynamic libraries loaded during start up of program are included in the hash of the enclave, i.e., any modification of any of these libraries will change MrEnclave . In that sense, the dynamic libraries are integrity protected since any modifications will result in a failed attestation. One can determine the dynamic libraries which are loaded during startup with command ldd for native binaries. Note that depending on how the binary is build (static linking, dynamic linking, for Alpine Linux or for Ubuntu), ldd might only print the dynamic libraries used to start the enclave. To enable the loading of protected dynamic libraries , i.e., encrypted or authenticated shared libraries, set environment variable SCONE_ALLOW_DLOPEN=1 in your session policy. If you want to disallow this, do not define SCONE_ALLOW_DLOPEN .","title":"What are protected dynamic libraries?"},{"location":"faq/#what-are-unprotected-dynamic-libraries","text":"An unprotected shared library , i.e., a shared library that is neither encrypted nor authenticated by the filesystem shield). It is loaded after the program has started by the application with a call to function dlopen (or similar). These libraries are not integrity protected by MrEnclave since they are loaded after MrEnclave was computed. To ensure the integrity of these shared library they have to be located in an authenticated or an encrypted file region. To enable the loading of unprotected dynamic libraries after startup, set environment variable SCONE_ALLOW_DLOPEN=2 . This will also enable loading libraries during startup. Never use SCONE_ALLOW_DLOPEN=2 in production mode .","title":"What are unprotected dynamic libraries?"},{"location":"faq/#encrypted-code-and-libraries","text":"One can encrypt code by pushing your main code in a shared library that is stored in an encrypted file region. In this way, you can protect the integrity as well as the confidentiality of your code.","title":"Encrypted Code and Libraries"},{"location":"faq/#library-path","text":"If you experience undefined symbols like __cxa_init_primary_exception , ensure that your library path ( LD_LIBRARY_PATH ) is set such that if finds the SCONE libraries. For example, inside our SCONE crosscompiler container, you might set export LD_LIBRARY_PATH = /opt/scone/cross-compiler/x86_64-linux-musl/lib/","title":"Library Path"},{"location":"faq/#resource-usage","text":"","title":"Resource Usage"},{"location":"faq/#cpu-utilization-number-of-threads-is-higher-than-in-native-mode","text":"By default, we use multiple threads to serve inside the enclave and to serve system calls outside the enclave. If theses threads have no work to do, they go to sleep. You can reduce the number of threads / CPU utilization by specifying a SCONE configuration file which uses less threads. For example, you could use one thread inside the enclave and one outside the enclave with this configuration file: cat > /etc/sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF","title":"CPU utilization / number of threads is higher than in native mode"},{"location":"faq/#the-cpu-utilization-is-still-higher-than-in-native-mode","text":"In our experience, then newest version of SCONE needs less than 1-2% CPU utilization when a service is idling. If the CPU utilization of a service is higher than the native version during idle periods, you could try to tune the the backoff behavior of the queues by setting parameters L and P appropriately. Note that the standard values should in most cases do not need any tuning.","title":"The CPU utilization is still higher than in native mode"},{"location":"faq/#the-memory-usage-is-higher-than-in-native-mode","text":"The issue is that SGX v1 enclaves must allocate all memory at startup since enclaves are fixed (- this will change with SGXv2). You can reduce memory consumption by setting SCONE_HEAP and SCONE_STACK . In SGX v2 we will allocate memory on demand, i.e., we will be more memory efficient. Note that in SGX v2, we will be able to dynamically adjust the size of the heap and the stack sizes automatically, i.e., one does not need to allocate all memory in the beginning. This will also reduce the startup times.","title":"The memory usage is higher than in native mode"},{"location":"faq/#high-startup-times","text":"Since in SGX v1 one needs to allocate all memory at the start of an enclave, startup times can be very large. In SGX v2, we will be able to dynamically scale the size of an enclave during runtime. Since we will allocate less memory during startup time, this will reduce the startup times.","title":"High startup times"},{"location":"faq/#side-channel-protection","text":"The newest microcode of new Intel CPUs protects against L1TF side channels when hyperthreading is disabled. Please ensure that your CPU microcode is up-to-date: You can follow the following instruction to update the microcode of your CPU .","title":"Side-Channel Protection"},{"location":"faq/#unimplemented-functions","text":"","title":"Unimplemented Functions"},{"location":"faq/#some-function-returns-enosys","text":"Sometimes it is difficult to diagnose why a function fails. In most cases, the issue is that we do not yet support fork . You can check which functions might not be supported by running your application with environment variable SCONE_LOG=7 .","title":"Some function returns ENOSYS"},{"location":"faq/#driver-issues","text":"","title":"Driver Issues"},{"location":"faq/#no-access-to-sgx-device","text":"Ensure that you installed the Intel SGX driver on your machine and that the SGX driver is accessible in your container .","title":"No Access to SGX device"},{"location":"firstcontainer/","text":"Creating Your First SCONE program Hello World in Simulation Mode Let's start with a simple hello world program that we run inside a container on top of SCONE. We first need to start the SCONE crosscompiler. The crosscompiler container image is hosted in a private repository on Docker hub and can be started with the help of docker: docker run -it sconecuratedimages/crosscompilers A docker engine must be installed and you need access to sconecuratedimages/crosscompilers You need to install a docker engine . In some docker installations, you might have to replace \"docker\" by \"sudo docker\". Send us an email to get access to the image. Even in simulation mode, we require some CPU features. Please ensure that your CPU has the right features by executing: docker run --rm sconecuratedimages/apps:check_cpuid This should output (amongst other messages): CPU has all features to run SCONE in Simulation Mode. Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with the SCONE crosscompiler (i.e., gcc): gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we did not give the container access to SGX, the program runs in simulation mode , i.e., the SCONE software runs but we do not use Intel SGX enclaves. Use simulation mode only for development and debugging This mode must not be used for production since programs do not run inside of enclaves . Simulation mode will run on modern Intel CPUs - even those without Intel SGX. It might, however, fail on old CPUs without AES hardware support. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 501194b1da9d4e86828353349cc7f9ef310b0dd1 Enclave hash: a01127f2190ed5ecd21f9fd432e4d07f7f250ad1e1808d9c0305e75505383c44 Hello World The output shows that SCONE is running in simulation mode : export SCONE_MODE=sim Background Info The most convenient way to use SCONE for development is to enable automatic (a.k.a. AUTO ) mode 1 . In AUTO mode, you neither need access to SGX-capable CPUs nor do you need to install any new software on your host: you only need to have access to a Docker engine. If you have access to an SGX-capable CPU and you give the container access to the sgx device, SCONE will run applications inside of SGX enclaves. Otherwise, the applications will run in simulation mode. Let's see in the next chapter how to run the hello world program inside an Intel SGX enclave. This is the default mode: see description of environment variable SCONE_MODE . \u21a9 Just send an email with your free Docker ID to info@scontain.com . \u21a9","title":"Simulation Mode"},{"location":"firstcontainer/#creating-your-first-scone-program","text":"","title":"Creating Your First SCONE program"},{"location":"firstcontainer/#hello-world-in-simulation-mode","text":"Let's start with a simple hello world program that we run inside a container on top of SCONE. We first need to start the SCONE crosscompiler. The crosscompiler container image is hosted in a private repository on Docker hub and can be started with the help of docker: docker run -it sconecuratedimages/crosscompilers A docker engine must be installed and you need access to sconecuratedimages/crosscompilers You need to install a docker engine . In some docker installations, you might have to replace \"docker\" by \"sudo docker\". Send us an email to get access to the image. Even in simulation mode, we require some CPU features. Please ensure that your CPU has the right features by executing: docker run --rm sconecuratedimages/apps:check_cpuid This should output (amongst other messages): CPU has all features to run SCONE in Simulation Mode. Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with the SCONE crosscompiler (i.e., gcc): gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we did not give the container access to SGX, the program runs in simulation mode , i.e., the SCONE software runs but we do not use Intel SGX enclaves. Use simulation mode only for development and debugging This mode must not be used for production since programs do not run inside of enclaves . Simulation mode will run on modern Intel CPUs - even those without Intel SGX. It might, however, fail on old CPUs without AES hardware support. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 501194b1da9d4e86828353349cc7f9ef310b0dd1 Enclave hash: a01127f2190ed5ecd21f9fd432e4d07f7f250ad1e1808d9c0305e75505383c44 Hello World The output shows that SCONE is running in simulation mode : export SCONE_MODE=sim","title":"Hello World in Simulation Mode"},{"location":"firstcontainer/#background-info","text":"The most convenient way to use SCONE for development is to enable automatic (a.k.a. AUTO ) mode 1 . In AUTO mode, you neither need access to SGX-capable CPUs nor do you need to install any new software on your host: you only need to have access to a Docker engine. If you have access to an SGX-capable CPU and you give the container access to the sgx device, SCONE will run applications inside of SGX enclaves. Otherwise, the applications will run in simulation mode. Let's see in the next chapter how to run the hello world program inside an Intel SGX enclave. This is the default mode: see description of environment variable SCONE_MODE . \u21a9 Just send an email with your free Docker ID to info@scontain.com . \u21a9","title":"Background Info"},{"location":"glossary/","text":"Glossary attestation Process of proving the integrity and authenticity of the attestee's software and/or hardware component to a verifier. This includes validating if a given software is executed on a given hardware. local attestation Attestation executed locally, e.g., one software component validates the integrity and authenticity of another software component which is executed on the same hardware. In Intel SGX, the CPU creates a report containing integrity information of the attested enclave whose keyed-MAC can only be verified, and changed for that matter, by the verifier enclave running on the same platform. remote attestation Attestation executed remotely, i.e., the component which does the validation and the component which is validated are executed on different machines. In Intel SGX, the report received during local attestation is signed with the quoting enclave's private key making the integrity of the quote - the signed report - remotely verifiable. mutual attestation Process of two components proving to each other the integrity and authenticity of their software and/or hardware components. This can include multiple aspects like that neither their code nor their filesystem was modified. Configuration and Attestation Service (CAS) The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client. SCONE CAS The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client. Local Attestation Service (LAS) A per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave. SCONE LAS The SCONE Local Attestation Service (LAS) is a per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave. secure boot A boot procedure which allows only the execution of firmware, bootloaders and operating systems which are digitally signed by a (well) defined set of acceptable signers. measured boot A boot procedure which measures the state of the system at each boot step. This measurement can be accessed to verify the current state of a given system. Compared to secure boot, measured boot will not prevent an \"insecure\" state of the system. cloud-native application An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers. cloud provider An entity providing cloud services (PaaS, IaaS, MaaS etc.) to its customers. It is assumed that a cloud provider is in physical or logical control of the hardware and system software used to provide the cloud services. container An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions. Kubernetes An open source container orchestration platform. More information at https://kubernetes.io/ KubeApps A dashboard to deploy and manage Kubernetes based applications. It can be used to deploy and manage SCONE-based confidential applications. curated image A container image of a popular service maintained by scontain.com. enclave This is an alias for SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC. Docker image A snapshot of a container's state that can be used to initialize new containers with the same state. Docker registry A Docker registry stores Docker images for the purpose of easy distribution comparable to the app stores of Android or iOS. EPC A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads. microservice A rather small component which offers a single service. service provider A company operating an application - typically, making these available via the Internet. We use this a general term that includes different models like Software as a Service (SaaS) providers as well as Hosted service providers etc. SCONE SCONE (Secure CONtainer Environment) is a software platform for confidential computing allowing the trustworthy execution of unmodified x86 source code within Intel SGX enclaves. It consists of components enabling the execution inside enclaves such as the SCONE runtime and the C, C++, C#, Fortran, Go, and Rust SCONE cross-compilers, components ensuring the trustworthiness of this execution and deployment in clouds such as the CAS and SCONE client. SCONE Docker image A SCONE Docker image is a Docker image that contains an SCONE-enabled executable and is additionally annotated via image labels with metadata allowing the attestation of the started SCONE-enabled executable and the image's file system content. SCONE container A SCONE container is a running instance of a SCONE Docker image. SCONE microservice Microservice which is a SCONE-enabled executable. SCONE-enabled executable An executable created by a SCONE cross-compiler. The actual program will be executed within an enclave and utilises the SCONE runtime. SCONE runtime The runtime environment necessary to execute a SCONE-enabled executable. At the moment this consists of a modified C-library based on the musl library. SCONE cross-compiler Compilers for various programming languages such as C, C++, Rust, Go, and Fortran which compile source code into a SCONE-enabled executable. SCONE client A program that is used to configure SCONE-enabled executables. It allows the user to push confidential configurations to the CAS and encrypt files to ensure their content is only accessible by specific SCONE-enabled executables executed inside enclaves. SCONE infrastructure The SCONE infrastructure summarises all components necessary to deploy and run a SCONE-enabled executable. This includes Docker components like the Docker daemon and the Docker registry as well as the SCONE client and additional services like CAS and LAS. Secure container A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE. Secure program A program that executes inside an enclave. SGX (Software Guard eXtension) A CPU extension by Intel that permits to create SGX enclaves. SGX enclave A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC. threading SCONE uses different kind of threads: ethread : a thread that executes application threads inside of an enclave lthread : an application thread. Typically, created by the application directly or indirectly via a pthread_create call. In SCONE, this pthread_create call will create a lthread . The lthread is executed by some ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly. sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave trusted computing base (TCB) The set of hardware and software components which can break the security policy. Therefore one has to trust that these components are not malicious or faulty. trusted execution environment A trusted execution environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity. TEE A Trusted Execution Environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity. release mode In SGX we distinguish between debug, pre-release and release mode. In release mode, the enclave will be launched in production mode, i.e., any access - including debug access - will be prevented. Moreover, the code is compiled with optimizations switched on and without debug symbols. CLI A Command Line Interface (CLI) is a textual interface to a computer. It is useful for scripting (i.e., automation) and typically, used for system and service administration. CC Confidential computing is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest. Confidential Computing Confidential computing (CC) is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest. vanilla In computer science, a system and or a software is called vanilla when there is no need to customize it for a certain use case. confidentiality In the context of information security the term confidentiality means that information is only made available to authorized entities. In the context of SCONE this means that information (data, code, secrets, and policies) are encrypted and the encryption key is only accessible to authorized entities. Authorization is defined in a SCONE CAS policy and access to this policy is also defined in the policy itself. integrity In the context of information security the term integrity means that one ensures the accuracy and completeness of information. Integrity implies freshness, i.e., that one reads the last data that was stored. While the freshness of SGX enclaves are ensured by hardware and in transit it is ensured by TLS, the freshness of data at rest is explicitly enforced by SCONE. MRENCLAVE An enclave is identified by a hash value which is called MrEnclave. This hash value is determined by the initial content of the pages of an enclave and their access rights. This means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave.","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#attestation","text":"Process of proving the integrity and authenticity of the attestee's software and/or hardware component to a verifier. This includes validating if a given software is executed on a given hardware.","title":"attestation"},{"location":"glossary/#local-attestation","text":"Attestation executed locally, e.g., one software component validates the integrity and authenticity of another software component which is executed on the same hardware. In Intel SGX, the CPU creates a report containing integrity information of the attested enclave whose keyed-MAC can only be verified, and changed for that matter, by the verifier enclave running on the same platform.","title":"local attestation"},{"location":"glossary/#remote-attestation","text":"Attestation executed remotely, i.e., the component which does the validation and the component which is validated are executed on different machines. In Intel SGX, the report received during local attestation is signed with the quoting enclave's private key making the integrity of the quote - the signed report - remotely verifiable.","title":"remote attestation"},{"location":"glossary/#mutual-attestation","text":"Process of two components proving to each other the integrity and authenticity of their software and/or hardware components. This can include multiple aspects like that neither their code nor their filesystem was modified.","title":"mutual attestation"},{"location":"glossary/#configuration-and-attestation-service-cas","text":"The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client.","title":"Configuration and Attestation Service (CAS)"},{"location":"glossary/#scone-cas","text":"The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client.","title":"SCONE CAS"},{"location":"glossary/#local-attestation-service-las","text":"A per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave.","title":"Local Attestation Service (LAS)"},{"location":"glossary/#scone-las","text":"The SCONE Local Attestation Service (LAS) is a per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave.","title":"SCONE LAS"},{"location":"glossary/#secure-boot","text":"A boot procedure which allows only the execution of firmware, bootloaders and operating systems which are digitally signed by a (well) defined set of acceptable signers.","title":"secure boot"},{"location":"glossary/#measured-boot","text":"A boot procedure which measures the state of the system at each boot step. This measurement can be accessed to verify the current state of a given system. Compared to secure boot, measured boot will not prevent an \"insecure\" state of the system.","title":"measured boot"},{"location":"glossary/#cloud-native-application","text":"An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers.","title":"cloud-native application"},{"location":"glossary/#cloud-provider","text":"An entity providing cloud services (PaaS, IaaS, MaaS etc.) to its customers. It is assumed that a cloud provider is in physical or logical control of the hardware and system software used to provide the cloud services.","title":"cloud provider"},{"location":"glossary/#container","text":"An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions.","title":"container"},{"location":"glossary/#kubernetes","text":"An open source container orchestration platform. More information at https://kubernetes.io/","title":"Kubernetes"},{"location":"glossary/#kubeapps","text":"A dashboard to deploy and manage Kubernetes based applications. It can be used to deploy and manage SCONE-based confidential applications.","title":"KubeApps"},{"location":"glossary/#curated-image","text":"A container image of a popular service maintained by scontain.com.","title":"curated image"},{"location":"glossary/#enclave","text":"This is an alias for SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.","title":"enclave"},{"location":"glossary/#docker-image","text":"A snapshot of a container's state that can be used to initialize new containers with the same state.","title":"Docker image"},{"location":"glossary/#docker-registry","text":"A Docker registry stores Docker images for the purpose of easy distribution comparable to the app stores of Android or iOS.","title":"Docker registry"},{"location":"glossary/#epc","text":"A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads.","title":"EPC"},{"location":"glossary/#microservice","text":"A rather small component which offers a single service.","title":"microservice"},{"location":"glossary/#service-provider","text":"A company operating an application - typically, making these available via the Internet. We use this a general term that includes different models like Software as a Service (SaaS) providers as well as Hosted service providers etc.","title":"service provider"},{"location":"glossary/#scone","text":"SCONE (Secure CONtainer Environment) is a software platform for confidential computing allowing the trustworthy execution of unmodified x86 source code within Intel SGX enclaves. It consists of components enabling the execution inside enclaves such as the SCONE runtime and the C, C++, C#, Fortran, Go, and Rust SCONE cross-compilers, components ensuring the trustworthiness of this execution and deployment in clouds such as the CAS and SCONE client.","title":"SCONE"},{"location":"glossary/#scone-docker-image","text":"A SCONE Docker image is a Docker image that contains an SCONE-enabled executable and is additionally annotated via image labels with metadata allowing the attestation of the started SCONE-enabled executable and the image's file system content.","title":"SCONE Docker image"},{"location":"glossary/#scone-container","text":"A SCONE container is a running instance of a SCONE Docker image.","title":"SCONE container"},{"location":"glossary/#scone-microservice","text":"Microservice which is a SCONE-enabled executable.","title":"SCONE microservice"},{"location":"glossary/#scone-enabled-executable","text":"An executable created by a SCONE cross-compiler. The actual program will be executed within an enclave and utilises the SCONE runtime.","title":"SCONE-enabled executable"},{"location":"glossary/#scone-runtime","text":"The runtime environment necessary to execute a SCONE-enabled executable. At the moment this consists of a modified C-library based on the musl library.","title":"SCONE runtime"},{"location":"glossary/#scone-cross-compiler","text":"Compilers for various programming languages such as C, C++, Rust, Go, and Fortran which compile source code into a SCONE-enabled executable.","title":"SCONE cross-compiler"},{"location":"glossary/#scone-client","text":"A program that is used to configure SCONE-enabled executables. It allows the user to push confidential configurations to the CAS and encrypt files to ensure their content is only accessible by specific SCONE-enabled executables executed inside enclaves.","title":"SCONE client"},{"location":"glossary/#scone-infrastructure","text":"The SCONE infrastructure summarises all components necessary to deploy and run a SCONE-enabled executable. This includes Docker components like the Docker daemon and the Docker registry as well as the SCONE client and additional services like CAS and LAS.","title":"SCONE infrastructure"},{"location":"glossary/#secure-container","text":"A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE.","title":"Secure container"},{"location":"glossary/#secure-program","text":"A program that executes inside an enclave.","title":"Secure program"},{"location":"glossary/#sgx-software-guard-extension","text":"A CPU extension by Intel that permits to create SGX enclaves.","title":"SGX (Software Guard eXtension)"},{"location":"glossary/#sgx-enclave","text":"A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.","title":"SGX enclave"},{"location":"glossary/#threading","text":"SCONE uses different kind of threads: ethread : a thread that executes application threads inside of an enclave lthread : an application thread. Typically, created by the application directly or indirectly via a pthread_create call. In SCONE, this pthread_create call will create a lthread . The lthread is executed by some ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly. sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave","title":"threading"},{"location":"glossary/#trusted-computing-base-tcb","text":"The set of hardware and software components which can break the security policy. Therefore one has to trust that these components are not malicious or faulty.","title":"trusted computing base (TCB)"},{"location":"glossary/#trusted-execution-environment","text":"A trusted execution environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity.","title":"trusted execution environment"},{"location":"glossary/#tee","text":"A Trusted Execution Environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity.","title":"TEE"},{"location":"glossary/#release-mode","text":"In SGX we distinguish between debug, pre-release and release mode. In release mode, the enclave will be launched in production mode, i.e., any access - including debug access - will be prevented. Moreover, the code is compiled with optimizations switched on and without debug symbols.","title":"release mode"},{"location":"glossary/#cli","text":"A Command Line Interface (CLI) is a textual interface to a computer. It is useful for scripting (i.e., automation) and typically, used for system and service administration.","title":"CLI"},{"location":"glossary/#cc","text":"Confidential computing is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest.","title":"CC"},{"location":"glossary/#confidential-computing","text":"Confidential computing (CC) is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest.","title":"Confidential Computing"},{"location":"glossary/#vanilla","text":"In computer science, a system and or a software is called vanilla when there is no need to customize it for a certain use case.","title":"vanilla"},{"location":"glossary/#confidentiality","text":"In the context of information security the term confidentiality means that information is only made available to authorized entities. In the context of SCONE this means that information (data, code, secrets, and policies) are encrypted and the encryption key is only accessible to authorized entities. Authorization is defined in a SCONE CAS policy and access to this policy is also defined in the policy itself.","title":"confidentiality"},{"location":"glossary/#integrity","text":"In the context of information security the term integrity means that one ensures the accuracy and completeness of information. Integrity implies freshness, i.e., that one reads the last data that was stored. While the freshness of SGX enclaves are ensured by hardware and in transit it is ensured by TLS, the freshness of data at rest is explicitly enforced by SCONE.","title":"integrity"},{"location":"glossary/#mrenclave","text":"An enclave is identified by a hash value which is called MrEnclave. This hash value is determined by the initial content of the pages of an enclave and their access rights. This means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave.","title":"MRENCLAVE"},{"location":"groupcacheUseCase/","text":"groupcache Example groupcache is a memcached-like library written in GO: it implements a peer-to-peer caching service. We use groupcache to show how to build a little more complex GO program with SCONE. Typically, one would build a container image for groupcache with a Dockerfile . Since our emphasis is to explain how to build such programs, we first show the individual steps to compile and execute groupcache and second, please read how to build container images with a multi-stage build . Note You might want to read how to compile GO programs with SCONE first. Building groupcache - without shielding First, start a crosscompiler container. Give it access to the sgx device if your host has this device - otherwise, just drop argument --device=/dev/isgx : docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Second, install the go command to simplify the building of groupcache : apk update apk add go git curl Now you can build the groupcache library as follows: go get -compiler gccgo -u github.com/golang/groupcache Note that flag -compiler gccgo is required to ensure that the scone-gccgo is used to compile groupcache . That's it! Example OK, we should show how to use group cache. We show this for a simple application from fiorix : cat > groupcache.go << EOF // Simple groupcache example: https://github.com/golang/groupcache // Running 3 instances: // go run groupcache.go -addr=:8080 -pool=http://127.0.0.1:8080,http://127.0.0.1:8081,http://127.0.0.1:8082 // go run groupcache.go -addr=:8081 -pool=http://127.0.0.1:8081,http://127.0.0.1:8080,http://127.0.0.1:8082 // go run groupcache.go -addr=:8082 -pool=http://127.0.0.1:8082,http://127.0.0.1:8080,http://127.0.0.1:8081 // Testing: // curl localhost:8080/color?name=red package main import ( \"errors\" \"flag\" \"log\" \"net/http\" \"strings\" \"github.com/golang/groupcache\" ) var Store = map[string][]byte{ \"red\": []byte(\"#FF0000\"), \"green\": []byte(\"#00FF00\"), \"blue\": []byte(\"#0000FF\"), } var Group = groupcache.NewGroup(\"foobar\", 64<<20, groupcache.GetterFunc( func(ctx groupcache.Context, key string, dest groupcache.Sink) error { log.Println(\"looking up\", key) v, ok := Store[key] if !ok { return errors.New(\"color not found\") } dest.SetBytes(v) return nil }, )) func main() { addr := flag.String(\"addr\", \":8080\", \"server address\") peers := flag.String(\"pool\", \"http://localhost:8080\", \"server pool list\") flag.Parse() http.HandleFunc(\"/color\", func(w http.ResponseWriter, r *http.Request) { color := r.FormValue(\"name\") var b []byte err := Group.Get(nil, color, groupcache.AllocatingByteSliceSink(&b)) if err != nil { http.Error(w, err.Error(), http.StatusNotFound) return } w.Write(b) w.Write([]byte{'\\n'}) }) p := strings.Split(*peers, \",\") pool := groupcache.NewHTTPPool(p[0]) pool.Set(p...) http.ListenAndServe(*addr, nil) } EOF Let's compile this with scone-gccgo : export SCONE_HEAP = 1G go build -compiler gccgo -buildmode = exe -gccgoflags -g groupcache.go Run groupcache in the background: ./groupcache -addr = :8080 -pool = http://127.0.0.1:8080 & And let's query groupcache: curl localhost:8080/color?name = green #00FF00 curl localhost:8080/color?name = red #FF0000 Shielding While the above code runs inside of an enclave, there are multiple security issues if that code would run in an untrusted environment: The peers communicate via http instead of https. This means that executing in an enclave does not improve the security since an attacker can just look into the network traffic of groupcache. The use of https instead of http would require a certificate and a private key. The arguments (i.e., -addr and -pool) are passed via command line, i.e., we can neither trust the integrity nor the confidentiality of these arguments. This groupcache service logs error messages on stderr. How can we be sure if the code runs indeed in an enclave? After all, SCONE supports simulation mode. Alternative: Manual Modifications One could manually modify the program to use https instead of http , one could encrypt the output with AES. However, this would require that we have to change groupcache not only to encrypt all output but also to manage the key for encrypting the output. The private key of the certificate is typically stored in an unencrypted file and protected via the access control of the file system. Since we do not trust the operating system, we would need to encrypt the private key in the file system. We would need to attest the groupcache and after successful attestation pass the encryption keys and the arguments to groupcache via a secure channel. Alternative: SCONE shielding Many programs would require such or similar changes as groupcache . Hence, SCONE provides a way to shield programs without the need to modify these programs. This main advantages of that approach is that one does not have to put in the engineering to modify the code - which is difficult and bug-prone one can easily keep up with upstream code changes without the need to continuously patch the upstream code one does not risk a lock-in by having SGX-specific or SCONE-specific code modifications We will show in later sections how we can shield this application with the help of SCONE such that no source code changes are necessary, and we only need to define a description what shields should be activated In this way, we can address all of the above issues that we described.","title":"GO example"},{"location":"groupcacheUseCase/#groupcache-example","text":"groupcache is a memcached-like library written in GO: it implements a peer-to-peer caching service. We use groupcache to show how to build a little more complex GO program with SCONE. Typically, one would build a container image for groupcache with a Dockerfile . Since our emphasis is to explain how to build such programs, we first show the individual steps to compile and execute groupcache and second, please read how to build container images with a multi-stage build . Note You might want to read how to compile GO programs with SCONE first.","title":"groupcache Example"},{"location":"groupcacheUseCase/#building-groupcache-without-shielding","text":"First, start a crosscompiler container. Give it access to the sgx device if your host has this device - otherwise, just drop argument --device=/dev/isgx : docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Second, install the go command to simplify the building of groupcache : apk update apk add go git curl Now you can build the groupcache library as follows: go get -compiler gccgo -u github.com/golang/groupcache Note that flag -compiler gccgo is required to ensure that the scone-gccgo is used to compile groupcache . That's it!","title":"Building groupcache - without shielding"},{"location":"groupcacheUseCase/#example","text":"OK, we should show how to use group cache. We show this for a simple application from fiorix : cat > groupcache.go << EOF // Simple groupcache example: https://github.com/golang/groupcache // Running 3 instances: // go run groupcache.go -addr=:8080 -pool=http://127.0.0.1:8080,http://127.0.0.1:8081,http://127.0.0.1:8082 // go run groupcache.go -addr=:8081 -pool=http://127.0.0.1:8081,http://127.0.0.1:8080,http://127.0.0.1:8082 // go run groupcache.go -addr=:8082 -pool=http://127.0.0.1:8082,http://127.0.0.1:8080,http://127.0.0.1:8081 // Testing: // curl localhost:8080/color?name=red package main import ( \"errors\" \"flag\" \"log\" \"net/http\" \"strings\" \"github.com/golang/groupcache\" ) var Store = map[string][]byte{ \"red\": []byte(\"#FF0000\"), \"green\": []byte(\"#00FF00\"), \"blue\": []byte(\"#0000FF\"), } var Group = groupcache.NewGroup(\"foobar\", 64<<20, groupcache.GetterFunc( func(ctx groupcache.Context, key string, dest groupcache.Sink) error { log.Println(\"looking up\", key) v, ok := Store[key] if !ok { return errors.New(\"color not found\") } dest.SetBytes(v) return nil }, )) func main() { addr := flag.String(\"addr\", \":8080\", \"server address\") peers := flag.String(\"pool\", \"http://localhost:8080\", \"server pool list\") flag.Parse() http.HandleFunc(\"/color\", func(w http.ResponseWriter, r *http.Request) { color := r.FormValue(\"name\") var b []byte err := Group.Get(nil, color, groupcache.AllocatingByteSliceSink(&b)) if err != nil { http.Error(w, err.Error(), http.StatusNotFound) return } w.Write(b) w.Write([]byte{'\\n'}) }) p := strings.Split(*peers, \",\") pool := groupcache.NewHTTPPool(p[0]) pool.Set(p...) http.ListenAndServe(*addr, nil) } EOF Let's compile this with scone-gccgo : export SCONE_HEAP = 1G go build -compiler gccgo -buildmode = exe -gccgoflags -g groupcache.go Run groupcache in the background: ./groupcache -addr = :8080 -pool = http://127.0.0.1:8080 & And let's query groupcache: curl localhost:8080/color?name = green #00FF00 curl localhost:8080/color?name = red #FF0000","title":"Example"},{"location":"groupcacheUseCase/#shielding","text":"While the above code runs inside of an enclave, there are multiple security issues if that code would run in an untrusted environment: The peers communicate via http instead of https. This means that executing in an enclave does not improve the security since an attacker can just look into the network traffic of groupcache. The use of https instead of http would require a certificate and a private key. The arguments (i.e., -addr and -pool) are passed via command line, i.e., we can neither trust the integrity nor the confidentiality of these arguments. This groupcache service logs error messages on stderr. How can we be sure if the code runs indeed in an enclave? After all, SCONE supports simulation mode.","title":"Shielding"},{"location":"groupcacheUseCase/#alternative-manual-modifications","text":"One could manually modify the program to use https instead of http , one could encrypt the output with AES. However, this would require that we have to change groupcache not only to encrypt all output but also to manage the key for encrypting the output. The private key of the certificate is typically stored in an unencrypted file and protected via the access control of the file system. Since we do not trust the operating system, we would need to encrypt the private key in the file system. We would need to attest the groupcache and after successful attestation pass the encryption keys and the arguments to groupcache via a secure channel.","title":"Alternative: Manual Modifications"},{"location":"groupcacheUseCase/#alternative-scone-shielding","text":"Many programs would require such or similar changes as groupcache . Hence, SCONE provides a way to shield programs without the need to modify these programs. This main advantages of that approach is that one does not have to put in the engineering to modify the code - which is difficult and bug-prone one can easily keep up with upstream code changes without the need to continuously patch the upstream code one does not risk a lock-in by having SGX-specific or SCONE-specific code modifications We will show in later sections how we can shield this application with the help of SCONE such that no source code changes are necessary, and we only need to define a description what shields should be activated In this way, we can address all of the above issues that we described.","title":"Alternative: SCONE shielding"},{"location":"hardwaremode/","text":"Running \"Hello World\" inside of an enclave So far , we showed how to run a hello world program using simulation mode . Let's show how to run this program in hardware mode , i.e., the hello world program runs inside an Intel SGX enclave. Actually, the only change is to give the container access to the SGX device via option --device=/dev/isgx . Detailed Description We first need to start a container which includes the SCONE crosscompiler and give the container access to the Intel SGX driver: docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers The docker engine and the Intel SGX driver must be installed. Read about how to install a docker engine and to install the Intel SGX driver . In some installations, you might have to replace \"docker\" by \"sudo docker\". To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Hardware mode is only supported in Linux, since the Intel SGX driver is only available on Linux. Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we have given the container access to the SGX driver, this runs in hardware mode . Use this mode only for development and debugging The program runs inside of a hardware enclave . However, the enclave is in debug mode, i.e., one can actually introspect the content of the enclave. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 9d601c360ce9b6100e35dc42ec2800c1c20478328a0d4450d8d5163c00289dea Hello World The output shows that SCONE is running in hardware mode : export SCONE_MODE=hw Background Info For ease of use, we create all Docker images such that applications run inside of enclaves if enclaves are available ( AUTO mode ). If SGX is not available, they run in simulation mode, i.e., outside of an enclave but all SCONE software is running. To disable simulation mode, you can set environment variable SCONE_MODE=HW : docker run --device=/dev/isgx -e \"SCONE_MODE=HW\" -it sconecuratedimages/crosscompilers If you would start your container in hardware mode but forget to give it access to the sgx device, i.e., docker run -e \"SCONE_MODE=HW\" -it sconecuratedimages/crosscompilers compilation of the helloworld will succeed but running the helloworld program will fail: bash-4.4# echo $SCONE_MODE HW bash-4.4# ls -l /dev/isgx ls: /dev/isgx: No such file or directory bash-4.4# ./helloworld [SCONE|ERROR] ./tools/starter-exec.c:993:_dl_main(): Could not create enclave: Error opening SGX device When running your software in operations, you would force the programs to run inside of enclaves: this can be enforced with the help of SCONE configuration and attestation service . Environment Variables To simplify the development with SCONE, you can control the behavior of SCONE with a set of environment variables, i.e., variables defined by your shell. Section SCONE Environment Variables describes these in details.","title":"Hardware Mode"},{"location":"hardwaremode/#running-hello-world-inside-of-an-enclave","text":"So far , we showed how to run a hello world program using simulation mode . Let's show how to run this program in hardware mode , i.e., the hello world program runs inside an Intel SGX enclave. Actually, the only change is to give the container access to the SGX device via option --device=/dev/isgx .","title":"Running \"Hello World\" inside of  an enclave"},{"location":"hardwaremode/#detailed-description","text":"We first need to start a container which includes the SCONE crosscompiler and give the container access to the Intel SGX driver: docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers The docker engine and the Intel SGX driver must be installed. Read about how to install a docker engine and to install the Intel SGX driver . In some installations, you might have to replace \"docker\" by \"sudo docker\". To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Hardware mode is only supported in Linux, since the Intel SGX driver is only available on Linux. Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we have given the container access to the SGX driver, this runs in hardware mode . Use this mode only for development and debugging The program runs inside of a hardware enclave . However, the enclave is in debug mode, i.e., one can actually introspect the content of the enclave. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 9d601c360ce9b6100e35dc42ec2800c1c20478328a0d4450d8d5163c00289dea Hello World The output shows that SCONE is running in hardware mode : export SCONE_MODE=hw","title":"Detailed Description"},{"location":"hardwaremode/#background-info","text":"For ease of use, we create all Docker images such that applications run inside of enclaves if enclaves are available ( AUTO mode ). If SGX is not available, they run in simulation mode, i.e., outside of an enclave but all SCONE software is running. To disable simulation mode, you can set environment variable SCONE_MODE=HW : docker run --device=/dev/isgx -e \"SCONE_MODE=HW\" -it sconecuratedimages/crosscompilers If you would start your container in hardware mode but forget to give it access to the sgx device, i.e., docker run -e \"SCONE_MODE=HW\" -it sconecuratedimages/crosscompilers compilation of the helloworld will succeed but running the helloworld program will fail: bash-4.4# echo $SCONE_MODE HW bash-4.4# ls -l /dev/isgx ls: /dev/isgx: No such file or directory bash-4.4# ./helloworld [SCONE|ERROR] ./tools/starter-exec.c:993:_dl_main(): Could not create enclave: Error opening SGX device When running your software in operations, you would force the programs to run inside of enclaves: this can be enforced with the help of SCONE configuration and attestation service . Environment Variables To simplify the development with SCONE, you can control the behavior of SCONE with a set of environment variables, i.e., variables defined by your shell. Section SCONE Environment Variables describes these in details.","title":"Background Info"},{"location":"hello_world_kubernetes/","text":"Confidential Computing with SCONE This tutorial shows how to build and deploy containerized CC App (Confidential Compute App) with the help of SCONE. We show how to run this CC App in a Kubernetes cluster. This is a deep-dive to introduce some of the concepts of SCONE using a hands-on example. Note that the CC App we build is not yet sufficiently secure : e.g., the Python code of the base image can be manipulated. We will show in the second part of this tutorial how to run this securely and how to simplify some of the steps. The tutorial is organized as follows: Setup describes the initial setup required to run this example. Hello World! shows how to build a Python Hello-World! and run this in an enclave in a Kubernetes cluster. This enclaved service does not yet know any secrets. Hello World! with remote attestation provides a secret GREETING to Hello-World . For this, we define a first SCONE policy to run Hello-World! : this policy passes the secret as an environment variable. It does not yet use https since it misses a certificate and a private key. Hello World! with TLS certificates auto-generated by SCONE CAS extends the policy to inject a certificate and a private key into the file system of Hello-World! So far, our policy only ensures that the expected Python engine runs but not that actually our Hello-World! program runs. Encrypt your source code : we encrypt the Hello-World! program to ensure not only its confidentiality but also its integrity such that an adversary cannot change our program. SCONE ensures that the filesystem was not modified. Another way to look at this is that the Hello-World! program itself and the location where it is executed, is use to authenticate the program and to get the decryption key and all the secrets like the certificates. In the second part of this tutorial, we will show how to simplify the setup how to ensure that all Python libraries are encrypted how to ensure that this program can only run on the nodes of your Kubernetes cluster how to provide access control to Hello-World! , i.e., only authorized clients can access the service, and how to run Hello-World! in release mode Script An executable version of this tutorial can be cloned as follows: git clone https://github.com/scontain/hello-world-scone-on-kubernetes.git The code has some more error handling than given in this page. Also, some aspects of the code are not discussed in this tutorial. Unless you set variable, IMAGENAME , it will eventually fail with an error message since it will not be able to push a certain image. The reason is that you will not be able to execute an encrypted image of another session . You need to define environment variable IMAGENAME export IMAGENAME = ... such that it points to an image name that you are permitted to push. By default this will be set to export IMAGENAME = sconecuratedimages/kubernetes:hello-k8s-scone0.3 You can run this demo by executing even if you do not yet have access to the community edition : cd hello-world-scone-on-kubernetes ./run_hello_world.sh There will be some error messages if you do not have access to the community edition but they should all be tolerated. Setup A Kubernetes cluster and a separate namespace This tutorial assumes that you have access to a Kubernetes cluster (through kubectl ), and that you are deploying everything to a separate namespace, called hello-scone . Namespaces are a good way to separate resources, not to mention how it makes the clean-up process a lot easier at the end: kubectl create namespace hello-scone Don't forget to specify it by adding -n hello-scone to your kubectl commands. Set yourself a workspace A lot of files are created through this tutorial, and they have to be somewhere. Choose whatever directory you want to run the commands below, but it might be a good idea to create a temporary one: cd $( mktemp -d ) We use the following base image: export BASE_IMAGE=sconecuratedimages/apps:python-3.7.3-alpine3.10 Have fun! Set a Configuration and Attestation Service (CAS) When remote attestation enters the scene, you will need a CAS to provide attestation, configuration and secrets. Export now the address of your CAS. If you don't have your own CAS, use our public one at scone-cas.cf: export SCONE_CAS_ADDR = scone-cas.cf Now, generate client certificates (needed to submit new sessions to CAS): mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Hello World! Let's start by writing a simple Python webserver: it only returns the message Hello World! and the value of the environment variable GREETING . mkdir -p app cat > app/server.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 8080), HTTPHelloWorldHandler) httpd.serve_forever() EOF To build this application with SCONE, simply use our Python 3.7 curated image as base. The Python interpreter will run inside of an enclave! cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 8080 COPY app /app CMD [ \"python\", \"/app/server.py\" ] EOF Set the name of the image. If you already have an image, skip the building process. export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.1 docker build --pull . -t $IMAGE && docker push $IMAGE Deploying the application with Kubernetes is also simple: we just need a deployment and a service exposing it. Write the Kubernetes manifests: cat > app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: hello-world replicas: 1 template: metadata: labels: run: hello-world spec: containers: - name: hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: GREETING value: howdy! volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: hello-world labels: run: hello-world spec: ports: - port: 8080 protocol: TCP selector: run: hello-world EOF Now, submit the manifests to the Kubernetes API, using kubectl command: kubectl create -f app.yaml -n hello-scone NOTE : SGX applications need access to the Intel SGX driver, a char device located at /dev/isgx (in the host). If you are in a cluster context, it means that you need such device mounted into the container, and that's the reasoning behind the addition of the volume dev-isgx to the Kubernetes manifest. If you don't have access, or if you are not sure whether the underlying infrastructure has SGX installed, you can run in simulated mode, by adding SCONE_MODE=sim to the environment. This will emulate an enclave for you by encripting the main memory (Intel SGX security guarantees do not apply here). NOTE : You need special privileges to access a host device. Kubernetes does not allow a fine-grained access to such devices (like a Docker --device option does), so you need to run your container in privileged mode (i.e. your container has access to ALL host devices). This is defined in the container spec, as a securityContext policy ( privileged: true ). Now that everything is deployed, you can access your Python app running on your cluster. Forward your local 8080 port to the service port: kubectl port-forward svc/hello-world 8080 :8080 -n hello-scone The application will be available at your http://localhost:8080 : $ curl localhost:8080 Hello World! Environment GREETING is: howdy! Run with remote attestation SCONE provides a remote attestation feature, so you make sure your application is running unmodified. It's also possible to have secrets and configuration delivered directly to the attested enclave! The remote attestation is provided by two components: LAS (local attestation service, runs on the cluster) and CAS (a trusted service that runs elsewhere. We provide a public one in scone-cas.cf). You can deploy LAS to your cluster with the help of a DaemonSet, deploying one LAS instance per cluster node. As your application has to contact the LAS container running in the same host, we use the default Docker interface (172.17.0.1) as our LAS address. cat > las.yaml << EOF apiVersion: apps/v1 kind: DaemonSet metadata: name: local-attestation labels: k8s-app: local-attestation spec: selector: matchLabels: k8s-app: local-attestation template: metadata: labels: k8s-app: local-attestation spec: hostNetwork: true volumes: - name: dev-isgx hostPath: path: /dev/isgx containers: - name: local-attestation image: sconecuratedimages/services:las volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true ports: - containerPort: 18766 hostPort: 18766 EOF kubectl create -f las.yaml -n hello-scone To setup remote attestation, you will need a session file and the MRENCLAVE , which is a unique signature of your application. Extract MRENCLAVE of your application by running its container with the environment variable SCONE_HASH set to 1 : MRENCLAVE = ` docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ` Then create a session file. Please note that we are also defining a secret GREETING . Only the MRENCLAVES registered in this session file will be allowed to see such secret. cat > session.yaml << EOF name: hello-k8s-scone version: \"0.2\" services: - name: application image_name: $IMAGE mrenclaves: [$MRENCLAVE] command: python /app/server.py pwd: / environment: GREETING: hello from SCONE!!! images: - name: $IMAGE EOF Now, post the session file to your CAS of choice: curl -v -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yaml -X POST https:// $SCONE_CAS_ADDR :8081/v1/sessions Once you submit the session file, you just need to inject the CAS address into your Deployment manifest. To showcase that, we're creating a new Deployment: cat > attested-app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world spec: selector: matchLabels: run: attested-hello-world replicas: 1 template: metadata: labels: run: attested-hello-world spec: containers: - name: attested-hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: hello-k8s-scone/application - name: SCONE_LAS_ADDR value: 172.17.0.1:18766 volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: attested-hello-world labels: run: attested-hello-world spec: ports: - port: 8080 protocol: TCP selector: run: attested-hello-world EOF NOTE : The environment defined in the Kubernetes manifest won't be considered once your app is attested. The environment will be retrieved from your session file. Deploy your attested app: kubectl create -f attested-app.yaml -n hello-scone Once again, forward your local port 8081 to the service port: kubectl port-forward svc/attested-hello-world 8081 :8080 -n hello-scone The attested application will be available at your http://localhost:8081 : $ curl localhost:8081 Hello World! Environment GREETING is: hello from SCONE!!! TLS with certificates auto-generated by CAS The CAS service can also generate secrets and certificates automatically. Combined with access policies, it means that such auto-generated secrets and certificates will be seen only by certain applications. No human (e.g. a system operator) will ever see them: they only exist inside of SGX enclaves. To showcase such feature, let's use the same application as last example. But now, we serve the traffic over TLS, and we let CAS generate the server certificates. Start by rewriting our application to serve with TLS: cat > app/server-tls.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os import socket import ssl class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 4443), HTTPHelloWorldHandler) httpd.socket = ssl.wrap_socket(httpd.socket, keyfile=\"/app/key.pem\", certfile=\"/app/cert.pem\", server_side=True) httpd.serve_forever() EOF Our updated Dockerfile looks like this: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app /app CMD [ \"/usr/local/bin/python\" ] EOF Build the updated image: export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.2 docker build --pull . -t $IMAGE && docker push $IMAGE The magic is done in the session file. First, extract the MRENCLAVE again: MRENCLAVE = $( docker run -i --rm --device /dev/isgx -e \"SCONE_HASH=1\" $IMAGE ) Now, create a Session file to be submitted to CAS. The certificates are defined in the secrets field, and are injected into the filesystem through images.injection_files : cat > session-tls-certs.yaml << EOF name: hello-k8s-scone-tls-certs version: \"0.2\" services: - name: application image_name: $IMAGE mrenclaves: [$MRENCLAVE] command: python /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with TLS and auto-generated certs!!! images: - name: $IMAGE injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls-certs.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session The steps to run your app are similar to before: let's create a Kubernetes manifest and submit it. cat > attested-app-tls-certs.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls-certs spec: selector: matchLabels: run: attested-hello-world-tls-certs replicas: 1 template: metadata: labels: run: attested-hello-world-tls-certs spec: containers: - name: attested-hello-world-tls-certs image: $IMAGE imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: hello-k8s-scone-tls-certs/application - name: SCONE_LAS_ADDR value: \"172.17.0.1\" volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls-certs labels: run: attested-hello-world-tls-certs spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls-certs EOF kubectl create -f attested-app-tls-certs.yaml -n hello-scone Now it's time to access your app. Again, forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls-certs 8083 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8083 Hello World! $GREETING is: hello from SCONE with TLS and auto-generated certs!!! Encrypt your source code Moving further, you can run the exact same application, but now the server source code will be encrypted using SCONE's file protection feature, and only the Python interpreter that you register will be able to read them (after being attested by CAS). Encrypt the source code and filesystem Let's encrypt the source code using SCONE's Fileshield. Run a SCONE CLI container with access to the files: docker run -it --rm --device /dev/isgx -v $PWD :/tutorial $BASE_IMAGE sh Inside the container, create an encrypted region /app and add all the files in /tutorial/app (the plain files) to it. Lastly, encrypt the key itself. cd /tutorial rm -rf app_image && mkdir -p app_image/app cd app_image scone fspf create fspf.pb scone fspf addr fspf.pb / --not-protected --kernel / scone fspf addr fspf.pb /app --encrypted --kernel /app scone fspf addf fspf.pb /app /tutorial/app /tutorial/app_image/app scone fspf encrypt fspf.pb > /tutorial/app/keytag The contents of /tutorial/app_image/app are now encrypted. Try to cat them from inside the container: $ cd /tutorial/app_image/app && ls server-tls.py $ cat server-tls.py $\ufffd\u0287 ( E@\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffdId\ufffdZ\ufffd\ufffd\ufffdg3uc\ufffd\ufffdm\u046a\ufffdZ.$\ufffd__\ufffd! ) \u04b9S\ufffd\ufffd\u0660\ufffd\ufffd\ufffd ( \ufffd\ufffd \ufffdX\ufffdW\u03d0\ufffd { $\ufffd\ufffd\ufffd | \ufffd\u05d6H\ufffd\ufffd\u01d3\ufffd\ufffd! ; 2 \ufffd\ufffd\ufffd\ufffd>@z4-\ufffdh\ufffd\ufffd3i\u077es\ufffdt\ufffd7\ufffd<>H4:\ufffd\ufffd\ufffd\ufffd ( 9 = \ufffda ) \ufffdj?\ufffd\ufffdp\ufffd\ufffd\ufffd\ufffdq\ufffd\u07e73\ufffd } \ufffd\ufffdH\u0638a\ufffd | \ufffd\ufffd\ufffdw-\ufffdq\ufffd\ufffd96\ufffd\ufffd\ufffdo\ufffd9\u0303\ufffdEv\ufffdv\ufffd\ufffd*$\ufffd\ufffd\ufffd\ufffdTU/\ufffd\ufffd\ufffd\u04d4\ufffd\ufffdv\ufffdG\ufffd\ufffd\ufffdT\ufffd1<\u06b9\ufffd\ufffdC#p | i\ufffd\ufffdA\u01a2\u02c5_!6\ufffd\ufffd\ufffdF\ufffd\ufffd\ufffdw\ufffd@ \ufffdC\ufffd\ufffdJ\ufffd\ufffd\ufffd+81\ufffd8\ufffd Build the new image You can now exit the container. Let's build the server image with our files, now encrypted: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app_image / CMD [ \"/usr/local/bin/python\" ] EOF Choose the new image name and build it: export IMAGE3 = sconecuratedimages/kubernetes:hello-k8s-scone0.3 docker build --pull . -t $IMAGE3 && docker push $IMAGE3 Run in Kubernetes Time to deploy our server to Kubernetes. Let's setup the attestation first, so we make sure that only our application (identified by its MRENCLAVE ) will have access to the secrets to read the encrypted filesystem, as well as our secret greeting. Extract the MRENCLAVE : MRENCLAVE = $( docker run -i --rm --device /dev/isgx -e \"SCONE_HASH=1\" $IMAGE3 ) Extract SCONE_FSPF_KEY and SCONE_FSPF_TAG : export SCONE_FSPF_KEY = $( cat app/keytag | awk '{print $11}' ) export SCONE_FSPF_TAG = $( cat app/keytag | awk '{print $9}' ) export SCONE_FSPF = /fspf.pb Create a session file: cat > session-tls.yaml << EOF name: hello-k8s-scone-tls version: \"0.2\" services: - name: application image_name: $IMAGE3 mrenclaves: [$MRENCLAVE] command: python /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with encrypted source code and auto-generated certs!!! fspf_path: $SCONE_FSPF fspf_key: $SCONE_FSPF_KEY fspf_tag: $SCONE_FSPF_TAG images: - name: $IMAGE3 injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session Create the manifest and submit it to your Kubernetes cluster: cat > attested-app-tls.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls spec: selector: matchLabels: run: attested-hello-world-tls replicas: 1 template: metadata: labels: run: attested-hello-world-tls spec: containers: - name: attested-hello-world-tls image: $IMAGE3 imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: hello-k8s-scone-tls/application - name: SCONE_LAS_ADDR value: 172.17.0.1 volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls labels: run: attested-hello-world-tls spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls EOF kubectl create -f attested-app-tls.yaml -n hello-scone Time to access your app. Forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls 8082 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8082 Hello World! $GREETING is: hello from SCONE with encrypted source code and auto-generated certs!!! Clean up To clean up all the resources created in this tutorial, simply delete the namespace: kubectl delete namespace hello-scone Author: Clenimar","title":"Hello World Kubernetes"},{"location":"hello_world_kubernetes/#confidential-computing-with-scone","text":"This tutorial shows how to build and deploy containerized CC App (Confidential Compute App) with the help of SCONE. We show how to run this CC App in a Kubernetes cluster. This is a deep-dive to introduce some of the concepts of SCONE using a hands-on example. Note that the CC App we build is not yet sufficiently secure : e.g., the Python code of the base image can be manipulated. We will show in the second part of this tutorial how to run this securely and how to simplify some of the steps. The tutorial is organized as follows: Setup describes the initial setup required to run this example. Hello World! shows how to build a Python Hello-World! and run this in an enclave in a Kubernetes cluster. This enclaved service does not yet know any secrets. Hello World! with remote attestation provides a secret GREETING to Hello-World . For this, we define a first SCONE policy to run Hello-World! : this policy passes the secret as an environment variable. It does not yet use https since it misses a certificate and a private key. Hello World! with TLS certificates auto-generated by SCONE CAS extends the policy to inject a certificate and a private key into the file system of Hello-World! So far, our policy only ensures that the expected Python engine runs but not that actually our Hello-World! program runs. Encrypt your source code : we encrypt the Hello-World! program to ensure not only its confidentiality but also its integrity such that an adversary cannot change our program. SCONE ensures that the filesystem was not modified. Another way to look at this is that the Hello-World! program itself and the location where it is executed, is use to authenticate the program and to get the decryption key and all the secrets like the certificates. In the second part of this tutorial, we will show how to simplify the setup how to ensure that all Python libraries are encrypted how to ensure that this program can only run on the nodes of your Kubernetes cluster how to provide access control to Hello-World! , i.e., only authorized clients can access the service, and how to run Hello-World! in release mode","title":"Confidential Computing with SCONE"},{"location":"hello_world_kubernetes/#script","text":"An executable version of this tutorial can be cloned as follows: git clone https://github.com/scontain/hello-world-scone-on-kubernetes.git The code has some more error handling than given in this page. Also, some aspects of the code are not discussed in this tutorial. Unless you set variable, IMAGENAME , it will eventually fail with an error message since it will not be able to push a certain image. The reason is that you will not be able to execute an encrypted image of another session . You need to define environment variable IMAGENAME export IMAGENAME = ... such that it points to an image name that you are permitted to push. By default this will be set to export IMAGENAME = sconecuratedimages/kubernetes:hello-k8s-scone0.3 You can run this demo by executing even if you do not yet have access to the community edition : cd hello-world-scone-on-kubernetes ./run_hello_world.sh There will be some error messages if you do not have access to the community edition but they should all be tolerated.","title":"Script"},{"location":"hello_world_kubernetes/#setup","text":"","title":"Setup"},{"location":"hello_world_kubernetes/#a-kubernetes-cluster-and-a-separate-namespace","text":"This tutorial assumes that you have access to a Kubernetes cluster (through kubectl ), and that you are deploying everything to a separate namespace, called hello-scone . Namespaces are a good way to separate resources, not to mention how it makes the clean-up process a lot easier at the end: kubectl create namespace hello-scone Don't forget to specify it by adding -n hello-scone to your kubectl commands.","title":"A Kubernetes cluster and a separate namespace"},{"location":"hello_world_kubernetes/#set-yourself-a-workspace","text":"A lot of files are created through this tutorial, and they have to be somewhere. Choose whatever directory you want to run the commands below, but it might be a good idea to create a temporary one: cd $( mktemp -d ) We use the following base image: export BASE_IMAGE=sconecuratedimages/apps:python-3.7.3-alpine3.10 Have fun!","title":"Set yourself a workspace"},{"location":"hello_world_kubernetes/#set-a-configuration-and-attestation-service-cas","text":"When remote attestation enters the scene, you will need a CAS to provide attestation, configuration and secrets. Export now the address of your CAS. If you don't have your own CAS, use our public one at scone-cas.cf: export SCONE_CAS_ADDR = scone-cas.cf Now, generate client certificates (needed to submit new sessions to CAS): mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi","title":"Set a Configuration and Attestation Service (CAS)"},{"location":"hello_world_kubernetes/#hello-world","text":"Let's start by writing a simple Python webserver: it only returns the message Hello World! and the value of the environment variable GREETING . mkdir -p app cat > app/server.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 8080), HTTPHelloWorldHandler) httpd.serve_forever() EOF To build this application with SCONE, simply use our Python 3.7 curated image as base. The Python interpreter will run inside of an enclave! cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 8080 COPY app /app CMD [ \"python\", \"/app/server.py\" ] EOF Set the name of the image. If you already have an image, skip the building process. export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.1 docker build --pull . -t $IMAGE && docker push $IMAGE Deploying the application with Kubernetes is also simple: we just need a deployment and a service exposing it. Write the Kubernetes manifests: cat > app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: hello-world replicas: 1 template: metadata: labels: run: hello-world spec: containers: - name: hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: GREETING value: howdy! volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: hello-world labels: run: hello-world spec: ports: - port: 8080 protocol: TCP selector: run: hello-world EOF Now, submit the manifests to the Kubernetes API, using kubectl command: kubectl create -f app.yaml -n hello-scone NOTE : SGX applications need access to the Intel SGX driver, a char device located at /dev/isgx (in the host). If you are in a cluster context, it means that you need such device mounted into the container, and that's the reasoning behind the addition of the volume dev-isgx to the Kubernetes manifest. If you don't have access, or if you are not sure whether the underlying infrastructure has SGX installed, you can run in simulated mode, by adding SCONE_MODE=sim to the environment. This will emulate an enclave for you by encripting the main memory (Intel SGX security guarantees do not apply here). NOTE : You need special privileges to access a host device. Kubernetes does not allow a fine-grained access to such devices (like a Docker --device option does), so you need to run your container in privileged mode (i.e. your container has access to ALL host devices). This is defined in the container spec, as a securityContext policy ( privileged: true ). Now that everything is deployed, you can access your Python app running on your cluster. Forward your local 8080 port to the service port: kubectl port-forward svc/hello-world 8080 :8080 -n hello-scone The application will be available at your http://localhost:8080 : $ curl localhost:8080 Hello World! Environment GREETING is: howdy!","title":"Hello World!"},{"location":"hello_world_kubernetes/#run-with-remote-attestation","text":"SCONE provides a remote attestation feature, so you make sure your application is running unmodified. It's also possible to have secrets and configuration delivered directly to the attested enclave! The remote attestation is provided by two components: LAS (local attestation service, runs on the cluster) and CAS (a trusted service that runs elsewhere. We provide a public one in scone-cas.cf). You can deploy LAS to your cluster with the help of a DaemonSet, deploying one LAS instance per cluster node. As your application has to contact the LAS container running in the same host, we use the default Docker interface (172.17.0.1) as our LAS address. cat > las.yaml << EOF apiVersion: apps/v1 kind: DaemonSet metadata: name: local-attestation labels: k8s-app: local-attestation spec: selector: matchLabels: k8s-app: local-attestation template: metadata: labels: k8s-app: local-attestation spec: hostNetwork: true volumes: - name: dev-isgx hostPath: path: /dev/isgx containers: - name: local-attestation image: sconecuratedimages/services:las volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true ports: - containerPort: 18766 hostPort: 18766 EOF kubectl create -f las.yaml -n hello-scone To setup remote attestation, you will need a session file and the MRENCLAVE , which is a unique signature of your application. Extract MRENCLAVE of your application by running its container with the environment variable SCONE_HASH set to 1 : MRENCLAVE = ` docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ` Then create a session file. Please note that we are also defining a secret GREETING . Only the MRENCLAVES registered in this session file will be allowed to see such secret. cat > session.yaml << EOF name: hello-k8s-scone version: \"0.2\" services: - name: application image_name: $IMAGE mrenclaves: [$MRENCLAVE] command: python /app/server.py pwd: / environment: GREETING: hello from SCONE!!! images: - name: $IMAGE EOF Now, post the session file to your CAS of choice: curl -v -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yaml -X POST https:// $SCONE_CAS_ADDR :8081/v1/sessions Once you submit the session file, you just need to inject the CAS address into your Deployment manifest. To showcase that, we're creating a new Deployment: cat > attested-app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world spec: selector: matchLabels: run: attested-hello-world replicas: 1 template: metadata: labels: run: attested-hello-world spec: containers: - name: attested-hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: hello-k8s-scone/application - name: SCONE_LAS_ADDR value: 172.17.0.1:18766 volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: attested-hello-world labels: run: attested-hello-world spec: ports: - port: 8080 protocol: TCP selector: run: attested-hello-world EOF NOTE : The environment defined in the Kubernetes manifest won't be considered once your app is attested. The environment will be retrieved from your session file. Deploy your attested app: kubectl create -f attested-app.yaml -n hello-scone Once again, forward your local port 8081 to the service port: kubectl port-forward svc/attested-hello-world 8081 :8080 -n hello-scone The attested application will be available at your http://localhost:8081 : $ curl localhost:8081 Hello World! Environment GREETING is: hello from SCONE!!!","title":"Run with remote attestation"},{"location":"hello_world_kubernetes/#tls-with-certificates-auto-generated-by-cas","text":"The CAS service can also generate secrets and certificates automatically. Combined with access policies, it means that such auto-generated secrets and certificates will be seen only by certain applications. No human (e.g. a system operator) will ever see them: they only exist inside of SGX enclaves. To showcase such feature, let's use the same application as last example. But now, we serve the traffic over TLS, and we let CAS generate the server certificates. Start by rewriting our application to serve with TLS: cat > app/server-tls.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os import socket import ssl class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 4443), HTTPHelloWorldHandler) httpd.socket = ssl.wrap_socket(httpd.socket, keyfile=\"/app/key.pem\", certfile=\"/app/cert.pem\", server_side=True) httpd.serve_forever() EOF Our updated Dockerfile looks like this: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app /app CMD [ \"/usr/local/bin/python\" ] EOF Build the updated image: export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.2 docker build --pull . -t $IMAGE && docker push $IMAGE The magic is done in the session file. First, extract the MRENCLAVE again: MRENCLAVE = $( docker run -i --rm --device /dev/isgx -e \"SCONE_HASH=1\" $IMAGE ) Now, create a Session file to be submitted to CAS. The certificates are defined in the secrets field, and are injected into the filesystem through images.injection_files : cat > session-tls-certs.yaml << EOF name: hello-k8s-scone-tls-certs version: \"0.2\" services: - name: application image_name: $IMAGE mrenclaves: [$MRENCLAVE] command: python /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with TLS and auto-generated certs!!! images: - name: $IMAGE injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls-certs.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session The steps to run your app are similar to before: let's create a Kubernetes manifest and submit it. cat > attested-app-tls-certs.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls-certs spec: selector: matchLabels: run: attested-hello-world-tls-certs replicas: 1 template: metadata: labels: run: attested-hello-world-tls-certs spec: containers: - name: attested-hello-world-tls-certs image: $IMAGE imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: hello-k8s-scone-tls-certs/application - name: SCONE_LAS_ADDR value: \"172.17.0.1\" volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls-certs labels: run: attested-hello-world-tls-certs spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls-certs EOF kubectl create -f attested-app-tls-certs.yaml -n hello-scone Now it's time to access your app. Again, forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls-certs 8083 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8083 Hello World! $GREETING is: hello from SCONE with TLS and auto-generated certs!!!","title":"TLS with certificates auto-generated by CAS"},{"location":"hello_world_kubernetes/#encrypt-your-source-code","text":"Moving further, you can run the exact same application, but now the server source code will be encrypted using SCONE's file protection feature, and only the Python interpreter that you register will be able to read them (after being attested by CAS).","title":"Encrypt your source code"},{"location":"hello_world_kubernetes/#encrypt-the-source-code-and-filesystem","text":"Let's encrypt the source code using SCONE's Fileshield. Run a SCONE CLI container with access to the files: docker run -it --rm --device /dev/isgx -v $PWD :/tutorial $BASE_IMAGE sh Inside the container, create an encrypted region /app and add all the files in /tutorial/app (the plain files) to it. Lastly, encrypt the key itself. cd /tutorial rm -rf app_image && mkdir -p app_image/app cd app_image scone fspf create fspf.pb scone fspf addr fspf.pb / --not-protected --kernel / scone fspf addr fspf.pb /app --encrypted --kernel /app scone fspf addf fspf.pb /app /tutorial/app /tutorial/app_image/app scone fspf encrypt fspf.pb > /tutorial/app/keytag The contents of /tutorial/app_image/app are now encrypted. Try to cat them from inside the container: $ cd /tutorial/app_image/app && ls server-tls.py $ cat server-tls.py $\ufffd\u0287 ( E@\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffdId\ufffdZ\ufffd\ufffd\ufffdg3uc\ufffd\ufffdm\u046a\ufffdZ.$\ufffd__\ufffd! ) \u04b9S\ufffd\ufffd\u0660\ufffd\ufffd\ufffd ( \ufffd\ufffd \ufffdX\ufffdW\u03d0\ufffd { $\ufffd\ufffd\ufffd | \ufffd\u05d6H\ufffd\ufffd\u01d3\ufffd\ufffd! ; 2 \ufffd\ufffd\ufffd\ufffd>@z4-\ufffdh\ufffd\ufffd3i\u077es\ufffdt\ufffd7\ufffd<>H4:\ufffd\ufffd\ufffd\ufffd ( 9 = \ufffda ) \ufffdj?\ufffd\ufffdp\ufffd\ufffd\ufffd\ufffdq\ufffd\u07e73\ufffd } \ufffd\ufffdH\u0638a\ufffd | \ufffd\ufffd\ufffdw-\ufffdq\ufffd\ufffd96\ufffd\ufffd\ufffdo\ufffd9\u0303\ufffdEv\ufffdv\ufffd\ufffd*$\ufffd\ufffd\ufffd\ufffdTU/\ufffd\ufffd\ufffd\u04d4\ufffd\ufffdv\ufffdG\ufffd\ufffd\ufffdT\ufffd1<\u06b9\ufffd\ufffdC#p | i\ufffd\ufffdA\u01a2\u02c5_!6\ufffd\ufffd\ufffdF\ufffd\ufffd\ufffdw\ufffd@ \ufffdC\ufffd\ufffdJ\ufffd\ufffd\ufffd+81\ufffd8\ufffd","title":"Encrypt the source code and filesystem"},{"location":"hello_world_kubernetes/#build-the-new-image","text":"You can now exit the container. Let's build the server image with our files, now encrypted: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app_image / CMD [ \"/usr/local/bin/python\" ] EOF Choose the new image name and build it: export IMAGE3 = sconecuratedimages/kubernetes:hello-k8s-scone0.3 docker build --pull . -t $IMAGE3 && docker push $IMAGE3","title":"Build the new image"},{"location":"hello_world_kubernetes/#run-in-kubernetes","text":"Time to deploy our server to Kubernetes. Let's setup the attestation first, so we make sure that only our application (identified by its MRENCLAVE ) will have access to the secrets to read the encrypted filesystem, as well as our secret greeting. Extract the MRENCLAVE : MRENCLAVE = $( docker run -i --rm --device /dev/isgx -e \"SCONE_HASH=1\" $IMAGE3 ) Extract SCONE_FSPF_KEY and SCONE_FSPF_TAG : export SCONE_FSPF_KEY = $( cat app/keytag | awk '{print $11}' ) export SCONE_FSPF_TAG = $( cat app/keytag | awk '{print $9}' ) export SCONE_FSPF = /fspf.pb Create a session file: cat > session-tls.yaml << EOF name: hello-k8s-scone-tls version: \"0.2\" services: - name: application image_name: $IMAGE3 mrenclaves: [$MRENCLAVE] command: python /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with encrypted source code and auto-generated certs!!! fspf_path: $SCONE_FSPF fspf_key: $SCONE_FSPF_KEY fspf_tag: $SCONE_FSPF_TAG images: - name: $IMAGE3 injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session Create the manifest and submit it to your Kubernetes cluster: cat > attested-app-tls.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls spec: selector: matchLabels: run: attested-hello-world-tls replicas: 1 template: metadata: labels: run: attested-hello-world-tls spec: containers: - name: attested-hello-world-tls image: $IMAGE3 imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: hello-k8s-scone-tls/application - name: SCONE_LAS_ADDR value: 172.17.0.1 volumeMounts: - mountPath: /dev/isgx name: dev-isgx securityContext: privileged: true volumes: - name: dev-isgx hostPath: path: /dev/isgx --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls labels: run: attested-hello-world-tls spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls EOF kubectl create -f attested-app-tls.yaml -n hello-scone Time to access your app. Forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls 8082 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8082 Hello World! $GREETING is: hello from SCONE with encrypted source code and auto-generated certs!!!","title":"Run in Kubernetes"},{"location":"hello_world_kubernetes/#clean-up","text":"To clean up all the resources created in this tutorial, simply delete the namespace: kubectl delete namespace hello-scone Author: Clenimar","title":"Clean up"},{"location":"helm/","text":"Helm and Confidential Applications Helm is the package manager for Kubernetes . You can install SCONE-based confidential applications with the help of Helm. Note that we do not need to trust Helm nor the Kubernetes cluster: SCONE can protect the confidentiality and the integrity of applications even if Helm or Kubernetes would be compromised. The objective is of course that one can outsource the management of the cluster and its operations to an external entity and still keep in control of one's applications and data. Trust is established with an confidential applications via attestation. In the case of SCONE, this can actually be delegated to TLS. How exactly this is achieved, we will explain in a later section. Prerequisites The following prerequisites are required for using Helm to install confidential applications: A Kubernetes cluster, some of the nodes have Intel SGX and an Intel SGX driver installed, Helm 3 client, and access to our git repository https://github.com/scontain/sconeapp If you need help installing large clusters, just drop us an email . sconeapps: Curated Confidential Applications We support a variety of applications executing inside of SGX enclaves. These applications can be installed in various ways. One of the convenient ways to install the applications is with the help of Helm. We support this right now only for commercial customers, i.e., for the following steps you need to get access to the SCONE standard edition. Send us an email to get access. sconeapps is a private Helm repository, we need to grant you access and you need a GitHub token with \"repo\" permissions. Click here to issue a GitHub token . Use it to add this repository to Helm: export GH_TOKEN = ... helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update Using Helm See all charts available: helm search repo sconeapps Installing one chart: helm install my-database sconeapps/database This repository contains the following charts: cas : deploy SCONE Configuration and Attestation Service (CAS) to Kubernetes . database : Umbrella chart to deploy to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy; las : Deploy SCONE Local Attestation Service (LAS) on all Kubernetes node; mariadb : Deploy MariaDB SCONE, i.e., MariaDB running inside of SGX enclaves; maxscale : Deploy MaxScale SCONE, i.e., Maxscale running inside of SGX enclaves and (optionally) an HAProxy as Ingress; We have a quite a large number of applications running inside of enclaves. Just let us know what application you need and we add a way to deploy with the help of Helm. Example The database chart will deploy mariadb and maxscale charts together. The example below will deploy 6 instances of MariaDB SCONE: each one will act as a different shard, and also be attested by the provided CAS. MaxScale will generate a maxscale.cnf using the in-cluster DNS entries for each MariaDB instance. The last line enables an HAProxy Ingress exposing the MaxScale replicas to the external world. export SCONE_CAS_ADDR = scone-cas.cf helm install my-database sconeapps/database \\ --set global.mariadb.replicaCount = 6 \\ --set mariadb-scone.scone.attestation.cas = $SCONE_CAS_ADDR \\ --set mariadb-scone.scone.attestation.config_id = database/db \\ --set maxscale.generateConfig = true \\ --set maxscale.haproxy-ingress.enabled = true To check other examples and the complete set of options to further customize these charts, please refer to later sections of this chapter.","title":"Helm"},{"location":"helm/#helm-and-confidential-applications","text":"Helm is the package manager for Kubernetes . You can install SCONE-based confidential applications with the help of Helm. Note that we do not need to trust Helm nor the Kubernetes cluster: SCONE can protect the confidentiality and the integrity of applications even if Helm or Kubernetes would be compromised. The objective is of course that one can outsource the management of the cluster and its operations to an external entity and still keep in control of one's applications and data. Trust is established with an confidential applications via attestation. In the case of SCONE, this can actually be delegated to TLS. How exactly this is achieved, we will explain in a later section.","title":"Helm and Confidential Applications"},{"location":"helm/#prerequisites","text":"The following prerequisites are required for using Helm to install confidential applications: A Kubernetes cluster, some of the nodes have Intel SGX and an Intel SGX driver installed, Helm 3 client, and access to our git repository https://github.com/scontain/sconeapp If you need help installing large clusters, just drop us an email .","title":"Prerequisites"},{"location":"helm/#sconeapps-curated-confidential-applications","text":"We support a variety of applications executing inside of SGX enclaves. These applications can be installed in various ways. One of the convenient ways to install the applications is with the help of Helm. We support this right now only for commercial customers, i.e., for the following steps you need to get access to the SCONE standard edition. Send us an email to get access. sconeapps is a private Helm repository, we need to grant you access and you need a GitHub token with \"repo\" permissions. Click here to issue a GitHub token . Use it to add this repository to Helm: export GH_TOKEN = ... helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update","title":"sconeapps: Curated Confidential Applications"},{"location":"helm/#using-helm","text":"See all charts available: helm search repo sconeapps Installing one chart: helm install my-database sconeapps/database This repository contains the following charts: cas : deploy SCONE Configuration and Attestation Service (CAS) to Kubernetes . database : Umbrella chart to deploy to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy; las : Deploy SCONE Local Attestation Service (LAS) on all Kubernetes node; mariadb : Deploy MariaDB SCONE, i.e., MariaDB running inside of SGX enclaves; maxscale : Deploy MaxScale SCONE, i.e., Maxscale running inside of SGX enclaves and (optionally) an HAProxy as Ingress; We have a quite a large number of applications running inside of enclaves. Just let us know what application you need and we add a way to deploy with the help of Helm.","title":"Using Helm"},{"location":"helm/#example","text":"The database chart will deploy mariadb and maxscale charts together. The example below will deploy 6 instances of MariaDB SCONE: each one will act as a different shard, and also be attested by the provided CAS. MaxScale will generate a maxscale.cnf using the in-cluster DNS entries for each MariaDB instance. The last line enables an HAProxy Ingress exposing the MaxScale replicas to the external world. export SCONE_CAS_ADDR = scone-cas.cf helm install my-database sconeapps/database \\ --set global.mariadb.replicaCount = 6 \\ --set mariadb-scone.scone.attestation.cas = $SCONE_CAS_ADDR \\ --set mariadb-scone.scone.attestation.config_id = database/db \\ --set maxscale.generateConfig = true \\ --set maxscale.haproxy-ingress.enabled = true To check other examples and the complete set of options to further customize these charts, please refer to later sections of this chapter.","title":"Example"},{"location":"helm_cas/","text":"SCONE CAS: Deploy and Attest SCONE CAS is the configuration and attestation service that manages policies and attests services. We explain how to start SCONE CAS with the help of helm and how to attest it to ensure that it was properly deployed. Prerequisites The following prerequisites are required for using Helm to install confidential applications: A Kubernetes cluster and the helm setup is already done Deploying SCONE CAS The sconeapps/cas chart will deploy SCONE CAS . If we do not need to set any special affinity options and we want to deploy the development CAS, you can just execute: helm install cas sconeapps/cas This starts SCONE CAS in the default Kubernetes namespace. As an alternative, you can deploy SCONE CAS with KubeApps . Configuration Options You can learn about the configuration options by executing: helm install cas sconeapps/cas --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different image by first defining environment variable SCONE_CAS_IMAGE appropriately. You can do this as follows: helm install cas sconeapps/cas --set image = $SCONE_CAS_IMAGE You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status cas Attesting SCONE CAS Since we do not know if SCONE CAS was properly started, we need to attest SCONE CAS . Typically, the attestation will be performed by another already enclaved service. You can also perform the attestation manually. We assume that you trust your local computer that is properly executes the attestation. You can attest your SCONE CAS as follows. First, forward the traffic from your local computer to the release that you just started: kubectl port-forward service/cas 8081 :8081 Second, determine MRENCLAVE of the SCONE CAS image. Note for commercial customers, we provide a secure way to determine MRENCLAVE . A generic way to determine MRENCLAVE is as follows. We set the environment variable IMAGE to the name of the SCONE CAS image. If you changed the SCONE CAS image, we assume that you set the environment variable SCONE_CAS_IMAGE accordingly (see above). We then compute MRENCLAVE of SCONE CAS - assuming that you trust your local computer: export IMAGE = ${ SCONE_CAS_IMAGE :- sconecuratedimages /services: cas .trust.group-out-of-date } export CAS_MRENCLAVE = $( docker pull $IMAGE > /dev/null ; docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE cas ) The result you can view like this: $ echo $CAS_MRENCLAVE 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794 You can attest SCONE CAS on your local machine in executing the following (set CLI_IMAGE to an image that contains the SCONE CLI): docker run -e SCONE_MODE = SIM -it --rm $CLI_IMAGE scone cas attest -G --only_for_testing-debug host.docker.internal $CAS_MRENCLAVE It will print the following output if the SCONE CAS runs inside of an enclave: CAS host.docker.internal at https://host.docker.internal:8081/ is trustworthy and it will store the certificate of the SCONE CAS . This certificate can be used later to verify that we connect to the attested SCONE CAS . For attesting a SCONE CAS in production mode, you should use hardware mode (set DEVICE to your SGX device, i.e., /dev/sgx or /dev/isgx ): docker run --device = $DEVICE -e SCONE_MODE = HW -it --rm $CLI_IMAGE scone cas attest host.docker.internal $CAS_MRENCLAVE You can also try to attest with a wrong CAS_MRENCLAVE (e.g. by incrementing by 1): export CAS_MRENCLAVE = 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696795 docker run -e SCONE_MODE = SIM -it --rm $BASE_IMAGE scone cas attest -G --only_for_testing-debug host.docker.internal $CAS_MRENCLAVE The output would like this: Error: Error during attestation of CAS Caused by: Enclave hash does not match expectation: 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794","title":"CAS Deploy & Attest"},{"location":"helm_cas/#scone-cas-deploy-and-attest","text":"SCONE CAS is the configuration and attestation service that manages policies and attests services. We explain how to start SCONE CAS with the help of helm and how to attest it to ensure that it was properly deployed.","title":"SCONE CAS: Deploy and Attest"},{"location":"helm_cas/#prerequisites","text":"The following prerequisites are required for using Helm to install confidential applications: A Kubernetes cluster and the helm setup is already done","title":"Prerequisites"},{"location":"helm_cas/#deploying-scone-cas","text":"The sconeapps/cas chart will deploy SCONE CAS . If we do not need to set any special affinity options and we want to deploy the development CAS, you can just execute: helm install cas sconeapps/cas This starts SCONE CAS in the default Kubernetes namespace. As an alternative, you can deploy SCONE CAS with KubeApps .","title":"Deploying SCONE CAS"},{"location":"helm_cas/#configuration-options","text":"You can learn about the configuration options by executing: helm install cas sconeapps/cas --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different image by first defining environment variable SCONE_CAS_IMAGE appropriately. You can do this as follows: helm install cas sconeapps/cas --set image = $SCONE_CAS_IMAGE You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status cas","title":"Configuration Options"},{"location":"helm_cas/#attesting-scone-cas","text":"Since we do not know if SCONE CAS was properly started, we need to attest SCONE CAS . Typically, the attestation will be performed by another already enclaved service. You can also perform the attestation manually. We assume that you trust your local computer that is properly executes the attestation. You can attest your SCONE CAS as follows. First, forward the traffic from your local computer to the release that you just started: kubectl port-forward service/cas 8081 :8081 Second, determine MRENCLAVE of the SCONE CAS image. Note for commercial customers, we provide a secure way to determine MRENCLAVE . A generic way to determine MRENCLAVE is as follows. We set the environment variable IMAGE to the name of the SCONE CAS image. If you changed the SCONE CAS image, we assume that you set the environment variable SCONE_CAS_IMAGE accordingly (see above). We then compute MRENCLAVE of SCONE CAS - assuming that you trust your local computer: export IMAGE = ${ SCONE_CAS_IMAGE :- sconecuratedimages /services: cas .trust.group-out-of-date } export CAS_MRENCLAVE = $( docker pull $IMAGE > /dev/null ; docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE cas ) The result you can view like this: $ echo $CAS_MRENCLAVE 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794 You can attest SCONE CAS on your local machine in executing the following (set CLI_IMAGE to an image that contains the SCONE CLI): docker run -e SCONE_MODE = SIM -it --rm $CLI_IMAGE scone cas attest -G --only_for_testing-debug host.docker.internal $CAS_MRENCLAVE It will print the following output if the SCONE CAS runs inside of an enclave: CAS host.docker.internal at https://host.docker.internal:8081/ is trustworthy and it will store the certificate of the SCONE CAS . This certificate can be used later to verify that we connect to the attested SCONE CAS . For attesting a SCONE CAS in production mode, you should use hardware mode (set DEVICE to your SGX device, i.e., /dev/sgx or /dev/isgx ): docker run --device = $DEVICE -e SCONE_MODE = HW -it --rm $CLI_IMAGE scone cas attest host.docker.internal $CAS_MRENCLAVE You can also try to attest with a wrong CAS_MRENCLAVE (e.g. by incrementing by 1): export CAS_MRENCLAVE = 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696795 docker run -e SCONE_MODE = SIM -it --rm $BASE_IMAGE scone cas attest -G --only_for_testing-debug host.docker.internal $CAS_MRENCLAVE The output would like this: Error: Error during attestation of CAS Caused by: Enclave hash does not match expectation: 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794","title":"Attesting SCONE CAS"},{"location":"helm_las/","text":"Deploying SCONE LAS SCONE LAS is the SCONE Local Attestation Service. It has to run on each platform, i.e., on each server, on which confidential applications should run. We explain how to start SCONE LAS with the help of helm . Note that SCONE LAS is implicitly attested by SCONE CAS and hence, we do not need to attest SCONE LAS explicitly. Prerequisites The following prerequisites are required for using Helm to install confidential applications: A Kubernetes cluster and the helm setup is already done Deploying SCONE LAS The sconeapps/las chart will deploy SCONE LAS : helm install las sconeapps/las This starts SCONE LAS in the default Kubernetes namespace. As an alternative, you can deploy SCONE LAS with KubeApps . Configuration Options You can learn about the configuration options by executing: helm install las sconeapps/las --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different SCONE LAS image by first defining environment variable SCONE_LAS_IMAGE appropriately. You can do this as follows: helm install las sconeapps/las --set image = $SCONE_LAS_IMAGE You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status las","title":"LAS Deploy"},{"location":"helm_las/#deploying-scone-las","text":"SCONE LAS is the SCONE Local Attestation Service. It has to run on each platform, i.e., on each server, on which confidential applications should run. We explain how to start SCONE LAS with the help of helm . Note that SCONE LAS is implicitly attested by SCONE CAS and hence, we do not need to attest SCONE LAS explicitly.","title":"Deploying SCONE LAS"},{"location":"helm_las/#prerequisites","text":"The following prerequisites are required for using Helm to install confidential applications: A Kubernetes cluster and the helm setup is already done","title":"Prerequisites"},{"location":"helm_las/#deploying-scone-las_1","text":"The sconeapps/las chart will deploy SCONE LAS : helm install las sconeapps/las This starts SCONE LAS in the default Kubernetes namespace. As an alternative, you can deploy SCONE LAS with KubeApps .","title":"Deploying SCONE LAS"},{"location":"helm_las/#configuration-options","text":"You can learn about the configuration options by executing: helm install las sconeapps/las --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different SCONE LAS image by first defining environment variable SCONE_LAS_IMAGE appropriately. You can do this as follows: helm install las sconeapps/las --set image = $SCONE_LAS_IMAGE You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status las","title":"Configuration Options"},{"location":"hostexample/","text":"Host Execution We now show how one can compile a simple hello world program in a container and how to execute the program in the container in simulation mode and on the host in hardware mode . Installation In this example, we assume that you run on a host and have installed the Intel SGX driver and a docker engine . Driver installation is not strictly necessary: without the driver, the program will be automatically be executed in simulation mode on the host. Detailed Description We first need to start a container which includes the SCONE crosscompiler which is based on Ubuntu 1 : docker run -it -v \" $PWD \" :/src sconecuratedimages/crosscompilers:ubuntu We map the local directory of the host into the container (via option -v ) to be able to executed the generated binary on the host. Now execute the following command inside the container to create the hello world program: cd / src cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program with some debug output in the container: SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World The output shows that SCONE is running in simulation mode : \" export SCONE_MODE=sim \" Note Our patched docker engine automatically maps the sgx device inside of containers. In this case, the program would actually be executed in hardware mode. Execution on host Now, exit the container by executing: exit On the host, you can now execute the generated helloworld program 2 . First, we need to ensure that there exists a SCONE configuration file . We store this in the local directory: cat > sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF Now, we start the program with debug messages and using the configuration file in the local directory: SCONE_CONFIG = \" $PWD \" /sgx-musl.conf SCONE_VERSION = 1 ./helloworld In case your host has the Intel sgx driver installed, the output will show that it is executed in hardware mode on the host: ... export SCONE_MODE=hw ... Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World If you do not have the sgx driver installed, the program runs in simulation mode. The Ubuntu crosscompiler creates static binaries that can run on a Linux host. \u21a9 The host must run Linux. \u21a9","title":"Host Execution"},{"location":"hostexample/#host-execution","text":"We now show how one can compile a simple hello world program in a container and how to execute the program in the container in simulation mode and on the host in hardware mode .","title":"Host Execution"},{"location":"hostexample/#installation","text":"In this example, we assume that you run on a host and have installed the Intel SGX driver and a docker engine . Driver installation is not strictly necessary: without the driver, the program will be automatically be executed in simulation mode on the host.","title":"Installation"},{"location":"hostexample/#detailed-description","text":"We first need to start a container which includes the SCONE crosscompiler which is based on Ubuntu 1 : docker run -it -v \" $PWD \" :/src sconecuratedimages/crosscompilers:ubuntu We map the local directory of the host into the container (via option -v ) to be able to executed the generated binary on the host. Now execute the following command inside the container to create the hello world program: cd / src cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program with some debug output in the container: SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World The output shows that SCONE is running in simulation mode : \" export SCONE_MODE=sim \" Note Our patched docker engine automatically maps the sgx device inside of containers. In this case, the program would actually be executed in hardware mode.","title":"Detailed Description"},{"location":"hostexample/#execution-on-host","text":"Now, exit the container by executing: exit On the host, you can now execute the generated helloworld program 2 . First, we need to ensure that there exists a SCONE configuration file . We store this in the local directory: cat > sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF Now, we start the program with debug messages and using the configuration file in the local directory: SCONE_CONFIG = \" $PWD \" /sgx-musl.conf SCONE_VERSION = 1 ./helloworld In case your host has the Intel sgx driver installed, the output will show that it is executed in hardware mode on the host: ... export SCONE_MODE=hw ... Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World If you do not have the sgx driver installed, the program runs in simulation mode. The Ubuntu crosscompiler creates static binaries that can run on a Linux host. \u21a9 The host must run Linux. \u21a9","title":"Execution on host"},{"location":"iexec_install/","text":"Patched SGX driver and Docker For running together with the iExec platform, you need to install a patched docker engine as well as a patched Intel SGX driver for now. The reason for that is that the iExec containers are so far started without explicitly mapping the isgx device into these containers. The patched docker engine and the patched sgx driver will make the sgx device available to all Docker containers that run on that host. To perform this installation, we provide a simple script. Ensure that this host does not contain any important data nor does it run any services, since this script will uninstall the current docker engine and replace it by the patched engine. Note that you might need to provide your docker credentials to be able to download the LAS images: if the installation fails please log into docker and rerun the script. Also, you need to send us email that we can give you access to the LAS (local attestation service) image. Ssh to the machine that you want to install and then copy the following statement and execute: curl -fssl https://raw.githubusercontent.com/scontain/install_patched_dependencies/master/install-host-prerequisites.sh | bash","title":"Patched driver & engine"},{"location":"iexec_install/#patched-sgx-driver-and-docker","text":"For running together with the iExec platform, you need to install a patched docker engine as well as a patched Intel SGX driver for now. The reason for that is that the iExec containers are so far started without explicitly mapping the isgx device into these containers. The patched docker engine and the patched sgx driver will make the sgx device available to all Docker containers that run on that host. To perform this installation, we provide a simple script. Ensure that this host does not contain any important data nor does it run any services, since this script will uninstall the current docker engine and replace it by the patched engine. Note that you might need to provide your docker credentials to be able to download the LAS images: if the installation fails please log into docker and rerun the script. Also, you need to send us email that we can give you access to the LAS (local attestation service) image. Ssh to the machine that you want to install and then copy the following statement and execute: curl -fssl https://raw.githubusercontent.com/scontain/install_patched_dependencies/master/install-host-prerequisites.sh | bash","title":"Patched SGX driver and Docker"},{"location":"installation/","text":"SCONE Installation We recommend to use Alpine Linux for container images using SCONE and and Ubuntu 16.04 LTS or Ubuntu 18.04 LTS for the hosts that run these container images. To ensure that your Ubuntu host has all software installed to run SCONE containers, you can just run: curl -fssl https://raw.githubusercontent.com/scontain/install_dependencies/master/install-host-prerequisites.sh | bash This script will check if the required components are already installed and installs only the components that have not yet been installed. Customized Installation You can customize the SCONE installation based on your needs. Depending on how you want to use SCONE, you could instead install software components on a per need basis: Simulation mode: if you want to run SCONE inside a container in simulation mode, you just need to install the docker engine on your host. Hardware mode: If you want to run your applications in hardware mode inside of containers, you need to install the docker engine and install the Intel SGX driver on your host. Running on a host: Running you applications on a host, you need to install the Intel SGX driver on the host. Running an iExec platform: The iExec containers are so far started without explicitly mapping the isgx device into these containers. Hence, you need to install the patched docker engine and the patched sgx driver. Running on a VM: Running you applications on a virtual machine, you need to install the Intel SGX driver on the VM and you have to ensure that your hypervisor supports SGX: You might want to install a patched version of KVM . Ensure that your CPU runs the newest microcode by updating the CPU microcode .","title":"Installation overview"},{"location":"installation/#scone-installation","text":"We recommend to use Alpine Linux for container images using SCONE and and Ubuntu 16.04 LTS or Ubuntu 18.04 LTS for the hosts that run these container images. To ensure that your Ubuntu host has all software installed to run SCONE containers, you can just run: curl -fssl https://raw.githubusercontent.com/scontain/install_dependencies/master/install-host-prerequisites.sh | bash This script will check if the required components are already installed and installs only the components that have not yet been installed.","title":"SCONE Installation"},{"location":"installation/#customized-installation","text":"You can customize the SCONE installation based on your needs. Depending on how you want to use SCONE, you could instead install software components on a per need basis: Simulation mode: if you want to run SCONE inside a container in simulation mode, you just need to install the docker engine on your host. Hardware mode: If you want to run your applications in hardware mode inside of containers, you need to install the docker engine and install the Intel SGX driver on your host. Running on a host: Running you applications on a host, you need to install the Intel SGX driver on the host. Running an iExec platform: The iExec containers are so far started without explicitly mapping the isgx device into these containers. Hence, you need to install the patched docker engine and the patched sgx driver. Running on a VM: Running you applications on a virtual machine, you need to install the Intel SGX driver on the VM and you have to ensure that your hypervisor supports SGX: You might want to install a patched version of KVM . Ensure that your CPU runs the newest microcode by updating the CPU microcode .","title":"Customized Installation"},{"location":"k8s_concepts/","text":"Kubernetes and SCONE Basics In this section, we describe how you can build confidential compute applications with the help of SCONE that run on top of a vanilla Kubernetes cluster. This means that you do not need to modify Kubernetes in any way. We assume that you are already familiar with basic Kubernetes concepts. We first introduce some confidential computing concepts and continue with a in-depth hello world tutorial . Over time, we will add more information explaining how to reduce the complexity of building, deploying and operating confidential applications on top of Kubernetes. SCONE Confidential Computing Concepts When developing a CC application, we need to consider the following concepts: (native) service : In Kubernetes, a service is the concept of exposing an application as a network service at one or more network ports. The network ports are actually served by one or more processes - in the context of confidential computing, all of these processes would need to be protected. To refer to service that is not protected by SCONE , we use the term native service . enclaved service : SCONE permits to run each process of a service inside of an SGX enclave . Hence, we call these enclaved services . All other services, we call native services . If such an enclave is in release mode , its memory cannot be read by any other process nor by the operating system or the hypervisor or any user. container : a container is a way to deploy services. In most cases, this will be a Docker container. Note that a container can be entered by users and the users can inspect all files in the container as well as the memory of the processes - unless the processes run in enclave s in release mode . containerized CC App : an application consisting of enclaved service. The enclaved services cannot be inspected by users. However, the file system can still be inspected unless we encrypt the file system. (transparently) encrypted volume : an enclaved service can get access to transparently encrypted volumes. To Kubernetes, these look like standard volumes. However, users that log into a container will only see encrypted files. An authorized enclaved service the files look like plain files because SCONE transparently decrypts/encrypts the files inside of the enclave. pod : a pod is a Kubernetes concept. It is the minimal deployable unit consisting of one or more containers executed on the same node. A pod can consist of enclaved as well as native services. CC pod is a SCONE concept. It is a pod that contains only enclaved services and all volumes are encrypted. Also, all communication between enclaved services is encrypted. IP address : a pod has an IP address. The network communication can be inspected unless it is encrypted. We explain below how SCONE helps to provide enclaved services with certificates such that secure channels can be established between enclaved services as well as between containerized CC Apps. TLS : SCONE can enforce that all enclaved services of a containerized CC App communicate with each other via TLS. This is the case for both enclaved services running in the same pod as well as enclaved services across pods. server certs : SCONE can generate server certificates for services. These certificates are generated by an enclaved service (SCONE CAS) and can directly be injected into the filesystem of an enclaved service: they are only visible to the enclaved service, i.e., they do not exist outside the enclave. client certs : SCONE can also generate client certificates. SCONE CAS can provides a containerized CC app with policy-controlled CA (as part of the SCONE CAS). In this way, one can enforce access control to services: only services with the appropriate client certificate are permitted to access a service. CA certs : SCONE can inject the CA certificates for the clients as well as the services. In this way, one can perform a mutual attestation via TLS: only if both communication partners run inside of enclaves, were properly initialized, have encrypted volumes that were not tempered with and run an untempered code, will be able to establish a TLS connection.","title":"Concepts"},{"location":"k8s_concepts/#kubernetes-and-scone-basics","text":"In this section, we describe how you can build confidential compute applications with the help of SCONE that run on top of a vanilla Kubernetes cluster. This means that you do not need to modify Kubernetes in any way. We assume that you are already familiar with basic Kubernetes concepts. We first introduce some confidential computing concepts and continue with a in-depth hello world tutorial . Over time, we will add more information explaining how to reduce the complexity of building, deploying and operating confidential applications on top of Kubernetes.","title":"Kubernetes and SCONE Basics"},{"location":"k8s_concepts/#scone-confidential-computing-concepts","text":"When developing a CC application, we need to consider the following concepts: (native) service : In Kubernetes, a service is the concept of exposing an application as a network service at one or more network ports. The network ports are actually served by one or more processes - in the context of confidential computing, all of these processes would need to be protected. To refer to service that is not protected by SCONE , we use the term native service . enclaved service : SCONE permits to run each process of a service inside of an SGX enclave . Hence, we call these enclaved services . All other services, we call native services . If such an enclave is in release mode , its memory cannot be read by any other process nor by the operating system or the hypervisor or any user. container : a container is a way to deploy services. In most cases, this will be a Docker container. Note that a container can be entered by users and the users can inspect all files in the container as well as the memory of the processes - unless the processes run in enclave s in release mode . containerized CC App : an application consisting of enclaved service. The enclaved services cannot be inspected by users. However, the file system can still be inspected unless we encrypt the file system. (transparently) encrypted volume : an enclaved service can get access to transparently encrypted volumes. To Kubernetes, these look like standard volumes. However, users that log into a container will only see encrypted files. An authorized enclaved service the files look like plain files because SCONE transparently decrypts/encrypts the files inside of the enclave. pod : a pod is a Kubernetes concept. It is the minimal deployable unit consisting of one or more containers executed on the same node. A pod can consist of enclaved as well as native services. CC pod is a SCONE concept. It is a pod that contains only enclaved services and all volumes are encrypted. Also, all communication between enclaved services is encrypted. IP address : a pod has an IP address. The network communication can be inspected unless it is encrypted. We explain below how SCONE helps to provide enclaved services with certificates such that secure channels can be established between enclaved services as well as between containerized CC Apps. TLS : SCONE can enforce that all enclaved services of a containerized CC App communicate with each other via TLS. This is the case for both enclaved services running in the same pod as well as enclaved services across pods. server certs : SCONE can generate server certificates for services. These certificates are generated by an enclaved service (SCONE CAS) and can directly be injected into the filesystem of an enclaved service: they are only visible to the enclaved service, i.e., they do not exist outside the enclave. client certs : SCONE can also generate client certificates. SCONE CAS can provides a containerized CC app with policy-controlled CA (as part of the SCONE CAS). In this way, one can enforce access control to services: only services with the appropriate client certificate are permitted to access a service. CA certs : SCONE can inject the CA certificates for the clients as well as the services. In this way, one can perform a mutual attestation via TLS: only if both communication partners run inside of enclaves, were properly initialized, have encrypted volumes that were not tempered with and run an untempered code, will be able to establish a TLS connection.","title":"SCONE Confidential Computing Concepts"},{"location":"kubeapps/","text":"KubeApps You can use KubeApps to deploy and manage your confidential applications. KubeApps is a dashboard for helm . While probably most applications will be be deployed via the helm CLI , using a dashboard is a convenient way to inspect the running applications and to test new applications. Deploy and Manage We provide a catalog with sconeapps , i.e., curated, confidential applications that can be installed with the help of helm or via point and click using KubeApps. KubeApps provides you with a view of available sconeapps , like this: You can select an application that you want to start and deploy it as follows. Deploy a sconeapp When deploying an application, you can customize its configuration values. For example, for LAS, i.e., the SCONE's local attestation service, you will be able to configure the parameters of the Helm chart that is used to install this application: Inspecting Applications KubeApps is a dashboard that can show you all running applications. You can select an application for inspection. Inspecting LAS A view of the LAS application that we started above, will look as follows: Deploying Kubeapps sconeapps is a private Helm repository, we need to grant you access and you need a GitHub token that has permission to access this \"repo\". Define an environment variable that contains this token: export GH_TOKEN = ... Use it to add this repository to start KubeApps : if [ \" $GH_TOKEN \" x = \"x\" ] ; then echo \"You need to set you github token: https://github.com/settings/tokens/new\" else cat > kubeapps_values.yml <<EOF apprepository: initialRepos: - name: sconeapps url: https://${GH_TOKEN}@raw.githubusercontent.com/scontain/sconeapps/master/ - name: bitnami url: https://charts.bitnami.com/bitnami EOF fi You can now start KubeApps with the help of helm as follows: kubectl create namespace kubeapps || echo \"Does namespace 'kubeapps' already exists?\" helm install -f kubeapps_values.yml kubeapps --namespace kubeapps bitnami/kubeapps --set useHelm3 = true Access Control For production, you should define a Role-based Access Control. For testing, you might want to create a service account: kubectl create serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole = cluster-admin --serviceaccount = default:kubeapps-operator To log into the KubeApps dashboard, you need to determine the API Token: APITOKEN = $( kubectl get -n default secret $( kubectl get -n default serviceaccount kubeapps-operator -o jsonpath = '{.secrets[].name}' ) -o go-template = '{{.data.token | base64decode}}' && echo ) echo $APITOKEN","title":"KubeApps"},{"location":"kubeapps/#kubeapps","text":"You can use KubeApps to deploy and manage your confidential applications. KubeApps is a dashboard for helm . While probably most applications will be be deployed via the helm CLI , using a dashboard is a convenient way to inspect the running applications and to test new applications.","title":"KubeApps"},{"location":"kubeapps/#deploy-and-manage","text":"We provide a catalog with sconeapps , i.e., curated, confidential applications that can be installed with the help of helm or via point and click using KubeApps. KubeApps provides you with a view of available sconeapps , like this: You can select an application that you want to start and deploy it as follows.","title":"Deploy and Manage"},{"location":"kubeapps/#deploy-a-sconeapp","text":"When deploying an application, you can customize its configuration values. For example, for LAS, i.e., the SCONE's local attestation service, you will be able to configure the parameters of the Helm chart that is used to install this application:","title":"Deploy a sconeapp"},{"location":"kubeapps/#inspecting-applications","text":"KubeApps is a dashboard that can show you all running applications. You can select an application for inspection.","title":"Inspecting Applications"},{"location":"kubeapps/#inspecting-las","text":"A view of the LAS application that we started above, will look as follows:","title":"Inspecting LAS"},{"location":"kubeapps/#deploying-kubeapps","text":"sconeapps is a private Helm repository, we need to grant you access and you need a GitHub token that has permission to access this \"repo\". Define an environment variable that contains this token: export GH_TOKEN = ... Use it to add this repository to start KubeApps : if [ \" $GH_TOKEN \" x = \"x\" ] ; then echo \"You need to set you github token: https://github.com/settings/tokens/new\" else cat > kubeapps_values.yml <<EOF apprepository: initialRepos: - name: sconeapps url: https://${GH_TOKEN}@raw.githubusercontent.com/scontain/sconeapps/master/ - name: bitnami url: https://charts.bitnami.com/bitnami EOF fi You can now start KubeApps with the help of helm as follows: kubectl create namespace kubeapps || echo \"Does namespace 'kubeapps' already exists?\" helm install -f kubeapps_values.yml kubeapps --namespace kubeapps bitnami/kubeapps --set useHelm3 = true","title":"Deploying Kubeapps"},{"location":"kubeapps/#access-control","text":"For production, you should define a Role-based Access Control. For testing, you might want to create a service account: kubectl create serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole = cluster-admin --serviceaccount = default:kubeapps-operator To log into the KubeApps dashboard, you need to determine the API Token: APITOKEN = $( kubectl get -n default secret $( kubectl get -n default serviceaccount kubeapps-operator -o jsonpath = '{.secrets[].name}' ) -o go-template = '{{.data.token | base64decode}}' && echo ) echo $APITOKEN","title":"Access Control"},{"location":"memory_dump/","text":"Finding Secrets... One of the main advantages of SGX technology is that it cannot only protect against external attackers but also against internal attackers that have root access. We introduce a simple approach to demonstrate that your application is indeed running inside of an enclave and that its secrets are not accessible. Note To ensure that your SCONE application runs inside of an enclave and to distribute secrets, please to use the SCONE configuration and attestation service . To demonstrate how one can check that SGX/SCONE can protect against users with root access 1 , let's consider a simplistic example : we store some secret in a variable secret 2 . By default, binaries are not encrypted. Hence, do not store any secrets in a binary as it is done in this simplistic example. If you really need to to protect your binaries - for example, since you have some legacy code with embedded secrets - SCONE can protect these secrets by permitting to encrypt shared libraries. Example We use the SCONE crosscompiler image and start an container with access to the SGX device: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Let's create some program that stores a secret ( MYBIGS ) in a local variable secret : cat > mysecret.c << EOF #include <stdio.h> #include <unistd.h> const char *code_is_not_encrypted=\"THIS_IS_NOT_SECRET\"; int main() { char secret[7]; secret[0] ='M'; secret[1] ='Y'; secret[2] ='B'; secret[3] ='I'; secret[4] ='G'; secret[5] ='S'; secret[6] =0; printf(\"'%s' SECRET at %lx\\n\", secret, secret); printf(\"Kill with Ctrl-C.\\n\"); for(;;) sleep(1); // loop forever } EOF Compile this program with the SCONE crosscompiler (i.e., gcc): gcc -g -o mysecret mysecret.c Simulation Mode You can run this program in SIMULATION MODE , i.e., this program does not protect your secrets: SCONE_VERSION = 1 SCONE_MODE = SIM SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Log into a different terminal on your host . Let us figure out the process ID of the mysecret program: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) Now, we can dump the memory of this process via the /proc filesystem. You can determine the different memory regions of your process via cat /proc/$SPID/maps and the memory is stored in /proc/$SPID/mem . We can use the following Python program to write all pages to stdout : cat > dumpstack.py << EOF import sys, os, string, re pid = sys.argv[1] maps_file = open(\"/proc/%s/maps\" % pid, 'r') mem_file = open(\"/proc/%s/mem\" % pid, 'r') r=0 for line in maps_file.readlines(): # for each mapped region w=line.rsplit(None, 1)[-1] # last word if w != \"/dev/isgx\" and w != \"[vvar]\" and w != \"[vdso]\" and w != \"[vsyscall]\": m = re.match(r'([0-9A-Fa-f]+)-([0-9A-Fa-f]+) ([-r])', line) r += 1 p = 0 if m.group(3) == 'r': # if this is a readable region start = int(m.group(1), 16) end = int(m.group(2), 16) while start < end: try: mem_file.seek(start) # seek to region start chunk = mem_file.read(4096) # read region contents sys.stdout.write(chunk) p += 1 if p > 1000: sys.stderr.write(\"region = %02d, index=%x \\r\" % (r,start)) p = 0 start += 4096 except: pass sys.stderr.write(\"\\n\") EOF Run this program and grep for the prefix of our secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will take some time but eventually it will print the full secret MYBIGS . Hardware Mode Let us now run the program in hardware mode. First, ensure that you kill the original program by typing control-C . Let's start mysecret in an enclave, i.e., in hardware mode inside the container: SCONE_VERSION = 1 SCONE_MODE = HW SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Update environment variable SPID in a second terminal on your host: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) and then try to find the secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will run for much less time and in particular, it will not print any secrets. Note, however, that secrets stored in the binaries can be found because the binary is not encrypted: a copy of the original binary - which is used to start the enclave - stays in main memory outside the enclave. Let's look for the string THIS_IS_NOT_SECRET in our example application. We can find this secret as follows: sudo python dumpstack.py $SPID | strings -n 5 | grep THIS_IS_NOT_SECRET Note that the enclave runs in this example runs in debug mode, i.e., one can still attach to this enclave with scone-gdb . To prevent access via the debugger, you need to run your enclave in production mode . \u21a9 Note that an adversary could analyse the binary and figure out the secret. The standard way to provide an enclave with secrets is to use SCONE CAS . \u21a9","title":"Finding Secrets"},{"location":"memory_dump/#finding-secrets","text":"One of the main advantages of SGX technology is that it cannot only protect against external attackers but also against internal attackers that have root access. We introduce a simple approach to demonstrate that your application is indeed running inside of an enclave and that its secrets are not accessible. Note To ensure that your SCONE application runs inside of an enclave and to distribute secrets, please to use the SCONE configuration and attestation service . To demonstrate how one can check that SGX/SCONE can protect against users with root access 1 , let's consider a simplistic example : we store some secret in a variable secret 2 . By default, binaries are not encrypted. Hence, do not store any secrets in a binary as it is done in this simplistic example. If you really need to to protect your binaries - for example, since you have some legacy code with embedded secrets - SCONE can protect these secrets by permitting to encrypt shared libraries.","title":"Finding Secrets..."},{"location":"memory_dump/#example","text":"We use the SCONE crosscompiler image and start an container with access to the SGX device: docker pull sconecuratedimages/crosscompilers docker run --device = /dev/isgx -it sconecuratedimages/crosscompilers Let's create some program that stores a secret ( MYBIGS ) in a local variable secret : cat > mysecret.c << EOF #include <stdio.h> #include <unistd.h> const char *code_is_not_encrypted=\"THIS_IS_NOT_SECRET\"; int main() { char secret[7]; secret[0] ='M'; secret[1] ='Y'; secret[2] ='B'; secret[3] ='I'; secret[4] ='G'; secret[5] ='S'; secret[6] =0; printf(\"'%s' SECRET at %lx\\n\", secret, secret); printf(\"Kill with Ctrl-C.\\n\"); for(;;) sleep(1); // loop forever } EOF Compile this program with the SCONE crosscompiler (i.e., gcc): gcc -g -o mysecret mysecret.c","title":"Example"},{"location":"memory_dump/#simulation-mode","text":"You can run this program in SIMULATION MODE , i.e., this program does not protect your secrets: SCONE_VERSION = 1 SCONE_MODE = SIM SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Log into a different terminal on your host . Let us figure out the process ID of the mysecret program: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) Now, we can dump the memory of this process via the /proc filesystem. You can determine the different memory regions of your process via cat /proc/$SPID/maps and the memory is stored in /proc/$SPID/mem . We can use the following Python program to write all pages to stdout : cat > dumpstack.py << EOF import sys, os, string, re pid = sys.argv[1] maps_file = open(\"/proc/%s/maps\" % pid, 'r') mem_file = open(\"/proc/%s/mem\" % pid, 'r') r=0 for line in maps_file.readlines(): # for each mapped region w=line.rsplit(None, 1)[-1] # last word if w != \"/dev/isgx\" and w != \"[vvar]\" and w != \"[vdso]\" and w != \"[vsyscall]\": m = re.match(r'([0-9A-Fa-f]+)-([0-9A-Fa-f]+) ([-r])', line) r += 1 p = 0 if m.group(3) == 'r': # if this is a readable region start = int(m.group(1), 16) end = int(m.group(2), 16) while start < end: try: mem_file.seek(start) # seek to region start chunk = mem_file.read(4096) # read region contents sys.stdout.write(chunk) p += 1 if p > 1000: sys.stderr.write(\"region = %02d, index=%x \\r\" % (r,start)) p = 0 start += 4096 except: pass sys.stderr.write(\"\\n\") EOF Run this program and grep for the prefix of our secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will take some time but eventually it will print the full secret MYBIGS .","title":"Simulation Mode"},{"location":"memory_dump/#hardware-mode","text":"Let us now run the program in hardware mode. First, ensure that you kill the original program by typing control-C . Let's start mysecret in an enclave, i.e., in hardware mode inside the container: SCONE_VERSION = 1 SCONE_MODE = HW SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Update environment variable SPID in a second terminal on your host: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) and then try to find the secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will run for much less time and in particular, it will not print any secrets. Note, however, that secrets stored in the binaries can be found because the binary is not encrypted: a copy of the original binary - which is used to start the enclave - stays in main memory outside the enclave. Let's look for the string THIS_IS_NOT_SECRET in our example application. We can find this secret as follows: sudo python dumpstack.py $SPID | strings -n 5 | grep THIS_IS_NOT_SECRET Note that the enclave runs in this example runs in debug mode, i.e., one can still attach to this enclave with scone-gdb . To prevent access via the debugger, you need to run your enclave in production mode . \u21a9 Note that an adversary could analyse the binary and figure out the secret. The standard way to provide an enclave with secrets is to use SCONE CAS . \u21a9","title":"Hardware Mode"},{"location":"microcode/","text":"Microcode update / Side-channel Mitigation Intel SGX is susceptible to certain side channel attacks in which an attacker can read the secrets of an enclave E by starting an enclave A on the same core as E . This attack only works for secrets of E stored in a L1 cache and if A and E run on the same core. This attack is referred to as enclave-to-enclave (E2E) attack. E2E does not work if E and A run on different cores since each core has its own dedicated L1 cache. The current Intel microcode ensures that the L1 cache of a core is flushed when a thread leaves the enclave. In this way, if A runs after E , it will not be able to read any secrets from L1 . Note that flushing L1 is an expensive operation. SCONE keeps the threads running inside of an enclave and in this way avoids the cost of flushing L1 on each system call. The microcode update by Intel protects against L1TF only if hyperthreading is disabled! If enclave A would run on one hyperthread and E on the second hyperthread of the same core, A could still read the values of E stored in L1 using the L1TF side channel. Hence, you must also disable the hyperthreading inside of your BIOS 1 . If hyperthreading is switched on/off, is checked during attestation. In other words, one can ensure during attestation that the enclave is protected against L1TF. According to the Intel documentation , an update microcode combined with switched off hyperthreading will fully mitigate L1TF and E2E for Intel SGX . Microcode Update on Ubuntu Note that the BIOS loads the microcode during boot. The operating system can load a newer version of the microcode, for example, in case there is no new microcode update available yet for your BIOS. In this case, the OS has to load the new microcode on each new boot. On Ubuntu, you can upgrade your CPU to the newest microcode as follows: TMPDIR = $( mktemp -d ) cd $TMPDIR git clone https://github.com/intel/Intel-Linux-Processor-Microcode-Data-Files.git cd Intel-Linux-Processor-Microcode-Data-Files sudo apt-get update sudo apt-get install -y intel-microcode if [ -f /sys/devices/system/cpu/microcode/reload ] ; then if [ -d /lib/firmware ] ; then mkdir -p OLD cp -rf /lib/firmware/intel-ucode OLD sudo cp -rf intel-ucode /lib/firmware echo \"1\" | sudo tee /sys/devices/system/cpu/microcode/reload else echo \"Error: microcode directory does not exist\" fi else echo \"Error: is intel-micrcode really installed?\" fi You can also execute the a script that we provide: curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_microcode.sh | bash In case your microcode is already up-to-date, it will print a message like this: Already newest version of microcode installed: [ 0.000000] microcode: microcode updated early to revision 0xca, date = 2019-10-03 In case your microcode is not yet up-to-date, it will print a message like this: Updated microcode from version: to version: [ 0.000000] microcode: microcode updated early to revision 0xca, date = 2019-10-03 When you do not have the newest microcode installed, you can load this microcode during reboot as follows: cat > /tmp/load-intel-ucode.sh << EOF #!/bin/bash echo \"1\" | sudo tee /sys/devices/system/cpu/microcode/reload EOF sudo mv /tmp/load-intel-ucode.sh /lib/firmware/load-intel-ucode.sh chmod a+x /lib/firmware/load-intel-ucode.sh # the following cmd pops up the editor: add the @reboot... via the editor crontab -e @reboot /lib/firmware/load-intel-ucode.sh Checking CPU Features CPU microcode updates that protects against side channels like L1TF are not available for all CPUs. You can check that your microcode has up to date protection features enabled by executing the following: docker run --rm sconecuratedimages/apps:check_cpuid We have published an alternative to switching off hyperthreading in Usenix ATC 2018 : we ensure that both hyperthreads of a core run inside of the same enclave. \u21a9","title":"Updating CPU microcode"},{"location":"microcode/#microcode-update-side-channel-mitigation","text":"Intel SGX is susceptible to certain side channel attacks in which an attacker can read the secrets of an enclave E by starting an enclave A on the same core as E . This attack only works for secrets of E stored in a L1 cache and if A and E run on the same core. This attack is referred to as enclave-to-enclave (E2E) attack. E2E does not work if E and A run on different cores since each core has its own dedicated L1 cache. The current Intel microcode ensures that the L1 cache of a core is flushed when a thread leaves the enclave. In this way, if A runs after E , it will not be able to read any secrets from L1 . Note that flushing L1 is an expensive operation. SCONE keeps the threads running inside of an enclave and in this way avoids the cost of flushing L1 on each system call. The microcode update by Intel protects against L1TF only if hyperthreading is disabled! If enclave A would run on one hyperthread and E on the second hyperthread of the same core, A could still read the values of E stored in L1 using the L1TF side channel. Hence, you must also disable the hyperthreading inside of your BIOS 1 . If hyperthreading is switched on/off, is checked during attestation. In other words, one can ensure during attestation that the enclave is protected against L1TF. According to the Intel documentation , an update microcode combined with switched off hyperthreading will fully mitigate L1TF and E2E for Intel SGX .","title":"Microcode update / Side-channel Mitigation"},{"location":"microcode/#microcode-update-on-ubuntu","text":"Note that the BIOS loads the microcode during boot. The operating system can load a newer version of the microcode, for example, in case there is no new microcode update available yet for your BIOS. In this case, the OS has to load the new microcode on each new boot. On Ubuntu, you can upgrade your CPU to the newest microcode as follows: TMPDIR = $( mktemp -d ) cd $TMPDIR git clone https://github.com/intel/Intel-Linux-Processor-Microcode-Data-Files.git cd Intel-Linux-Processor-Microcode-Data-Files sudo apt-get update sudo apt-get install -y intel-microcode if [ -f /sys/devices/system/cpu/microcode/reload ] ; then if [ -d /lib/firmware ] ; then mkdir -p OLD cp -rf /lib/firmware/intel-ucode OLD sudo cp -rf intel-ucode /lib/firmware echo \"1\" | sudo tee /sys/devices/system/cpu/microcode/reload else echo \"Error: microcode directory does not exist\" fi else echo \"Error: is intel-micrcode really installed?\" fi You can also execute the a script that we provide: curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_microcode.sh | bash In case your microcode is already up-to-date, it will print a message like this: Already newest version of microcode installed: [ 0.000000] microcode: microcode updated early to revision 0xca, date = 2019-10-03 In case your microcode is not yet up-to-date, it will print a message like this: Updated microcode from version: to version: [ 0.000000] microcode: microcode updated early to revision 0xca, date = 2019-10-03 When you do not have the newest microcode installed, you can load this microcode during reboot as follows: cat > /tmp/load-intel-ucode.sh << EOF #!/bin/bash echo \"1\" | sudo tee /sys/devices/system/cpu/microcode/reload EOF sudo mv /tmp/load-intel-ucode.sh /lib/firmware/load-intel-ucode.sh chmod a+x /lib/firmware/load-intel-ucode.sh # the following cmd pops up the editor: add the @reboot... via the editor crontab -e @reboot /lib/firmware/load-intel-ucode.sh","title":"Microcode Update on Ubuntu"},{"location":"microcode/#checking-cpu-features","text":"CPU microcode updates that protects against side channels like L1TF are not available for all CPUs. You can check that your microcode has up to date protection features enabled by executing the following: docker run --rm sconecuratedimages/apps:check_cpuid We have published an alternative to switching off hyperthreading in Usenix ATC 2018 : we ensure that both hyperthreads of a core run inside of the same enclave. \u21a9","title":"Checking CPU Features"},{"location":"multistagebuild/","text":"Multi-Stage Build As we mentioned in the context of the dockerfile example , that you should not include the SCONE platform in the images you build - at least if you intent to push you images to public repositories. The easiest way to achieve this, is to use multi-stage builds. The idea is to build you application with the scone cross-compiler image (i.e., sconecuratedimages/crosscompilers ) image and then copy the application to another container with a different base image. You must ensure that you copy all parts of your application are included. If you use static linking , this can be easier than using dynamic linking. We show how to generate a Docker image of a dynamically linked application: we show this for groupcache . Getting access You need access to a private docker hub repository sconecuratedimages/crosscompilers to execute this example. Send us your docker hub ID to get access to this repository. We do want to make sure that the image is as small as possible and in particular, that the image must not contain the SCONE crosscompilers. Hence, we use a multi-stage build during which we copy all dependencies of groupcache : cat > Dockerfile << EOF FROM sconecuratedimages/crosscompilers RUN apk update \\ && apk add git curl go \\ && go get -compiler gccgo -u github.com/golang/groupcache \\ && curl -fsSL --output groupcache.go https://gist.githubusercontent.com/fiorix/816117cfc7573319b72d/raw/797d2ed5b567dcffb8ebd8896a3d7671b1a44b31/groupcache.go \\ && export SCONE_HEAP=1G \\ && go build -compiler gccgo -buildmode=exe groupcache.go FROM alpine:latest COPY --from=0 /groupcache / COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 COPY --from=0 /opt/scone/lib/ld-scone-x86_64.so.1 /opt/scone/lib/ld-scone-x86_64.so.1 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 COPY --from=0 /etc/sgx-musl.conf /etc/sgx-musl.conf CMD sh -c \"SCONE_HEAP=1G /groupcache\" EOF Note that one can figure out the libraries to copy with command ldd groupcache . Let's generate an image groupcache with this Dockerfile: docker build --pull -t groupcache . The size of the groupcache image is about 65MB. You can run this container by executing: docker run --rm --publish 8080 :8080 groupcache You can now query this service from a different terminal on the host this service, e.g.,: curl localhost:8080/color?name = green Warning This service has multiple security issues : we show how to address these with the help of the SCONE shields in a later section. Screencast","title":"Multi-stage build"},{"location":"multistagebuild/#multi-stage-build","text":"As we mentioned in the context of the dockerfile example , that you should not include the SCONE platform in the images you build - at least if you intent to push you images to public repositories. The easiest way to achieve this, is to use multi-stage builds. The idea is to build you application with the scone cross-compiler image (i.e., sconecuratedimages/crosscompilers ) image and then copy the application to another container with a different base image. You must ensure that you copy all parts of your application are included. If you use static linking , this can be easier than using dynamic linking. We show how to generate a Docker image of a dynamically linked application: we show this for groupcache . Getting access You need access to a private docker hub repository sconecuratedimages/crosscompilers to execute this example. Send us your docker hub ID to get access to this repository. We do want to make sure that the image is as small as possible and in particular, that the image must not contain the SCONE crosscompilers. Hence, we use a multi-stage build during which we copy all dependencies of groupcache : cat > Dockerfile << EOF FROM sconecuratedimages/crosscompilers RUN apk update \\ && apk add git curl go \\ && go get -compiler gccgo -u github.com/golang/groupcache \\ && curl -fsSL --output groupcache.go https://gist.githubusercontent.com/fiorix/816117cfc7573319b72d/raw/797d2ed5b567dcffb8ebd8896a3d7671b1a44b31/groupcache.go \\ && export SCONE_HEAP=1G \\ && go build -compiler gccgo -buildmode=exe groupcache.go FROM alpine:latest COPY --from=0 /groupcache / COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 COPY --from=0 /opt/scone/lib/ld-scone-x86_64.so.1 /opt/scone/lib/ld-scone-x86_64.so.1 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 COPY --from=0 /etc/sgx-musl.conf /etc/sgx-musl.conf CMD sh -c \"SCONE_HEAP=1G /groupcache\" EOF Note that one can figure out the libraries to copy with command ldd groupcache . Let's generate an image groupcache with this Dockerfile: docker build --pull -t groupcache . The size of the groupcache image is about 65MB. You can run this container by executing: docker run --rm --publish 8080 :8080 groupcache You can now query this service from a different terminal on the host this service, e.g.,: curl localhost:8080/color?name = green Warning This service has multiple security issues : we show how to address these with the help of the SCONE shields in a later section.","title":"Multi-Stage Build"},{"location":"multistagebuild/#screencast","text":"","title":"Screencast"},{"location":"network-shield/","text":"Network Shield Introduction Enclaves may need to communicate with each other. In case this communication is not encrypted, men in the middle (MitM) can eavesdrop on, alter or forge messages. Note that, in the context of enclaves, a third party, network operator, system administrator, and even the operating system may be considered MitM. Even if the application itself encrypts its traffic, there are no guarantees that the remote end resides inside of an enclave. The network shield solves both of these problems. Key characteristics of the network shield include: Confidentiality and integrity of network communication: Encryption and message authentication (through TLS) prevent eavesdropping or message forgery. Authentication of remote entities : The network shield ensures only communication with previously authorized remote peers is allowed, and those peers also run in a protected enclave. Fully transparent : The user application does not have to be changed in any way in order to make use of the network protection. Only the shield itself needs to be configured. Security by default : The network shield is always enabled by default, it has to be explicitly disabled (if required). Using unrecognized methods, flags or connections will be prevented. Problems Addressed SCONE provides a network shield that automatically wraps TCP connections in TLS. The objectives of the network shield are as follows: Encryption of communication : Some services - like some versions of memcached - that were designed at a time when communication within a data center was considered secure, might not support TLS protection of TCP connections. Access control : The network shield can also be used for automatic access control, i.e., we can limit what clients can connect to a service. To do so, the network shield automatically generates TLS certificates that a specific to a session (i.e., an instance of a security policy). TLS over TLS : in some data centers, TLS has to be terminated within the load balancers to enforce central access control to services running within a data center. The issue is that the communication within the data center is unprotected. With the help network shield one can implement TLS-over-TLS for outgoing connections as well as terminating the additional TLS connection for incoming connections. Example: Access Control Let's look at an example in which transparently protect all connections between the nginx-server and the nginx-proxy . In this case, we only permit connections from the nginx-proxy , i.e., only clients that have a valid nginx-proxy certificate are permitted to connect to the nginx-server . version: '1.0beta' alias: nginx-benchmark services: - name: nginx-server command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"http_server.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-server incoming: ports: - port: 10000 authorized-clients: services: - name: nginx-proxy - name: nginx-proxy command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"http_proxy.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-proxy outgoing: authorized-servers: services: - name: nginx-server port: 10000 incoming: ports: - port: 10001 unprotected: true The http configuration of the server looks as follows: events { worker_connections 512; use select; } error_log stderr info; pid /tmp/nginx_server.pid; master_process off; daemon off; http { server { root ./content/; listen 10000; access_log off; open_file_cache max=1000 inactive=120s; location / { tcp_nodelay on; keepalive_requests 2000000000; keepalive_timeout 300s; } } } Example: TLS-over-TLS We can wrap the TLS connection of nginx server that already uses https : version: '1.0beta' alias: nginx-benchmark services: - name: nginx-server command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"https_server.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-server incoming: ports: - port: 10000 authorized-clients: services: - name: nginx-proxy - name: nginx-proxy command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"https_proxy.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-proxy outgoing: authorized-servers: services: - name: nginx-server port: 10000 incoming: ports: - port: 10001 unprotected: true The nginx configuration looks as follows: events { worker_connections 512; } error_log stderr warn; pid /tmp/nginx_server.pid; master_process off; daemon off; http { server { root ./content/; listen 10000 ssl; access_log off; open_file_cache max=1000 inactive=120s; location / { tcp_nodelay on; keepalive_requests 2000000000; keepalive_timeout 300s; } ssl_protocols TLSv1.2; ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384\"; ssl_ecdh_curve \"secp384r1:prime256v1\"; # Note that the curve of the certificate, prime256v1 in this case, MUST be listed here, otherwise the connection fails. ECDHE still chooses the first curve if possible. ssl_prefer_server_ciphers on; ssl_session_tickets off; ssl_session_cache off; ssl_certificate_key ./pki/srv.pem.key; ssl_certificate ./pki/srv.bundle.pem.crt; } }","title":"Network Shield"},{"location":"network-shield/#network-shield","text":"","title":"Network Shield"},{"location":"network-shield/#introduction","text":"Enclaves may need to communicate with each other. In case this communication is not encrypted, men in the middle (MitM) can eavesdrop on, alter or forge messages. Note that, in the context of enclaves, a third party, network operator, system administrator, and even the operating system may be considered MitM. Even if the application itself encrypts its traffic, there are no guarantees that the remote end resides inside of an enclave. The network shield solves both of these problems. Key characteristics of the network shield include: Confidentiality and integrity of network communication: Encryption and message authentication (through TLS) prevent eavesdropping or message forgery. Authentication of remote entities : The network shield ensures only communication with previously authorized remote peers is allowed, and those peers also run in a protected enclave. Fully transparent : The user application does not have to be changed in any way in order to make use of the network protection. Only the shield itself needs to be configured. Security by default : The network shield is always enabled by default, it has to be explicitly disabled (if required). Using unrecognized methods, flags or connections will be prevented.","title":"Introduction"},{"location":"network-shield/#problems-addressed","text":"SCONE provides a network shield that automatically wraps TCP connections in TLS. The objectives of the network shield are as follows: Encryption of communication : Some services - like some versions of memcached - that were designed at a time when communication within a data center was considered secure, might not support TLS protection of TCP connections. Access control : The network shield can also be used for automatic access control, i.e., we can limit what clients can connect to a service. To do so, the network shield automatically generates TLS certificates that a specific to a session (i.e., an instance of a security policy). TLS over TLS : in some data centers, TLS has to be terminated within the load balancers to enforce central access control to services running within a data center. The issue is that the communication within the data center is unprotected. With the help network shield one can implement TLS-over-TLS for outgoing connections as well as terminating the additional TLS connection for incoming connections.","title":"Problems Addressed"},{"location":"network-shield/#example-access-control","text":"Let's look at an example in which transparently protect all connections between the nginx-server and the nginx-proxy . In this case, we only permit connections from the nginx-proxy , i.e., only clients that have a valid nginx-proxy certificate are permitted to connect to the nginx-server . version: '1.0beta' alias: nginx-benchmark services: - name: nginx-server command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"http_server.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-server incoming: ports: - port: 10000 authorized-clients: services: - name: nginx-proxy - name: nginx-proxy command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"http_proxy.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-proxy outgoing: authorized-servers: services: - name: nginx-server port: 10000 incoming: ports: - port: 10001 unprotected: true The http configuration of the server looks as follows: events { worker_connections 512; use select; } error_log stderr info; pid /tmp/nginx_server.pid; master_process off; daemon off; http { server { root ./content/; listen 10000; access_log off; open_file_cache max=1000 inactive=120s; location / { tcp_nodelay on; keepalive_requests 2000000000; keepalive_timeout 300s; } } }","title":"Example: Access Control"},{"location":"network-shield/#example-tls-over-tls","text":"We can wrap the TLS connection of nginx server that already uses https : version: '1.0beta' alias: nginx-benchmark services: - name: nginx-server command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"https_server.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-server incoming: ports: - port: 10000 authorized-clients: services: - name: nginx-proxy - name: nginx-proxy command: [\"nginx\", \"-p\", \"/nginx\", \"-c\", \"https_proxy.conf\"] pwd: / attestation: disabled-for-testing-purposes: true network: hostname: nginx-proxy outgoing: authorized-servers: services: - name: nginx-server port: 10000 incoming: ports: - port: 10001 unprotected: true The nginx configuration looks as follows: events { worker_connections 512; } error_log stderr warn; pid /tmp/nginx_server.pid; master_process off; daemon off; http { server { root ./content/; listen 10000 ssl; access_log off; open_file_cache max=1000 inactive=120s; location / { tcp_nodelay on; keepalive_requests 2000000000; keepalive_timeout 300s; } ssl_protocols TLSv1.2; ssl_ciphers \"ECDHE-ECDSA-AES256-GCM-SHA384\"; ssl_ecdh_curve \"secp384r1:prime256v1\"; # Note that the curve of the certificate, prime256v1 in this case, MUST be listed here, otherwise the connection fails. ECDHE still chooses the first curve if possible. ssl_prefer_server_ciphers on; ssl_session_tickets off; ssl_session_cache off; ssl_certificate_key ./pki/srv.pem.key; ssl_certificate ./pki/srv.bundle.pem.crt; } }","title":"Example: TLS-over-TLS"},{"location":"news/","text":"SCONE News NEWS (2018-06-01) SCONE adds a new curated image for PySpark . Also, check out our other usescases . NEWS (2018-03-17): SCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine NEWS (2018-03-07): SCONE adds an updated curated Node image (sconecuratedimages/apps:node-8.9.4-alpine) NEWS (2018-02-02): SCONE adds support for Zookeeper (sconecuratedimages/apps:zookeeper-alpine). NEWS (2018-02-02): SCONE adds support for Python 3.6.4 (sconecuratedimages/apps:python-3.5-alpine). NEWS (2018-01-27): SCONE adds support for Java . NEWS (2018-01-27): SCONE adds curated Node image (sconecuratedimages/apps:node-8-alpine). NEWS (2018-01-27): SCONE adds curated MongoDB image (sconecuratedimages/apps:mongodb-alpine). NEWS (2018-01-27): SCONE adds curated Memcached image (sconecuratedimages/apps:memcached-alpine). NEWS (2018-01-27): SCONE adds curated Vault image (sconecuratedimages/apps:vault-alpine). SOON TO COME: While SGX is affected by Spectre, SCONE adds protection against Spectre (variants 1 and 2)","title":"SCONE News"},{"location":"news/#scone-news","text":"NEWS (2018-06-01) SCONE adds a new curated image for PySpark . Also, check out our other usescases . NEWS (2018-03-17): SCONE adds an updated curated nginx image (sconecuratedimages/apps:nginx-1.13-alpine NEWS (2018-03-07): SCONE adds an updated curated Node image (sconecuratedimages/apps:node-8.9.4-alpine) NEWS (2018-02-02): SCONE adds support for Zookeeper (sconecuratedimages/apps:zookeeper-alpine). NEWS (2018-02-02): SCONE adds support for Python 3.6.4 (sconecuratedimages/apps:python-3.5-alpine). NEWS (2018-01-27): SCONE adds support for Java . NEWS (2018-01-27): SCONE adds curated Node image (sconecuratedimages/apps:node-8-alpine). NEWS (2018-01-27): SCONE adds curated MongoDB image (sconecuratedimages/apps:mongodb-alpine). NEWS (2018-01-27): SCONE adds curated Memcached image (sconecuratedimages/apps:memcached-alpine). NEWS (2018-01-27): SCONE adds curated Vault image (sconecuratedimages/apps:vault-alpine). SOON TO COME: While SGX is affected by Spectre, SCONE adds protection against Spectre (variants 1 and 2)","title":"SCONE News"},{"location":"openvino/","text":"OpenVino Use Case OpenVino was designed to optimize AI inferencing . This toolkit allows developers to deploy pre-trained deep learning models through its inference engine with high performance and with smaller model sizes. Thus, OpenVino is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We demonstrate how to run OpenVino with SCONE in the following screencast: If you want to evaluate the OpenVino image, send us an email .","title":"OpenVino"},{"location":"openvino/#openvino-use-case","text":"OpenVino was designed to optimize AI inferencing . This toolkit allows developers to deploy pre-trained deep learning models through its inference engine with high performance and with smaller model sizes. Thus, OpenVino is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We demonstrate how to run OpenVino with SCONE in the following screencast: If you want to evaluate the OpenVino image, send us an email .","title":"OpenVino Use Case"},{"location":"outline/","text":"Execution Modes To simplify not only getting started with SCONE but also using SCONE, we support multiple ways to develop and run SCONE-based applications. Depending on what execution mode you want to use, you need to install different software components. Execution Modes: simulation mode inside of a container: use this mode to check out SCONE or to develop software on machines without Intel SGX support (e.g., a Mac). hardware mode inside of a container: this mode requires that you install the Intel SGX driver on your host. host mode : compile inside of a container and execute on the host. Dockerfile : build program and container image with the help of a Dockerfile. iExec platform : you can run your SCONE application on the iExec platform. Language Support After gaining access to the SCONE container images 1 , you can compile and run the hello world program as shown in this section . After that, check out how you could automate the compilation with the help of a Dockerfile . If you want to run applications languages not supported by GNU gcc compiler, please read the descriptions in the appropriate section in menu Language Support : in addition to C , C++ , Fortran , GO and Rust , we also support Python , Java , and JavaScript/ Node.js . Please send us an email if you need access to another programming language. Terminology We might use some terms in the SCONE documentation that we do not explicitly introduce. We maintain a Glossary that defines some of the important terms that we use within the SCONE technical documentation. You need to gain permissions to be able to run the examples given in this documentation. Please send an email with your free Docker ID to info@scontain.com . \u21a9","title":"Overview"},{"location":"outline/#execution-modes","text":"To simplify not only getting started with SCONE but also using SCONE, we support multiple ways to develop and run SCONE-based applications. Depending on what execution mode you want to use, you need to install different software components. Execution Modes: simulation mode inside of a container: use this mode to check out SCONE or to develop software on machines without Intel SGX support (e.g., a Mac). hardware mode inside of a container: this mode requires that you install the Intel SGX driver on your host. host mode : compile inside of a container and execute on the host. Dockerfile : build program and container image with the help of a Dockerfile. iExec platform : you can run your SCONE application on the iExec platform.","title":"Execution Modes"},{"location":"outline/#language-support","text":"After gaining access to the SCONE container images 1 , you can compile and run the hello world program as shown in this section . After that, check out how you could automate the compilation with the help of a Dockerfile . If you want to run applications languages not supported by GNU gcc compiler, please read the descriptions in the appropriate section in menu Language Support : in addition to C , C++ , Fortran , GO and Rust , we also support Python , Java , and JavaScript/ Node.js . Please send us an email if you need access to another programming language. Terminology We might use some terms in the SCONE documentation that we do not explicitly introduce. We maintain a Glossary that defines some of the important terms that we use within the SCONE technical documentation. You need to gain permissions to be able to run the examples given in this documentation. Please send an email with your free Docker ID to info@scontain.com . \u21a9","title":"Language Support"},{"location":"peer-to-peer-attestation/","text":"Peer-to-Peer Attestation Motivation CAS supports peers to attest each others. The term peer refers an instance of a program such that each peer might be operated by a different organizations, and these organizations might not always trust each other to be honest, however they need to cooperate to achieve their business objectives. A dishonest organization might try to operate a modified program to gain advantages. As we explain below, it might even be sufficient for an organization to change the configuration of the program or some files on the disk to change the program's behavior to its advantage. For example, the behavior of a Python program is not only determined by the Python engine that executes the Python program but also the set of Python files and dynamic link libraries and configuration files in the filesystem. An organization could try to modify one of the program's Python files or a file belonging to the Python standard library to change the behavior of the program to its advantage. General Approach For a set of mutually distrusting organizations to be able to work together, we propose the following approach all agree to run the same, agreed-upon program : This program could be stored in a global git repository and is identified by a hash value - like the commit timestamp. same initial state : all files accessed by the program are correctly initialized. cryptographic access control : only the program itself knows the encryption key to modify these files and to protect the confidentiality as well as the integrity, all files can be transparently encrypted. trusted execution : the behavior of the program cannot be modified during run-time by, for example, modifying the memory, files or the operating system. To achieve this, we will explain that peers can attest each other via TLS certificates, very that the peers run in the context of a honest SCONE CAS (SCONE Configuration and Attestation Service), and a peer needs to check the security policy of it peers. Peer Attestation The basic idea of peer attestation is as follows. A peer program A is connected via TLS to a peer B and A wants to ensure that it can trust program B . To do so, A verifies the TLS certificate of B . This certificate, B_CERT , is issued in the context of a CAS session in which B runs, i.e., it is issued and signed by the CAS, B_CAS , that B is using. B_CERT includes a chain of certificates which also includes the TLS certificate of B_CAS . Peer A can now perform the following steps: A checks that B_CERT was issued by B_CAS by attesting that B_CAS is a honest CAS running inside of a production enclave. See CAS Attestation for more details on how to attest CAS itself. B_CERT contains the session name B_session and A downloads this CAS session from B_CAS using its certificate determined during attestation to exclude man-in-the middle attacks: to ensure that B's session is as expected, B checks that it is an instance of an agreed upon session template. Session The session B_session defines how the files are initialized and which programs can access the files. Usually there are two programs: the programm that provides a service, and an initialization program (we call it pull_code ) that encrypts and initializes the filesystem. The session specifies a certificate B_CERT that is issued by CAS in such a way that the private key belonging to B_CERT is only known inside of CAS and program B . The idea of the pull_code is to clone existing code and files from a remote git repository and store this in an encrypted file region. It ensures the integrity of the cloned code by connecting via a secure channel to the git repository, verifying the identity of the git repository and checking out a given commit time stamp. Additionally, one could extend this to only support to accept signed commits, and to run the git repository also inside of an enclave The behavior of pull_code is completely defined by its MrEnclave and its environment variables which are specified in session B_session . In other words, peer A can verify that pull_code does exactly that what it is supposed to do. A dishonest organization could try to circumvent pull_code by trying to modify a session B_CERT such that during attestation is the expected format. We can prevent this by specifying inside the session that it cannot be deleted nor can it be updated: this ensures that the current session is immutable. However, there could be a predecessor session that was mutable and in which a program under control of an adversary initialized the files. To be able to detect this, we check that the session B_session is the first session of this name, i.e., it has no predecessor field. Program Attestation The term program attestation refers to the process of helping to ensure that an enclave was started and that the file system is in the correct state. In the context of the peer-to-peer attestation, this helps to ensure that the file system is actually properly initialized and only a program B with the correct MrEnclave has access to the private key of B_CERT . In the SCONE platform, CAS performs a program attestation of a program in context of a session to ensure that it is permitted to pass arguments, environment variables, and the file system keys to the program. CAS performs the following steps: it is checked that a program runs the same, agreed-upon program code : in the above example, this would be the Python engine which can be attested via its expected MrEnclave , and the program itself runs inside of an SGX enclave in production mode , i.e., it cannot be accessed during runtime by other programs the content of files were not modified , i.e., all files are in the expected state. MrEnclave and the expected file state are defined in the session. Summary The state of a program consists of multiple components, like a Python engine that can be attested via its MrEnclave , and Python code and dynamically-linked libraries stored in the file system which is protected via the SCONE file shield . We ensure that the the initial state of the file system was correctly initialized by performing this initialization via a trusted program running inside an enclave, and the attestation with the help of TLS certificates checks all components that affect the behavior of a program.","title":"Peer-to-Peer Attestation"},{"location":"peer-to-peer-attestation/#peer-to-peer-attestation","text":"","title":"Peer-to-Peer Attestation"},{"location":"peer-to-peer-attestation/#motivation","text":"CAS supports peers to attest each others. The term peer refers an instance of a program such that each peer might be operated by a different organizations, and these organizations might not always trust each other to be honest, however they need to cooperate to achieve their business objectives. A dishonest organization might try to operate a modified program to gain advantages. As we explain below, it might even be sufficient for an organization to change the configuration of the program or some files on the disk to change the program's behavior to its advantage. For example, the behavior of a Python program is not only determined by the Python engine that executes the Python program but also the set of Python files and dynamic link libraries and configuration files in the filesystem. An organization could try to modify one of the program's Python files or a file belonging to the Python standard library to change the behavior of the program to its advantage.","title":"Motivation"},{"location":"peer-to-peer-attestation/#general-approach","text":"For a set of mutually distrusting organizations to be able to work together, we propose the following approach all agree to run the same, agreed-upon program : This program could be stored in a global git repository and is identified by a hash value - like the commit timestamp. same initial state : all files accessed by the program are correctly initialized. cryptographic access control : only the program itself knows the encryption key to modify these files and to protect the confidentiality as well as the integrity, all files can be transparently encrypted. trusted execution : the behavior of the program cannot be modified during run-time by, for example, modifying the memory, files or the operating system. To achieve this, we will explain that peers can attest each other via TLS certificates, very that the peers run in the context of a honest SCONE CAS (SCONE Configuration and Attestation Service), and a peer needs to check the security policy of it peers.","title":"General Approach"},{"location":"peer-to-peer-attestation/#peer-attestation","text":"The basic idea of peer attestation is as follows. A peer program A is connected via TLS to a peer B and A wants to ensure that it can trust program B . To do so, A verifies the TLS certificate of B . This certificate, B_CERT , is issued in the context of a CAS session in which B runs, i.e., it is issued and signed by the CAS, B_CAS , that B is using. B_CERT includes a chain of certificates which also includes the TLS certificate of B_CAS . Peer A can now perform the following steps: A checks that B_CERT was issued by B_CAS by attesting that B_CAS is a honest CAS running inside of a production enclave. See CAS Attestation for more details on how to attest CAS itself. B_CERT contains the session name B_session and A downloads this CAS session from B_CAS using its certificate determined during attestation to exclude man-in-the middle attacks: to ensure that B's session is as expected, B checks that it is an instance of an agreed upon session template.","title":"Peer Attestation"},{"location":"peer-to-peer-attestation/#session","text":"The session B_session defines how the files are initialized and which programs can access the files. Usually there are two programs: the programm that provides a service, and an initialization program (we call it pull_code ) that encrypts and initializes the filesystem. The session specifies a certificate B_CERT that is issued by CAS in such a way that the private key belonging to B_CERT is only known inside of CAS and program B . The idea of the pull_code is to clone existing code and files from a remote git repository and store this in an encrypted file region. It ensures the integrity of the cloned code by connecting via a secure channel to the git repository, verifying the identity of the git repository and checking out a given commit time stamp. Additionally, one could extend this to only support to accept signed commits, and to run the git repository also inside of an enclave The behavior of pull_code is completely defined by its MrEnclave and its environment variables which are specified in session B_session . In other words, peer A can verify that pull_code does exactly that what it is supposed to do. A dishonest organization could try to circumvent pull_code by trying to modify a session B_CERT such that during attestation is the expected format. We can prevent this by specifying inside the session that it cannot be deleted nor can it be updated: this ensures that the current session is immutable. However, there could be a predecessor session that was mutable and in which a program under control of an adversary initialized the files. To be able to detect this, we check that the session B_session is the first session of this name, i.e., it has no predecessor field.","title":"Session"},{"location":"peer-to-peer-attestation/#program-attestation","text":"The term program attestation refers to the process of helping to ensure that an enclave was started and that the file system is in the correct state. In the context of the peer-to-peer attestation, this helps to ensure that the file system is actually properly initialized and only a program B with the correct MrEnclave has access to the private key of B_CERT . In the SCONE platform, CAS performs a program attestation of a program in context of a session to ensure that it is permitted to pass arguments, environment variables, and the file system keys to the program. CAS performs the following steps: it is checked that a program runs the same, agreed-upon program code : in the above example, this would be the Python engine which can be attested via its expected MrEnclave , and the program itself runs inside of an SGX enclave in production mode , i.e., it cannot be accessed during runtime by other programs the content of files were not modified , i.e., all files are in the expected state. MrEnclave and the expected file state are defined in the session.","title":"Program Attestation"},{"location":"peer-to-peer-attestation/#summary","text":"The state of a program consists of multiple components, like a Python engine that can be attested via its MrEnclave , and Python code and dynamically-linked libraries stored in the file system which is protected via the SCONE file shield . We ensure that the the initial state of the file system was correctly initialized by performing this initialization via a trusted program running inside an enclave, and the attestation with the help of TLS certificates checks all components that affect the behavior of a program.","title":"Summary"},{"location":"performance/","text":"Performance of SCONE-based Programs SCONE-Based SGX PySpark Performance We designed and implemented a SCONE-based SGX-PySpark 1 - a secure distributed data analytics system which is based on PySpark and relies on SCONE and Intel SGX to provide strong security guarantees. To demonstrate the performance of SGX-PySpark use of a standard data analytics benchmark, i.e.,TPC-H, to demonstrate that SCONE-based SGX-PySpark supports the same range of queries as native PySpark. The figure below presents the latency comparison between SGX-PySpark with native PySpark in processing TPC-H queries. The performance overhead incurred when running Python processes inside enclaves is not significant compared to the native PySpark: the average slowdown is only 22% for TPC-H. Python Performance For many of the standard Python benchmarks, our SCONE PyPy (i.e., the just in time Python engine) inside of an SGX enclave actually runs faster than CPython in native mode. Yes, we get for many of the benchmarks a speedup over native performance: For a detail performance analysis have a look at the normalized PyPy performance shows that overheads are for the standard Python benchmarks are acceptable. Performance Factors The performance of running programs inside of enclaves depends on various factors. The main factors are the following: Locality of memory accesses : if the working set of a process does not fit in the EPC (extended page cache), the process will suffer page faults. During a page fault, a page in the EPC will be selected by the SGX driver, re-encrypted and stored in main memory. The overhead of a process inside of an enclave grows with the page fault rate. If a program has an option to reduce the memory footprint (not include debug symbols in binaries, compiled to reduce size, etc), this will often result in better performance. system calls : each system call requires that memory-based arguments are copied from the enclave to the outside memory and memory-based return values are copied from the main memory into the enclave. Exiting and returning to an enclave takes at least 8000 cycles 2 . SCONE uses an asynchronous system call interface that ensures that threads do not need to exit the enclave to perform a system call. threading : SCONE provides application-level threading. This ensures that in case an application thread waits for some event like the return of a system call, we can switch to a new application thread without the need to exit the enclave. For getting optimal performance in your applications, SCONE provides multiple tuning parameters ( see here ): The default SCONE configuration file often provides reasonable performance but optimal performance - at least for SGXv1 hardware - can often only be achieved by selecting the right number of ethreads , sthreads and heap size . Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. DOI: https://doi.org/10.1145/3308558.3314129 \u21a9 Recent CPU microcode updates have increased this number further. \u21a9","title":"Applications"},{"location":"performance/#performance-of-scone-based-programs","text":"","title":"Performance of SCONE-based Programs"},{"location":"performance/#scone-based-sgx-pyspark-performance","text":"We designed and implemented a SCONE-based SGX-PySpark 1 - a secure distributed data analytics system which is based on PySpark and relies on SCONE and Intel SGX to provide strong security guarantees. To demonstrate the performance of SGX-PySpark use of a standard data analytics benchmark, i.e.,TPC-H, to demonstrate that SCONE-based SGX-PySpark supports the same range of queries as native PySpark. The figure below presents the latency comparison between SGX-PySpark with native PySpark in processing TPC-H queries. The performance overhead incurred when running Python processes inside enclaves is not significant compared to the native PySpark: the average slowdown is only 22% for TPC-H.","title":"SCONE-Based SGX PySpark Performance"},{"location":"performance/#python-performance","text":"For many of the standard Python benchmarks, our SCONE PyPy (i.e., the just in time Python engine) inside of an SGX enclave actually runs faster than CPython in native mode. Yes, we get for many of the benchmarks a speedup over native performance: For a detail performance analysis have a look at the normalized PyPy performance shows that overheads are for the standard Python benchmarks are acceptable.","title":"Python Performance"},{"location":"performance/#performance-factors","text":"The performance of running programs inside of enclaves depends on various factors. The main factors are the following: Locality of memory accesses : if the working set of a process does not fit in the EPC (extended page cache), the process will suffer page faults. During a page fault, a page in the EPC will be selected by the SGX driver, re-encrypted and stored in main memory. The overhead of a process inside of an enclave grows with the page fault rate. If a program has an option to reduce the memory footprint (not include debug symbols in binaries, compiled to reduce size, etc), this will often result in better performance. system calls : each system call requires that memory-based arguments are copied from the enclave to the outside memory and memory-based return values are copied from the main memory into the enclave. Exiting and returning to an enclave takes at least 8000 cycles 2 . SCONE uses an asynchronous system call interface that ensures that threads do not need to exit the enclave to perform a system call. threading : SCONE provides application-level threading. This ensures that in case an application thread waits for some event like the return of a system call, we can switch to a new application thread without the need to exit the enclave. For getting optimal performance in your applications, SCONE provides multiple tuning parameters ( see here ): The default SCONE configuration file often provides reasonable performance but optimal performance - at least for SGXv1 hardware - can often only be achieved by selecting the right number of ethreads , sthreads and heap size . Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. DOI: https://doi.org/10.1145/3308558.3314129 \u21a9 Recent CPU microcode updates have increased this number further. \u21a9","title":"Performance Factors"},{"location":"print-arg-env/","text":"Secure Arguments and Environment Variables We show how to pass arguments and environment variables in a secure fashion to an application with the help of CAS. SCONE CAS manages the keys of an application. The keys are unter complete control of the application: only services given explicit control by the application get access to the keys. Prerequisites We assume that you already started a CAS instance on the local host, as well as started a LAS instance on the local host, We also assume that this example is executed in the context of Ubuntu. For other Linux variants, you might need to use a different cross-compiler image. Alternatively, you could execute this example in the crosscompiler container but you need to ensure that the container can establish https connections to CAS and LAS. Simple Example We create a simple C program to print the arguments as well as the environment variables: cat > print-arg-env.c <<EOF #include <stdio.h> extern char **__environ; int main (int argc, char **argv) { printf(\"argv:\"); for (int i = 0; i < argc; i++) { printf(\" %s\", argv[i]); } printf(\"\\n\"); char** envp = __environ; printf(\"environ:\\n\"); while (*envp != NULL) { printf(\"%s\\n\", *envp); envp++; } return 42; } EOF We can compile this natively as follows: gcc print-arg-env.c -g -O3 -o print-arg-env And we can now run this with some some arguments and some environment variables: ENV2 = World ENV1 = ! ./print-arg-env Hello This results in an output similar to this: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Let's cross-compile this program to run inside of enclaves as follows: docker run --device = /dev/isgx -it --privileged -v ` pwd ` :/work sconecuratedimages/crosscompilers:ubuntu16.04 scone-gcc /work/print-arg-env.c -g -O3 -o /work/scone-print-arg-env We can now determine MRENCLAVE as follows: export MRENCLAVE = ` SCONE_HASH = 1 ./scone-print-arg-env ` echo MRENCLAVE of scone-print-arg-env is $MRENCLAVE We can now run the program inside of an enclave but not under the control of CAS as follows: ENV2 = World ENV1 = ! ./scone-print-arg-env Hello This will result in the same output as before: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Without CAS: No Confidentiality and Integrity of Arguments and Environment Variables Note that neither the integrity nor the confidentiality of these arguments are protected. To protect these and to ensure that the program is indeed executed inside an enclave, we need to run the program in the context of CAS.** Secure Arguments and Environment Variables In the next step, we control the environment variables and the arguments with the help of CAS. When does the SCONE runtime use CAS? When environment variable SCONE_CONFIG_ID is not set, the SCONE runtime will not contact CAS. If it is set, we will contact SCONE_CAS_ADDR . If SCONE_CAS_ADDR is not set, we contact by default host \u201ecas\u201c. The SCONE runtime will also contact SCONE_LAS_ADDR for local attestation. If SCONE_LAS_ADDR is not set, we contact by default host \u201elas\u201c. We assume that you already started a CAS in the background via docker-compose up -d cas on the local machine: export SCONE_CAS_ADDR = 127 .0.0.1 Alternatively, you can use the public CAS server at scone-cas.cf : export SCONE_CAS_ADDR = scone-cas.cf We also assume that you already started a LAS in the background via docker-compose up -d las on the local machine: export SCONE_LAS_ADDR = 127 .0.0.1 We can now create a new session on cas as follows: cat > session.yml <<EOF name: secure-arguments-example digest: somedigest services: - name: scone-print-arg-env mrenclaves: [$MRENCLAVE] tags: [thisissometag1] command: ./scone-print-arg-env arg1 arg2 arg3 environment: SCONE_MODE: hw env1: running env2: in env3: enclave pwd: / EOF Let's make sure that we have a client certificate to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar to this: Created Session[id=ce81e4cd-eed4-4f12-9bfc-c93e3dd4b454, name=secure-arguments, status=Pending]c Note that if you try to upload to a session that already exists, you will get an error code like this: Could not create successor session. Invalid previous session digest: Some(\"5de101e4980f312527e1c308dfe6c7bc465660c0fea6b98983efa5baf94bf362\") != None In other words, if you want to update a session, you need to define a predecessor key in the session that specifies the current session. This is to ensure that a client does not overwrite a just updated session, i.e., it permits to detect write/write conflicts. Executing Print-Arg-Env Let us now execute scone-print-arg-env in the context of session secure-arguments-example . To do so, we need to pass in environment variable SCONE_CONFIG_ID which contains the name of the session and we need to set SCONE_CAS_ADDR as well as SCONE_LAS_ADDR as we have shown above. SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env This will print the following output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave Note that the environment variables - except those expected by the SCONE runtime to locate LAS, CAS and the session - are ignored. This means even if we pass additional arguments and environment variables, the output does not change. The execution of IRGNORED_ENV = 1 SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env IGNORED1 IGNORED2 results in the same output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave The SCONE variables provided by the environment are not protected in any way and we designed SCONE assuming that an attacker could modify these environment variables. This could lead to denial of service attacks but not to the exposure of any secrets. Integrity of SCONE environment variables An adversary could change SCONE_CONFIG_ID , SCONE_CAS_ADDR and SCONE_LAS_ADDR but in this case, the application does not start (attestation fails) up or it does not get access to the secrets and configuration variables to do its job.","title":"Secure Arguments"},{"location":"print-arg-env/#secure-arguments-and-environment-variables","text":"We show how to pass arguments and environment variables in a secure fashion to an application with the help of CAS. SCONE CAS manages the keys of an application. The keys are unter complete control of the application: only services given explicit control by the application get access to the keys.","title":"Secure Arguments and Environment Variables"},{"location":"print-arg-env/#prerequisites","text":"We assume that you already started a CAS instance on the local host, as well as started a LAS instance on the local host, We also assume that this example is executed in the context of Ubuntu. For other Linux variants, you might need to use a different cross-compiler image. Alternatively, you could execute this example in the crosscompiler container but you need to ensure that the container can establish https connections to CAS and LAS.","title":"Prerequisites"},{"location":"print-arg-env/#simple-example","text":"We create a simple C program to print the arguments as well as the environment variables: cat > print-arg-env.c <<EOF #include <stdio.h> extern char **__environ; int main (int argc, char **argv) { printf(\"argv:\"); for (int i = 0; i < argc; i++) { printf(\" %s\", argv[i]); } printf(\"\\n\"); char** envp = __environ; printf(\"environ:\\n\"); while (*envp != NULL) { printf(\"%s\\n\", *envp); envp++; } return 42; } EOF We can compile this natively as follows: gcc print-arg-env.c -g -O3 -o print-arg-env And we can now run this with some some arguments and some environment variables: ENV2 = World ENV1 = ! ./print-arg-env Hello This results in an output similar to this: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Let's cross-compile this program to run inside of enclaves as follows: docker run --device = /dev/isgx -it --privileged -v ` pwd ` :/work sconecuratedimages/crosscompilers:ubuntu16.04 scone-gcc /work/print-arg-env.c -g -O3 -o /work/scone-print-arg-env We can now determine MRENCLAVE as follows: export MRENCLAVE = ` SCONE_HASH = 1 ./scone-print-arg-env ` echo MRENCLAVE of scone-print-arg-env is $MRENCLAVE We can now run the program inside of an enclave but not under the control of CAS as follows: ENV2 = World ENV1 = ! ./scone-print-arg-env Hello This will result in the same output as before: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Without CAS: No Confidentiality and Integrity of Arguments and Environment Variables Note that neither the integrity nor the confidentiality of these arguments are protected. To protect these and to ensure that the program is indeed executed inside an enclave, we need to run the program in the context of CAS.**","title":"Simple Example"},{"location":"print-arg-env/#secure-arguments-and-environment-variables_1","text":"In the next step, we control the environment variables and the arguments with the help of CAS. When does the SCONE runtime use CAS? When environment variable SCONE_CONFIG_ID is not set, the SCONE runtime will not contact CAS. If it is set, we will contact SCONE_CAS_ADDR . If SCONE_CAS_ADDR is not set, we contact by default host \u201ecas\u201c. The SCONE runtime will also contact SCONE_LAS_ADDR for local attestation. If SCONE_LAS_ADDR is not set, we contact by default host \u201elas\u201c. We assume that you already started a CAS in the background via docker-compose up -d cas on the local machine: export SCONE_CAS_ADDR = 127 .0.0.1 Alternatively, you can use the public CAS server at scone-cas.cf : export SCONE_CAS_ADDR = scone-cas.cf We also assume that you already started a LAS in the background via docker-compose up -d las on the local machine: export SCONE_LAS_ADDR = 127 .0.0.1 We can now create a new session on cas as follows: cat > session.yml <<EOF name: secure-arguments-example digest: somedigest services: - name: scone-print-arg-env mrenclaves: [$MRENCLAVE] tags: [thisissometag1] command: ./scone-print-arg-env arg1 arg2 arg3 environment: SCONE_MODE: hw env1: running env2: in env3: enclave pwd: / EOF Let's make sure that we have a client certificate to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar to this: Created Session[id=ce81e4cd-eed4-4f12-9bfc-c93e3dd4b454, name=secure-arguments, status=Pending]c Note that if you try to upload to a session that already exists, you will get an error code like this: Could not create successor session. Invalid previous session digest: Some(\"5de101e4980f312527e1c308dfe6c7bc465660c0fea6b98983efa5baf94bf362\") != None In other words, if you want to update a session, you need to define a predecessor key in the session that specifies the current session. This is to ensure that a client does not overwrite a just updated session, i.e., it permits to detect write/write conflicts.","title":"Secure Arguments and Environment Variables"},{"location":"print-arg-env/#executing-print-arg-env","text":"Let us now execute scone-print-arg-env in the context of session secure-arguments-example . To do so, we need to pass in environment variable SCONE_CONFIG_ID which contains the name of the session and we need to set SCONE_CAS_ADDR as well as SCONE_LAS_ADDR as we have shown above. SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env This will print the following output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave Note that the environment variables - except those expected by the SCONE runtime to locate LAS, CAS and the session - are ignored. This means even if we pass additional arguments and environment variables, the output does not change. The execution of IRGNORED_ENV = 1 SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env IGNORED1 IGNORED2 results in the same output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave The SCONE variables provided by the environment are not protected in any way and we designed SCONE assuming that an attacker could modify these environment variables. This could lead to denial of service attacks but not to the exposure of any secrets. Integrity of SCONE environment variables An adversary could change SCONE_CONFIG_ID , SCONE_CAS_ADDR and SCONE_LAS_ADDR but in this case, the application does not start (attestation fails) up or it does not get access to the secrets and configuration variables to do its job.","title":"Executing Print-Arg-Env"},{"location":"pypyscone/","text":"PyPy for SCONE PyPy is a Just In Time compiler for Python. We maintain a container image that includes PyPy running inside of an enclave. The speed of pypy inside of an enclave is in many cases faster than running natively (i.e., outside an enclave) using the standard CPython interpreter. To compare the performance we are using the standard python / pypy speed center and add pypy for SCONE. Below you can see a graph that gives an overview of the performance of different Python variants. This depicts normalized performance using pypy as the baseline, i.e., smaller values show better performance. For example, a ratio of 65 means that the program is indeed 65 times slower than pypy running in native mode (i.e., these are not percent values). We can see in this graph that pypy can achieve for some benchmarks dramatic speedups. pypy running inside of an enclave is in almost all benchmarks faster than native CPython: You can compare the performance in more details using the speedcenter website maintained by UFCG .","title":"PyPy"},{"location":"pypyscone/#pypy-for-scone","text":"PyPy is a Just In Time compiler for Python. We maintain a container image that includes PyPy running inside of an enclave. The speed of pypy inside of an enclave is in many cases faster than running natively (i.e., outside an enclave) using the standard CPython interpreter. To compare the performance we are using the standard python / pypy speed center and add pypy for SCONE. Below you can see a graph that gives an overview of the performance of different Python variants. This depicts normalized performance using pypy as the baseline, i.e., smaller values show better performance. For example, a ratio of 65 means that the program is indeed 65 times slower than pypy running in native mode (i.e., these are not percent values). We can see in this graph that pypy can achieve for some benchmarks dramatic speedups. pypy running inside of an enclave is in almost all benchmarks faster than native CPython: You can compare the performance in more details using the speedcenter website maintained by UFCG .","title":"PyPy for SCONE"},{"location":"pyspark/","text":"PySpark Use Case PySpark is the Python API to Spark . We will add some more documentation about the curated PySpark image later. Until then, you can have a look at our PySpark screencast: If you want to evaluate the PySpark image, send us an email .","title":"PySpark"},{"location":"pyspark/#pyspark-use-case","text":"PySpark is the Python API to Spark . We will add some more documentation about the curated PySpark image later. Until then, you can have a look at our PySpark screencast: If you want to evaluate the PySpark image, send us an email .","title":"PySpark Use Case"},{"location":"scone-use-case-overview/","text":"Overview The SCONE Platform is a generic platform to execute applications inside of trusted execution environments like Intel SGX. It can be used in different settings with multiple stakeholders that might not all trust each other: User , Application Provider , Data Owner , and Auditor . SCONE protects the IP (intellectual property) of multiple stakeholders . In this section, we introduce the different stakeholders and sketch how SCONE can be used to protect the data, code, and secrets not only from attackers but also from other stakeholders. We also give some recommendations - in the form of forward links - on how one could proceed reading about SCONE. Attacker In SCONE, we assume a very powerful attacker that has root access on the hosts on which we execute applications. The attacker can hence dump the memory of the application using standard features of the operating system. In this way, the attacker can gain access to the code, the data and all the keys that are stored in main memory. Note that the attacker can read all files of the application since operating system based access control does not apply to an attacker with root access. Moreover, any operating system based encryption of files does prevent file system accesses by the attacker either. If the application encrypts the files itself, the encryption key has to be stored somewhere in main memory - otherwise, the application would not be able to encrypt and decrypt files. An attacker can dump the main memory to get a hold of this encryption key. User A user wants to protect his computations, data and keys from accesses by other parties and in particular, by accesses by attackers with root access rights: Protection against read accesses by attackers protects the confidentiality of the data, keys, and code. Protection against read accesses by attackers protects the integrity of the data, keys, and code. SCONE provides a simple CLI (command line interface) to execute applications on remote hosts. This CLI supports to encrypt files on the user's computer and push these files to a remote computer. Only the application itself can access the files: SCONE performs a transparent attestation of the application to ensure that neither the application or the file system has been modified . Only then, SCONE transparently encrypts/decrypts the files. In this way, SCONE ensures the confidentiality and integrity of the user provided files. Application Provider An application provider can create container images like, for example, blender , a 3D creation suite. SCONE helps the application provider to run unmodified applications inside of TEEs (trusted execution environments) such that the application is protected from accesses by attackers. SCONE can also transparently encrypt/decrypt the files stored in the file system. The application container image can, for example, be created with the help of a Dockerfile and in some cases, it might need a multistage build . We describe how to build a Go program as part of this tutorial. An application provider can push its container images to a public repository like Docker hub or some private repository. The application provider can encrypt parts of the files to ensure the confidentiality and integrity of the files. For example, some of the application code might be written in Python and hence, the application provider must protect the integrity and confidentiality of that code: Actually, the situation is even more difficult. The user might want to give the application access to some encrypted input and output files (as explained above). The application provider should not have the right to access the files since this is the data that the user must protect. The user should not have access to the files provided by the application provider. In particular, SCONE manages the keys such that the SCONE runtime of the application can get access to the keys but neither the application provider nor the user will see these keys. We describe later how to do this for a simple decentralized copy application . Data Owner A data owner is another stakeholder. The data owner provides data that can be processed by the application on behalf of the user. The data owner wants to protect its data not only accesses by access by attackers but also by direct access by the user and/or the application provider. The data owner therefore encrypts its data and only permits accesses by some explicitly specified application. SCONE supports security policies that ensure that the interests of the different stakeholders are satisfied. This is supported by security policies that are defined by the individual stakeholders. Parts of these policies can be exported to and imported from other policies. In this way, one can compose security policies of mutually distrusting stakeholders. Auditor In very critical application domains, neither the data owner nor the user might trust neither the application provider nor the SCONE platform sufficiently much, to entrust their data & computations to the application. In these domains, an auditor can help to establish trust in the application . The auditor is an independent entity that can access all source code of the application as well as the SCONE platform. The auditor works typically offline, i.e., it is not involved in any of the computations. Instead, she determines which MrEnclaves together with what file system state (i.e., tag) can be trusted by the data owner and the user. The auditor will also check the SCONE Configuration and Attestation Service (CAS): SCONE CAS enforces the security policies and manages the secrets on behalf of the SCONE platform. The auditor publishes the trusted MrEnclave and the file system tag of SCONE CAS. The data owner and the user can specify in their security policies a set of MrEnclaves, each with a specific file system tag, that they trust. Moreover, before submitting a security policy to SCONE CAS, they check that the instance of SCONE CAS has the correct MrEnclave as well as the correct initial file state. This is achieved with the help of the Intel attestation service. \u00a9 scontain.com ,2020. Questions or Suggestions?","title":"Overview"},{"location":"scone-use-case-overview/#overview","text":"The SCONE Platform is a generic platform to execute applications inside of trusted execution environments like Intel SGX. It can be used in different settings with multiple stakeholders that might not all trust each other: User , Application Provider , Data Owner , and Auditor . SCONE protects the IP (intellectual property) of multiple stakeholders . In this section, we introduce the different stakeholders and sketch how SCONE can be used to protect the data, code, and secrets not only from attackers but also from other stakeholders. We also give some recommendations - in the form of forward links - on how one could proceed reading about SCONE.","title":"Overview"},{"location":"scone-use-case-overview/#attacker","text":"In SCONE, we assume a very powerful attacker that has root access on the hosts on which we execute applications. The attacker can hence dump the memory of the application using standard features of the operating system. In this way, the attacker can gain access to the code, the data and all the keys that are stored in main memory. Note that the attacker can read all files of the application since operating system based access control does not apply to an attacker with root access. Moreover, any operating system based encryption of files does prevent file system accesses by the attacker either. If the application encrypts the files itself, the encryption key has to be stored somewhere in main memory - otherwise, the application would not be able to encrypt and decrypt files. An attacker can dump the main memory to get a hold of this encryption key.","title":"Attacker"},{"location":"scone-use-case-overview/#user","text":"A user wants to protect his computations, data and keys from accesses by other parties and in particular, by accesses by attackers with root access rights: Protection against read accesses by attackers protects the confidentiality of the data, keys, and code. Protection against read accesses by attackers protects the integrity of the data, keys, and code. SCONE provides a simple CLI (command line interface) to execute applications on remote hosts. This CLI supports to encrypt files on the user's computer and push these files to a remote computer. Only the application itself can access the files: SCONE performs a transparent attestation of the application to ensure that neither the application or the file system has been modified . Only then, SCONE transparently encrypts/decrypts the files. In this way, SCONE ensures the confidentiality and integrity of the user provided files.","title":"User"},{"location":"scone-use-case-overview/#application-provider","text":"An application provider can create container images like, for example, blender , a 3D creation suite. SCONE helps the application provider to run unmodified applications inside of TEEs (trusted execution environments) such that the application is protected from accesses by attackers. SCONE can also transparently encrypt/decrypt the files stored in the file system. The application container image can, for example, be created with the help of a Dockerfile and in some cases, it might need a multistage build . We describe how to build a Go program as part of this tutorial. An application provider can push its container images to a public repository like Docker hub or some private repository. The application provider can encrypt parts of the files to ensure the confidentiality and integrity of the files. For example, some of the application code might be written in Python and hence, the application provider must protect the integrity and confidentiality of that code: Actually, the situation is even more difficult. The user might want to give the application access to some encrypted input and output files (as explained above). The application provider should not have the right to access the files since this is the data that the user must protect. The user should not have access to the files provided by the application provider. In particular, SCONE manages the keys such that the SCONE runtime of the application can get access to the keys but neither the application provider nor the user will see these keys. We describe later how to do this for a simple decentralized copy application .","title":"Application Provider"},{"location":"scone-use-case-overview/#data-owner","text":"A data owner is another stakeholder. The data owner provides data that can be processed by the application on behalf of the user. The data owner wants to protect its data not only accesses by access by attackers but also by direct access by the user and/or the application provider. The data owner therefore encrypts its data and only permits accesses by some explicitly specified application. SCONE supports security policies that ensure that the interests of the different stakeholders are satisfied. This is supported by security policies that are defined by the individual stakeholders. Parts of these policies can be exported to and imported from other policies. In this way, one can compose security policies of mutually distrusting stakeholders.","title":"Data Owner"},{"location":"scone-use-case-overview/#auditor","text":"In very critical application domains, neither the data owner nor the user might trust neither the application provider nor the SCONE platform sufficiently much, to entrust their data & computations to the application. In these domains, an auditor can help to establish trust in the application . The auditor is an independent entity that can access all source code of the application as well as the SCONE platform. The auditor works typically offline, i.e., it is not involved in any of the computations. Instead, she determines which MrEnclaves together with what file system state (i.e., tag) can be trusted by the data owner and the user. The auditor will also check the SCONE Configuration and Attestation Service (CAS): SCONE CAS enforces the security policies and manages the secrets on behalf of the SCONE platform. The auditor publishes the trusted MrEnclave and the file system tag of SCONE CAS. The data owner and the user can specify in their security policies a set of MrEnclaves, each with a specific file system tag, that they trust. Moreover, before submitting a security policy to SCONE CAS, they check that the instance of SCONE CAS has the correct MrEnclave as well as the correct initial file state. This is achieved with the help of the Intel attestation service. \u00a9 scontain.com ,2020. Questions or Suggestions?","title":"Auditor"},{"location":"scone_affinity/","text":"SCONE Affinity CPU Affinity CPU affinity refers to the strong preference a task has for a particular CPU (core). When a task is made to always run on a particular core, the chances of experiencing cache misses reduces. This effectively improves the performance and thus throughput of a program. Affinity in SCONE SCONE has three types of threads: lthreads : these are threads that execute the application generated threads (like pthreads) inside of an enclave. ethreads : these are threads that execute the lthreads inside of enclaves. The number of ethreads has to be defined in the SCONE configuration file . SCONE performs an n:m mapping of lthreads to ethreads. This includes workstealing in the sense that before an ethread goes to sleep, it checks if another ethread has more than one lthread ready to execute. sthreads : run outside the enclave (i.e., in native mode). The sthreads execute the syscalls on behalf of the lthreads. CPU affinity in SCONE is supported by locking ethreads (as well as sthreads) to particular cores/cpus. The affinity of ethreads can be specified in the scone configuration file passed to the SCONE runtime. By default, an ethread has zero cpu preference. This is also the default behavior on Linux. An lthread has no affinity by default, but can be pinned to a particular ethread and a particular core if the ethread is pinned. \u200b \u200b By default, a lthread has zero cpu preference, even if it's parent had affinity for particular ehtread. This is the default behavior on Linux. Number of available CPUs In SCONE, then number of available CPUs is reported as the number of ethreads . This means the number of virtual cores/CPUS is determined by the default SCONE SCONE configuration file ( /etc/sgx-musl.conf ) unless the path is changed via the environment variable SCONE_CONFIG . Using CPU Affinity The standard C library (musl-libc in the case of SCONE), exposes the inquisition and setting of CPU preference by an application through two system calls. #define _GNU_SOURCE #include <sched.h> int sched_setaffinity ( pid_t pid , size_t cpusetsize , const cpu_set_t * mask ); int sched_getaffinity ( pid_t pid , size_t cpusetsize , cpu_set_t * mask ); Check man 2 SCHED_AFFINITY for a complete description. Configuring CPU affinity The CPU affinity for an arbitrary application can be effected by specifying the affinity a particular ethread has. By doing so, all the lthreads that will be locked to the ethreads, will effectively run on the CPUS on which the ethreads run. This will guarantee cpu prefrence is propagated to userspace. Author: Pamenas","title":"affinity"},{"location":"scone_affinity/#scone-affinity","text":"","title":"SCONE Affinity"},{"location":"scone_affinity/#cpu-affinity","text":"CPU affinity refers to the strong preference a task has for a particular CPU (core). When a task is made to always run on a particular core, the chances of experiencing cache misses reduces. This effectively improves the performance and thus throughput of a program.","title":"CPU Affinity"},{"location":"scone_affinity/#affinity-in-scone","text":"SCONE has three types of threads: lthreads : these are threads that execute the application generated threads (like pthreads) inside of an enclave. ethreads : these are threads that execute the lthreads inside of enclaves. The number of ethreads has to be defined in the SCONE configuration file . SCONE performs an n:m mapping of lthreads to ethreads. This includes workstealing in the sense that before an ethread goes to sleep, it checks if another ethread has more than one lthread ready to execute. sthreads : run outside the enclave (i.e., in native mode). The sthreads execute the syscalls on behalf of the lthreads. CPU affinity in SCONE is supported by locking ethreads (as well as sthreads) to particular cores/cpus. The affinity of ethreads can be specified in the scone configuration file passed to the SCONE runtime. By default, an ethread has zero cpu preference. This is also the default behavior on Linux. An lthread has no affinity by default, but can be pinned to a particular ethread and a particular core if the ethread is pinned. \u200b \u200b By default, a lthread has zero cpu preference, even if it's parent had affinity for particular ehtread. This is the default behavior on Linux.","title":"Affinity in SCONE"},{"location":"scone_affinity/#number-of-available-cpus","text":"In SCONE, then number of available CPUs is reported as the number of ethreads . This means the number of virtual cores/CPUS is determined by the default SCONE SCONE configuration file ( /etc/sgx-musl.conf ) unless the path is changed via the environment variable SCONE_CONFIG .","title":"Number of available CPUs"},{"location":"scone_affinity/#using-cpu-affinity","text":"The standard C library (musl-libc in the case of SCONE), exposes the inquisition and setting of CPU preference by an application through two system calls. #define _GNU_SOURCE #include <sched.h> int sched_setaffinity ( pid_t pid , size_t cpusetsize , const cpu_set_t * mask ); int sched_getaffinity ( pid_t pid , size_t cpusetsize , cpu_set_t * mask ); Check man 2 SCHED_AFFINITY for a complete description.","title":"Using CPU Affinity"},{"location":"scone_affinity/#configuring-cpu-affinity","text":"The CPU affinity for an arbitrary application can be effected by specifying the affinity a particular ethread has. By doing so, all the lthreads that will be locked to the ethreads, will effectively run on the CPUS on which the ethreads run. This will guarantee cpu prefrence is propagated to userspace. Author: Pamenas","title":"Configuring CPU affinity"},{"location":"scone_file_shield/","text":"SCONE IMAGE Objective: Simplify the shielding of files and the execution of services inside of enclaves. We want to be able to convert an image such that the files in the image are authenticated and optionally, be encrypted binaries inside of the image a converted to run inside of enclaves can given access to the encryption key to potential users A client that runs a container of this image, can simply encryption of all volumes that are mapped into the container simply map encrypted volumes inside of containers and also ensure the freshness of the files a client can simply access the secret information from the producer of the image Note: In many applications, we might split the file shielding along the volumes that we map into a container. There is a top-level FSPF that gets its information from the stack file of that container. Example scone image protect : we can generate a fspf for an existing image and update the image with the help of a stack file like this: aptsigner: image: aptsigner target_num_containers: 1 command: aptsign ... volumes: - /infinit/myself/etc-mysql:/packages - /infinit/myself/apt-keyring:/keyring The stack file gives us the image names and the volumes that might be mapped into the containers. We can authenticate the root FSPF with a public key and optionally, encrypt the fspf with a symmetric key. The symmetric key would permit customers to decrypt the FSPF. scone image protect # Description --encrypt 0-1 encrypt all files currently stored in this image --signer KEY 0-1 certificate use to sign FSPF (optionally) --mrsigner MRSIGNER 0-1 key to sign enclave (optionally) --service NAME 0-1 default - encrypt all services in the stack file --intag TAG 0-1 default - scone --stack FILE 1 stackfile used to define volumes --binary CMD 0-n ensure that this binary runs inside an enclave We store for each image the signer key, the tag and the encryption key in CAS. We can give access to these keys via a token. A token has a limited lifetime, typically, one year. Encrypt Volume A user can run a container by specifying a stack file. This stack file contains all information for SCONE to figure out the signatures to run the services / images. However, we need to run the stack in the context of the same session in which the volume was encrypted (or, the session was given access to these keys). Encrypt volume and store tag in CAS: scone volume encrypt --as tenant --name vol This call will fail if the volume is already encrypted. To simplify encryption, you will be able to create an already encrypted volume (via flag --encrypt ). The call will also fail, if no session is currently active. The key is stored in the current session as volume/tenant/vol/key and the tag as volume/tenant/vol/tag . Updating TAG In case the volume has been updated and you want to map it into a container, you need first to update the TAG in CAS. To update the tag, perform the following scone volume encrypt --as tenant --name vol --update-tag --no-check scone volume encrypt Description --update-tag update the TAG in CAS --reencrypt change the encryption key --check do not perform integrity check if volume is already encrypted A container can update the TAG and store the update TAG in a file and encrypt the TAG with a shared key volume/tenant/vol/tagkey . Example: nginx sconedocs: image: 127.0.0.1:5000/sconetainer:fss command: /bin/nginx -p /nginx -c nginx.conf volumes: - /infinit/scone/sconedocs:/nginx - /infinit/scone/sconedocs-etc:/nginx-etc","title":"SCONE IMAGE"},{"location":"scone_file_shield/#scone-image","text":"Objective: Simplify the shielding of files and the execution of services inside of enclaves. We want to be able to convert an image such that the files in the image are authenticated and optionally, be encrypted binaries inside of the image a converted to run inside of enclaves can given access to the encryption key to potential users A client that runs a container of this image, can simply encryption of all volumes that are mapped into the container simply map encrypted volumes inside of containers and also ensure the freshness of the files a client can simply access the secret information from the producer of the image Note: In many applications, we might split the file shielding along the volumes that we map into a container. There is a top-level FSPF that gets its information from the stack file of that container.","title":"SCONE IMAGE"},{"location":"scone_file_shield/#example","text":"scone image protect : we can generate a fspf for an existing image and update the image with the help of a stack file like this: aptsigner: image: aptsigner target_num_containers: 1 command: aptsign ... volumes: - /infinit/myself/etc-mysql:/packages - /infinit/myself/apt-keyring:/keyring The stack file gives us the image names and the volumes that might be mapped into the containers. We can authenticate the root FSPF with a public key and optionally, encrypt the fspf with a symmetric key. The symmetric key would permit customers to decrypt the FSPF. scone image protect # Description --encrypt 0-1 encrypt all files currently stored in this image --signer KEY 0-1 certificate use to sign FSPF (optionally) --mrsigner MRSIGNER 0-1 key to sign enclave (optionally) --service NAME 0-1 default - encrypt all services in the stack file --intag TAG 0-1 default - scone --stack FILE 1 stackfile used to define volumes --binary CMD 0-n ensure that this binary runs inside an enclave We store for each image the signer key, the tag and the encryption key in CAS. We can give access to these keys via a token. A token has a limited lifetime, typically, one year.","title":"Example"},{"location":"scone_file_shield/#encrypt-volume","text":"A user can run a container by specifying a stack file. This stack file contains all information for SCONE to figure out the signatures to run the services / images. However, we need to run the stack in the context of the same session in which the volume was encrypted (or, the session was given access to these keys). Encrypt volume and store tag in CAS: scone volume encrypt --as tenant --name vol This call will fail if the volume is already encrypted. To simplify encryption, you will be able to create an already encrypted volume (via flag --encrypt ). The call will also fail, if no session is currently active. The key is stored in the current session as volume/tenant/vol/key and the tag as volume/tenant/vol/tag .","title":"Encrypt Volume"},{"location":"scone_file_shield/#updating-tag","text":"In case the volume has been updated and you want to map it into a container, you need first to update the TAG in CAS. To update the tag, perform the following scone volume encrypt --as tenant --name vol --update-tag --no-check scone volume encrypt Description --update-tag update the TAG in CAS --reencrypt change the encryption key --check do not perform integrity check if volume is already encrypted A container can update the TAG and store the update TAG in a file and encrypt the TAG with a shared key volume/tenant/vol/tagkey .","title":"Updating TAG"},{"location":"scone_file_shield/#example-nginx","text":"sconedocs: image: 127.0.0.1:5000/sconetainer:fss command: /bin/nginx -p /nginx -c nginx.conf volumes: - /infinit/scone/sconedocs:/nginx - /infinit/scone/sconedocs-etc:/nginx-etc","title":"Example: nginx"},{"location":"scone_semantic_versioning/","text":"Scone Semantic Versioning Following Semantic Versioning , the SCONE platform is automatically assigned a version number SCONE_VERSION_NUMBER = MAJOR.MINOR.PATCH whereby MAJOR is incremented when some breaking change happens like an underlying protocol changes or we disable some deprecated feature. MINOR is incremented whenever we add a new feature that is backward compatible. PATCH is incremented whenever we fix some bug or cleanup the code. The version is automatically computed by our system, i.e., for each release our build system computes the new version of the platform. Since we started to release periodically (the current plan is every week), you will see a frequent increase in version numbers. This does not mean that clients of the SCONE platform need to update every week as well. Setting your own speed of upgrades We keep the old version of our curated docker images on docker hub and in this way, everybody can set their own speed for upgrading to newer versions of SCONE. One can stay, for example, cutting edge with the newest version of SCONE or one can upgrade only if a new major version is released. Our recommendation is to upgrade all your images at the same time to the same SCONE version. In particular, try to avoid to mix and match versions with two different major versions. Moreover, we recommend to integrate the upgrade process into your CI (Continuous Integration) pipeline: try to upgrade to newer versions automatically to ensure that you stay up-to-date with bug fixes. If you do so, pull in your application a specific image version and even better the image with the specific image digest that you tested in your CI pipeline. The most important point is to always upgrade SCONE CAS (Configuration and Attestation Service) first! This ensures that in case we introduce a breaking change that SCONE CAS can attest new applications as well as old applications (that are not yet updated). While we might have some fallback such that an older CAS can attest a newer version of an application, we strongly recommend to first upgrade CAS before upgrading an application. SCONE CURATED IMAGE VERSIONING Each of the SCONE curated images has a version number. Actually, we push the same image with multiple names to support multiple pull / upgrade strategies. For example, consider image pypy3 . We are releasing different versions of pypy3 for different versions of Alpine Linux and for different versions of the SCONE platform - just to make sure that everybody can find the version they need: Image name = {Application Name and Version}-{Operating System Version}[-SCONE Versions] For the application, we might define various versions like: pypy3 : latest supported pypy3 version - use this if you want to keep cutting edge pypy3-6 : latest supported pypy3 version with major version 6 pypy3-6.0 : latest supported pypy3 version with major version 6 and minor version 0 pypy3-6.0.0 : pypy3 version 6.0.0 - use this if you do not want to upgrade automatically For the operating system, we might define various versions like: alpine3.6 : an old, stable version of Alpine Linux alpine3.7 : a little less old, stable version of Alpine Linux ... alpine : the latest stable version of Alpine Linux that we support - use this to keep up to date with current image. alpine-edge : the newest, not yet stable version of Alpine Linux (i.e, development version of Alpine) - use this for example in development if you need the newest version of an image. For the SCONE platform itself, we will provide various versions: EMPTY : do not specify the SCONE version if you want to keep up to date with the most recent version of the SCONE platform -scone2 : latest supported SCONE version with major version 2 -scone3 : latest supported SCONE version with major version 3 ... -scone2.0 : latest supported SCONE version with major version 2 and minor version 0 -scone2.1 : latest supported SCONE version with major version 2 and minor version 1 ... -scone2.0.0 : SCONE version 2.0.0 - use this if you do not want to upgrade automatically to any other version than 2.0.0 -scone2.0.1 : SCONE version 2.0.1 - use this if you do not want to upgrade automatically to any other version than 2.0.1 ... Note that not all version combinations will be available in the community version of the SCONE platform. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Scone Semantic Versioning"},{"location":"scone_semantic_versioning/#scone-semantic-versioning","text":"Following Semantic Versioning , the SCONE platform is automatically assigned a version number SCONE_VERSION_NUMBER = MAJOR.MINOR.PATCH whereby MAJOR is incremented when some breaking change happens like an underlying protocol changes or we disable some deprecated feature. MINOR is incremented whenever we add a new feature that is backward compatible. PATCH is incremented whenever we fix some bug or cleanup the code. The version is automatically computed by our system, i.e., for each release our build system computes the new version of the platform. Since we started to release periodically (the current plan is every week), you will see a frequent increase in version numbers. This does not mean that clients of the SCONE platform need to update every week as well.","title":"Scone Semantic Versioning"},{"location":"scone_semantic_versioning/#setting-your-own-speed-of-upgrades","text":"We keep the old version of our curated docker images on docker hub and in this way, everybody can set their own speed for upgrading to newer versions of SCONE. One can stay, for example, cutting edge with the newest version of SCONE or one can upgrade only if a new major version is released. Our recommendation is to upgrade all your images at the same time to the same SCONE version. In particular, try to avoid to mix and match versions with two different major versions. Moreover, we recommend to integrate the upgrade process into your CI (Continuous Integration) pipeline: try to upgrade to newer versions automatically to ensure that you stay up-to-date with bug fixes. If you do so, pull in your application a specific image version and even better the image with the specific image digest that you tested in your CI pipeline. The most important point is to always upgrade SCONE CAS (Configuration and Attestation Service) first! This ensures that in case we introduce a breaking change that SCONE CAS can attest new applications as well as old applications (that are not yet updated). While we might have some fallback such that an older CAS can attest a newer version of an application, we strongly recommend to first upgrade CAS before upgrading an application.","title":"Setting your own speed of upgrades"},{"location":"scone_semantic_versioning/#scone-curated-image-versioning","text":"Each of the SCONE curated images has a version number. Actually, we push the same image with multiple names to support multiple pull / upgrade strategies. For example, consider image pypy3 . We are releasing different versions of pypy3 for different versions of Alpine Linux and for different versions of the SCONE platform - just to make sure that everybody can find the version they need: Image name = {Application Name and Version}-{Operating System Version}[-SCONE Versions] For the application, we might define various versions like: pypy3 : latest supported pypy3 version - use this if you want to keep cutting edge pypy3-6 : latest supported pypy3 version with major version 6 pypy3-6.0 : latest supported pypy3 version with major version 6 and minor version 0 pypy3-6.0.0 : pypy3 version 6.0.0 - use this if you do not want to upgrade automatically For the operating system, we might define various versions like: alpine3.6 : an old, stable version of Alpine Linux alpine3.7 : a little less old, stable version of Alpine Linux ... alpine : the latest stable version of Alpine Linux that we support - use this to keep up to date with current image. alpine-edge : the newest, not yet stable version of Alpine Linux (i.e, development version of Alpine) - use this for example in development if you need the newest version of an image. For the SCONE platform itself, we will provide various versions: EMPTY : do not specify the SCONE version if you want to keep up to date with the most recent version of the SCONE platform -scone2 : latest supported SCONE version with major version 2 -scone3 : latest supported SCONE version with major version 3 ... -scone2.0 : latest supported SCONE version with major version 2 and minor version 0 -scone2.1 : latest supported SCONE version with major version 2 and minor version 1 ... -scone2.0.0 : SCONE version 2.0.0 - use this if you do not want to upgrade automatically to any other version than 2.0.0 -scone2.0.1 : SCONE version 2.0.1 - use this if you do not want to upgrade automatically to any other version than 2.0.1 ... Note that not all version combinations will be available in the community version of the SCONE platform. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"SCONE CURATED IMAGE VERSIONING"},{"location":"scone_session/","text":"scone session A session is a unique namespace that is used for the management of keys associated with the execution of an application. scone session manages keys and their access rights in the context of scone-based applications. Motivation The management of keys is one of the important problems to be addressed when building secure cloud applications. Here are some typical questions that need to be addressed: how to generate keys in a secure fashion? how to share keys between different applications? how to prevent unauthorized access to keys? how to ensure that keys stay confidential - despite, for example, administrator churn. how to ensure that no human can read keys? More advanced questions that we need to address is on how to ensure company policies, like how to enforce company policy regarding which versions of a binary are permitted to be executed? how to restrict the teams that can certain applications? Another related topic is the management of the private key of certificates. how to provide enclaves with certificates and the associated private key in a secure fashion? Related question that we need to address is on how to make this practical to use. In particular, we need to address the following question: how can we achieve all this without the need to re-engineer our applications? Approach scone session helps you to address the above questions. Before we describe the details of the scone session command line interface, we motivate its design from the threat model that we must address. Problem: No trusted party to manage access rights Traditionally, most systems have the concept of an administrator that can manage the access rights to resources. For example, a root user in an operating system or a database can determine which resources a user can access. In SCONE, we do not want to require an administrator that controls the access to secrets. Instead, we would like to empower each user such that this user can control access to her resources without any administrator being able to overwrite the access control policy of the user. However, a user can decide to share resources with others. For example, a user could share access to the key of an encrypted volume with other users. Approach: Session Concept Our objective is to provide key management without introducing the need to have a trusted party that is responsible to administrate access rights. The main idea is that each user can create a session (an alternative name would be a namespace ) in which the user has complete control over access rights to the keys created within this session. Problem: No Trusted Input and Output One of the issues that we face in the context of secure trusted computing is that we can neither trust the cloud computers and in many cases, not even our development machines. In some cases, the cloud machines might even be more secure than our development machines. Hence, we need to address the following problem: Problem: Any key that is received via an input device or output via an output device cannot be considered to be confidential anymore (- unless the key itself is encrypted) Approach: Keys are always encrypted To address this problem, we need to ensure that keys are never input or output as plain text. To achieve this, we support generation of keys and management of keys that never require keys to be transferred as plain text. Problem: Authorization If there is no trusted input and output, how can we know that certain requests / commands were indeed issued by a specific user? For example, a command is issued by a user O to give user U access to a key K that O has created. How do we ensure that it is indeed the intention of O to give U access to K ? After all, the input for issuing this command my be inserted by U or some other interested party. Approach: Multi-factor authentication and authorization While we do not trust that the input is confidential and under the control of the user, we assume that we have a reasonably secure way to authenticate the user and to ask for an authorization from the user. For the authentication and authorization, we can specify multiple devices that perform the multi-factor authentication and authorization. The authentication and authorization will be performed as a combination of face ID, finger print ID, and PIN codes running on one or more devices. Command line interface: scone session scone session create scone session create starts a new session and activates this session, i.e., makes this the default session. Outputs the name of the session on success. Options scone session create # Description --approve URL 0-n URL for approval of certain operations --fingerprint FP 1 per URL defines the fingerprint of the public key for the preceeding --approve URL --tenant TENANT 1 name of tenant --name NAME 1 name to identify session --no [OP,] 0-1 define what operations that do not require approval.(default: ALL = all operations require approval) We support the following commands / operations: Operation (OP) Approval? Description create required create a session. Approval to verify the arguments of the command and to ensure that issued to correct CAS key required create a key. Approval to verify the arguments of the command and to ensure that issued to correct CAS share required share the keys of this session with another session. Verify that command was issued by owner and that arguments are correct. join required join another session (if that session shared all its key with this session). Verify that command was issued by owner and that arguments are correct. checkpoint required checkpoint session information. Verify that command was issued by owner and that arguments are correct. restore required restores a session checkpoint. Verify that command was issued by owner and that arguments are correct. Caveats scone session create creates credentials to identify this session and to be able to modify the session. The session credentials are stored in file $HOME/.scone-session/TENANT/NAME/SESSIONID.cred : this file is encrypted with the seal key of scone session . If this file is corrupted or deleted, access to the session keys is not possible anymore . Hence, this file should be backed up. In case the scone CLI runs inside of a container, this file should be mapped into a volume. Since this file is encrypted with the seal key of scone session , if the CPU that created this file fails or the CPU microcode is updated, this file cannot be accessed anymore . Hence, the owner of the session should share the access rights on multiple machines. We support this with the help of scone session share and scone session join . Alternative: only support scone session join and authorize via approval only. This would help with software upgrade too. Authorize new client via session join. Session information is also stored in a CAS. You can copy the session information from one CAS to another CAS with the help of scone session checkpoint and scone session restore . Moreover, you can use these two commands to move a session from one CAS to another CAS. To be able to support updated scone clients, you must enable that a session permits additional clients with different MRENCLAVEs. Also, to support updated CAS. We support this via scone session join to permit new clients to get existing session information. To support new CAS, a user has to perform a checkpoint and restore operation. Example: scone session create --tenant myself --name www \\ --approve ap.com/myself --fingerprint 43 :51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\ --approve ap2.com/myboss --fingerprint 55 :45:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\ scone session key Creates a key or a certificate and makes it available at a given key name. scone session key --name KEYNAME [ --replace ] \u2014-type TYPE [ --access [ session ] :MRENCLAVE ]] Options scone session key Description --scope SCOPE limits visibility of key: final, session, full. final = only participants specified with option --access are permitted to receive the key. session = one can add more enclaves that can access the key but it is not possible to extend this beyond the current session. full no limit to what extend the user can share this key with other sessions. -- type TYPE determines the type of key. TYPE=[DIGIT:N,ALPHA:N,ALPHANUM:N,PRINT:N,CERT:N] scone session share --key KEYNAME --gentoken --session SESSIONID Use cases: 1) We share the public key and the private key of an image with a client scone session share --key image/NAME/fspf-key --pubkey signer --gentoken --duration 366 2) We share the complete session with another client: scone session share --gentoken --all 3) Share all keys related to a stack that was created scone session share --stack FILE --gentoken Prints a token that can be used to access all keys to be able to run this stack scone session join --session SESSION-ID --token TOKEN Use case: Replace the current session by another session: scone session join --token TOKEN We effectively join a already existing session ID with a new client. scone session import --token TOKEN scone session import --token TOKEN [ --replace ] Import all keys related to the TOKEN. If --replace is given, keys are updated in case they already exist. Otherwise, an error message is issued. scone session checkpoint Copies the current state of a session from the CAS and stores it encrypted on the local file system. scone session checkpoint scone session restore --session SESSIONID --CAS URL Take the checkpoint of session SESSIONID and stores it to CAS referred to via URL. If the session already exists, this will result in a failure. scone session remove --force Removes a current session from the CAS. scone session activate --session SESSIONID Switches session to another session with ID SESSIONID Example: Sconedocs website Root ... We need to specify the path, the key and the tag of the file system protection file: image: aptsigner fspf_path: /nginx/fspf.pb fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag: 5930bffbd9ea2f1317e6872b032334db We get this information from the CAS for the current session. Key directory is /image/aptsigner/fspf_path , ... Example: Signing APT packages Our objective is to sign Debian/Ubuntu packages without ever having the private key be visible in the clear. In particular, we do not want be able ourselves to change our minds and make the keys visible to us or others. Approach: We package the software in a container image that contains all code required to sign packages. To do so, we have two volumes: 1) packages : contains the package to be signed 2) keyring : Sharing Keys Access to keys: public key: share with everybody encrpytion key: share access with token generate token: with limited lifetime \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"scone session"},{"location":"scone_session/#scone-session","text":"A session is a unique namespace that is used for the management of keys associated with the execution of an application. scone session manages keys and their access rights in the context of scone-based applications.","title":"scone session"},{"location":"scone_session/#motivation","text":"The management of keys is one of the important problems to be addressed when building secure cloud applications. Here are some typical questions that need to be addressed: how to generate keys in a secure fashion? how to share keys between different applications? how to prevent unauthorized access to keys? how to ensure that keys stay confidential - despite, for example, administrator churn. how to ensure that no human can read keys? More advanced questions that we need to address is on how to ensure company policies, like how to enforce company policy regarding which versions of a binary are permitted to be executed? how to restrict the teams that can certain applications? Another related topic is the management of the private key of certificates. how to provide enclaves with certificates and the associated private key in a secure fashion? Related question that we need to address is on how to make this practical to use. In particular, we need to address the following question: how can we achieve all this without the need to re-engineer our applications?","title":"Motivation"},{"location":"scone_session/#approach","text":"scone session helps you to address the above questions. Before we describe the details of the scone session command line interface, we motivate its design from the threat model that we must address.","title":"Approach"},{"location":"scone_session/#problem-no-trusted-party-to-manage-access-rights","text":"Traditionally, most systems have the concept of an administrator that can manage the access rights to resources. For example, a root user in an operating system or a database can determine which resources a user can access. In SCONE, we do not want to require an administrator that controls the access to secrets. Instead, we would like to empower each user such that this user can control access to her resources without any administrator being able to overwrite the access control policy of the user. However, a user can decide to share resources with others. For example, a user could share access to the key of an encrypted volume with other users.","title":"Problem: No trusted party to manage access rights"},{"location":"scone_session/#approach-session-concept","text":"Our objective is to provide key management without introducing the need to have a trusted party that is responsible to administrate access rights. The main idea is that each user can create a session (an alternative name would be a namespace ) in which the user has complete control over access rights to the keys created within this session.","title":"Approach: Session Concept"},{"location":"scone_session/#problem-no-trusted-input-and-output","text":"One of the issues that we face in the context of secure trusted computing is that we can neither trust the cloud computers and in many cases, not even our development machines. In some cases, the cloud machines might even be more secure than our development machines. Hence, we need to address the following problem: Problem: Any key that is received via an input device or output via an output device cannot be considered to be confidential anymore (- unless the key itself is encrypted)","title":"Problem: No Trusted Input and Output"},{"location":"scone_session/#approach-keys-are-always-encrypted","text":"To address this problem, we need to ensure that keys are never input or output as plain text. To achieve this, we support generation of keys and management of keys that never require keys to be transferred as plain text.","title":"Approach: Keys are always encrypted"},{"location":"scone_session/#problem-authorization","text":"If there is no trusted input and output, how can we know that certain requests / commands were indeed issued by a specific user? For example, a command is issued by a user O to give user U access to a key K that O has created. How do we ensure that it is indeed the intention of O to give U access to K ? After all, the input for issuing this command my be inserted by U or some other interested party.","title":"Problem: Authorization"},{"location":"scone_session/#approach-multi-factor-authentication-and-authorization","text":"While we do not trust that the input is confidential and under the control of the user, we assume that we have a reasonably secure way to authenticate the user and to ask for an authorization from the user. For the authentication and authorization, we can specify multiple devices that perform the multi-factor authentication and authorization. The authentication and authorization will be performed as a combination of face ID, finger print ID, and PIN codes running on one or more devices.","title":"Approach: Multi-factor authentication and authorization"},{"location":"scone_session/#command-line-interface-scone-session","text":"","title":"Command line interface: scone session"},{"location":"scone_session/#scone-session-create","text":"scone session create starts a new session and activates this session, i.e., makes this the default session. Outputs the name of the session on success.","title":"scone session create"},{"location":"scone_session/#options","text":"scone session create # Description --approve URL 0-n URL for approval of certain operations --fingerprint FP 1 per URL defines the fingerprint of the public key for the preceeding --approve URL --tenant TENANT 1 name of tenant --name NAME 1 name to identify session --no [OP,] 0-1 define what operations that do not require approval.(default: ALL = all operations require approval) We support the following commands / operations: Operation (OP) Approval? Description create required create a session. Approval to verify the arguments of the command and to ensure that issued to correct CAS key required create a key. Approval to verify the arguments of the command and to ensure that issued to correct CAS share required share the keys of this session with another session. Verify that command was issued by owner and that arguments are correct. join required join another session (if that session shared all its key with this session). Verify that command was issued by owner and that arguments are correct. checkpoint required checkpoint session information. Verify that command was issued by owner and that arguments are correct. restore required restores a session checkpoint. Verify that command was issued by owner and that arguments are correct.","title":"Options"},{"location":"scone_session/#caveats","text":"scone session create creates credentials to identify this session and to be able to modify the session. The session credentials are stored in file $HOME/.scone-session/TENANT/NAME/SESSIONID.cred : this file is encrypted with the seal key of scone session . If this file is corrupted or deleted, access to the session keys is not possible anymore . Hence, this file should be backed up. In case the scone CLI runs inside of a container, this file should be mapped into a volume. Since this file is encrypted with the seal key of scone session , if the CPU that created this file fails or the CPU microcode is updated, this file cannot be accessed anymore . Hence, the owner of the session should share the access rights on multiple machines. We support this with the help of scone session share and scone session join . Alternative: only support scone session join and authorize via approval only. This would help with software upgrade too. Authorize new client via session join. Session information is also stored in a CAS. You can copy the session information from one CAS to another CAS with the help of scone session checkpoint and scone session restore . Moreover, you can use these two commands to move a session from one CAS to another CAS. To be able to support updated scone clients, you must enable that a session permits additional clients with different MRENCLAVEs. Also, to support updated CAS. We support this via scone session join to permit new clients to get existing session information. To support new CAS, a user has to perform a checkpoint and restore operation.","title":"Caveats"},{"location":"scone_session/#example","text":"scone session create --tenant myself --name www \\ --approve ap.com/myself --fingerprint 43 :51:43:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\ --approve ap2.com/myboss --fingerprint 55 :45:a1:b5:fc:8b:b7:0a:3a:a9:b1:0f:66:73:a8 \\","title":"Example:"},{"location":"scone_session/#scone-session-key","text":"Creates a key or a certificate and makes it available at a given key name. scone session key --name KEYNAME [ --replace ] \u2014-type TYPE [ --access [ session ] :MRENCLAVE ]]","title":"scone session key"},{"location":"scone_session/#options_1","text":"scone session key Description --scope SCOPE limits visibility of key: final, session, full. final = only participants specified with option --access are permitted to receive the key. session = one can add more enclaves that can access the key but it is not possible to extend this beyond the current session. full no limit to what extend the user can share this key with other sessions. -- type TYPE determines the type of key. TYPE=[DIGIT:N,ALPHA:N,ALPHANUM:N,PRINT:N,CERT:N]","title":"Options"},{"location":"scone_session/#scone-session-share-key-keyname-gentoken-session-sessionid","text":"Use cases: 1) We share the public key and the private key of an image with a client scone session share --key image/NAME/fspf-key --pubkey signer --gentoken --duration 366 2) We share the complete session with another client: scone session share --gentoken --all 3) Share all keys related to a stack that was created scone session share --stack FILE --gentoken Prints a token that can be used to access all keys to be able to run this stack","title":"scone session share --key KEYNAME  --gentoken --session SESSIONID"},{"location":"scone_session/#scone-session-join-session-session-id-token-token","text":"Use case: Replace the current session by another session: scone session join --token TOKEN We effectively join a already existing session ID with a new client.","title":"scone session join --session SESSION-ID --token TOKEN"},{"location":"scone_session/#scone-session-import-token-token","text":"scone session import --token TOKEN [ --replace ] Import all keys related to the TOKEN. If --replace is given, keys are updated in case they already exist. Otherwise, an error message is issued.","title":"scone session import --token TOKEN"},{"location":"scone_session/#scone-session-checkpoint","text":"Copies the current state of a session from the CAS and stores it encrypted on the local file system. scone session checkpoint","title":"scone session checkpoint"},{"location":"scone_session/#scone-session-restore-session-sessionid-cas-url","text":"Take the checkpoint of session SESSIONID and stores it to CAS referred to via URL. If the session already exists, this will result in a failure.","title":"scone session restore --session SESSIONID --CAS URL"},{"location":"scone_session/#scone-session-remove-force","text":"Removes a current session from the CAS.","title":"scone session remove --force"},{"location":"scone_session/#scone-session-activate-session-sessionid","text":"Switches session to another session with ID SESSIONID","title":"scone session activate --session SESSIONID"},{"location":"scone_session/#example-sconedocs-website","text":"Root ... We need to specify the path, the key and the tag of the file system protection file: image: aptsigner fspf_path: /nginx/fspf.pb fspf_key: 970f4925bb7b221461f3d1a3f17450aa42844539de24f5acc1b45b8c140f9467 fspf_tag: 5930bffbd9ea2f1317e6872b032334db We get this information from the CAS for the current session. Key directory is /image/aptsigner/fspf_path , ...","title":"Example: Sconedocs website"},{"location":"scone_session/#example-signing-apt-packages","text":"Our objective is to sign Debian/Ubuntu packages without ever having the private key be visible in the clear. In particular, we do not want be able ourselves to change our minds and make the keys visible to us or others. Approach: We package the software in a container image that contains all code required to sign packages. To do so, we have two volumes: 1) packages : contains the package to be signed 2) keyring :","title":"Example: Signing APT packages"},{"location":"scone_session/#sharing-keys","text":"Access to keys: public key: share with everybody encrpytion key: share access with token generate token: with limited lifetime \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Sharing Keys"},{"location":"sconeinstall/","text":"Installing SCONE Typically, you will use the SCONE crosscompilers and CLI inside of a container. Hence, just start a container that contains SCONE in simulation mode or in hardware mode . In case you want to executed your programs on a host , you can compile in a container and move the generated binary to the host and execute on this host. In case you want to distribute your programs in container images and execute them in containers, you probably want to use a Dockerfile to compile your program and generate your container image . \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Installing SCONE"},{"location":"sconeinstall/#installing-scone","text":"Typically, you will use the SCONE crosscompilers and CLI inside of a container. Hence, just start a container that contains SCONE in simulation mode or in hardware mode . In case you want to executed your programs on a host , you can compile in a container and move the generated binary to the host and execute on this host. In case you want to distribute your programs in container images and execute them in containers, you probably want to use a Dockerfile to compile your program and generate your container image . \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Installing SCONE"},{"location":"secure_remote_execution/","text":"Secure Remote Execution We show how to execute applications in an always encrypted fashion on a remote, untrusted machine via a simple CLI (Command Line Interface). The CLI runs inside a container on a trusted client computer: Prerequisites Remote host installation You need to install the SGX driver and Docker engine . So far, we support the execution on remote hosts / VMs via ssh and via the iExec platform. To start a container on a remote host via this CLI , you need to give the CLI the credentials of this hosts. These credentials are stored in a volume stored on the client's computer. In this example, we use directory $PWD/conf/ to store the credentials. Ensure that only you have access to this directory to protect the credentials. This directory must be mapped in the container at location /conf : eval docker run -it --rm -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli add-host --alias caroline --hostname 1 .2.3.4 This adds an alias ( caroline ) for the host identified by option --hostname . The argument --hostname can either be a hostname or an IP address. The CLI generates a key pair inside the container and stores it in /conf . The public key is used to grant access to the container on the remote host. The output of add-host will look something like this: ssh ubuntu@1.2.3.4 \"echo \\\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDXVqr0diJjgMPx7WNTQWJVhV3ea8L1I/8mDZTsQUH5gazB+laIiM7tEAKbddbessItmA9bLOZEq4CIrYmtXpjG2tBSES2YpqNhQ8+3r4U3ozSBD1XAc6OzRmnHP+wmuVpCQw0QxBKj0Qq5MCehmLIXLYephOtjVWwsA3EprVHIq0+/wYZp4mU3evCgcvE46nxOrmHzu5X4iDUMSY59XHdavIVkkS87qp4RlyFQa0gBRHOfcGEVJ3oS/pmKHxasdLwjiVmCovLG4RPS88RoKCf8zWis8vpPUKt/04xjlj4gqsV/U7VR2S/kFcvq1yuvOno+BhcsGme2U2CKTV4Y16ZJ scone-CLI\\\" >> .ssh/authorized_keys\" eval executes the output and gives your local client to give the container access to the remote host. Execution To execute an application on a remote host, you first have to add this host via add-host as described above. We show how to execute the simple copy application that we introduce in the next section of this tutorial . This copy application takes some input files in a given directory INPUTS and copies these files to directory OUTPUTS in a very elaborate way: encrypt the files in directory INPUTS inside the container running on the client machine push the encrypted files to a remote file service - which is operated by an entity that we do not know start the copy application container on a remote host - which is given via option --host HOST a script running inside the container pulls the encrypted files from the remote file service the script starts the copy application inside of an enclave the SCONE runtime transparently decrypts the input files and transparently encrypts the output files the SCONE runtime ensures both the confidentiality as well as the integrity of the files the copy applications copies the input files to the output files and then terminates the shell script pushes the encrypted output files to the remote file service the CLI pulls the output files from the remote file service and decrypts the files and stores the files inside the OUTPUT directory. The copy application is stored in image sconecuratedimages/iexecsgx:copy_demo on hub.docker.com . You can execute this image as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline Example: Copy Demo You could perform the following steps to see the copy_demo in action. Create some directory and a file to copy. mkdir -p INPUTS OUTPUTS echo \"Hello World\" > INPUTS/f1.txt Now execute the copy application (as shown above): docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline The file f1.txt was copied to directory OUTPUTS . You can check the output, by executing: cat OUTPUTS/f1.txt which results in the output: Hello World Example: Blender We show to run blender with a blender file. To do so, show to render some blender file from our collaborator iExec as well as some other blender files accessible via github. Setup Let's set up an EXAMPLE directory and create some sub-directories for input and output files. We also need to specify a remote file service like transfer.sh or filepush.co/upload , and an instance of the SCONE CAS configuration and attestation service. mkdir -p EXAMPLE TRANSFER = \"transfer.sh\" CAS = scone-cas.cf cd EXAMPLE mkdir -p conf INPUTS OUTPUTS To reduce the typing, let's define an alias for the secure remote execution of blender: alias aeblender = \"docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender --host caroline -r $TRANSFER -s $CAS \" We assume that you added the host (in this case caroline ) already via add-host (see above). Render Image Let's first download a blender file: curl https://raw.githubusercontent.com/iExecBlockchainComputing/iexec-dapps-registry/master/iExecBlockchainComputing/Blender/iexec-rlc.blend -o INPUTS/iexec-rlc.blend and we render this blender file remotely ensuring all artifacts are always encrypted : aeblender We can now look at the files in OUTPUT directory: open OUTPUTS/0001.png Which shows the following picture which was computed using always encrypted approach: Cube Let's download another blender file from github: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/cube.blend and render this blender file on the remote and untrusted host caroline : aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted : Helicopter Example Another example from github : curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/scene-Helicopter-27-cycles.blend We are execution with the same command as above: aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted : Car Example We show one more example: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/bmw27_cpu.blend and render this (as above): aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted : Advanced Example You can also run blender with Python scripts that describe your images and movies - see blender use case for details. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Secure Remote Execution"},{"location":"secure_remote_execution/#secure-remote-execution","text":"We show how to execute applications in an always encrypted fashion on a remote, untrusted machine via a simple CLI (Command Line Interface). The CLI runs inside a container on a trusted client computer:","title":"Secure Remote Execution"},{"location":"secure_remote_execution/#prerequisites","text":"Remote host installation You need to install the SGX driver and Docker engine . So far, we support the execution on remote hosts / VMs via ssh and via the iExec platform. To start a container on a remote host via this CLI , you need to give the CLI the credentials of this hosts. These credentials are stored in a volume stored on the client's computer. In this example, we use directory $PWD/conf/ to store the credentials. Ensure that only you have access to this directory to protect the credentials. This directory must be mapped in the container at location /conf : eval docker run -it --rm -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli add-host --alias caroline --hostname 1 .2.3.4 This adds an alias ( caroline ) for the host identified by option --hostname . The argument --hostname can either be a hostname or an IP address. The CLI generates a key pair inside the container and stores it in /conf . The public key is used to grant access to the container on the remote host. The output of add-host will look something like this: ssh ubuntu@1.2.3.4 \"echo \\\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDXVqr0diJjgMPx7WNTQWJVhV3ea8L1I/8mDZTsQUH5gazB+laIiM7tEAKbddbessItmA9bLOZEq4CIrYmtXpjG2tBSES2YpqNhQ8+3r4U3ozSBD1XAc6OzRmnHP+wmuVpCQw0QxBKj0Qq5MCehmLIXLYephOtjVWwsA3EprVHIq0+/wYZp4mU3evCgcvE46nxOrmHzu5X4iDUMSY59XHdavIVkkS87qp4RlyFQa0gBRHOfcGEVJ3oS/pmKHxasdLwjiVmCovLG4RPS88RoKCf8zWis8vpPUKt/04xjlj4gqsV/U7VR2S/kFcvq1yuvOno+BhcsGme2U2CKTV4Y16ZJ scone-CLI\\\" >> .ssh/authorized_keys\" eval executes the output and gives your local client to give the container access to the remote host.","title":"Prerequisites"},{"location":"secure_remote_execution/#execution","text":"To execute an application on a remote host, you first have to add this host via add-host as described above. We show how to execute the simple copy application that we introduce in the next section of this tutorial . This copy application takes some input files in a given directory INPUTS and copies these files to directory OUTPUTS in a very elaborate way: encrypt the files in directory INPUTS inside the container running on the client machine push the encrypted files to a remote file service - which is operated by an entity that we do not know start the copy application container on a remote host - which is given via option --host HOST a script running inside the container pulls the encrypted files from the remote file service the script starts the copy application inside of an enclave the SCONE runtime transparently decrypts the input files and transparently encrypts the output files the SCONE runtime ensures both the confidentiality as well as the integrity of the files the copy applications copies the input files to the output files and then terminates the shell script pushes the encrypted output files to the remote file service the CLI pulls the output files from the remote file service and decrypts the files and stores the files inside the OUTPUT directory. The copy application is stored in image sconecuratedimages/iexecsgx:copy_demo on hub.docker.com . You can execute this image as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline","title":"Execution"},{"location":"secure_remote_execution/#example-copy-demo","text":"You could perform the following steps to see the copy_demo in action. Create some directory and a file to copy. mkdir -p INPUTS OUTPUTS echo \"Hello World\" > INPUTS/f1.txt Now execute the copy application (as shown above): docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline The file f1.txt was copied to directory OUTPUTS . You can check the output, by executing: cat OUTPUTS/f1.txt which results in the output: Hello World","title":"Example: Copy Demo"},{"location":"secure_remote_execution/#example-blender","text":"We show to run blender with a blender file. To do so, show to render some blender file from our collaborator iExec as well as some other blender files accessible via github.","title":"Example: Blender"},{"location":"secure_remote_execution/#setup","text":"Let's set up an EXAMPLE directory and create some sub-directories for input and output files. We also need to specify a remote file service like transfer.sh or filepush.co/upload , and an instance of the SCONE CAS configuration and attestation service. mkdir -p EXAMPLE TRANSFER = \"transfer.sh\" CAS = scone-cas.cf cd EXAMPLE mkdir -p conf INPUTS OUTPUTS To reduce the typing, let's define an alias for the secure remote execution of blender: alias aeblender = \"docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender --host caroline -r $TRANSFER -s $CAS \" We assume that you added the host (in this case caroline ) already via add-host (see above).","title":"Setup"},{"location":"secure_remote_execution/#render-image","text":"Let's first download a blender file: curl https://raw.githubusercontent.com/iExecBlockchainComputing/iexec-dapps-registry/master/iExecBlockchainComputing/Blender/iexec-rlc.blend -o INPUTS/iexec-rlc.blend and we render this blender file remotely ensuring all artifacts are always encrypted : aeblender We can now look at the files in OUTPUT directory: open OUTPUTS/0001.png Which shows the following picture which was computed using always encrypted approach:","title":"Render Image"},{"location":"secure_remote_execution/#cube","text":"Let's download another blender file from github: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/cube.blend and render this blender file on the remote and untrusted host caroline : aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted :","title":"Cube"},{"location":"secure_remote_execution/#helicopter-example","text":"Another example from github : curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/scene-Helicopter-27-cycles.blend We are execution with the same command as above: aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted :","title":"Helicopter Example"},{"location":"secure_remote_execution/#car-example","text":"We show one more example: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/bmw27_cpu.blend and render this (as above): aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted :","title":"Car Example"},{"location":"secure_remote_execution/#advanced-example","text":"You can also run blender with Python scripts that describe your images and movies - see blender use case for details. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Advanced Example"},{"location":"sgxinstall/","text":"Installation of SGX driver To install the SGX 2.0 driver on Linux distributions, follow the official description . Alternatively, on a modern Ubuntu system on which you have sudo access, you could execute the following: curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_sgx_driver.sh | bash Check if the SGX driver is installed Note that in systems in which the CPU supports SGX but the BIOS disables SGX, the driver might successfully install and load but using the SGX driver fails. Check on the host as well as inside your containers that the SGX device /dev/isgx is visible: ls /dev/isgx >/dev/null 2 >1 && echo \"SGX Driver installed\" || echo \"SGX Driver NOT installed\" Checking availability of SGX device inside of containers Docker does not automatically map the SGX device inside of containers. We provide, however, a patched Docker engine and a patched SGX driver that together permit to automatically map the sgx device inside of containers. You can run the checks to see if the SGX device gets mapped automatically into a container, i.e., if you run the patched docker engine. # preferred alternative: required for swarms to work: SGX device is available in all containers by default docker run --rm sconecuratedimages/checksgx || echo \"SGX device is not automatically mapped inside of container\" If the SGX device is not automatically mapped into the container, you can try to map the device as follows into the container: # alternative: use --device option without --privileged flag docker run --device = /dev/isgx --rm sconecuratedimages/checksgx || echo \"--device=/dev/isgx: failed to map SGX device inside of container\" In the unlikely case that the device is not mapped in the container, you can try to see if the container must be privileged or if we might need to remap the device ids: # last alternative: use --device option with --privileged flag sudo docker run -v /dev/isgx:/dev/isgx --privileged --rm sconecuratedimages/checksgx || echo \"SGX device NOT available inside of container\" Use the first alternative that works in your installation to give containers access to the SGX device. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Installing Intel SGX driver"},{"location":"sgxinstall/#installation-of-sgx-driver","text":"To install the SGX 2.0 driver on Linux distributions, follow the official description . Alternatively, on a modern Ubuntu system on which you have sudo access, you could execute the following: curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_sgx_driver.sh | bash","title":"Installation of SGX driver"},{"location":"sgxinstall/#check-if-the-sgx-driver-is-installed","text":"Note that in systems in which the CPU supports SGX but the BIOS disables SGX, the driver might successfully install and load but using the SGX driver fails. Check on the host as well as inside your containers that the SGX device /dev/isgx is visible: ls /dev/isgx >/dev/null 2 >1 && echo \"SGX Driver installed\" || echo \"SGX Driver NOT installed\"","title":"Check if the SGX driver is installed"},{"location":"sgxinstall/#checking-availability-of-sgx-device-inside-of-containers","text":"Docker does not automatically map the SGX device inside of containers. We provide, however, a patched Docker engine and a patched SGX driver that together permit to automatically map the sgx device inside of containers. You can run the checks to see if the SGX device gets mapped automatically into a container, i.e., if you run the patched docker engine. # preferred alternative: required for swarms to work: SGX device is available in all containers by default docker run --rm sconecuratedimages/checksgx || echo \"SGX device is not automatically mapped inside of container\" If the SGX device is not automatically mapped into the container, you can try to map the device as follows into the container: # alternative: use --device option without --privileged flag docker run --device = /dev/isgx --rm sconecuratedimages/checksgx || echo \"--device=/dev/isgx: failed to map SGX device inside of container\" In the unlikely case that the device is not mapped in the container, you can try to see if the container must be privileged or if we might need to remap the device ids: # last alternative: use --device option with --privileged flag sudo docker run -v /dev/isgx:/dev/isgx --privileged --rm sconecuratedimages/checksgx || echo \"SGX device NOT available inside of container\" Use the first alternative that works in your installation to give containers access to the SGX device. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Checking availability of SGX device inside of containers"},{"location":"ssh/","text":"ssh Setup ssh is standard way to log securely into remote hosts. The SCONE CLI requires that you can log into all hosts of your docker swarm(s) without the need for providing a password. We describe in this section how you could set this up. scone The scone utility executes commands via ssh on the SGX-capable machine to install software as well as to deploy and monitor containers. Since we potentially execute many ssh commands, you need to configure ssh such that you can log into the SGX machines without having to type a password, you can use the basename of your SGX machines to login, and ssh is permitted to reuse connections to reduce the execution time of the scone commands. To do so, you need to configure ssh on your development machine and/or your container in which you run scone as well as on the sgx hosts that you are using. Development machine For each SGX host inside of your swarm, you should add a host alias to your ssh configuration. Host alias To reduce your typing overhead, scone assumes that each host has a unique basename and that you configured ssh such that you can log into the host via this basename. For example, instead of typing node2.my.very.long.domain.com , you must configure ssh such that ssh node2 is a shortcut for ssh node2.my.very.long.domain.com . As a caveat, this basename must be sufficient for other hosts in the same swarm to reach node2 . In the above figure, manager must be able to resolve node2 to the IP address of node2.my.very.long.domain.com . By default most swarms are setup this way. On your development machine (or, more precisely in your development container), you need to add an alias node2 for node2.my.very.long.domain.com , you could add the following lines to your ssh config (stored in $HOME/.ssh/config ): Host node2 HostName node2.my.very.long.domain.com Port 22 User scone IdentityFile ~/.ssh/id_rsa ssh connection reuse To be able to reuse a ssh connection, you must configure ssh appropriately. You should set ControlMaster to auto and you have to specify a control path via option ControlPath . You can define a generic path like ~/.ssh/ssh_mux_%h_%p_%r - this can be the same for all hosts. For example, for some host alice you might add the following lines to $HOME/.ssh/config : Host alice ControlMaster auto ControlPath ~/.ssh/ssh_mux_%h_%p_%r user ubuntu port 10101 hostname sshproxy.cloudprovider.com Container Configuration To simplify the ssh setup inside of containers, you might want to map your ssh configuration residing in your home directory into the containers in which you run the scone CLI. Since you probably have a different user ID inside and outside the container, you might want to copy the original ssh configuration: > docker run -it -v $HOME /.ssh:/root/.xssh sconecuratedimages/sconecli Inside the container, copy the external ssh configuration: $ cp -rf $HOME /.xssh/* $HOME /.ssh SSH Agent In the container running the scone CLI, you need to start a ssh-agent in the container in which you run the scone CLI: $ SA = $( ssh-agent ) $ eval \" $SA \" and add your public key by executing: $ ssh-add Ensure that you are now able to log into all hosts of your swarm. SGX Host Setup scone expects to have password-less access to all SGX hosts that you want to use for Scone. To do so, you need to add one of your public ssh keys to ~/.ssh/authorized_keys on all SGX host. For example, you could add your public key ~/.ssh/id_rsa.pub to file ~/.ssh/authorized_keys on each of these hosts. Some commands are required to be executed with sudo . We assume that the user has the right to perform a password-less sudo on the SGX hosts. In case the user does not yet have this right, the user should be added to /etc/sudoers . Typically, one would add the user with the help of command visudo . The entry for user alice might look like this: alice ALL =( ALL ) NOPASSWD: ALL SSH Credentials In case you do not want to use your standard credentials inside of a container, you need to ensure that you have a pair of authentication keys inside the container. If there exists no public key $HOME/.ssh/id_rsa.pub (often the case if you use a container), you can generate a new pair by executing: $ ssh-keygen -b 4096 -t rsa Append the generated public key HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/authorized_keys on the SGX hosts for which you to be able to log in without a password. Also, ensure that your $HOME/.ssh/config contains an entry for each host of your swarm (see above). \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"ssh Setup"},{"location":"ssh/#ssh-setup","text":"ssh is standard way to log securely into remote hosts. The SCONE CLI requires that you can log into all hosts of your docker swarm(s) without the need for providing a password. We describe in this section how you could set this up.","title":"ssh Setup"},{"location":"ssh/#scone","text":"The scone utility executes commands via ssh on the SGX-capable machine to install software as well as to deploy and monitor containers. Since we potentially execute many ssh commands, you need to configure ssh such that you can log into the SGX machines without having to type a password, you can use the basename of your SGX machines to login, and ssh is permitted to reuse connections to reduce the execution time of the scone commands. To do so, you need to configure ssh on your development machine and/or your container in which you run scone as well as on the sgx hosts that you are using.","title":"scone"},{"location":"ssh/#development-machine","text":"For each SGX host inside of your swarm, you should add a host alias to your ssh configuration.","title":"Development machine"},{"location":"ssh/#host-alias","text":"To reduce your typing overhead, scone assumes that each host has a unique basename and that you configured ssh such that you can log into the host via this basename. For example, instead of typing node2.my.very.long.domain.com , you must configure ssh such that ssh node2 is a shortcut for ssh node2.my.very.long.domain.com . As a caveat, this basename must be sufficient for other hosts in the same swarm to reach node2 . In the above figure, manager must be able to resolve node2 to the IP address of node2.my.very.long.domain.com . By default most swarms are setup this way. On your development machine (or, more precisely in your development container), you need to add an alias node2 for node2.my.very.long.domain.com , you could add the following lines to your ssh config (stored in $HOME/.ssh/config ): Host node2 HostName node2.my.very.long.domain.com Port 22 User scone IdentityFile ~/.ssh/id_rsa","title":"Host alias"},{"location":"ssh/#ssh-connection-reuse","text":"To be able to reuse a ssh connection, you must configure ssh appropriately. You should set ControlMaster to auto and you have to specify a control path via option ControlPath . You can define a generic path like ~/.ssh/ssh_mux_%h_%p_%r - this can be the same for all hosts. For example, for some host alice you might add the following lines to $HOME/.ssh/config : Host alice ControlMaster auto ControlPath ~/.ssh/ssh_mux_%h_%p_%r user ubuntu port 10101 hostname sshproxy.cloudprovider.com","title":"ssh connection reuse"},{"location":"ssh/#container-configuration","text":"To simplify the ssh setup inside of containers, you might want to map your ssh configuration residing in your home directory into the containers in which you run the scone CLI. Since you probably have a different user ID inside and outside the container, you might want to copy the original ssh configuration: > docker run -it -v $HOME /.ssh:/root/.xssh sconecuratedimages/sconecli Inside the container, copy the external ssh configuration: $ cp -rf $HOME /.xssh/* $HOME /.ssh","title":"Container Configuration"},{"location":"ssh/#ssh-agent","text":"In the container running the scone CLI, you need to start a ssh-agent in the container in which you run the scone CLI: $ SA = $( ssh-agent ) $ eval \" $SA \" and add your public key by executing: $ ssh-add Ensure that you are now able to log into all hosts of your swarm.","title":"SSH Agent"},{"location":"ssh/#sgx-host-setup","text":"scone expects to have password-less access to all SGX hosts that you want to use for Scone. To do so, you need to add one of your public ssh keys to ~/.ssh/authorized_keys on all SGX host. For example, you could add your public key ~/.ssh/id_rsa.pub to file ~/.ssh/authorized_keys on each of these hosts. Some commands are required to be executed with sudo . We assume that the user has the right to perform a password-less sudo on the SGX hosts. In case the user does not yet have this right, the user should be added to /etc/sudoers . Typically, one would add the user with the help of command visudo . The entry for user alice might look like this: alice ALL =( ALL ) NOPASSWD: ALL","title":"SGX Host Setup"},{"location":"ssh/#ssh-credentials","text":"In case you do not want to use your standard credentials inside of a container, you need to ensure that you have a pair of authentication keys inside the container. If there exists no public key $HOME/.ssh/id_rsa.pub (often the case if you use a container), you can generate a new pair by executing: $ ssh-keygen -b 4096 -t rsa Append the generated public key HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/id_rsa.pub* to file * HOME/.ssh/authorized_keys on the SGX hosts for which you to be able to log in without a password. Also, ensure that your $HOME/.ssh/config contains an entry for each host of your swarm (see above). \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"SSH Credentials"},{"location":"technical_summary/","text":"SCONE Technical Summary SCONE is a platform to build and run secure applications with the help of Intel SGX (Software Guard eXtensions) 1 . In a nutshell, our objective is to run applications such that data is always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against attackers with root access . Our aim is it to make it as easy as possible to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran. Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in - even into SCONE. SCONE can be used on top of Kubernetes and Docker. So, what problems can SCONE help to solve? Secure application configuration SCONE provides applications with secrets in a secure fashion. Why is that a problem? Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways: secure configuration files . SCONE can transparently decrypt encrypted configuration files, i.e., without the need to modify the application. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work. secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. Why would I need this? Consider the MySQL example from above. You can pass user passwords via environment variables like MYSQL_ROOT_PASSWORD and MYSQL_PASSWORD to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database. secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments. Transparent attestation SCONE verifies that the correct code is running before passing any configuration info to the application. To ensure this, SCONE provides a local attestation and configuration service : this service provides only the code with the correct signature ( MrEnclave ) with its secrets: certificates, arguments, environment variables and keys. It also provides the application with a certificate that shows that the application runs inside an enclave. Note that this can be done completely transparent to the application, i.e., no application source code changes are required: the encrypted certificate can be stored in the file system where the application expects its certificates. For debugging and development, you can run code inside of enclaves without attestation. Two applications can ensure that they run inside enclaves via TLS authentication. In this way we can ensure that the client certificate and the server certificate was issued by the SCONE CAS, i.e., both communication partners run inside of enclaves and have the expected MrEnclave . Secure main memory An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to protect the main memory : no access by adversaries - even those who have root access, no access by the operating system - even if compromised, no access by the hypervisor - even if compromised, and no access by the cloud provider, and no access by evil maids - despite having physical access to the host. Integration with secure key store Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one can generate keys and stores in SCONE CAS. SCONE also supports the integration with a keystore like Vault. SCONE can run Vault inside of an enclave to protect Vaults secrets in main memory. Transparent TLS encryption Some popular applications like memcached or Zookeeper 2 do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections: the connections are terminated inside of the enclave. In this way, the plain text is never seen by the operating system or any adversary. Note that one should not use an external process for TLS termination 3 . Transparent file protection SCONE protects the integrity and confidentiality of files via transparent file protection . This protection does not require any source code changes. A file can either be integrity-protected only (i.e., the file is stored in plain text but modifications are detected) or confidentiality- and integrity-protected (i.e., the file is encrypted and modifications are detected). Ease of use We provide an easy to use CLI supporting always encrypted execution as well as simplify the construction of remote applications that support always encryption. \u00a9 scontain.com , 2020. Questions or Suggestions? We plan to support alternative trusted execution environments in future releases of SCONE. \u21a9 Zookeeper replicates its state amongst a group of servers. Zookeeper does not support protecting the communication between these servers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected. \u21a9 Memcached could be protected, for example, with the help of a stunnel . The communication between memcached and stunnel is not encrypted and hence, adversaries with root access would see the unencrypted traffic. \u21a9","title":"Technical summary of SCONE"},{"location":"technical_summary/#scone-technical-summary","text":"SCONE is a platform to build and run secure applications with the help of Intel SGX (Software Guard eXtensions) 1 . In a nutshell, our objective is to run applications such that data is always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against attackers with root access . Our aim is it to make it as easy as possible to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran. Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in - even into SCONE. SCONE can be used on top of Kubernetes and Docker.","title":"SCONE Technical Summary"},{"location":"technical_summary/#so-what-problems-can-scone-help-to-solve","text":"","title":"So, what problems can SCONE help to solve?"},{"location":"technical_summary/#secure-application-configuration","text":"SCONE provides applications with secrets in a secure fashion. Why is that a problem? Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways: secure configuration files . SCONE can transparently decrypt encrypted configuration files, i.e., without the need to modify the application. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work. secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. Why would I need this? Consider the MySQL example from above. You can pass user passwords via environment variables like MYSQL_ROOT_PASSWORD and MYSQL_PASSWORD to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database. secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments.","title":"Secure application configuration"},{"location":"technical_summary/#transparent-attestation","text":"SCONE verifies that the correct code is running before passing any configuration info to the application. To ensure this, SCONE provides a local attestation and configuration service : this service provides only the code with the correct signature ( MrEnclave ) with its secrets: certificates, arguments, environment variables and keys. It also provides the application with a certificate that shows that the application runs inside an enclave. Note that this can be done completely transparent to the application, i.e., no application source code changes are required: the encrypted certificate can be stored in the file system where the application expects its certificates. For debugging and development, you can run code inside of enclaves without attestation. Two applications can ensure that they run inside enclaves via TLS authentication. In this way we can ensure that the client certificate and the server certificate was issued by the SCONE CAS, i.e., both communication partners run inside of enclaves and have the expected MrEnclave .","title":"Transparent attestation"},{"location":"technical_summary/#secure-main-memory","text":"An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to protect the main memory : no access by adversaries - even those who have root access, no access by the operating system - even if compromised, no access by the hypervisor - even if compromised, and no access by the cloud provider, and no access by evil maids - despite having physical access to the host.","title":"Secure main memory"},{"location":"technical_summary/#integration-with-secure-key-store","text":"Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one can generate keys and stores in SCONE CAS. SCONE also supports the integration with a keystore like Vault. SCONE can run Vault inside of an enclave to protect Vaults secrets in main memory.","title":"Integration with secure key store"},{"location":"technical_summary/#transparent-tls-encryption","text":"Some popular applications like memcached or Zookeeper 2 do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections: the connections are terminated inside of the enclave. In this way, the plain text is never seen by the operating system or any adversary. Note that one should not use an external process for TLS termination 3 .","title":"Transparent TLS encryption"},{"location":"technical_summary/#transparent-file-protection","text":"SCONE protects the integrity and confidentiality of files via transparent file protection . This protection does not require any source code changes. A file can either be integrity-protected only (i.e., the file is stored in plain text but modifications are detected) or confidentiality- and integrity-protected (i.e., the file is encrypted and modifications are detected).","title":"Transparent file protection"},{"location":"technical_summary/#ease-of-use","text":"We provide an easy to use CLI supporting always encrypted execution as well as simplify the construction of remote applications that support always encryption. \u00a9 scontain.com , 2020. Questions or Suggestions? We plan to support alternative trusted execution environments in future releases of SCONE. \u21a9 Zookeeper replicates its state amongst a group of servers. Zookeeper does not support protecting the communication between these servers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected. \u21a9 Memcached could be protected, for example, with the help of a stunnel . The communication between memcached and stunnel is not encrypted and hence, adversaries with root access would see the unencrypted traffic. \u21a9","title":"Ease of use"},{"location":"teemon/","text":"TEEMon Introduction We designed and implemented TEEMon \u2014 a real-time performance monitoring and analysis tool for Intel SGX based applications. TEEMon provides fine-grained performance metrics during runtime, including SGX metrics e.g., total EPC pages, free EPC pages, pages marked as old, pages evicted to main memory, pages added to enclaves, pages reclaimed from main memory, etc. It also performs the analysis to identify the causes of performance bottlenecks. It is integrated with open-source tools like Prometheus and Grafana to offer a comprehensive monitoring solution running inside Docker containers and providing a wide-ranging set of SGX metrics such as and visualizations with a low performance overhead. TEEMon is integrated with Kubernetes to monitor the performance of applications running inside SGX enclaves We integrated TEEMon with Kubernetes to monitor the performance of an application running inside more than 6000 distributed SGX enclaves using SCONE (See the following screencast)","title":"Monitoring tool"},{"location":"teemon/#teemon","text":"","title":"TEEMon"},{"location":"teemon/#introduction","text":"We designed and implemented TEEMon \u2014 a real-time performance monitoring and analysis tool for Intel SGX based applications. TEEMon provides fine-grained performance metrics during runtime, including SGX metrics e.g., total EPC pages, free EPC pages, pages marked as old, pages evicted to main memory, pages added to enclaves, pages reclaimed from main memory, etc. It also performs the analysis to identify the causes of performance bottlenecks. It is integrated with open-source tools like Prometheus and Grafana to offer a comprehensive monitoring solution running inside Docker containers and providing a wide-ranging set of SGX metrics such as and visualizations with a low performance overhead.","title":"Introduction"},{"location":"teemon/#teemon-is-integrated-with-kubernetes-to-monitor-the-performance-of-applications-running-inside-sgx-enclaves","text":"We integrated TEEMon with Kubernetes to monitor the performance of an application running inside more than 6000 distributed SGX enclaves using SCONE (See the following screencast)","title":"TEEMon is integrated with Kubernetes to monitor the performance of applications running inside SGX enclaves"},{"location":"tensorflow/","text":"Palaemon Tensorflow Classification Demo We created a Tensorflow demo to simplify your evaluating TensorFlow running inside of Intel SGX. To get access to the TensorFlow image, send us an email . Try it out the demo by executing: docker run -it --privileged sconecuratedimages/datasystems:tensorflow-full sh Test Tensorflow with SCONE by performing image classification cd /demo/ && ./run.sh \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"TensorFlow"},{"location":"tensorflow/#palaemon-tensorflow-classification-demo","text":"We created a Tensorflow demo to simplify your evaluating TensorFlow running inside of Intel SGX. To get access to the TensorFlow image, send us an email . Try it out the demo by executing: docker run -it --privileged sconecuratedimages/datasystems:tensorflow-full sh Test Tensorflow with SCONE by performing image classification cd /demo/ && ./run.sh \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Palaemon Tensorflow Classification Demo"},{"location":"tensorflowlite/","text":"TensorFlow Lite Use Case TensorFlow Lite was designed for on-device machine learning inference with low latency and a small binary size . Hence, TensorFlow Lite is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We will add some more documentation about the curated TensorFlow Lite image later. Until then, you can have a look at our TensorFlow Lite screencast: If you want to evaluate the TensorFlow Lite image, send us an email . \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"TensorFlow Lite"},{"location":"tensorflowlite/#tensorflow-lite-use-case","text":"TensorFlow Lite was designed for on-device machine learning inference with low latency and a small binary size . Hence, TensorFlow Lite is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We will add some more documentation about the curated TensorFlow Lite image later. Until then, you can have a look at our TensorFlow Lite screencast: If you want to evaluate the TensorFlow Lite image, send us an email . \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"TensorFlow Lite Use Case"},{"location":"usecases/","text":"SCONE Use Cases Overview In this chapter, we introduce some use cases of how SCONE can be used. Most of these use cases will be based on curated images 1 that users can subscribe to. So far, we have the following use cases demos: blender : Blender 3D creation suite. We show how to protect the blender image while permitting clients that deploy this image to customize the image. They can customize without having access to the key used to protect that image. PySpark : Apache Spark with Python inside of Intel SGX enclaves with the help of SCONE. TensorFlow : Run TensorFlow inside of Intel SGX enclaves with the help of SCONE. TensorFlow Lite : TensorFlow Lite inside of Intel SGX enclaves with the help of SCONE. OpenVino : Run OpenVino based applications inside of Intel SGX enclaves with the help of SCONE. We will add more use cases over time. \u00a9 scontain.com , 2020. Questions or Suggestions? A curated image is a container image of a popular service maintained by scontain.com. \u21a9","title":"Introduction"},{"location":"usecases/#scone-use-cases-overview","text":"In this chapter, we introduce some use cases of how SCONE can be used. Most of these use cases will be based on curated images 1 that users can subscribe to. So far, we have the following use cases demos: blender : Blender 3D creation suite. We show how to protect the blender image while permitting clients that deploy this image to customize the image. They can customize without having access to the key used to protect that image. PySpark : Apache Spark with Python inside of Intel SGX enclaves with the help of SCONE. TensorFlow : Run TensorFlow inside of Intel SGX enclaves with the help of SCONE. TensorFlow Lite : TensorFlow Lite inside of Intel SGX enclaves with the help of SCONE. OpenVino : Run OpenVino based applications inside of Intel SGX enclaves with the help of SCONE. We will add more use cases over time. \u00a9 scontain.com , 2020. Questions or Suggestions? A curated image is a container image of a popular service maintained by scontain.com. \u21a9","title":"SCONE Use Cases Overview"},{"location":"vault/","text":"Scone Vault Vault is a popular and nicely designed secret management system. We maintain a hardened version of Vault that runs with Scone. It addresses the following issues: Vault keeps secrets in the clear in the main memory. Even if a secret is encrypted when stored externally, they are stored in the clear in main memory, e.g., just before they are sent to a client. An attacker with root access could hence dump the memory of Vault to retrieve the secrets. Vault's encryption key to read from and to write to the external storage is stored in the main memory. Again, an attacker could dump the memory to gain access to Vault's encryption key. Vault Image We maintain a container image sconecuratedimages/apps:scone-vault-latest which contains the latest version of Vault (0.8.1), runs inside of an SGX enclaves (default is a pre-release enclave), on top of Alpine 3.6 This protects against dumping of the main memory (- when run in release mode). Demo This image also contains a demo that you can try out. This demo shows how to set up the configuration of an nginx instance. To run this demo, you first need to checkout the demo repostiory: git clone https://github.com/scontain/scone-vault cd scone-vault you need to ensure to have a docker compose file with content as follows: cat > docker-compose.yml <<EOF version: '3.2' services: vault: image: sconecuratedimages/apps:scone-vault-latest command: sh -c \"cd build_dir && ./start_vault.sh\" environment: - VAULT_DEV_ROOT_TOKEN_ID=RootToken volumes: - ./:/build_dir cap_add: - IPC_LOCK scone-vault-nginx: image: sconecuratedimages/apps:nginx-1.13.8-alpine environment: - URL=\"http://vault:8200\" - INDEX=nginx - VAULT_ADDR=\"http://vault:8200\" - TOKEN=RootToken command: sh -c \"cd build_dir && ./install-deps.sh && ./bench.sh\" volumes: - ./:/build_dir depends_on: - vault EOF Next, try it out by executing: docker-compose up Please ensure to execute docker-compose down before starting it with up again. Note that the script start_vault.sh is used to start Vault server and inject some secrets used for Nginx. Details You can perform the individual steps manually as described below. Run the demo container using docker-compose: docker-compose run scone-vault-nginx sh Go to the deployment directory: cd /build_dir/ Install dependencies: ./install-deps.sh Now, run the benchmark to test if SCONE Vault is setting up the configuration for Nginx. ./bench.sh Integration with SCONE CAS For added security, we recommend the following to run Vault in a release enclave to prevent attackers from dumping the content of the Vault enclave and to integrate the execution of Vault with SCONE CAS such that: the command line arguments and environment variables are passed to Vault in an encrypted fashion stdin/stdout/stderr of Vault are encrypted CAS can pass a TLS private / public key to Vault the Vault encryption key can be stored by SCONE CAS Vault's HSM PIN pin can be specified by the VAULT_HSM_PIN environment variable. We can securely set the environment variable with the help of SCONE CAS since SCONE can pass encrypted environment variables to applications. Please contact us, if you are interested in a setup together with SCONE CAS. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Vault"},{"location":"vault/#scone-vault","text":"Vault is a popular and nicely designed secret management system. We maintain a hardened version of Vault that runs with Scone. It addresses the following issues: Vault keeps secrets in the clear in the main memory. Even if a secret is encrypted when stored externally, they are stored in the clear in main memory, e.g., just before they are sent to a client. An attacker with root access could hence dump the memory of Vault to retrieve the secrets. Vault's encryption key to read from and to write to the external storage is stored in the main memory. Again, an attacker could dump the memory to gain access to Vault's encryption key.","title":"Scone Vault"},{"location":"vault/#vault-image","text":"We maintain a container image sconecuratedimages/apps:scone-vault-latest which contains the latest version of Vault (0.8.1), runs inside of an SGX enclaves (default is a pre-release enclave), on top of Alpine 3.6 This protects against dumping of the main memory (- when run in release mode).","title":"Vault Image"},{"location":"vault/#demo","text":"This image also contains a demo that you can try out. This demo shows how to set up the configuration of an nginx instance. To run this demo, you first need to checkout the demo repostiory: git clone https://github.com/scontain/scone-vault cd scone-vault you need to ensure to have a docker compose file with content as follows: cat > docker-compose.yml <<EOF version: '3.2' services: vault: image: sconecuratedimages/apps:scone-vault-latest command: sh -c \"cd build_dir && ./start_vault.sh\" environment: - VAULT_DEV_ROOT_TOKEN_ID=RootToken volumes: - ./:/build_dir cap_add: - IPC_LOCK scone-vault-nginx: image: sconecuratedimages/apps:nginx-1.13.8-alpine environment: - URL=\"http://vault:8200\" - INDEX=nginx - VAULT_ADDR=\"http://vault:8200\" - TOKEN=RootToken command: sh -c \"cd build_dir && ./install-deps.sh && ./bench.sh\" volumes: - ./:/build_dir depends_on: - vault EOF Next, try it out by executing: docker-compose up Please ensure to execute docker-compose down before starting it with up again. Note that the script start_vault.sh is used to start Vault server and inject some secrets used for Nginx.","title":"Demo"},{"location":"vault/#details","text":"You can perform the individual steps manually as described below. Run the demo container using docker-compose: docker-compose run scone-vault-nginx sh Go to the deployment directory: cd /build_dir/ Install dependencies: ./install-deps.sh Now, run the benchmark to test if SCONE Vault is setting up the configuration for Nginx. ./bench.sh","title":"Details"},{"location":"vault/#integration-with-scone-cas","text":"For added security, we recommend the following to run Vault in a release enclave to prevent attackers from dumping the content of the Vault enclave and to integrate the execution of Vault with SCONE CAS such that: the command line arguments and environment variables are passed to Vault in an encrypted fashion stdin/stdout/stderr of Vault are encrypted CAS can pass a TLS private / public key to Vault the Vault encryption key can be stored by SCONE CAS Vault's HSM PIN pin can be specified by the VAULT_HSM_PIN environment variable. We can securely set the environment variable with the help of SCONE CAS since SCONE can pass encrypted environment variables to applications. Please contact us, if you are interested in a setup together with SCONE CAS. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Integration with SCONE CAS"},{"location":"whyscone/","text":"SCONE vs Intel SGX SDK In this section, we present the advantages of SCONE compared with Intel SGX SDK. The table below shows why should we use SCONE for Confidential Computing instead of Intel SGX SDK. Features Intel SGX SDK SCONE Platform SLA: Startup times Slow Efficient startup/attestation SLA: Scheduling - SLA-based scheduling SLA: Efficiency Many enclave exits Reduced enclave exits Security: CVEs CVE handling by application CVEs addressed by platform Security: policy No policy support Advanced-policy support Security: platform - Integrated OS and Application Sec. Security: Side-channel No protection Side-channel protection Monitoring: SLA - SLA-based monitoring Monitoring: SGX - SGX-resources & scheduling Encryption at rest / in transit Source code changes required No source code changes Encryption at use Source code changes required No source code changes Attestation Explicit code required Automatic by SCONE Key Provisioning Explicit code required Automatic by SCONE CI/CD Integration - Modern IDE Languages C/C++ Most modern languages (C/C++, Python, Rust, Java, Nodejs, R, ...) Portability Intel SGX-specific (eventually other CPUs) TCO Higher Lower","title":"SCONE vs Intel SGX SDK"},{"location":"whyscone/#scone-vs-intel-sgx-sdk","text":"In this section, we present the advantages of SCONE compared with Intel SGX SDK. The table below shows why should we use SCONE for Confidential Computing instead of Intel SGX SDK. Features Intel SGX SDK SCONE Platform SLA: Startup times Slow Efficient startup/attestation SLA: Scheduling - SLA-based scheduling SLA: Efficiency Many enclave exits Reduced enclave exits Security: CVEs CVE handling by application CVEs addressed by platform Security: policy No policy support Advanced-policy support Security: platform - Integrated OS and Application Sec. Security: Side-channel No protection Side-channel protection Monitoring: SLA - SLA-based monitoring Monitoring: SGX - SGX-resources & scheduling Encryption at rest / in transit Source code changes required No source code changes Encryption at use Source code changes required No source code changes Attestation Explicit code required Automatic by SCONE Key Provisioning Explicit code required Automatic by SCONE CI/CD Integration - Modern IDE Languages C/C++ Most modern languages (C/C++, Python, Rust, Java, Nodejs, R, ...) Portability Intel SGX-specific (eventually other CPUs) TCO Higher Lower","title":"SCONE vs Intel SGX SDK"},{"location":"windows10/","text":"Windows 10 First, SCONE-based applications run on top of Linux. We actually support multiple flavors like Alpine Linux, Ubuntu, Fedora and RHEL/Centos. Our recommended distribution for SCONE-based application is Alpine Linux since it is ideally suited to run in contains. Second, you can run SCONE-based applications on top of Windows 10/HyperV hosts by running a Linux VM. You can test this by running SCONE-based applications in simulation mode in a Docker engine - which runs on Windows 10 systems with HyperV support. Of course, you would like to run your applications in hardware mode. To do so, you need to give your Linux VM access to SGX. You can follow our Windows tutorial to learn how to install Alpine Linux and to run SCONE-based applications on top of Windows 10. Note that these SCONE-based applications run with paging on, i.e., the memory size of these applications can be as large as 32GB on current CPUs and much larger on future CPUs. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Windows 10"},{"location":"windows10/#windows-10","text":"First, SCONE-based applications run on top of Linux. We actually support multiple flavors like Alpine Linux, Ubuntu, Fedora and RHEL/Centos. Our recommended distribution for SCONE-based application is Alpine Linux since it is ideally suited to run in contains. Second, you can run SCONE-based applications on top of Windows 10/HyperV hosts by running a Linux VM. You can test this by running SCONE-based applications in simulation mode in a Docker engine - which runs on Windows 10 systems with HyperV support. Of course, you would like to run your applications in hardware mode. To do so, you need to give your Linux VM access to SGX. You can follow our Windows tutorial to learn how to install Alpine Linux and to run SCONE-based applications on top of Windows 10. Note that these SCONE-based applications run with paging on, i.e., the memory size of these applications can be as large as 32GB on current CPUs and much larger on future CPUs. \u00a9 scontain.com , 2020. Questions or Suggestions?","title":"Windows 10"}]}