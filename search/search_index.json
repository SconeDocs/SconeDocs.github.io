{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Confidential Computing with SCONE We added some advanced confidential compute uses cases (2020-03-20) We explain how one can use the SCONE confidential computing platform to implement some advanced use cases. We show how to build confidential cloud-native applications (see Confidential Document Management ), how to implement confidential federated machine learning , and how to establish trust with the help of a shared codebase using a SCONE confidential deployment . We improved the integration with Azure services (2021-03-19) SCONE supports MAA (Microsoft Attestation Service) in addition to Intel DCAP/EPID attestation: select as part of the attestation policy. Our policies can also retrieve secrets from AKV (Azure Key Vault) and securely distribute these to attested applications . In addition to a Flask-based Python application , we added a tutorial to show how to use MAA and AKV in the context of confidential AKS Register at our container registry to get access to our community edition (2021-03-19) You can register a free account at https://gitlab.scontain.com to get access to our community edition. For more details, please have a look at our Scontain Registry documentation . SCONE Version 5.2.1 released (2020-03-12) We released a new minor version which includes a sequence of bug fixes and new features like binary_fs , new CAS features (audit log), and updated sconeapps. New episode of SCONE Confidential Computing Telenovela released (2021-02-23) Meet our lovely protagonists Alice and Bob who have been working from their home office. They need to protect their teleconferences against Mallory - who has started to work as a cloud admin. Confidential document management (2021-02-04) We added a new confidential document management service that includes nginx as a proxy, a REST API written in Python, memcached as rate-limiter, and MariaDB as a database. All services run inside of enclaves. All communication is encrypted. All code - including the Python code - is protected and attested. All services implicitly attest each other. One can deploy this with helm on Azure Kubernetes Services (AKS) or any Kubernetes cluster with access to Intel SGX. Improved support for Azure (2021-02-01) We support the Azure SGX Plugin in our helm charts. You can now just specify --useSGXDevPlugin=azure to use the SGX Plugin and --set sgxEpcMem=16 (in MiB) to specify the required EPC size. We also added/updated helm charts for mariadb, maxscale, memcached, nginx, openvino, pytorch, spark, teemon, tensorflow, TensorFlowLite , and Zookeeper . CAS Namespace Support (2021-01-03) The newest version of SCONE CAS supports namespaces . Operating Confidential Applications (2020-12-12) We describe in the new operations section how to detect known security vulnerabilities and how to mitigate these. SCONE Confidential Computing University (2020-11-05) The first screencasts of a new Confidential Computing Course are online. This is a new university course that will be offered next year, and over the next weeks, we will add some lectures that introduce the basic concepts of confidential computing and how to implement these with the help of SCONE. Sconify existing native images (2020-09-22) SCONE supports a single step conversion of existing, native container images into encrypted container images with services running inside of SGX enclaves and secrets are provisioned automatically with the help of a security policy. We support a standard edition and for evaluation, a community edition . Enhanced SCONE Policy Language released (2020-08-12) The new version provides more control over the information in the generated certificates. Moreover, secrets can now be exported/imported to/from sessions located on another CAS instance (this is part of release 4.2). Visual Studio Code and C# Support added (2020-06-13) We combined Visual Studio Code with our SCONE cross-compiler. . For mono, the program execution as well as the compilation itself runs inside of an enclave. See a simple C# hello world example. Kubernetes Support added (2020-06-01) SCONE supports deploying confidential applications to Kubernetes. We explain the basic concepts and we describe how to deploy confidential applications with helm like MariaDB . See also our deep-dive tutorial to show how to build and run an encrypted Python program in a Kubernetes deployment and to use some new features such as policy-based certificate generation and injection . Your browser does not support the video tag. SCONE - in a nutshell The objective of SCONE is to protect the confidentiality , integrity and consistency of an application's data , secrets , and code everywhere , i.e., in main memory - at runtime , on disk - at rest , and on the network - during communication and during the entire lifetime of the application by addressing vulnerabilities by supporting software updates of the application, and firmware and hardware updates. while reducing the cost and effort of migrating to SGX with the help of a lift and shift approach, and of operating SGX-based applications by permitting to outsource most of the maintenance to cloud providers ensuring excellent security by supporting a security policy to store, generate, share secrets (e.g., key pairs and certificates) and inject these into applications, and a defense-in-depth approach to protect also against known and unknown vulnerabilities and supporting classical applications running on bare metal servers, modern cloud-native applications running in containers and VMs, integration in CI/CD pipelines, deployment with helm on Kubernetes, ensuring state of the art availability with the help of Kubernetes, and using managed Kubernetes services. SCONE Executive Summary The SCONE confidential computing platform facilitates always encrypted execution : one can run services and applications such that neither the data nor the code is ever accessible as plain text - not even for root users. Only the application code itself can access the unencrypted data and code . SCONE simplifies encrypting the input, executing the service/application in encrypted memory on an untrusted host, transparently encrypting the output, and shipping the output back to the client. SCONE ( Secure CONtainer Environment ) supports the execution of confidential applications inside of containers running inside a Kubernetes cluster ( basic concepts ). SCONE also supports the execution of confidential applications inside of VMs (e.g., on top of Windows10 ) as well as directly on a host ( baremetal ). SCONE supports all common programming languages. It also supports air-gapped systems both with SGXv1 as well as SGXv2. The memory size of SCONE-based applications can be up to 32GB on current SGX-capable CPUs. The SGX specification updates published by Intel show that upcoming CPUs will support even larger enclaves, and SCONE will - on these CPUs - support applications with basically unlimited memory sizes . SCONE supports the execution of existing programs inside of enclaves ( at use encryption ): this includes both programs linked with glibc (default for Ubuntu, Centos, RHEL) as well as musl (default on Alpine Linux and hence, many container images). SCONE supports all popular programming languages and supports static and dynamic linking. For the development of applications, we support a crosscompiler, which is our recommended way to develop confidential applications. SCONE helps to ensure that data, communications, code, and the main memory are always encrypted . To do so, SCONE needs to verify (i.e., attest) that the expected application code is running in a trusted execution environment on a potentially untrusted host. Read our secure remote execution tutorial to see how to perform an encrypted remote execution in a single step. In this way, one can even execute encrypted code. We show how to execute encrypted Python scripts in the context of blender , an encrypted wordcount and a hello world program . SCONE can help you to encrypt your input and output data on your local computer. The keys are managed with the help of SCONE CAS (Configuration and Attestation Service). SCONE CAS itself runs, of course, inside an enclave. It can either run on the client-side or on a remote host. It can even be operated by an untrusted entity and still be trusted by CAS clients. SCONE supports multiple stakeholders ( confidential multiparty computation ) that do not necessarily trust each other. SCONE supports users, service providers, application providers, data providers, and infrastructure providers. They can all work together, and SCONE can ensure that each party can protect its own intellectual property. Some of the services, like SCONE CAS, can be operated by not necessarily trusted stakeholders since clients can verify that the services are in the correct state. If you are interested in confidential multi-party computations , we can give you access to a proof of concept that shows how to protect AI models and provide access control to the model, e.g., can only be executed on certain machines, and only certain arguments can be provided by the user - depending on a given SCONE policy. Just send us an email .","title":"Executive summary"},{"location":"#confidential-computing-with-scone","text":"We added some advanced confidential compute uses cases (2020-03-20) We explain how one can use the SCONE confidential computing platform to implement some advanced use cases. We show how to build confidential cloud-native applications (see Confidential Document Management ), how to implement confidential federated machine learning , and how to establish trust with the help of a shared codebase using a SCONE confidential deployment . We improved the integration with Azure services (2021-03-19) SCONE supports MAA (Microsoft Attestation Service) in addition to Intel DCAP/EPID attestation: select as part of the attestation policy. Our policies can also retrieve secrets from AKV (Azure Key Vault) and securely distribute these to attested applications . In addition to a Flask-based Python application , we added a tutorial to show how to use MAA and AKV in the context of confidential AKS Register at our container registry to get access to our community edition (2021-03-19) You can register a free account at https://gitlab.scontain.com to get access to our community edition. For more details, please have a look at our Scontain Registry documentation . SCONE Version 5.2.1 released (2020-03-12) We released a new minor version which includes a sequence of bug fixes and new features like binary_fs , new CAS features (audit log), and updated sconeapps. New episode of SCONE Confidential Computing Telenovela released (2021-02-23) Meet our lovely protagonists Alice and Bob who have been working from their home office. They need to protect their teleconferences against Mallory - who has started to work as a cloud admin. Confidential document management (2021-02-04) We added a new confidential document management service that includes nginx as a proxy, a REST API written in Python, memcached as rate-limiter, and MariaDB as a database. All services run inside of enclaves. All communication is encrypted. All code - including the Python code - is protected and attested. All services implicitly attest each other. One can deploy this with helm on Azure Kubernetes Services (AKS) or any Kubernetes cluster with access to Intel SGX. Improved support for Azure (2021-02-01) We support the Azure SGX Plugin in our helm charts. You can now just specify --useSGXDevPlugin=azure to use the SGX Plugin and --set sgxEpcMem=16 (in MiB) to specify the required EPC size. We also added/updated helm charts for mariadb, maxscale, memcached, nginx, openvino, pytorch, spark, teemon, tensorflow, TensorFlowLite , and Zookeeper . CAS Namespace Support (2021-01-03) The newest version of SCONE CAS supports namespaces . Operating Confidential Applications (2020-12-12) We describe in the new operations section how to detect known security vulnerabilities and how to mitigate these. SCONE Confidential Computing University (2020-11-05) The first screencasts of a new Confidential Computing Course are online. This is a new university course that will be offered next year, and over the next weeks, we will add some lectures that introduce the basic concepts of confidential computing and how to implement these with the help of SCONE. Sconify existing native images (2020-09-22) SCONE supports a single step conversion of existing, native container images into encrypted container images with services running inside of SGX enclaves and secrets are provisioned automatically with the help of a security policy. We support a standard edition and for evaluation, a community edition . Enhanced SCONE Policy Language released (2020-08-12) The new version provides more control over the information in the generated certificates. Moreover, secrets can now be exported/imported to/from sessions located on another CAS instance (this is part of release 4.2). Visual Studio Code and C# Support added (2020-06-13) We combined Visual Studio Code with our SCONE cross-compiler. . For mono, the program execution as well as the compilation itself runs inside of an enclave. See a simple C# hello world example. Kubernetes Support added (2020-06-01) SCONE supports deploying confidential applications to Kubernetes. We explain the basic concepts and we describe how to deploy confidential applications with helm like MariaDB . See also our deep-dive tutorial to show how to build and run an encrypted Python program in a Kubernetes deployment and to use some new features such as policy-based certificate generation and injection . Your browser does not support the video tag.","title":"Confidential Computing with SCONE"},{"location":"#scone-in-a-nutshell","text":"The objective of SCONE is to protect the confidentiality , integrity and consistency of an application's data , secrets , and code everywhere , i.e., in main memory - at runtime , on disk - at rest , and on the network - during communication and during the entire lifetime of the application by addressing vulnerabilities by supporting software updates of the application, and firmware and hardware updates. while reducing the cost and effort of migrating to SGX with the help of a lift and shift approach, and of operating SGX-based applications by permitting to outsource most of the maintenance to cloud providers ensuring excellent security by supporting a security policy to store, generate, share secrets (e.g., key pairs and certificates) and inject these into applications, and a defense-in-depth approach to protect also against known and unknown vulnerabilities and supporting classical applications running on bare metal servers, modern cloud-native applications running in containers and VMs, integration in CI/CD pipelines, deployment with helm on Kubernetes, ensuring state of the art availability with the help of Kubernetes, and using managed Kubernetes services.","title":"SCONE - in a nutshell"},{"location":"#scone-executive-summary","text":"The SCONE confidential computing platform facilitates always encrypted execution : one can run services and applications such that neither the data nor the code is ever accessible as plain text - not even for root users. Only the application code itself can access the unencrypted data and code . SCONE simplifies encrypting the input, executing the service/application in encrypted memory on an untrusted host, transparently encrypting the output, and shipping the output back to the client. SCONE ( Secure CONtainer Environment ) supports the execution of confidential applications inside of containers running inside a Kubernetes cluster ( basic concepts ). SCONE also supports the execution of confidential applications inside of VMs (e.g., on top of Windows10 ) as well as directly on a host ( baremetal ). SCONE supports all common programming languages. It also supports air-gapped systems both with SGXv1 as well as SGXv2. The memory size of SCONE-based applications can be up to 32GB on current SGX-capable CPUs. The SGX specification updates published by Intel show that upcoming CPUs will support even larger enclaves, and SCONE will - on these CPUs - support applications with basically unlimited memory sizes . SCONE supports the execution of existing programs inside of enclaves ( at use encryption ): this includes both programs linked with glibc (default for Ubuntu, Centos, RHEL) as well as musl (default on Alpine Linux and hence, many container images). SCONE supports all popular programming languages and supports static and dynamic linking. For the development of applications, we support a crosscompiler, which is our recommended way to develop confidential applications. SCONE helps to ensure that data, communications, code, and the main memory are always encrypted . To do so, SCONE needs to verify (i.e., attest) that the expected application code is running in a trusted execution environment on a potentially untrusted host. Read our secure remote execution tutorial to see how to perform an encrypted remote execution in a single step. In this way, one can even execute encrypted code. We show how to execute encrypted Python scripts in the context of blender , an encrypted wordcount and a hello world program . SCONE can help you to encrypt your input and output data on your local computer. The keys are managed with the help of SCONE CAS (Configuration and Attestation Service). SCONE CAS itself runs, of course, inside an enclave. It can either run on the client-side or on a remote host. It can even be operated by an untrusted entity and still be trusted by CAS clients. SCONE supports multiple stakeholders ( confidential multiparty computation ) that do not necessarily trust each other. SCONE supports users, service providers, application providers, data providers, and infrastructure providers. They can all work together, and SCONE can ensure that each party can protect its own intellectual property. Some of the services, like SCONE CAS, can be operated by not necessarily trusted stakeholders since clients can verify that the services are in the correct state. If you are interested in confidential multi-party computations , we can give you access to a proof of concept that shows how to protect AI models and provide access control to the model, e.g., can only be executed on certain machines, and only certain arguments can be provided by the user - depending on a given SCONE policy. Just send us an email .","title":"SCONE Executive Summary"},{"location":"C++/","text":"C++ Program Language Support SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation. Cross-compilation is required to support, in particular, statically linked binaries. This page focuses on the SCONE C++ cross compiler scone g++ (a.k.a. scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++. Determine which SGX device to mount with function determine_sgx_device Image Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode . Help If you need some help, just execute in the container: $ scone g++ --help Usage: x86_64-linux-musl-g++ [ options ] file... Options: ... Example Let's try to compile a simple program: cat > sqrt.cc << EOF #include <iostream> #include <cmath> using namespace std; int main() { int x = 0; while(x < 10) { double y = sqrt((double)x); cout << \"The square root of \" << x << \" is \" << y << endl; x++; } return 0; } EOF We compile the program with scone gcc or scone-gcc : scone g++ sqrt.cc -o sqrt Let's execute the binary and switch on debug outputs: SCONE_VERSION = 1 ./sqrt The output will look like: xport SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: ebf98279a2cae1179366f8b5a0fc007decdc5dd3dec2b92ddbf121c2e2bf22f4 The square root of 0 is 0 The square root of 1 is 1 The square root of 2 is 1.41421 The square root of 3 is 1.73205 The square root of 4 is 2 The square root of 5 is 2.23607 The square root of 6 is 2.44949 The square root of 7 is 2.64575 The square root of 8 is 2.82843 The square root of 9 is 3 Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. Screencast","title":"C++"},{"location":"C++/#c-program-language-support","text":"SCONE supports native compilation of C++ programs when combined with dynamic linking as well as cross-compilation. Cross-compilation is required to support, in particular, statically linked binaries. This page focuses on the SCONE C++ cross compiler scone g++ (a.k.a. scone-g++ ). This cross compiler is based on g++ and hence, the command line options are the same as those of g++. Determine which SGX device to mount with function determine_sgx_device","title":"C++ Program Language Support"},{"location":"C++/#image","text":"Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode .","title":"Image"},{"location":"C++/#help","text":"If you need some help, just execute in the container: $ scone g++ --help Usage: x86_64-linux-musl-g++ [ options ] file... Options: ...","title":"Help"},{"location":"C++/#example","text":"Let's try to compile a simple program: cat > sqrt.cc << EOF #include <iostream> #include <cmath> using namespace std; int main() { int x = 0; while(x < 10) { double y = sqrt((double)x); cout << \"The square root of \" << x << \" is \" << y << endl; x++; } return 0; } EOF We compile the program with scone gcc or scone-gcc : scone g++ sqrt.cc -o sqrt Let's execute the binary and switch on debug outputs: SCONE_VERSION = 1 ./sqrt The output will look like: xport SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: ebf98279a2cae1179366f8b5a0fc007decdc5dd3dec2b92ddbf121c2e2bf22f4 The square root of 0 is 0 The square root of 1 is 1 The square root of 2 is 1.41421 The square root of 3 is 1.73205 The square root of 4 is 2 The square root of 5 is 2.23607 The square root of 6 is 2.44949 The square root of 7 is 2.64575 The square root of 8 is 2.82843 The square root of 9 is 3","title":"Example"},{"location":"C++/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs.","title":"Debugging"},{"location":"C++/#screencast","text":"","title":"Screencast"},{"location":"C/","text":"C Program Language Support SCONE supports native compilation combined with dynamic linking as well as cross-compilation with static as well as dynamic linking. This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc. Image Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode . Help If you need some help, just execute in the container: $ scone gcc --help Usage: x86_64-linux-musl-gcc [ options ] file... Options: ... Example Let's try to compile a simple program: cat > fib.c << EOF #include <stdio.h> #include <stdlib.h> int main(int argc, char** argv) { int n=0, first = 0, second = 1, next = 0, c; if (argc > 1) n=atoi(argv[1]); printf(\"fib(%d)= 1\",n); for ( c = 1 ; c < n ; c++ ) { next = first + second; first = second; second = next; printf(\", %d\",next); } printf(\"\\n\"); } EOF We compile the program with scone gcc or scone-gcc or just gcc (all equivalent): scone gcc fib.c -o fib To compute fib(23), execute: SCONE_VERSION = 1 ./fib 23 The last line of the output should look as follows: fib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657 Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. Screencast","title":"C"},{"location":"C/#c-program-language-support","text":"SCONE supports native compilation combined with dynamic linking as well as cross-compilation with static as well as dynamic linking. This page focuses on the SCONE cross compiler. This cross compiler is based on gcc and hence, the command line options are the same as gcc.","title":"C Program Language Support"},{"location":"C/#image","text":"Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode .","title":"Image"},{"location":"C/#help","text":"If you need some help, just execute in the container: $ scone gcc --help Usage: x86_64-linux-musl-gcc [ options ] file... Options: ...","title":"Help"},{"location":"C/#example","text":"Let's try to compile a simple program: cat > fib.c << EOF #include <stdio.h> #include <stdlib.h> int main(int argc, char** argv) { int n=0, first = 0, second = 1, next = 0, c; if (argc > 1) n=atoi(argv[1]); printf(\"fib(%d)= 1\",n); for ( c = 1 ; c < n ; c++ ) { next = first + second; first = second; second = next; printf(\", %d\",next); } printf(\"\\n\"); } EOF We compile the program with scone gcc or scone-gcc or just gcc (all equivalent): scone gcc fib.c -o fib To compute fib(23), execute: SCONE_VERSION = 1 ./fib 23 The last line of the output should look as follows: fib(23)= 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657","title":"Example"},{"location":"C/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs.","title":"Debugging"},{"location":"C/#screencast","text":"","title":"Screencast"},{"location":"CASOverview/","text":"SCONE Configuration and Attestation Service (CAS) SCONE CAS manages the secrets - in particular, the keys - of an application. The application is in complete control of the secrets: only services given explicit permission by the application's policy get access to keys, encrypted data, encrypted code and policies. Key generation . SCONE CAS can generate keys on behalf of an application. The generation is performed inside of a trusted execution environment . Access to keys is controlled by a security policy controlled by the application. Neither root users nor SCONE CAS admins can access the keys nor the security policies. So far, SCONE CAS runs inside of SGX enclave s. Isolation . Users can run their own instances of SCONE CAS , i.e., one can isolate the secrets of different users and the secrets of different applications. Secure key and configuration provisioning without the need to change the source code of applications: secrets, keys, and configuration parameters are securely provisioned via command line arguments, environment variables and via transparently encrypted files. Access control . To modify or read a policy, a client needs to prove, via TLS, that it knows the private key belonging to a public key specified in the policy. SCONE CAS grants - without any exception - only such clients access to this policy. The client's access to a private key is typically also controlled by a policy - possibly, even the same policy. Note that only after a successful attestation, will a client can get access to its private keys. Management . The management of SCONE CAS can be delegated to a third party. The confidentiality and integrity of the policies and their secrets are ensured by CAS itself. Since the entity creating a policy has complete control over who can read or modify this policy, no admin managing SCONE CAS can overwrite the application's access control to a policy. SCONE CAS supports peer-to-peer based attestation of services operated by mutually distrusting peers. Encrypted Code . One can create images with encrypted Python code or Java or JavaScript or C# or any other JIT or interpreted code on a trusted host. Alternatively, this code could als be generated inside of an enclave. One can transparently attest and decrypt the code inside of an enclave . This can be done without the need to change the Python engine or the Java/... virtual machine. Note that SCONE CAS attests both the Python engine as well as the Python code.","title":"Overview"},{"location":"CASOverview/#scone-configuration-and-attestation-service-cas","text":"SCONE CAS manages the secrets - in particular, the keys - of an application. The application is in complete control of the secrets: only services given explicit permission by the application's policy get access to keys, encrypted data, encrypted code and policies. Key generation . SCONE CAS can generate keys on behalf of an application. The generation is performed inside of a trusted execution environment . Access to keys is controlled by a security policy controlled by the application. Neither root users nor SCONE CAS admins can access the keys nor the security policies. So far, SCONE CAS runs inside of SGX enclave s. Isolation . Users can run their own instances of SCONE CAS , i.e., one can isolate the secrets of different users and the secrets of different applications. Secure key and configuration provisioning without the need to change the source code of applications: secrets, keys, and configuration parameters are securely provisioned via command line arguments, environment variables and via transparently encrypted files. Access control . To modify or read a policy, a client needs to prove, via TLS, that it knows the private key belonging to a public key specified in the policy. SCONE CAS grants - without any exception - only such clients access to this policy. The client's access to a private key is typically also controlled by a policy - possibly, even the same policy. Note that only after a successful attestation, will a client can get access to its private keys. Management . The management of SCONE CAS can be delegated to a third party. The confidentiality and integrity of the policies and their secrets are ensured by CAS itself. Since the entity creating a policy has complete control over who can read or modify this policy, no admin managing SCONE CAS can overwrite the application's access control to a policy. SCONE CAS supports peer-to-peer based attestation of services operated by mutually distrusting peers. Encrypted Code . One can create images with encrypted Python code or Java or JavaScript or C# or any other JIT or interpreted code on a trusted host. Alternatively, this code could als be generated inside of an enclave. One can transparently attest and decrypt the code inside of an enclave . This can be done without the need to change the Python engine or the Java/... virtual machine. Note that SCONE CAS attests both the Python engine as well as the Python code.","title":"SCONE Configuration and Attestation Service (CAS)"},{"location":"CAS_cli-intro/","text":"SCONE CLI The SCONE Command Line Interface (CLI) is the primary tool for interaction with the SCONE platform. CLI State The SCONE CLI is stateful, which means that important state, such as attestation information and identity keys, are preserved between invocations. This state is stored in a configuration file (default location: ~/.cas/config.json ). Alternative locations can be used by setting the SCONE_CLI_CONFIG environment variable. The config file contains security-sensitive data and has to be protected by the user. Failure to do so will allow an attacker to impersonate the user. User Identity Communication with CAS is encrypted and authenticated to prevent eavesdropping and impersonation attacks. Users have to present an X.509 certificate to authenticate themselves. These certificates are also used to authorize session operations, such as reading or updating (see the documentation of the Session Description Language). For simplicity, the SCONE CLI generates a user certificate and private key on its own and stores it in the configuration file. scone self show shows the user's certificate in PEM format, as well as its public key hash, both of which can be used in the access control list of sessions. Alternatively, an external identity can be specified for all session and CAS configuration operations using the --identity <path> parameter, where <path> must point to a file containing a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate. This should be rarely necessary. CAS Trust Management SCONE CAS is the central point of knowledge and trust in the SCONE platform. It generates and stores secrets, and establishes trust into service enclaves by attesting and provisioning them. This role and the data involved is highly security-sensitive. Therefore, trust has to be established into CAS itself before it can be used. Without this, an adversary could impersonate as CAS and collect all of your application's secrets. scone cas attest is the central command to build trust into / attest a remote CAS. It takes the address of CAS and, optionally, the expected enclave measurement of a benign CAS enclave. It also ensures that the attested CAS software was signed by Scontain. If the attestation is successful, and the contacted CAS has the expected signer and optionally enclave measurement (its software is trusted), the CLI will store all information necessary to communicate securely with the CAS in the config file. Example attestation command: scone cas attest cas.example.com Additional command line switches allow relaxing the security policy, e.g. by allowing CAS to run on a machine with hyperthreading enabled ( -C ) or running on older, less secure SGX hardware ( -G ). Use scone cas attest --help for a full list of options. If the database encryption key was injected by the CAS owner (see CAS Database Encryption Key Injection ), attestation will be refused by default, because the CAS owner is granted access to all session secrets. If this does not violate your security policy, you can attest such a CAS by adding the argument --allow-cas-owner-secret-access . The first attested CAS will be used as the default CAS when performing CAS-related operations, such as creating sessions. Multiple CASes may be attested at the same time, though. scone cas list gives an overview and scone cas set-default allows switching to another default CAS. Attesting Specific CAS Versions By default, the CLI verifies that the running CAS software has the same or a newer security version ( ISVSVN ) as the CLI itself. There are two options to attest older or specific versions of CAS: Using an enclave measurement of that version (preferred). Example: scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e Note that 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e must be replaced with the proper enclave measurement. Using a specific security version ( ISVSVN ). Example: scone cas attest cas.example.com --isvsvn 42 Changing CAS Software Signer By default, the CLI verifies that the running CAS software was signed by Scontain. Although not recommended, it is also possible to attest a CAS signed by another key, using the --mrsigner , --isvprodid and --isvsvn arguments (all are required). Example: scone cas attest cas.example.com --mrsigner 2f46f979701db7229033ebf3f6d0ec25e7b04351de2f52269a3e6bbd32f6b192 --isvprodid 7 --isvsvn 3 Identifying CAS For multi-CAS use-cases the mere software identity of a CAS is not sufficient to ensure the confidentiality and integrity of secrets. In these scenarios also the individual identity of the CAS instance has to be took into consideration. Session secrets can be shared across CASes. For this, import and export statements have to contain the address and public key of the remote CAS. As addresses (DNS entries) can be easily manipulated by the adversary, the security of inter-CAS communication is based on their cryptographic keys. Addresses are a mere help to reach a CAS. Failure to properly identify a CAS instance opens the system to man-in-the-middle attacks where an adversary presents their CAS as legitimate remote CAS. CASes are identified by their public keys and/or certificates. scone cas show-identification produces identification information for already attested CASes. Further, scone cas attest can take the expected public key hash of the CAS or CAS software instance with the -c and -s options. Taken the previous example, the following command would ensure that the CAS to be attested has a specific CAS software key hash. $ scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e -s 4USPZDGq2FjrE2wh7FEErbrirCFZcu8nR7YaSjH3NjfJtEAfCc For secure multi-CAS scenarios the CAS identification information has to be exchanged via secure and authenticated channels between participants. CAS provisioning and configuration When a new CAS instance starts up for the first time, it operates in unprovisioned mode, waiting for someone to claim ownership and supply an initial configuration. This can be done through the scone cas provision command. Since scone cas attest will not work until CAS was configured, the provision command will also be used to attest the new CAS. Example provisioning command: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 with-attestation The following parameters must be present (please replace the example values): cas.example.com : The CAS address -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL : The CAS key hash. It can be found on the CAS console or log. This ensures you are connected to the right CAS instance. --config-file cas-config.toml : Path to the CAS configuration file in TOML, JSON or YAML format. The configuration will be uploaded to CAS after successful attestation. Please refer to the CAS configuration page regarding its content. --token e59a91c9dbf4538f32c4f20668bdbc08 : A Provisioning Token. It can be found on the CAS console or log. This ensures that only the rightful owner can supply a CAS configuration. with-attestation : Attest the CAS. This ensures that the CAS is running in a secure enclave before transferring the confidential configuration. Similar options as for scone cas attest can be specified. Use scone cas provision with-attestation --help for a full list of options. It is also possible to specify a custom --identity (see User Identity ). Attesting a specific CAS software version is possible by appending an expected enclave measurement, like with-attestation 8acd90af546bf50277ecef0e482e478b30cb8f9a7f6f55bb35ee1beebfc65000 Once successfully provisioned, the CAS will be added to the list of attested CAS instances, the CAS will continue to operate with the supplied configuration, and the CAS configuration can be updated later by the same user identity. Updating the CAS configuration can be done through the scone cas update-config command: scone cas update-config --cas cas.example.com --config-file new-cas-config.toml Make sure to specify the same --identity that was used for scone cas provision , if any. CAS Database Encryption Key Injection CAS always encrypts its database. By default, a secure encryption key is generated on enclave startup. This key is bound to the executing platform/machine, and not accessible outside the enclave, protecting CAS and session secrets. It is possible to make the encryption key available to equally trustworthy CAS instances on other machines, see CAS Backup . In some scenarios, it may be necessary to inject a database key in order to allow the CAS owner to inspect the database manually. This is possible during initial CAS provisioning, using the --database-key argument. Example: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 --database-key cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with-attestation Note Replace cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with your own key. It must be a 64-character hexadecimal string. When specifying a database key, CAS will generate new CAS keys and create a new encrypted database. Therefore, CAS must be attested again afterwards. Note Database key injection negatively impacts confidentiality as it makes CAS and session secrets accessible to the CAS owner. As a precaution, database key injection will be refused when using a CAS binary signed by Scontain, and attestation of the CAS will fail unless users give consent by supplying the argument --allow-cas-owner-secret-access . Note Database encryption key protection relies on the trusted execution environment. This applies to both securely generated and injected keys. If running in debug mode, simulation mode or native mode, the database key will be exposed to the platform owner. CAS Backup SCONE improves the integrity and confidentiality of programs. However, we still depend on the availability of the underlying system. Therefore, without proper backups all data is lost in case of disaster. SGX makes the situation even worse as it uses platform/machine dependent keys to encrypt data. Therefore, one cannot extract the stored data from a SSD of a failed machine, or rather one cannot decrypt or access it. CAS' backup feature prevents such scenarios as it enables multiple CAS on different machines to decrypt the CAS database encryption key. This works by encrypting the database encryption key individually for all CASes that are supposed to have access. Consequently, the primary CAS needs to learn about all of those CASes using a procedure we call backup registration . To register a backup CAS, make sure you own the primary CAS (see CAS provisioning and configuration ) and that both, the primary and the supposed backup CAS, are available/running (the backup CAS does not need to be provisioned) and execute the register-backup command: scone cas register-backup backup-cas-address If the command succeeds, the backup CAS gained access to the database. The primary CAS will accept any backup CAS with the same SGX signer identity (MRSIGNER), same SGX product ID (ISVPRODID), the same or higher SGX security version number (ISVSVN) and the same, or better SGX TCB state. Note At the moment, the CAS database should only be accessed by one CAS instance at a time. Otherwise, the database might be corrupted. Session Handling In SCONE, individual (microservice) applications security policies are described in sessions. Sessions are stored in session files - YAML-encoded descriptions of the session. The scone session commands provide verification and processing for sessions. A very simple (empty) session file (e.g. stored in sessions/my-example-session.001.yml ) could look like this: name : \"my-example-session\" version : \"0.2\" predecessor : ~ The Session Description Language documentation provides details about the syntax and content of session files. To check whether the provided file is a valid session description, one can use: scone cas session check \"sessions/my-example-session.001.yml\" The command will return a non-zero exit code when encountering a validation error. Once the user finalized the sesson and validation passes, the session may be uploaded to a CAS, by using: scone session create \"sessions/my-example-session.001.yml\" This will use the default CAS. To use a specific CAS instead: scone session create --cas cas.example2.com \"sessions/my-example-session.001.yml\" (Note that either way, the used CAS must have been attested through scone cas attest previously, or the command will fail.) As the complexity of sessions grows, variable substitution can be used to automate uploading of session templates (file sessions/my-example-session.002.yml ): name : \"my-example-session\" version : \"0.2\" services : - name : webservice command : \"./nginx\" mrenclaves : [ \"$NGINX_MRENCLAVE\" ] Note that we use a variable, $NGINX_MRENCLAVE , as the enclave measurement for a service. Variables can be used within all values in the session file. Since the session (identified by its name) was already created earlier, scone session create cannot be used a second time. Instead, the session needs to be updated: scone cas session update -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" When updating a session, the CLI will first retrieve the currently active session from the CAS, and fill in its hash as a predecessor of the updated session. This ensures a coherent, linear and verifiable session history. The -e switch causes variable $NGINX_MRENCLAVE to be replaced with the value 3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac . If a variable is present in the session file but not on the command line, an error is returned, and the session is not uploaded to CAS. Through --use-env , predefined environment variables can be used during substitution. Variables can be used with all cas session commands. Using scone session update --help , more advanced options can be shown. In some scenarios, it may be necessary to verify whether a session active on a CAS adheres a given policy, for instance to check the session's authenticity before exporting a secret to it. This can be done by using the verify CLI command: scone cas session verify -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" The command will fetch the active session from CAS (this requires the READ permission), check whether it matches the given session file after variable substitution, and print the session's hash. A non-zero exit code implies a mismatch. Utility commands Certificate key hashing Some configuration entries, like session access control policies, allow specifying certificate public key hashes. The CLI can be used to calculate these hashes for any PEM-encoded certificate: scone cert show-key-hash \"my-certificate.pem\" Example stdout output: 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1","title":"CLI"},{"location":"CAS_cli-intro/#scone-cli","text":"The SCONE Command Line Interface (CLI) is the primary tool for interaction with the SCONE platform.","title":"SCONE CLI"},{"location":"CAS_cli-intro/#cli-state","text":"The SCONE CLI is stateful, which means that important state, such as attestation information and identity keys, are preserved between invocations. This state is stored in a configuration file (default location: ~/.cas/config.json ). Alternative locations can be used by setting the SCONE_CLI_CONFIG environment variable. The config file contains security-sensitive data and has to be protected by the user. Failure to do so will allow an attacker to impersonate the user.","title":"CLI State"},{"location":"CAS_cli-intro/#user-identity","text":"Communication with CAS is encrypted and authenticated to prevent eavesdropping and impersonation attacks. Users have to present an X.509 certificate to authenticate themselves. These certificates are also used to authorize session operations, such as reading or updating (see the documentation of the Session Description Language). For simplicity, the SCONE CLI generates a user certificate and private key on its own and stores it in the configuration file. scone self show shows the user's certificate in PEM format, as well as its public key hash, both of which can be used in the access control list of sessions. Alternatively, an external identity can be specified for all session and CAS configuration operations using the --identity <path> parameter, where <path> must point to a file containing a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate. This should be rarely necessary.","title":"User Identity"},{"location":"CAS_cli-intro/#cas-trust-management","text":"SCONE CAS is the central point of knowledge and trust in the SCONE platform. It generates and stores secrets, and establishes trust into service enclaves by attesting and provisioning them. This role and the data involved is highly security-sensitive. Therefore, trust has to be established into CAS itself before it can be used. Without this, an adversary could impersonate as CAS and collect all of your application's secrets. scone cas attest is the central command to build trust into / attest a remote CAS. It takes the address of CAS and, optionally, the expected enclave measurement of a benign CAS enclave. It also ensures that the attested CAS software was signed by Scontain. If the attestation is successful, and the contacted CAS has the expected signer and optionally enclave measurement (its software is trusted), the CLI will store all information necessary to communicate securely with the CAS in the config file. Example attestation command: scone cas attest cas.example.com Additional command line switches allow relaxing the security policy, e.g. by allowing CAS to run on a machine with hyperthreading enabled ( -C ) or running on older, less secure SGX hardware ( -G ). Use scone cas attest --help for a full list of options. If the database encryption key was injected by the CAS owner (see CAS Database Encryption Key Injection ), attestation will be refused by default, because the CAS owner is granted access to all session secrets. If this does not violate your security policy, you can attest such a CAS by adding the argument --allow-cas-owner-secret-access . The first attested CAS will be used as the default CAS when performing CAS-related operations, such as creating sessions. Multiple CASes may be attested at the same time, though. scone cas list gives an overview and scone cas set-default allows switching to another default CAS.","title":"CAS Trust Management"},{"location":"CAS_cli-intro/#attesting-specific-cas-versions","text":"By default, the CLI verifies that the running CAS software has the same or a newer security version ( ISVSVN ) as the CLI itself. There are two options to attest older or specific versions of CAS: Using an enclave measurement of that version (preferred). Example: scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e Note that 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e must be replaced with the proper enclave measurement. Using a specific security version ( ISVSVN ). Example: scone cas attest cas.example.com --isvsvn 42","title":"Attesting Specific CAS Versions"},{"location":"CAS_cli-intro/#changing-cas-software-signer","text":"By default, the CLI verifies that the running CAS software was signed by Scontain. Although not recommended, it is also possible to attest a CAS signed by another key, using the --mrsigner , --isvprodid and --isvsvn arguments (all are required). Example: scone cas attest cas.example.com --mrsigner 2f46f979701db7229033ebf3f6d0ec25e7b04351de2f52269a3e6bbd32f6b192 --isvprodid 7 --isvsvn 3","title":"Changing CAS Software Signer"},{"location":"CAS_cli-intro/#identifying-cas","text":"For multi-CAS use-cases the mere software identity of a CAS is not sufficient to ensure the confidentiality and integrity of secrets. In these scenarios also the individual identity of the CAS instance has to be took into consideration. Session secrets can be shared across CASes. For this, import and export statements have to contain the address and public key of the remote CAS. As addresses (DNS entries) can be easily manipulated by the adversary, the security of inter-CAS communication is based on their cryptographic keys. Addresses are a mere help to reach a CAS. Failure to properly identify a CAS instance opens the system to man-in-the-middle attacks where an adversary presents their CAS as legitimate remote CAS. CASes are identified by their public keys and/or certificates. scone cas show-identification produces identification information for already attested CASes. Further, scone cas attest can take the expected public key hash of the CAS or CAS software instance with the -c and -s options. Taken the previous example, the following command would ensure that the CAS to be attested has a specific CAS software key hash. $ scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e -s 4USPZDGq2FjrE2wh7FEErbrirCFZcu8nR7YaSjH3NjfJtEAfCc For secure multi-CAS scenarios the CAS identification information has to be exchanged via secure and authenticated channels between participants.","title":"Identifying CAS"},{"location":"CAS_cli-intro/#cas-provisioning-and-configuration","text":"When a new CAS instance starts up for the first time, it operates in unprovisioned mode, waiting for someone to claim ownership and supply an initial configuration. This can be done through the scone cas provision command. Since scone cas attest will not work until CAS was configured, the provision command will also be used to attest the new CAS. Example provisioning command: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 with-attestation The following parameters must be present (please replace the example values): cas.example.com : The CAS address -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL : The CAS key hash. It can be found on the CAS console or log. This ensures you are connected to the right CAS instance. --config-file cas-config.toml : Path to the CAS configuration file in TOML, JSON or YAML format. The configuration will be uploaded to CAS after successful attestation. Please refer to the CAS configuration page regarding its content. --token e59a91c9dbf4538f32c4f20668bdbc08 : A Provisioning Token. It can be found on the CAS console or log. This ensures that only the rightful owner can supply a CAS configuration. with-attestation : Attest the CAS. This ensures that the CAS is running in a secure enclave before transferring the confidential configuration. Similar options as for scone cas attest can be specified. Use scone cas provision with-attestation --help for a full list of options. It is also possible to specify a custom --identity (see User Identity ). Attesting a specific CAS software version is possible by appending an expected enclave measurement, like with-attestation 8acd90af546bf50277ecef0e482e478b30cb8f9a7f6f55bb35ee1beebfc65000 Once successfully provisioned, the CAS will be added to the list of attested CAS instances, the CAS will continue to operate with the supplied configuration, and the CAS configuration can be updated later by the same user identity. Updating the CAS configuration can be done through the scone cas update-config command: scone cas update-config --cas cas.example.com --config-file new-cas-config.toml Make sure to specify the same --identity that was used for scone cas provision , if any.","title":"CAS provisioning and configuration"},{"location":"CAS_cli-intro/#cas-database-encryption-key-injection","text":"CAS always encrypts its database. By default, a secure encryption key is generated on enclave startup. This key is bound to the executing platform/machine, and not accessible outside the enclave, protecting CAS and session secrets. It is possible to make the encryption key available to equally trustworthy CAS instances on other machines, see CAS Backup . In some scenarios, it may be necessary to inject a database key in order to allow the CAS owner to inspect the database manually. This is possible during initial CAS provisioning, using the --database-key argument. Example: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 --database-key cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with-attestation Note Replace cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with your own key. It must be a 64-character hexadecimal string. When specifying a database key, CAS will generate new CAS keys and create a new encrypted database. Therefore, CAS must be attested again afterwards. Note Database key injection negatively impacts confidentiality as it makes CAS and session secrets accessible to the CAS owner. As a precaution, database key injection will be refused when using a CAS binary signed by Scontain, and attestation of the CAS will fail unless users give consent by supplying the argument --allow-cas-owner-secret-access . Note Database encryption key protection relies on the trusted execution environment. This applies to both securely generated and injected keys. If running in debug mode, simulation mode or native mode, the database key will be exposed to the platform owner.","title":"CAS Database Encryption Key Injection"},{"location":"CAS_cli-intro/#cas-backup","text":"SCONE improves the integrity and confidentiality of programs. However, we still depend on the availability of the underlying system. Therefore, without proper backups all data is lost in case of disaster. SGX makes the situation even worse as it uses platform/machine dependent keys to encrypt data. Therefore, one cannot extract the stored data from a SSD of a failed machine, or rather one cannot decrypt or access it. CAS' backup feature prevents such scenarios as it enables multiple CAS on different machines to decrypt the CAS database encryption key. This works by encrypting the database encryption key individually for all CASes that are supposed to have access. Consequently, the primary CAS needs to learn about all of those CASes using a procedure we call backup registration . To register a backup CAS, make sure you own the primary CAS (see CAS provisioning and configuration ) and that both, the primary and the supposed backup CAS, are available/running (the backup CAS does not need to be provisioned) and execute the register-backup command: scone cas register-backup backup-cas-address If the command succeeds, the backup CAS gained access to the database. The primary CAS will accept any backup CAS with the same SGX signer identity (MRSIGNER), same SGX product ID (ISVPRODID), the same or higher SGX security version number (ISVSVN) and the same, or better SGX TCB state. Note At the moment, the CAS database should only be accessed by one CAS instance at a time. Otherwise, the database might be corrupted.","title":"CAS Backup"},{"location":"CAS_cli-intro/#session-handling","text":"In SCONE, individual (microservice) applications security policies are described in sessions. Sessions are stored in session files - YAML-encoded descriptions of the session. The scone session commands provide verification and processing for sessions. A very simple (empty) session file (e.g. stored in sessions/my-example-session.001.yml ) could look like this: name : \"my-example-session\" version : \"0.2\" predecessor : ~ The Session Description Language documentation provides details about the syntax and content of session files. To check whether the provided file is a valid session description, one can use: scone cas session check \"sessions/my-example-session.001.yml\" The command will return a non-zero exit code when encountering a validation error. Once the user finalized the sesson and validation passes, the session may be uploaded to a CAS, by using: scone session create \"sessions/my-example-session.001.yml\" This will use the default CAS. To use a specific CAS instead: scone session create --cas cas.example2.com \"sessions/my-example-session.001.yml\" (Note that either way, the used CAS must have been attested through scone cas attest previously, or the command will fail.) As the complexity of sessions grows, variable substitution can be used to automate uploading of session templates (file sessions/my-example-session.002.yml ): name : \"my-example-session\" version : \"0.2\" services : - name : webservice command : \"./nginx\" mrenclaves : [ \"$NGINX_MRENCLAVE\" ] Note that we use a variable, $NGINX_MRENCLAVE , as the enclave measurement for a service. Variables can be used within all values in the session file. Since the session (identified by its name) was already created earlier, scone session create cannot be used a second time. Instead, the session needs to be updated: scone cas session update -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" When updating a session, the CLI will first retrieve the currently active session from the CAS, and fill in its hash as a predecessor of the updated session. This ensures a coherent, linear and verifiable session history. The -e switch causes variable $NGINX_MRENCLAVE to be replaced with the value 3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac . If a variable is present in the session file but not on the command line, an error is returned, and the session is not uploaded to CAS. Through --use-env , predefined environment variables can be used during substitution. Variables can be used with all cas session commands. Using scone session update --help , more advanced options can be shown. In some scenarios, it may be necessary to verify whether a session active on a CAS adheres a given policy, for instance to check the session's authenticity before exporting a secret to it. This can be done by using the verify CLI command: scone cas session verify -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" The command will fetch the active session from CAS (this requires the READ permission), check whether it matches the given session file after variable substitution, and print the session's hash. A non-zero exit code implies a mismatch.","title":"Session Handling"},{"location":"CAS_cli-intro/#utility-commands","text":"","title":"Utility commands"},{"location":"CAS_cli-intro/#certificate-key-hashing","text":"Some configuration entries, like session access control policies, allow specifying certificate public key hashes. The CLI can be used to calculate these hashes for any PEM-encoded certificate: scone cert show-key-hash \"my-certificate.pem\" Example stdout output: 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1","title":"Certificate key hashing"},{"location":"CAS_cli-intro_5.0.0/","text":"SCONE CLI The SCONE Command Line Interface (CLI) is the primary tool for interaction with the SCONE platform. CLI State The SCONE CLI is stateful, which means that important state, such as attestation information and identity keys, are preserved between invocations. This state is stored in a configuration file (default location: ~/.cas/config.json ). Alternative locations can be used by setting the SCONE_CLI_CONFIG environment variable. The config file contains security-sensitive data and has to be protected by the user. Failure to do so will allow an attacker to impersonate the user. User Identity Communication with CAS is encrypted and authenticated to prevent eavesdropping and impersonation attacks. Users have to present an X.509 certificate to authenticate themselves. These certificates are also used to authorize session operations, such as reading or updating (see the documentation of the Session Description Language). For simplicity, the SCONE CLI generates a user certificate and private key on its own and stores it in the configuration file. scone self show shows the user's certificate in PEM format, as well as its public key hash, both of which can be used in the access control list of sessions. Alternatively, an external identity can be specified for all session and CAS configuration operations using the --identity <path> parameter, where <path> must point to a file containing a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate. This should be rarely necessary. CAS Trust Management SCONE CAS is the central point of knowledge and trust in the SCONE platform. It generates and stores secrets, and establishes trust into service enclaves by attesting and provisioning them. This role and the data involved is highly security-sensitive. Therefore, trust has to be established into CAS itself before it can be used. Without this, an adversary could impersonate as CAS and collect all of your application's secrets. scone cas attest is the central command to build trust into / attest a remote CAS. It takes the address of CAS and, optionally, the expected enclave measurement of a benign CAS enclave. It also ensures that the attested CAS software was signed by Scontain. If the attestation is successful, and the contacted CAS has the expected signer and optionally enclave measurement (its software is trusted), the CLI will store all information necessary to communicate securely with the CAS in the config file. Example attestation command: scone cas attest cas.example.com Additional command line switches allow relaxing the security policy, e.g. by allowing CAS to run on a machine with hyperthreading enabled ( -C ) or running on older, less secure SGX hardware ( -G ). Use scone cas attest --help for a full list of options. If the database encryption key was injected by the CAS owner (see CAS Database Encryption Key Injection ), attestation will be refused by default, because the CAS owner is granted access to all session secrets. If this does not violate your security policy, you can attest such a CAS by adding the argument --allow-cas-owner-secret-access . The first attested CAS will be used as the default CAS when performing CAS-related operations, such as creating sessions. Multiple CASes may be attested at the same time, though. scone cas list gives an overview and scone cas set-default allows switching to another default CAS. Attesting Specific CAS Versions By default, the CLI verifies that the running CAS software has the same or a newer security version ( ISVSVN ) as the CLI itself. There are two options to attest older or specific versions of CAS: Using an enclave measurement of that version (preferred). Example: scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e Note that 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e must be replaced with the proper enclave measurement. Using a specific security version ( ISVSVN ). Example: scone cas attest cas.example.com --isvsvn 42 Changing CAS Software Signer By default, the CLI verifies that the running CAS software was signed by Scontain. Although not recommended, it is also possible to attest a CAS signed by another key, using the --mrsigner , --isvprodid and --isvsvn arguments (all are required). Example: scone cas attest cas.example.com --mrsigner 2f46f979701db7229033ebf3f6d0ec25e7b04351de2f52269a3e6bbd32f6b192 --isvprodid 7 --isvsvn 3 Identifying CAS For multi-CAS use-cases the mere software identity of a CAS is not sufficient to ensure the confidentiality and integrity of secrets. In these scenarios also the individual identity of the CAS instance has to be took into consideration. Session secrets can be shared across CASes. For this, import and export statements have to contain the address and public key of the remote CAS. As addresses (DNS entries) can be easily manipulated by the adversary, the security of inter-CAS communication is based on their cryptographic keys. Addresses are a mere help to reach a CAS. Failure to properly identify a CAS instance opens the system to man-in-the-middle attacks where an adversary presents their CAS as legitimate remote CAS. CASes are identified by their public keys and/or certificates. scone cas show-identification produces identification information for already attested CASes. Further, scone cas attest can take the expected public key hash of the CAS or CAS software instance with the -c and -s options. Taken the previous example, the following command would ensure that the CAS to be attested has a specific CAS software key hash. $ scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e -s 4USPZDGq2FjrE2wh7FEErbrirCFZcu8nR7YaSjH3NjfJtEAfCc For secure multi-CAS scenarios the CAS identification information has to be exchanged via secure and authenticated channels between participants. CAS provisioning and configuration When a new CAS instance starts up for the first time, it operates in unprovisioned mode, waiting for someone to claim ownership and supply an initial configuration. This can be done through the scone cas provision command. Since scone cas attest will not work until CAS was configured, the provision command will also be used to attest the new CAS. Example provisioning command: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 with-attestation The following parameters must be present (please replace the example values): cas.example.com : The CAS address -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL : The CAS key hash. It can be found on the CAS console or log. This ensures you are connected to the right CAS instance. --config-file cas-config.toml : Path to the CAS configuration file in TOML, JSON or YAML format. The configuration will be uploaded to CAS after successful attestation. Please refer to the CAS configuration page regarding its content. --token e59a91c9dbf4538f32c4f20668bdbc08 : A Provisioning Token. It can be found on the CAS console or log. This ensures that only the rightful owner can supply a CAS configuration. with-attestation : Attest the CAS. This ensures that the CAS is running in a secure enclave before transferring the confidential configuration. Similar options as for scone cas attest can be specified. Use scone cas provision with-attestation --help for a full list of options. It is also possible to specify a custom --identity (see User Identity ). Attesting a specific CAS software version is possible by appending an expected enclave measurement, like with-attestation 8acd90af546bf50277ecef0e482e478b30cb8f9a7f6f55bb35ee1beebfc65000 Once successfully provisioned, the CAS will be added to the list of attested CAS instances, the CAS will continue to operate with the supplied configuration, and the CAS configuration can be updated later by the same user identity. Updating the CAS configuration can be done through the scone cas update-config command: scone cas update-config --cas cas.example.com --config-file new-cas-config.toml Make sure to specify the same --identity that was used for scone cas provision , if any. CAS Database Encryption Key Injection CAS always encrypts its database. By default, a secure encryption key is generated on enclave startup. This key is bound to the executing platform/machine, and not accessible outside the enclave, protecting CAS and session secrets. It is possible to make the encryption key available to equally trustworthy CAS instances on other machines, see CAS Backup . In some scenarios, it may be necessary to inject a database key in order to allow the CAS owner to inspect the database manually. This is possible during initial CAS provisioning, using the --database-key argument. Example: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 --database-key cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with-attestation Note Replace cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with your own key. It must be a 64-character hexadecimal string. When specifying a database key, CAS will generate new CAS keys and create a new encrypted database. Therefore, CAS must be attested again afterwards. Note Database key injection negatively impacts confidentiality as it makes CAS and session secrets accessible to the CAS owner. As a precaution, database key injection will be refused when using a CAS binary signed by Scontain, and attestation of the CAS will fail unless users give consent by supplying the argument --allow-cas-owner-secret-access . Note Database encryption key protection relies on the trusted execution environment. This applies to both securely generated and injected keys. If running in debug mode, simulation mode or native mode, the database key will be exposed to the platform owner. CAS Backup SCONE improves the integrity and confidentiality of programs. However, we still depend on the availability of the underlying system. Therefore, without proper backups all data is lost in case of disaster. SGX makes the situation even worse as it uses platform/machine dependent keys to encrypt data. Therefore, one cannot extract the stored data from a SSD of a failed machine, or rather one cannot decrypt or access it. CAS' backup feature prevents such scenarios as it enables multiple CAS on different machines to decrypt the CAS database encryption key. This works by encrypting the database encryption key individually for all CASes that are supposed to have access. Consequently, the primary CAS needs to learn about all of those CASes using a procedure we call backup registration . To register a backup CAS, make sure you own the primary CAS (see CAS provisioning and configuration ) and that both, the primary and the supposed backup CAS, are available/running (the backup CAS does not need to be provisioned) and execute the register-backup command: scone cas register-backup backup-cas-address If the command succeeds, the backup CAS gained access to the database. The primary CAS will accept any backup CAS with the same SGX signer identity (MRSIGNER), same SGX product ID (ISVPRODID), the same or higher SGX security version number (ISVSVN) and the same, or better SGX TCB state. Note At the moment, the CAS database should only be accessed by one CAS instance at a time. Otherwise, the database might be corrupted. Session Handling In SCONE, individual (microservice) applications security policies are described in sessions. Sessions are stored in session files - YAML-encoded descriptions of the session. The scone session commands provide verification and processing for sessions. A very simple (empty) session file (e.g. stored in sessions/my-example-session.001.yml ) could look like this: name : \"my-example-session\" version : \"0.2\" predecessor : ~ The Session Description Language documentation provides details about the syntax and content of session files. To check whether the provided file is a valid session description, one can use: scone cas session check \"sessions/my-example-session.001.yml\" The command will return a non-zero exit code when encountering a validation error. Once the user finalized the sesson and validation passes, the session may be uploaded to a CAS, by using: scone session create \"sessions/my-example-session.001.yml\" This will use the default CAS. To use a specific CAS instead: scone session create --cas cas.example2.com \"sessions/my-example-session.001.yml\" (Note that either way, the used CAS must have been attested through scone cas attest previously, or the command will fail.) As the complexity of sessions grows, variable substitution can be used to automate uploading of session templates (file sessions/my-example-session.002.yml ): name : \"my-example-session\" version : \"0.2\" services : - name : webservice command : \"./nginx\" mrenclaves : [ \"$NGINX_MRENCLAVE\" ] Note that we use a variable, $NGINX_MRENCLAVE , as the enclave measurement for a service. Variables can be used within all values in the session file. Since the session (identified by its name) was already created earlier, scone session create cannot be used a second time. Instead, the session needs to be updated: scone cas session update -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" When updating a session, the CLI will first retrieve the currently active session from the CAS, and fill in its hash as a predecessor of the updated session. This ensures a coherent, linear and verifiable session history. The -e switch causes variable $NGINX_MRENCLAVE to be replaced with the value 3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac . If a variable is present in the session file but not on the command line, an error is returned, and the session is not uploaded to CAS. Through --use-env , predefined environment variables can be used during substitution. Variables can be used with all cas session commands. Using scone session update --help , more advanced options can be shown. In some scenarios, it may be necessary to verify whether a session active on a CAS adheres a given policy, for instance to check the session's authenticity before exporting a secret to it. This can be done by using the verify CLI command: scone cas session verify -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" The command will fetch the active session from CAS (this requires the READ permission), check whether it matches the given session file after variable substitution, and print the session's hash. A non-zero exit code implies a mismatch. Utility commands Certificate key hashing Some configuration entries, like session access control policies, allow specifying certificate public key hashes. The CLI can be used to calculate these hashes for any PEM-encoded certificate: scone cert show-key-hash \"my-certificate.pem\" Example stdout output: 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1","title":"SCONE CLI"},{"location":"CAS_cli-intro_5.0.0/#scone-cli","text":"The SCONE Command Line Interface (CLI) is the primary tool for interaction with the SCONE platform.","title":"SCONE CLI"},{"location":"CAS_cli-intro_5.0.0/#cli-state","text":"The SCONE CLI is stateful, which means that important state, such as attestation information and identity keys, are preserved between invocations. This state is stored in a configuration file (default location: ~/.cas/config.json ). Alternative locations can be used by setting the SCONE_CLI_CONFIG environment variable. The config file contains security-sensitive data and has to be protected by the user. Failure to do so will allow an attacker to impersonate the user.","title":"CLI State"},{"location":"CAS_cli-intro_5.0.0/#user-identity","text":"Communication with CAS is encrypted and authenticated to prevent eavesdropping and impersonation attacks. Users have to present an X.509 certificate to authenticate themselves. These certificates are also used to authorize session operations, such as reading or updating (see the documentation of the Session Description Language). For simplicity, the SCONE CLI generates a user certificate and private key on its own and stores it in the configuration file. scone self show shows the user's certificate in PEM format, as well as its public key hash, both of which can be used in the access control list of sessions. Alternatively, an external identity can be specified for all session and CAS configuration operations using the --identity <path> parameter, where <path> must point to a file containing a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate. This should be rarely necessary.","title":"User Identity"},{"location":"CAS_cli-intro_5.0.0/#cas-trust-management","text":"SCONE CAS is the central point of knowledge and trust in the SCONE platform. It generates and stores secrets, and establishes trust into service enclaves by attesting and provisioning them. This role and the data involved is highly security-sensitive. Therefore, trust has to be established into CAS itself before it can be used. Without this, an adversary could impersonate as CAS and collect all of your application's secrets. scone cas attest is the central command to build trust into / attest a remote CAS. It takes the address of CAS and, optionally, the expected enclave measurement of a benign CAS enclave. It also ensures that the attested CAS software was signed by Scontain. If the attestation is successful, and the contacted CAS has the expected signer and optionally enclave measurement (its software is trusted), the CLI will store all information necessary to communicate securely with the CAS in the config file. Example attestation command: scone cas attest cas.example.com Additional command line switches allow relaxing the security policy, e.g. by allowing CAS to run on a machine with hyperthreading enabled ( -C ) or running on older, less secure SGX hardware ( -G ). Use scone cas attest --help for a full list of options. If the database encryption key was injected by the CAS owner (see CAS Database Encryption Key Injection ), attestation will be refused by default, because the CAS owner is granted access to all session secrets. If this does not violate your security policy, you can attest such a CAS by adding the argument --allow-cas-owner-secret-access . The first attested CAS will be used as the default CAS when performing CAS-related operations, such as creating sessions. Multiple CASes may be attested at the same time, though. scone cas list gives an overview and scone cas set-default allows switching to another default CAS.","title":"CAS Trust Management"},{"location":"CAS_cli-intro_5.0.0/#attesting-specific-cas-versions","text":"By default, the CLI verifies that the running CAS software has the same or a newer security version ( ISVSVN ) as the CLI itself. There are two options to attest older or specific versions of CAS: Using an enclave measurement of that version (preferred). Example: scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e Note that 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e must be replaced with the proper enclave measurement. Using a specific security version ( ISVSVN ). Example: scone cas attest cas.example.com --isvsvn 42","title":"Attesting Specific CAS Versions"},{"location":"CAS_cli-intro_5.0.0/#changing-cas-software-signer","text":"By default, the CLI verifies that the running CAS software was signed by Scontain. Although not recommended, it is also possible to attest a CAS signed by another key, using the --mrsigner , --isvprodid and --isvsvn arguments (all are required). Example: scone cas attest cas.example.com --mrsigner 2f46f979701db7229033ebf3f6d0ec25e7b04351de2f52269a3e6bbd32f6b192 --isvprodid 7 --isvsvn 3","title":"Changing CAS Software Signer"},{"location":"CAS_cli-intro_5.0.0/#identifying-cas","text":"For multi-CAS use-cases the mere software identity of a CAS is not sufficient to ensure the confidentiality and integrity of secrets. In these scenarios also the individual identity of the CAS instance has to be took into consideration. Session secrets can be shared across CASes. For this, import and export statements have to contain the address and public key of the remote CAS. As addresses (DNS entries) can be easily manipulated by the adversary, the security of inter-CAS communication is based on their cryptographic keys. Addresses are a mere help to reach a CAS. Failure to properly identify a CAS instance opens the system to man-in-the-middle attacks where an adversary presents their CAS as legitimate remote CAS. CASes are identified by their public keys and/or certificates. scone cas show-identification produces identification information for already attested CASes. Further, scone cas attest can take the expected public key hash of the CAS or CAS software instance with the -c and -s options. Taken the previous example, the following command would ensure that the CAS to be attested has a specific CAS software key hash. $ scone cas attest cas.example.com 0a7f2f03e98a63656868a7083aa110210d244740fa73fa455130a1f926398e9e -s 4USPZDGq2FjrE2wh7FEErbrirCFZcu8nR7YaSjH3NjfJtEAfCc For secure multi-CAS scenarios the CAS identification information has to be exchanged via secure and authenticated channels between participants.","title":"Identifying CAS"},{"location":"CAS_cli-intro_5.0.0/#cas-provisioning-and-configuration","text":"When a new CAS instance starts up for the first time, it operates in unprovisioned mode, waiting for someone to claim ownership and supply an initial configuration. This can be done through the scone cas provision command. Since scone cas attest will not work until CAS was configured, the provision command will also be used to attest the new CAS. Example provisioning command: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 with-attestation The following parameters must be present (please replace the example values): cas.example.com : The CAS address -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL : The CAS key hash. It can be found on the CAS console or log. This ensures you are connected to the right CAS instance. --config-file cas-config.toml : Path to the CAS configuration file in TOML, JSON or YAML format. The configuration will be uploaded to CAS after successful attestation. Please refer to the CAS configuration page regarding its content. --token e59a91c9dbf4538f32c4f20668bdbc08 : A Provisioning Token. It can be found on the CAS console or log. This ensures that only the rightful owner can supply a CAS configuration. with-attestation : Attest the CAS. This ensures that the CAS is running in a secure enclave before transferring the confidential configuration. Similar options as for scone cas attest can be specified. Use scone cas provision with-attestation --help for a full list of options. It is also possible to specify a custom --identity (see User Identity ). Attesting a specific CAS software version is possible by appending an expected enclave measurement, like with-attestation 8acd90af546bf50277ecef0e482e478b30cb8f9a7f6f55bb35ee1beebfc65000 Once successfully provisioned, the CAS will be added to the list of attested CAS instances, the CAS will continue to operate with the supplied configuration, and the CAS configuration can be updated later by the same user identity. Updating the CAS configuration can be done through the scone cas update-config command: scone cas update-config --cas cas.example.com --config-file new-cas-config.toml Make sure to specify the same --identity that was used for scone cas provision , if any.","title":"CAS provisioning and configuration"},{"location":"CAS_cli-intro_5.0.0/#cas-database-encryption-key-injection","text":"CAS always encrypts its database. By default, a secure encryption key is generated on enclave startup. This key is bound to the executing platform/machine, and not accessible outside the enclave, protecting CAS and session secrets. It is possible to make the encryption key available to equally trustworthy CAS instances on other machines, see CAS Backup . In some scenarios, it may be necessary to inject a database key in order to allow the CAS owner to inspect the database manually. This is possible during initial CAS provisioning, using the --database-key argument. Example: scone cas provision cas.example.com -c 42ku6fcMt8fzj8br84Xg1ZmForSJDTuj6wt1pURRcYpVzJiCSL --config-file cas-config.toml --token e59a91c9dbf4538f32c4f20668bdbc08 --database-key cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with-attestation Note Replace cfadf5e00443f15e4670e253e353dda80eac0b1295903fdc96339554a78cde0a with your own key. It must be a 64-character hexadecimal string. When specifying a database key, CAS will generate new CAS keys and create a new encrypted database. Therefore, CAS must be attested again afterwards. Note Database key injection negatively impacts confidentiality as it makes CAS and session secrets accessible to the CAS owner. As a precaution, database key injection will be refused when using a CAS binary signed by Scontain, and attestation of the CAS will fail unless users give consent by supplying the argument --allow-cas-owner-secret-access . Note Database encryption key protection relies on the trusted execution environment. This applies to both securely generated and injected keys. If running in debug mode, simulation mode or native mode, the database key will be exposed to the platform owner.","title":"CAS Database Encryption Key Injection"},{"location":"CAS_cli-intro_5.0.0/#cas-backup","text":"SCONE improves the integrity and confidentiality of programs. However, we still depend on the availability of the underlying system. Therefore, without proper backups all data is lost in case of disaster. SGX makes the situation even worse as it uses platform/machine dependent keys to encrypt data. Therefore, one cannot extract the stored data from a SSD of a failed machine, or rather one cannot decrypt or access it. CAS' backup feature prevents such scenarios as it enables multiple CAS on different machines to decrypt the CAS database encryption key. This works by encrypting the database encryption key individually for all CASes that are supposed to have access. Consequently, the primary CAS needs to learn about all of those CASes using a procedure we call backup registration . To register a backup CAS, make sure you own the primary CAS (see CAS provisioning and configuration ) and that both, the primary and the supposed backup CAS, are available/running (the backup CAS does not need to be provisioned) and execute the register-backup command: scone cas register-backup backup-cas-address If the command succeeds, the backup CAS gained access to the database. The primary CAS will accept any backup CAS with the same SGX signer identity (MRSIGNER), same SGX product ID (ISVPRODID), the same or higher SGX security version number (ISVSVN) and the same, or better SGX TCB state. Note At the moment, the CAS database should only be accessed by one CAS instance at a time. Otherwise, the database might be corrupted.","title":"CAS Backup"},{"location":"CAS_cli-intro_5.0.0/#session-handling","text":"In SCONE, individual (microservice) applications security policies are described in sessions. Sessions are stored in session files - YAML-encoded descriptions of the session. The scone session commands provide verification and processing for sessions. A very simple (empty) session file (e.g. stored in sessions/my-example-session.001.yml ) could look like this: name : \"my-example-session\" version : \"0.2\" predecessor : ~ The Session Description Language documentation provides details about the syntax and content of session files. To check whether the provided file is a valid session description, one can use: scone cas session check \"sessions/my-example-session.001.yml\" The command will return a non-zero exit code when encountering a validation error. Once the user finalized the sesson and validation passes, the session may be uploaded to a CAS, by using: scone session create \"sessions/my-example-session.001.yml\" This will use the default CAS. To use a specific CAS instead: scone session create --cas cas.example2.com \"sessions/my-example-session.001.yml\" (Note that either way, the used CAS must have been attested through scone cas attest previously, or the command will fail.) As the complexity of sessions grows, variable substitution can be used to automate uploading of session templates (file sessions/my-example-session.002.yml ): name : \"my-example-session\" version : \"0.2\" services : - name : webservice command : \"./nginx\" mrenclaves : [ \"$NGINX_MRENCLAVE\" ] Note that we use a variable, $NGINX_MRENCLAVE , as the enclave measurement for a service. Variables can be used within all values in the session file. Since the session (identified by its name) was already created earlier, scone session create cannot be used a second time. Instead, the session needs to be updated: scone cas session update -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" When updating a session, the CLI will first retrieve the currently active session from the CAS, and fill in its hash as a predecessor of the updated session. This ensures a coherent, linear and verifiable session history. The -e switch causes variable $NGINX_MRENCLAVE to be replaced with the value 3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac . If a variable is present in the session file but not on the command line, an error is returned, and the session is not uploaded to CAS. Through --use-env , predefined environment variables can be used during substitution. Variables can be used with all cas session commands. Using scone session update --help , more advanced options can be shown. In some scenarios, it may be necessary to verify whether a session active on a CAS adheres a given policy, for instance to check the session's authenticity before exporting a secret to it. This can be done by using the verify CLI command: scone cas session verify -e \"NGINX_MRENCLAVE=3be304015fb399cf88d76a8f58f04e16997010dab573cb1a91eb6257c2a452ac\" \"sessions/my-example-session.002.yml\" The command will fetch the active session from CAS (this requires the READ permission), check whether it matches the given session file after variable substitution, and print the session's hash. A non-zero exit code implies a mismatch.","title":"Session Handling"},{"location":"CAS_cli-intro_5.0.0/#utility-commands","text":"","title":"Utility commands"},{"location":"CAS_cli-intro_5.0.0/#certificate-key-hashing","text":"Some configuration entries, like session access control policies, allow specifying certificate public key hashes. The CLI can be used to calculate these hashes for any PEM-encoded certificate: scone cert show-key-hash \"my-certificate.pem\" Example stdout output: 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1","title":"Certificate key hashing"},{"location":"CAS_cli/","text":"SCONE CLI We provide a CLI (Command Line Interface) to attest SCONE CAS, i.e., to ensure that we are connected to a CAS with an expected MrEnclave running inside an enclave. create a session, i.e., to upload a new session description. update a session, i.e., replace an existing session by a new session description. verify that a session matches a given session template. Prints the digest of the verified session on success. Note that this CLI is implemented as a Rust crate that can be used to interact with CAS directly from programs linked with this library. scone CLI scone 0.2.0 Your command line toolkit to interact with the scone infrastructure USAGE: scone [OPTIONS] <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --config <config> Sets a custom config file [env: SCONE_CLI_CONFIG=] [default: ~/.cas/config.json] SUBCOMMANDS: cas Communication with CAS cert Perform operations on X.509 certificates fspf Create and modify file system protection files help Prints this message or the help of the given subcommand(s) help-markdown Generate markdown help pages on stdout las Interact with the local attestation service (LAS) self Manage this instance of the SCONE CLI session Manage CAS sessions scone self scone self 0.2.0 Manage this instance of the SCONE CLI USAGE: scone self <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show Show the certificate and public key hash show-certificate Print only the certificate in PEM format show-certificate-hash Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead show-key-hash Print only the public key hash scone self show scone self show 0.2.0 Show the certificate and public key hash USAGE: scone self show FLAGS: -h, --help Prints help information -V, --version Prints version information scone self show-certificate scone self show-certificate 0.2.0 Print only the certificate in PEM format USAGE: scone self show-certificate FLAGS: -h, --help Prints help information -V, --version Prints version information scone self show-key-hash scone self show-key-hash 0.2.0 Print only the public key hash USAGE: scone self show-key-hash FLAGS: -h, --help Prints help information -V, --version Prints version information scone self show-certificate-hash scone self show-certificate-hash 0.2.0 Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead USAGE: scone self show-certificate-hash FLAGS: -h, --help Prints help information -V, --version Prints version information scone fspf scone fspf 0.2.0 Create and modify file system protection files USAGE: scone fspf <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: addf Add files (file protection data) to an existing fspf addr Add a protection region to an existing fspf create Create a new file system protection file encrypt Encrypt an existing fspf help Prints this message or the help of the given subcommand(s) show Show an existing fspf scone fspf create scone fspf create 0.2.0 Create a new file system protection file USAGE: scone fspf create <file> [version] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to store the created fspf at <version> FSPF format version to use, optional: latest version will be used by default scone fspf addr scone fspf addr 0.2.0 Add a protection region to an existing fspf USAGE: scone fspf addr [OPTIONS] <file> <embedpoint> <--not-protected|--encrypted|--authenticated> <--kernel <source-dir>|--ephemeral> FLAGS: -a, --authenticated The content of this region should be authenticated (but not encrypted) -e, --encrypted The content of this region should be encrypted --ephemeral The region's files are stored in volatile & untrusted memory -h, --help Prints help information -n, --not-protected The content of this region should NOT be protected -V, --version Prints version information OPTIONS: -c, --chunk-length <chunk-length> in bytes, Files will be protected with chunks of this size, only valid for protected regions -k, --key-length <key-length> in bits, The file protection will use keys with this size, only valid for protected regions --kernel <source-dir> The region's files are stored in the kernel's filesystem ARGS: <file> Location of the fspf <embedpoint> The location of the region in the protected file system scone fspf addf scone fspf addf 0.2.0 Add files (file protection data) to an existing fspf USAGE: scone fspf addf <file> <embedpoint> <source-dir> [encrypt-dir] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf <embedpoint> Region the files should be added to <source-dir> Source directory files to take from <encrypt-dir> Target directory for encrypted files (needed for encrypted regions) scone fspf encrypt scone fspf encrypt 0.2.0 Encrypt an existing fspf USAGE: scone fspf encrypt <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf scone fspf show scone fspf show 0.2.0 Show an existing fspf USAGE: scone fspf show [FLAGS] <file> [key] FLAGS: -h, --help Prints help information -t, --tag Only print tag -V, --version Prints version information ARGS: <file> Location of the fspf <key> Decryption key of the fspf scone cas scone cas 0.2.0 Communication with CAS USAGE: scone cas <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: attest Attest a CAS instance help Prints this message or the help of the given subcommand(s) list List attested CAS instances provision Take ownership of a CAS and supply its initial server-side configuration register-backup set-default Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command show-certificate Show certificate chain of an attested CAS instance show-identification Show identification information of an attested CAS that can be used to identify the instance update-config Update a server-side CAS configuration scone cas list scone cas list 0.2.0 List attested CAS instances USAGE: scone cas list FLAGS: -h, --help Prints help information -V, --version Prints version information scone cas attest scone cas attest 0.2.0 Attest a CAS instance USAGE: scone cas attest [FLAGS] [OPTIONS] <address> [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --allow-cas-owner-secret-access Whether to trust the CAS even if its database encryption key was injected by the CAS owner, and not generated securely within an enclave. If set, session secrets will be accessible to the CAS owner! -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash -s <cas-software-key-hash> Expected CAS software public key hash --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <address> CAS address <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled scone cas show-identification scone cas show-identification 0.2.0 Show identification information of an attested CAS that can be used to identify the instance USAGE: scone cas show-identification [FLAGS] [cas] FLAGS: -C, --cas-certificate PEM-encoded CAS X509 certificate -c, --cas-key-hash SHA256 Hash of CAS public key -S, --cas-software-certificate PEM-encoded CAS software X509 certificate -s, --cas-software-key-hash SHA256 Hash of CAS software public key -T, --certificate-chain Certificate chain of trust of PEM-encoded CAS and CAS software certificates [default] -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted scone cas show-certificate scone cas show-certificate 0.2.0 Show certificate chain of an attested CAS instance USAGE: scone cas show-certificate [cas] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted scone cas set-default scone cas set-default 0.2.0 Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command USAGE: scone cas set-default <cas> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS to become the new default CAS scone cas provision scone cas provision 0.2.0 Take ownership of a CAS and supply its initial server-side configuration USAGE: scone cas provision [OPTIONS] <address> -c <cas-key-hash> --config-file <config-file> --token <token> <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash. Can be found in the CAS console -s <cas-software-key-hash> Expected CAS software public key hash (optional). Can be found in the CAS console --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --database-key <database-key> Database key to encrypt the CAS database with. Must be a 64-character hexadecimal string. Allows CAS owner to decrypt the database at will. This impacts security of the CAS and session secrets, and is therefore discouraged. Database key injection will be refused by CAS binaries signed by Scontain --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate --token <token> Provisioning token (32-character hexadecimal string). Can be found in the CAS console ARGS: <address> CAS address SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) only_for_testing-without-attestation Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! with-attestation Attest the CAS instance during provisioning scone cas provision with-attestation scone cas provision with-attestation 0.2.0 Attest the CAS instance during provisioning USAGE: scone cas provision with-attestation [FLAGS] [OPTIONS] [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled scone cas provision only_for_testing-without-attestation scone cas provision only_for_testing-without-attestation 0.2.0 Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! USAGE: scone cas provision only_for_testing-without-attestation FLAGS: -h, --help Prints help information -V, --version Prints version information scone cas update-config scone cas update-config 0.2.0 Update a server-side CAS configuration USAGE: scone cas update-config [FLAGS] [OPTIONS] --config-file <config-file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate scone cas register-backup scone cas register-backup 0.2.0 USAGE: scone cas register-backup [FLAGS] [OPTIONS] <backup-cas> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <backup-cas> The address of the backup CAS scone session scone session 0.2.0 Manage CAS sessions USAGE: scone session <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: check Check the syntax of the provided session file. Exits with a non-zero exit code on validation error create Upload a new session to CAS. Prints the hash of the created session on success help Prints this message or the help of the given subcommand(s) read Load an active session from CAS and print it to stdout update Update an existing session in CAS. Prints the hash of the updated session on success verify Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success scone session check scone session check 0.2.0 Check the syntax of the provided session file. Exits with a non-zero exit code on validation error USAGE: scone session check [FLAGS] [OPTIONS] <file> FLAGS: -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session create scone session create 0.2.0 Upload a new session to CAS. Prints the hash of the created session on success USAGE: scone session create [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session update scone session update 0.2.0 Update an existing session in CAS. Prints the hash of the updated session on success USAGE: scone session update [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session verify scone session verify 0.2.0 Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success USAGE: scone session verify [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session read scone session read 0.2.0 Load an active session from CAS and print it to stdout USAGE: scone session read [FLAGS] [OPTIONS] <name> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <name> The name of the session to get from CAS and print scone las scone las 0.2.0 Interact with the local attestation service (LAS) USAGE: scone las <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) scone-epid-trust-anchor Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE scone las scone-epid-trust-anchor scone las scone-epid-trust-anchor 0.2.0 Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE. The information can only be verified, and thus trusted in a production environment, if IAS credentials (sp_id and sp_key) are provided. USAGE: scone las scone-epid-trust-anchor [FLAGS] [OPTIONS] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow LAS to run in debug mode, in which it CAN BE MANIPULATED AND SHOULDN'T BE TRUSTED (only for testing purposes!) -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --dev-env Whether the service provider ID is registered in the IAS debug environment -h, --help Prints help information -l The service provider ID's quote linkability setting -V, --version Prints version information OPTIONS: --las <las> LAS to connect to. Default port will be added if no port is specified [default: localhost] --sp-id <sp-id> The service provider ID used to verify the EPID quote with Intel IAS [default: 00000000000000000000000000000000] --sp-key <sp-key> The service provider KEY used to authenticate to Intel IAS scone cert scone cert 0.2.0 Perform operations on X.509 certificates USAGE: scone cert <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show-key-hash Print the hash of the certificate's public key scone cert show-key-hash scone cert show-key-hash 0.2.0 Print the hash of the certificate's public key USAGE: scone cert show-key-hash <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to the file containing the PEM-encoded X.509 certificate scone help-markdown scone help-markdown 0.2.0 Generate markdown help pages on stdout USAGE: scone help-markdown FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"CLI Reference"},{"location":"CAS_cli/#scone-cli","text":"We provide a CLI (Command Line Interface) to attest SCONE CAS, i.e., to ensure that we are connected to a CAS with an expected MrEnclave running inside an enclave. create a session, i.e., to upload a new session description. update a session, i.e., replace an existing session by a new session description. verify that a session matches a given session template. Prints the digest of the verified session on success. Note that this CLI is implemented as a Rust crate that can be used to interact with CAS directly from programs linked with this library.","title":"SCONE CLI"},{"location":"CAS_cli/#scone-cli_1","text":"scone 0.2.0 Your command line toolkit to interact with the scone infrastructure USAGE: scone [OPTIONS] <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --config <config> Sets a custom config file [env: SCONE_CLI_CONFIG=] [default: ~/.cas/config.json] SUBCOMMANDS: cas Communication with CAS cert Perform operations on X.509 certificates fspf Create and modify file system protection files help Prints this message or the help of the given subcommand(s) help-markdown Generate markdown help pages on stdout las Interact with the local attestation service (LAS) self Manage this instance of the SCONE CLI session Manage CAS sessions","title":"scone CLI"},{"location":"CAS_cli/#scone-self","text":"scone self 0.2.0 Manage this instance of the SCONE CLI USAGE: scone self <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show Show the certificate and public key hash show-certificate Print only the certificate in PEM format show-certificate-hash Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead show-key-hash Print only the public key hash","title":"scone self"},{"location":"CAS_cli/#scone-self-show","text":"scone self show 0.2.0 Show the certificate and public key hash USAGE: scone self show FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show"},{"location":"CAS_cli/#scone-self-show-certificate","text":"scone self show-certificate 0.2.0 Print only the certificate in PEM format USAGE: scone self show-certificate FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show-certificate"},{"location":"CAS_cli/#scone-self-show-key-hash","text":"scone self show-key-hash 0.2.0 Print only the public key hash USAGE: scone self show-key-hash FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show-key-hash"},{"location":"CAS_cli/#scone-self-show-certificate-hash","text":"scone self show-certificate-hash 0.2.0 Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead USAGE: scone self show-certificate-hash FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show-certificate-hash"},{"location":"CAS_cli/#scone-fspf","text":"scone fspf 0.2.0 Create and modify file system protection files USAGE: scone fspf <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: addf Add files (file protection data) to an existing fspf addr Add a protection region to an existing fspf create Create a new file system protection file encrypt Encrypt an existing fspf help Prints this message or the help of the given subcommand(s) show Show an existing fspf","title":"scone fspf"},{"location":"CAS_cli/#scone-fspf-create","text":"scone fspf create 0.2.0 Create a new file system protection file USAGE: scone fspf create <file> [version] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to store the created fspf at <version> FSPF format version to use, optional: latest version will be used by default","title":"scone fspf create"},{"location":"CAS_cli/#scone-fspf-addr","text":"scone fspf addr 0.2.0 Add a protection region to an existing fspf USAGE: scone fspf addr [OPTIONS] <file> <embedpoint> <--not-protected|--encrypted|--authenticated> <--kernel <source-dir>|--ephemeral> FLAGS: -a, --authenticated The content of this region should be authenticated (but not encrypted) -e, --encrypted The content of this region should be encrypted --ephemeral The region's files are stored in volatile & untrusted memory -h, --help Prints help information -n, --not-protected The content of this region should NOT be protected -V, --version Prints version information OPTIONS: -c, --chunk-length <chunk-length> in bytes, Files will be protected with chunks of this size, only valid for protected regions -k, --key-length <key-length> in bits, The file protection will use keys with this size, only valid for protected regions --kernel <source-dir> The region's files are stored in the kernel's filesystem ARGS: <file> Location of the fspf <embedpoint> The location of the region in the protected file system","title":"scone fspf addr"},{"location":"CAS_cli/#scone-fspf-addf","text":"scone fspf addf 0.2.0 Add files (file protection data) to an existing fspf USAGE: scone fspf addf <file> <embedpoint> <source-dir> [encrypt-dir] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf <embedpoint> Region the files should be added to <source-dir> Source directory files to take from <encrypt-dir> Target directory for encrypted files (needed for encrypted regions)","title":"scone fspf addf"},{"location":"CAS_cli/#scone-fspf-encrypt","text":"scone fspf encrypt 0.2.0 Encrypt an existing fspf USAGE: scone fspf encrypt <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf","title":"scone fspf encrypt"},{"location":"CAS_cli/#scone-fspf-show","text":"scone fspf show 0.2.0 Show an existing fspf USAGE: scone fspf show [FLAGS] <file> [key] FLAGS: -h, --help Prints help information -t, --tag Only print tag -V, --version Prints version information ARGS: <file> Location of the fspf <key> Decryption key of the fspf","title":"scone fspf show"},{"location":"CAS_cli/#scone-cas","text":"scone cas 0.2.0 Communication with CAS USAGE: scone cas <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: attest Attest a CAS instance help Prints this message or the help of the given subcommand(s) list List attested CAS instances provision Take ownership of a CAS and supply its initial server-side configuration register-backup set-default Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command show-certificate Show certificate chain of an attested CAS instance show-identification Show identification information of an attested CAS that can be used to identify the instance update-config Update a server-side CAS configuration","title":"scone cas"},{"location":"CAS_cli/#scone-cas-list","text":"scone cas list 0.2.0 List attested CAS instances USAGE: scone cas list FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone cas list"},{"location":"CAS_cli/#scone-cas-attest","text":"scone cas attest 0.2.0 Attest a CAS instance USAGE: scone cas attest [FLAGS] [OPTIONS] <address> [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --allow-cas-owner-secret-access Whether to trust the CAS even if its database encryption key was injected by the CAS owner, and not generated securely within an enclave. If set, session secrets will be accessible to the CAS owner! -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash -s <cas-software-key-hash> Expected CAS software public key hash --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <address> CAS address <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled","title":"scone cas attest"},{"location":"CAS_cli/#scone-cas-show-identification","text":"scone cas show-identification 0.2.0 Show identification information of an attested CAS that can be used to identify the instance USAGE: scone cas show-identification [FLAGS] [cas] FLAGS: -C, --cas-certificate PEM-encoded CAS X509 certificate -c, --cas-key-hash SHA256 Hash of CAS public key -S, --cas-software-certificate PEM-encoded CAS software X509 certificate -s, --cas-software-key-hash SHA256 Hash of CAS software public key -T, --certificate-chain Certificate chain of trust of PEM-encoded CAS and CAS software certificates [default] -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted","title":"scone cas show-identification"},{"location":"CAS_cli/#scone-cas-show-certificate","text":"scone cas show-certificate 0.2.0 Show certificate chain of an attested CAS instance USAGE: scone cas show-certificate [cas] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted","title":"scone cas show-certificate"},{"location":"CAS_cli/#scone-cas-set-default","text":"scone cas set-default 0.2.0 Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command USAGE: scone cas set-default <cas> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS to become the new default CAS","title":"scone cas set-default"},{"location":"CAS_cli/#scone-cas-provision","text":"scone cas provision 0.2.0 Take ownership of a CAS and supply its initial server-side configuration USAGE: scone cas provision [OPTIONS] <address> -c <cas-key-hash> --config-file <config-file> --token <token> <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash. Can be found in the CAS console -s <cas-software-key-hash> Expected CAS software public key hash (optional). Can be found in the CAS console --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --database-key <database-key> Database key to encrypt the CAS database with. Must be a 64-character hexadecimal string. Allows CAS owner to decrypt the database at will. This impacts security of the CAS and session secrets, and is therefore discouraged. Database key injection will be refused by CAS binaries signed by Scontain --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate --token <token> Provisioning token (32-character hexadecimal string). Can be found in the CAS console ARGS: <address> CAS address SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) only_for_testing-without-attestation Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! with-attestation Attest the CAS instance during provisioning","title":"scone cas provision"},{"location":"CAS_cli/#scone-cas-provision-with-attestation","text":"scone cas provision with-attestation 0.2.0 Attest the CAS instance during provisioning USAGE: scone cas provision with-attestation [FLAGS] [OPTIONS] [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled","title":"scone cas provision with-attestation"},{"location":"CAS_cli/#scone-cas-provision-only_for_testing-without-attestation","text":"scone cas provision only_for_testing-without-attestation 0.2.0 Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! USAGE: scone cas provision only_for_testing-without-attestation FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone cas provision only_for_testing-without-attestation"},{"location":"CAS_cli/#scone-cas-update-config","text":"scone cas update-config 0.2.0 Update a server-side CAS configuration USAGE: scone cas update-config [FLAGS] [OPTIONS] --config-file <config-file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate","title":"scone cas update-config"},{"location":"CAS_cli/#scone-cas-register-backup","text":"scone cas register-backup 0.2.0 USAGE: scone cas register-backup [FLAGS] [OPTIONS] <backup-cas> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <backup-cas> The address of the backup CAS","title":"scone cas register-backup"},{"location":"CAS_cli/#scone-session","text":"scone session 0.2.0 Manage CAS sessions USAGE: scone session <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: check Check the syntax of the provided session file. Exits with a non-zero exit code on validation error create Upload a new session to CAS. Prints the hash of the created session on success help Prints this message or the help of the given subcommand(s) read Load an active session from CAS and print it to stdout update Update an existing session in CAS. Prints the hash of the updated session on success verify Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success","title":"scone session"},{"location":"CAS_cli/#scone-session-check","text":"scone session check 0.2.0 Check the syntax of the provided session file. Exits with a non-zero exit code on validation error USAGE: scone session check [FLAGS] [OPTIONS] <file> FLAGS: -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session check"},{"location":"CAS_cli/#scone-session-create","text":"scone session create 0.2.0 Upload a new session to CAS. Prints the hash of the created session on success USAGE: scone session create [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session create"},{"location":"CAS_cli/#scone-session-update","text":"scone session update 0.2.0 Update an existing session in CAS. Prints the hash of the updated session on success USAGE: scone session update [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session update"},{"location":"CAS_cli/#scone-session-verify","text":"scone session verify 0.2.0 Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success USAGE: scone session verify [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session verify"},{"location":"CAS_cli/#scone-session-read","text":"scone session read 0.2.0 Load an active session from CAS and print it to stdout USAGE: scone session read [FLAGS] [OPTIONS] <name> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <name> The name of the session to get from CAS and print","title":"scone session read"},{"location":"CAS_cli/#scone-las","text":"scone las 0.2.0 Interact with the local attestation service (LAS) USAGE: scone las <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) scone-epid-trust-anchor Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE","title":"scone las"},{"location":"CAS_cli/#scone-las-scone-epid-trust-anchor","text":"scone las scone-epid-trust-anchor 0.2.0 Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE. The information can only be verified, and thus trusted in a production environment, if IAS credentials (sp_id and sp_key) are provided. USAGE: scone las scone-epid-trust-anchor [FLAGS] [OPTIONS] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow LAS to run in debug mode, in which it CAN BE MANIPULATED AND SHOULDN'T BE TRUSTED (only for testing purposes!) -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --dev-env Whether the service provider ID is registered in the IAS debug environment -h, --help Prints help information -l The service provider ID's quote linkability setting -V, --version Prints version information OPTIONS: --las <las> LAS to connect to. Default port will be added if no port is specified [default: localhost] --sp-id <sp-id> The service provider ID used to verify the EPID quote with Intel IAS [default: 00000000000000000000000000000000] --sp-key <sp-key> The service provider KEY used to authenticate to Intel IAS","title":"scone las scone-epid-trust-anchor"},{"location":"CAS_cli/#scone-cert","text":"scone cert 0.2.0 Perform operations on X.509 certificates USAGE: scone cert <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show-key-hash Print the hash of the certificate's public key","title":"scone cert"},{"location":"CAS_cli/#scone-cert-show-key-hash","text":"scone cert show-key-hash 0.2.0 Print the hash of the certificate's public key USAGE: scone cert show-key-hash <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to the file containing the PEM-encoded X.509 certificate","title":"scone cert show-key-hash"},{"location":"CAS_cli/#scone-help-markdown","text":"scone help-markdown 0.2.0 Generate markdown help pages on stdout USAGE: scone help-markdown FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone help-markdown"},{"location":"CAS_cli_reference_5.0.0/","text":"SCONE CLI We provide a CLI (Command Line Interface) to attest SCONE CAS, i.e., to ensure that we are connected to a CAS with an expected MrEnclave running inside an enclave. create a session, i.e., to upload a new session description. update a session, i.e., replace an existing session by a new session description. verify that a session matches a given session template. Prints the digest of the verified session on success. Note that this CLI is implemented as a Rust crate that can be used to interact with CAS directly from programs linked with this library. scone CLI scone 0.2.0 Your command line toolkit to interact with the scone infrastructure USAGE: scone [OPTIONS] <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --config <config> Sets a custom config file [env: SCONE_CLI_CONFIG=] [default: ~/.cas/config.json] SUBCOMMANDS: cas Communication with CAS cert Perform operations on X.509 certificates fspf Create and modify file system protection files help Prints this message or the help of the given subcommand(s) help-markdown Generate markdown help pages on stdout las Interact with the local attestation service (LAS) self Manage this instance of the SCONE CLI session Manage CAS sessions scone self scone self 0.2.0 Manage this instance of the SCONE CLI USAGE: scone self <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show Show the certificate and public key hash show-certificate Print only the certificate in PEM format show-certificate-hash Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead show-key-hash Print only the public key hash scone self show scone self show 0.2.0 Show the certificate and public key hash USAGE: scone self show FLAGS: -h, --help Prints help information -V, --version Prints version information scone self show-certificate scone self show-certificate 0.2.0 Print only the certificate in PEM format USAGE: scone self show-certificate FLAGS: -h, --help Prints help information -V, --version Prints version information scone self show-key-hash scone self show-key-hash 0.2.0 Print only the public key hash USAGE: scone self show-key-hash FLAGS: -h, --help Prints help information -V, --version Prints version information scone self show-certificate-hash scone self show-certificate-hash 0.2.0 Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead USAGE: scone self show-certificate-hash FLAGS: -h, --help Prints help information -V, --version Prints version information scone fspf scone fspf 0.2.0 Create and modify file system protection files USAGE: scone fspf <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: addf Add files (file protection data) to an existing fspf addr Add a protection region to an existing fspf create Create a new file system protection file encrypt Encrypt an existing fspf help Prints this message or the help of the given subcommand(s) show Show an existing fspf scone fspf create scone fspf create 0.2.0 Create a new file system protection file USAGE: scone fspf create <file> [version] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to store the created fspf at <version> FSPF format version to use, optional: latest version will be used by default scone fspf addr scone fspf addr 0.2.0 Add a protection region to an existing fspf USAGE: scone fspf addr [OPTIONS] <file> <embedpoint> <--not-protected|--encrypted|--authenticated> <--kernel <source-dir>|--ephemeral> FLAGS: -a, --authenticated The content of this region should be authenticated (but not encrypted) -e, --encrypted The content of this region should be encrypted --ephemeral The region's files are stored in volatile & untrusted memory -h, --help Prints help information -n, --not-protected The content of this region should NOT be protected -V, --version Prints version information OPTIONS: -c, --chunk-length <chunk-length> in bytes, Files will be protected with chunks of this size, only valid for protected regions -k, --key-length <key-length> in bits, The file protection will use keys with this size, only valid for protected regions --kernel <source-dir> The region's files are stored in the kernel's filesystem ARGS: <file> Location of the fspf <embedpoint> The location of the region in the protected file system scone fspf addf scone fspf addf 0.2.0 Add files (file protection data) to an existing fspf USAGE: scone fspf addf <file> <embedpoint> <source-dir> [encrypt-dir] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf <embedpoint> Region the files should be added to <source-dir> Source directory files to take from <encrypt-dir> Target directory for encrypted files (needed for encrypted regions) scone fspf encrypt scone fspf encrypt 0.2.0 Encrypt an existing fspf USAGE: scone fspf encrypt <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf scone fspf show scone fspf show 0.2.0 Show an existing fspf USAGE: scone fspf show [FLAGS] <file> [key] FLAGS: -h, --help Prints help information -t, --tag Only print tag -V, --version Prints version information ARGS: <file> Location of the fspf <key> Decryption key of the fspf scone cas scone cas 0.2.0 Communication with CAS USAGE: scone cas <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: attest Attest a CAS instance help Prints this message or the help of the given subcommand(s) list List attested CAS instances provision Take ownership of a CAS and supply its initial server-side configuration register-backup set-default Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command show-certificate Show certificate chain of an attested CAS instance show-identification Show identification information of an attested CAS that can be used to identify the instance update-config Update a server-side CAS configuration scone cas list scone cas list 0.2.0 List attested CAS instances USAGE: scone cas list FLAGS: -h, --help Prints help information -V, --version Prints version information scone cas attest scone cas attest 0.2.0 Attest a CAS instance USAGE: scone cas attest [FLAGS] [OPTIONS] <address> [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --allow-cas-owner-secret-access Whether to trust the CAS even if its database encryption key was injected by the CAS owner, and not generated securely within an enclave. If set, session secrets will be accessible to the CAS owner! -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash -s <cas-software-key-hash> Expected CAS software public key hash --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <address> CAS address <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled scone cas show-identification scone cas show-identification 0.2.0 Show identification information of an attested CAS that can be used to identify the instance USAGE: scone cas show-identification [FLAGS] [cas] FLAGS: -C, --cas-certificate PEM-encoded CAS X509 certificate -c, --cas-key-hash SHA256 Hash of CAS public key -S, --cas-software-certificate PEM-encoded CAS software X509 certificate -s, --cas-software-key-hash SHA256 Hash of CAS software public key -T, --certificate-chain Certificate chain of trust of PEM-encoded CAS and CAS software certificates [default] -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted scone cas show-certificate scone cas show-certificate 0.2.0 Show certificate chain of an attested CAS instance USAGE: scone cas show-certificate [cas] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted scone cas set-default scone cas set-default 0.2.0 Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command USAGE: scone cas set-default <cas> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS to become the new default CAS scone cas provision scone cas provision 0.2.0 Take ownership of a CAS and supply its initial server-side configuration USAGE: scone cas provision [OPTIONS] <address> -c <cas-key-hash> --config-file <config-file> --token <token> <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash. Can be found in the CAS console -s <cas-software-key-hash> Expected CAS software public key hash (optional). Can be found in the CAS console --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --database-key <database-key> Database key to encrypt the CAS database with. Must be a 64-character hexadecimal string. Allows CAS owner to decrypt the database at will. This impacts security of the CAS and session secrets, and is therefore discouraged. Database key injection will be refused by CAS binaries signed by Scontain --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate --token <token> Provisioning token (32-character hexadecimal string). Can be found in the CAS console ARGS: <address> CAS address SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) only_for_testing-without-attestation Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! with-attestation Attest the CAS instance during provisioning scone cas provision with-attestation scone cas provision with-attestation 0.2.0 Attest the CAS instance during provisioning USAGE: scone cas provision with-attestation [FLAGS] [OPTIONS] [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled scone cas provision only_for_testing-without-attestation scone cas provision only_for_testing-without-attestation 0.2.0 Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! USAGE: scone cas provision only_for_testing-without-attestation FLAGS: -h, --help Prints help information -V, --version Prints version information scone cas update-config scone cas update-config 0.2.0 Update a server-side CAS configuration USAGE: scone cas update-config [FLAGS] [OPTIONS] --config-file <config-file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate scone cas register-backup scone cas register-backup 0.2.0 USAGE: scone cas register-backup [FLAGS] [OPTIONS] <backup-cas> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <backup-cas> The address of the backup CAS scone session scone session 0.2.0 Manage CAS sessions USAGE: scone session <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: check Check the syntax of the provided session file. Exits with a non-zero exit code on validation error create Upload a new session to CAS. Prints the hash of the created session on success help Prints this message or the help of the given subcommand(s) read Load an active session from CAS and print it to stdout update Update an existing session in CAS. Prints the hash of the updated session on success verify Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success scone session check scone session check 0.2.0 Check the syntax of the provided session file. Exits with a non-zero exit code on validation error USAGE: scone session check [FLAGS] [OPTIONS] <file> FLAGS: -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session create scone session create 0.2.0 Upload a new session to CAS. Prints the hash of the created session on success USAGE: scone session create [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session update scone session update 0.2.0 Update an existing session in CAS. Prints the hash of the updated session on success USAGE: scone session update [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session verify scone session verify 0.2.0 Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success USAGE: scone session verify [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description scone session read scone session read 0.2.0 Load an active session from CAS and print it to stdout USAGE: scone session read [FLAGS] [OPTIONS] <name> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <name> The name of the session to get from CAS and print scone las scone las 0.2.0 Interact with the local attestation service (LAS) USAGE: scone las <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) scone-epid-trust-anchor Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE scone las scone-epid-trust-anchor scone las scone-epid-trust-anchor 0.2.0 Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE. The information can only be verified, and thus trusted in a production environment, if IAS credentials (sp_id and sp_key) are provided. USAGE: scone las scone-epid-trust-anchor [FLAGS] [OPTIONS] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow LAS to run in debug mode, in which it CAN BE MANIPULATED AND SHOULDN'T BE TRUSTED (only for testing purposes!) -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --dev-env Whether the service provider ID is registered in the IAS debug environment -h, --help Prints help information -l The service provider ID's quote linkability setting -V, --version Prints version information OPTIONS: --las <las> LAS to connect to. Default port will be added if no port is specified [default: localhost] --sp-id <sp-id> The service provider ID used to verify the EPID quote with Intel IAS [default: 00000000000000000000000000000000] --sp-key <sp-key> The service provider KEY used to authenticate to Intel IAS scone cert scone cert 0.2.0 Perform operations on X.509 certificates USAGE: scone cert <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show-key-hash Print the hash of the certificate's public key scone cert show-key-hash scone cert show-key-hash 0.2.0 Print the hash of the certificate's public key USAGE: scone cert show-key-hash <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to the file containing the PEM-encoded X.509 certificate scone help-markdown scone help-markdown 0.2.0 Generate markdown help pages on stdout USAGE: scone help-markdown FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"SCONE CLI"},{"location":"CAS_cli_reference_5.0.0/#scone-cli","text":"We provide a CLI (Command Line Interface) to attest SCONE CAS, i.e., to ensure that we are connected to a CAS with an expected MrEnclave running inside an enclave. create a session, i.e., to upload a new session description. update a session, i.e., replace an existing session by a new session description. verify that a session matches a given session template. Prints the digest of the verified session on success. Note that this CLI is implemented as a Rust crate that can be used to interact with CAS directly from programs linked with this library.","title":"SCONE CLI"},{"location":"CAS_cli_reference_5.0.0/#scone-cli_1","text":"scone 0.2.0 Your command line toolkit to interact with the scone infrastructure USAGE: scone [OPTIONS] <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --config <config> Sets a custom config file [env: SCONE_CLI_CONFIG=] [default: ~/.cas/config.json] SUBCOMMANDS: cas Communication with CAS cert Perform operations on X.509 certificates fspf Create and modify file system protection files help Prints this message or the help of the given subcommand(s) help-markdown Generate markdown help pages on stdout las Interact with the local attestation service (LAS) self Manage this instance of the SCONE CLI session Manage CAS sessions","title":"scone CLI"},{"location":"CAS_cli_reference_5.0.0/#scone-self","text":"scone self 0.2.0 Manage this instance of the SCONE CLI USAGE: scone self <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show Show the certificate and public key hash show-certificate Print only the certificate in PEM format show-certificate-hash Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead show-key-hash Print only the public key hash","title":"scone self"},{"location":"CAS_cli_reference_5.0.0/#scone-self-show","text":"scone self show 0.2.0 Show the certificate and public key hash USAGE: scone self show FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show"},{"location":"CAS_cli_reference_5.0.0/#scone-self-show-certificate","text":"scone self show-certificate 0.2.0 Print only the certificate in PEM format USAGE: scone self show-certificate FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show-certificate"},{"location":"CAS_cli_reference_5.0.0/#scone-self-show-key-hash","text":"scone self show-key-hash 0.2.0 Print only the public key hash USAGE: scone self show-key-hash FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show-key-hash"},{"location":"CAS_cli_reference_5.0.0/#scone-self-show-certificate-hash","text":"scone self show-certificate-hash 0.2.0 Print only the SHA-256 certificate hash. Using the certificate hash in session access control policies is deprecated, as it does not allow authentication with renewed certificates. Please use the key hash (show-key-hash) instead USAGE: scone self show-certificate-hash FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone self show-certificate-hash"},{"location":"CAS_cli_reference_5.0.0/#scone-fspf","text":"scone fspf 0.2.0 Create and modify file system protection files USAGE: scone fspf <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: addf Add files (file protection data) to an existing fspf addr Add a protection region to an existing fspf create Create a new file system protection file encrypt Encrypt an existing fspf help Prints this message or the help of the given subcommand(s) show Show an existing fspf","title":"scone fspf"},{"location":"CAS_cli_reference_5.0.0/#scone-fspf-create","text":"scone fspf create 0.2.0 Create a new file system protection file USAGE: scone fspf create <file> [version] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to store the created fspf at <version> FSPF format version to use, optional: latest version will be used by default","title":"scone fspf create"},{"location":"CAS_cli_reference_5.0.0/#scone-fspf-addr","text":"scone fspf addr 0.2.0 Add a protection region to an existing fspf USAGE: scone fspf addr [OPTIONS] <file> <embedpoint> <--not-protected|--encrypted|--authenticated> <--kernel <source-dir>|--ephemeral> FLAGS: -a, --authenticated The content of this region should be authenticated (but not encrypted) -e, --encrypted The content of this region should be encrypted --ephemeral The region's files are stored in volatile & untrusted memory -h, --help Prints help information -n, --not-protected The content of this region should NOT be protected -V, --version Prints version information OPTIONS: -c, --chunk-length <chunk-length> in bytes, Files will be protected with chunks of this size, only valid for protected regions -k, --key-length <key-length> in bits, The file protection will use keys with this size, only valid for protected regions --kernel <source-dir> The region's files are stored in the kernel's filesystem ARGS: <file> Location of the fspf <embedpoint> The location of the region in the protected file system","title":"scone fspf addr"},{"location":"CAS_cli_reference_5.0.0/#scone-fspf-addf","text":"scone fspf addf 0.2.0 Add files (file protection data) to an existing fspf USAGE: scone fspf addf <file> <embedpoint> <source-dir> [encrypt-dir] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf <embedpoint> Region the files should be added to <source-dir> Source directory files to take from <encrypt-dir> Target directory for encrypted files (needed for encrypted regions)","title":"scone fspf addf"},{"location":"CAS_cli_reference_5.0.0/#scone-fspf-encrypt","text":"scone fspf encrypt 0.2.0 Encrypt an existing fspf USAGE: scone fspf encrypt <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Location of the fspf","title":"scone fspf encrypt"},{"location":"CAS_cli_reference_5.0.0/#scone-fspf-show","text":"scone fspf show 0.2.0 Show an existing fspf USAGE: scone fspf show [FLAGS] <file> [key] FLAGS: -h, --help Prints help information -t, --tag Only print tag -V, --version Prints version information ARGS: <file> Location of the fspf <key> Decryption key of the fspf","title":"scone fspf show"},{"location":"CAS_cli_reference_5.0.0/#scone-cas","text":"scone cas 0.2.0 Communication with CAS USAGE: scone cas <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: attest Attest a CAS instance help Prints this message or the help of the given subcommand(s) list List attested CAS instances provision Take ownership of a CAS and supply its initial server-side configuration register-backup set-default Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command show-certificate Show certificate chain of an attested CAS instance show-identification Show identification information of an attested CAS that can be used to identify the instance update-config Update a server-side CAS configuration","title":"scone cas"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-list","text":"scone cas list 0.2.0 List attested CAS instances USAGE: scone cas list FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone cas list"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-attest","text":"scone cas attest 0.2.0 Attest a CAS instance USAGE: scone cas attest [FLAGS] [OPTIONS] <address> [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --allow-cas-owner-secret-access Whether to trust the CAS even if its database encryption key was injected by the CAS owner, and not generated securely within an enclave. If set, session secrets will be accessible to the CAS owner! -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash -s <cas-software-key-hash> Expected CAS software public key hash --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <address> CAS address <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled","title":"scone cas attest"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-show-identification","text":"scone cas show-identification 0.2.0 Show identification information of an attested CAS that can be used to identify the instance USAGE: scone cas show-identification [FLAGS] [cas] FLAGS: -C, --cas-certificate PEM-encoded CAS X509 certificate -c, --cas-key-hash SHA256 Hash of CAS public key -S, --cas-software-certificate PEM-encoded CAS software X509 certificate -s, --cas-software-key-hash SHA256 Hash of CAS software public key -T, --certificate-chain Certificate chain of trust of PEM-encoded CAS and CAS software certificates [default] -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted","title":"scone cas show-identification"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-show-certificate","text":"scone cas show-certificate 0.2.0 Show certificate chain of an attested CAS instance USAGE: scone cas show-certificate [cas] FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS of which to show the certificate chain, optional: default CAS if omitted","title":"scone cas show-certificate"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-set-default","text":"scone cas set-default 0.2.0 Set default CAS - The default CAS is always used if no CAS is explicitly specified, e.g. in the `session upload` command USAGE: scone cas set-default <cas> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <cas> CAS to become the new default CAS","title":"scone cas set-default"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-provision","text":"scone cas provision 0.2.0 Take ownership of a CAS and supply its initial server-side configuration USAGE: scone cas provision [OPTIONS] <address> -c <cas-key-hash> --config-file <config-file> --token <token> <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information OPTIONS: -c <cas-key-hash> Expected CAS public key hash. Can be found in the CAS console -s <cas-software-key-hash> Expected CAS software public key hash (optional). Can be found in the CAS console --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --database-key <database-key> Database key to encrypt the CAS database with. Must be a 64-character hexadecimal string. Allows CAS owner to decrypt the database at will. This impacts security of the CAS and session secrets, and is therefore discouraged. Database key injection will be refused by CAS binaries signed by Scontain --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate --token <token> Provisioning token (32-character hexadecimal string). Can be found in the CAS console ARGS: <address> CAS address SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) only_for_testing-without-attestation Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! with-attestation Attest the CAS instance during provisioning","title":"scone cas provision"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-provision-with-attestation","text":"scone cas provision with-attestation 0.2.0 Attest the CAS instance during provisioning USAGE: scone cas provision with-attestation [FLAGS] [OPTIONS] [mrenclave] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow CAS to run in debug mode, in which it CANNOT PROTECT SECRETS (only for testing purposes!) Only allowed if CAS signer is ignored -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) -h, --help Prints help information --only_for_testing-ignore-signer Do not verify CAS software signature (MRSIGNER). This allows using a CAS signed with test keys. If set, argument <mrenclave> becomes mandatory, unless --only_for_testing-trust-any was also set --only_for_testing-trust-any Trust ANY enclave measurement value, only allowed if CAS is in debug mode. With this option, `mrenclave` can be omitted, but the attested enclave can run ANY software. This is obviously not secure! -V, --version Prints version information OPTIONS: --isvprodid <isvprodid> Verify the given Independent Software Vendor Product ID (ISVPRODID). Required when using a custom MRSIGNER --isvsvn <isvsvn> Verify that CAS has a given or greater Independent Software Vendor Security Version Number (ISVSVN). Required when using a custom MRSIGNER --mrsigner <mrsigner> Verify the CAS software signature using the given MRSIGNER instead of the default Scontain MRSIGNER. Requires ISVPRODID and ISVSVN too ARGS: <mrenclave> Expected enclave measurement/MRENCLAVE of the CAS enclave (optional). The CAS software signature will always be verified, regardless of this argument; but if set, the default CAS version (ISVSVN) check will be disabled","title":"scone cas provision with-attestation"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-provision-only_for_testing-without-attestation","text":"scone cas provision only_for_testing-without-attestation 0.2.0 Do not attest the CAS. With this command selected, NO verification of the used CAS will be performed at all. This is obviously not secure! USAGE: scone cas provision only_for_testing-without-attestation FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone cas provision only_for_testing-without-attestation"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-update-config","text":"scone cas update-config 0.2.0 Update a server-side CAS configuration USAGE: scone cas update-config [FLAGS] [OPTIONS] --config-file <config-file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --config-file <config-file> Path to the file containing the server-side CAS configuration (in TOML, JSON or YAML format) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate","title":"scone cas update-config"},{"location":"CAS_cli_reference_5.0.0/#scone-cas-register-backup","text":"scone cas register-backup 0.2.0 USAGE: scone cas register-backup [FLAGS] [OPTIONS] <backup-cas> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <backup-cas> The address of the backup CAS","title":"scone cas register-backup"},{"location":"CAS_cli_reference_5.0.0/#scone-session","text":"scone session 0.2.0 Manage CAS sessions USAGE: scone session <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: check Check the syntax of the provided session file. Exits with a non-zero exit code on validation error create Upload a new session to CAS. Prints the hash of the created session on success help Prints this message or the help of the given subcommand(s) read Load an active session from CAS and print it to stdout update Update an existing session in CAS. Prints the hash of the updated session on success verify Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success","title":"scone session"},{"location":"CAS_cli_reference_5.0.0/#scone-session-check","text":"scone session check 0.2.0 Check the syntax of the provided session file. Exits with a non-zero exit code on validation error USAGE: scone session check [FLAGS] [OPTIONS] <file> FLAGS: -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session check"},{"location":"CAS_cli_reference_5.0.0/#scone-session-create","text":"scone session create 0.2.0 Upload a new session to CAS. Prints the hash of the created session on success USAGE: scone session create [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session create"},{"location":"CAS_cli_reference_5.0.0/#scone-session-update","text":"scone session update 0.2.0 Update an existing session in CAS. Prints the hash of the updated session on success USAGE: scone session update [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session update"},{"location":"CAS_cli_reference_5.0.0/#scone-session-verify","text":"scone session verify 0.2.0 Verify that a session active on CAS matches the given session. Prints the hash of the verified session on success USAGE: scone session verify [FLAGS] [OPTIONS] <file> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information --use-env Use the environment variables for variable substitution -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate -n, --name <name> Name of the session. Will replace the name stored in the file -e <VAR=VALUE>... Add or overwrite existing variables in the template ARGS: <file> Path to the file containing the session description","title":"scone session verify"},{"location":"CAS_cli_reference_5.0.0/#scone-session-read","text":"scone session read 0.2.0 Load an active session from CAS and print it to stdout USAGE: scone session read [FLAGS] [OPTIONS] <name> FLAGS: --only_for_testing-disable-attestation-verification Do not require the CAS to be attested. With this option set, NO verification of the used CAS will be performed at all. This is obviously not secure! -h, --help Prints help information -V, --version Prints version information OPTIONS: -c, --cas <cas> CAS to use (optional) --identity <identity> External identity file to use (optional). If omitted, the automatically generated CLI identity will be used. An identity file must contain a PEM-encoded PKCS#8 private key followed by a X.509v3 certificate ARGS: <name> The name of the session to get from CAS and print","title":"scone session read"},{"location":"CAS_cli_reference_5.0.0/#scone-las","text":"scone las 0.2.0 Interact with the local attestation service (LAS) USAGE: scone las <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) scone-epid-trust-anchor Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE","title":"scone las"},{"location":"CAS_cli_reference_5.0.0/#scone-las-scone-epid-trust-anchor","text":"scone las scone-epid-trust-anchor 0.2.0 Retrieve the EPID trust anchor of the SCONE QE (and verify it). Prints public key and enclave measurement of the SCONE QE. The information can only be verified, and thus trusted in a production environment, if IAS credentials (sp_id and sp_key) are provided. USAGE: scone las scone-epid-trust-anchor [FLAGS] [OPTIONS] FLAGS: -C, --accept-configuration-needed Accept CONFIGURATION NEEDED verification response (hyperthreading enabled, less secure) --only_for_testing-debug Allow LAS to run in debug mode, in which it CAN BE MANIPULATED AND SHOULDN'T BE TRUSTED (only for testing purposes!) -G, --accept-group-out-of-date Accept GROUP OUT OF DATE verification response (TCB out-of-date, dangerous!) --dev-env Whether the service provider ID is registered in the IAS debug environment -h, --help Prints help information -l The service provider ID's quote linkability setting -V, --version Prints version information OPTIONS: --las <las> LAS to connect to. Default port will be added if no port is specified [default: localhost] --sp-id <sp-id> The service provider ID used to verify the EPID quote with Intel IAS [default: 00000000000000000000000000000000] --sp-key <sp-key> The service provider KEY used to authenticate to Intel IAS","title":"scone las scone-epid-trust-anchor"},{"location":"CAS_cli_reference_5.0.0/#scone-cert","text":"scone cert 0.2.0 Perform operations on X.509 certificates USAGE: scone cert <SUBCOMMAND> FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: help Prints this message or the help of the given subcommand(s) show-key-hash Print the hash of the certificate's public key","title":"scone cert"},{"location":"CAS_cli_reference_5.0.0/#scone-cert-show-key-hash","text":"scone cert show-key-hash 0.2.0 Print the hash of the certificate's public key USAGE: scone cert show-key-hash <file> FLAGS: -h, --help Prints help information -V, --version Prints version information ARGS: <file> Path to the file containing the PEM-encoded X.509 certificate","title":"scone cert show-key-hash"},{"location":"CAS_cli_reference_5.0.0/#scone-help-markdown","text":"scone help-markdown 0.2.0 Generate markdown help pages on stdout USAGE: scone help-markdown FLAGS: -h, --help Prints help information -V, --version Prints version information","title":"scone help-markdown"},{"location":"CAS_config/","text":"Configuration Due to the ability to securely operate CAS remotely, its configuration is split into two independent parts: An availability configuration, supplied by the CAS operator at startup, and an owner configuration, provisioned by the CAS owner at runtime. Availability Configuration The availability configuration encompasses file paths and network interfaces - everything that is necessary in order to start a CAS instance. The configuration is read from a TOML -formatted file. This part of the configuration is not security-sensitive. It does not take part in remote attestation, is stored unencrypted, and can be changed by the CAS operator at any time. Example Configuration File [database] path = \"/etc/cas/cas.db\" [api] api_listen = \"0.0.0.0:8081\" enclave_listen = \"0.0.0.0:18765\" Database CAS stores its data in an SQLite database, the location of which is determined in by the database.path option of the configuration: database.path = \"/etc/cas/database.sqlite\" The database is always encrypted to protected the confidentiality of the contained secrets. The database encryption key is stored as cipher text in a separate file with the same filename, but ending with .key-store . Note Note that the key used to encrypt the database encryption key is derived using SGX' seal key derivation feature. Therefore, CAS can only decrypt the database encryption key and access the database if it is executed on the same machine which has created the .key-store file, or the key is explicitly made available to a CAS on another machine with CAS' Backup feature. In particular, that means if the machine hosting CAS breaks, e.g. due to hardware failure, or is not available anymore, for example, because it was a cloud machine, you'll access to all data stored by CAS is lost! Security in Debug Mode In debug mode, SGX generally does not protect your data as the debug interface exposes the enclave's memory. That means one can easily extract the database encryption key from a CAS running in SGX debug mode with our scone-gdb debugger. However, you might not be aware of this. Therefore, to counteract a false sense of security (and help debugging) CAS will dump its database encryption key anytime it does not run in production mode to the console and into a file next to the database file ending with .key . To reiterate: this is not a security vulnerability. Any slightly invested attacker could easily extract the key with a debugger. Interfaces At the moment CAS offers two interfaces: A client REST API with which clients can upload, upgrade and query sessions, and an enclave interface with which enclaves communicate with CAS. These configuration options will determine on which network interfaces and ports CAS will listen for incoming connection. Interfaces are defined with values satisfying std::net::SocketAddr 's FromStr implementation . The client API is configured via the api_listen option while the enclave interface is configured through the enclave_listen option: [api] enclave_listen = \"0.0.0.0:18765\" api_listen = \"0.0.0.0:8080\" Note A configuration with an IP address of 0.0.0.0 will make the CAS listen on any available network interface. Owner Configuration After its initial start, a new CAS instance remains in an unprovisioned state. In this state, its functionality is severely limited until someone claims ownership and provides a configuration. Since the owner configuration contains security-sensitive information, it is necessary to attest the CAS beforehand. The configuration will be stored encrypted as part of the CAS database. The SCONE CLI is the primary tool to manage the CAS' owner configuration: Initial provisioning: scone cas provision --config-file <...> Later configuration update: scone cas update-config --config-file <...> Please refer to the SCONE CLI documentation for a full description. The configuration file is usually provided in TOML format. Example Configuration File The following file can be used for a development environment: [api_identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" [ias] spid = \"32C2D32CD4F1C873625BBE65D973A702\" linkable_quotes = true sp_key = \"1c8b569cffd94a85ad11342bfad5a3ad\" base_uri = \"https://api.trustedservices.intel.com/sgx/dev/attestation/v3\" [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\" API Identity Contents of the TLS end-entity server and client certificate presented by CAS can be configured in the api.identity section: [api.identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" This section is not mandatory. If not specified, default values listed above will be used. In case the section is provided in the configuration file, it must contain at least common_name field. IAS Client The IAS client enables CAS to verify enclave EPID quotes using the Intel SGX Attestation Service (IAS) . EPID quotes will be used for remote attestation of program enclaves, SCONE Quoting Enclaves, and the CAS itself. At the moment, IAS client credentials are required for normal CAS operation. Its configuration consists of the Service Provider ID (SPID), linkability setting and authentication information. spid has to be a 32 character or 16 byte long hex string. linkable_quotes is either true or false . Both are provided to you when you enroll with Intel's IAS. base_url is the url endpoint at which the client will query Intel's attestation service. This setting is optional and will point towards the development environment endpoint of IAS by default. You have to adapt this setting if you want to use an IAS production identity. IAS is access-controlled. CAS supports Rev4 and Rev5 IAS authentication (choose either one, not both): You can specify your Rev4 X.509 certificate and private key through the ias.identity option. Refer to the section about private keys information regarding private key encoding. If you're using a Rev5 subscription key to authenticate your account, specify it under the sp_key option. [ias] spid = \"2EC5BE64E242BCC9E4670D49E5C7091E\" linkable_quotes = true base_url = \"https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3\" identity = \"\"\" -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- \"\"\" Intel Provisioning Certification Service for ECDSA Attestation Client (DCAP) The Intel Provisioning Certification Service for ECDSA Attestation Client enables CAS to verify DCAP quotes created by the Intel Quoting Enclave. DCAP quotes can be used as an alternative to EPID quotes for remote attestation of program enclaves. Configuring a DCAP client credential is optional. The configuration encompasses a subscription key. This key is provided to you when you enroll with Intel's Provisioning Certification Service. [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\" Private Keys We support PKCS#8 encoded private keys. By default, openssl will encode elliptic curve keys in the incompatible format specified in SEC 1: Elliptic Curve Cryptography . You can use the openssl cli to convert these keys into PKCS#8 : openssl pkcs8 -in pkey.pem -topk8 --nocrypt -out pkey.pkcs8.pem PKCS#1-encoded private keys, e.g. key with ------BEGIN RSA PRIVATE KEY----- , can be converted into PKCS#8 with the following command: openssl pkcs8 -topk8 -inform PEM -outform PEM -nocrypt -in pkcs1.key -out pkcs8.key","title":"CAS Configuration"},{"location":"CAS_config/#configuration","text":"Due to the ability to securely operate CAS remotely, its configuration is split into two independent parts: An availability configuration, supplied by the CAS operator at startup, and an owner configuration, provisioned by the CAS owner at runtime.","title":"Configuration"},{"location":"CAS_config/#availability-configuration","text":"The availability configuration encompasses file paths and network interfaces - everything that is necessary in order to start a CAS instance. The configuration is read from a TOML -formatted file. This part of the configuration is not security-sensitive. It does not take part in remote attestation, is stored unencrypted, and can be changed by the CAS operator at any time.","title":"Availability Configuration"},{"location":"CAS_config/#example-configuration-file","text":"[database] path = \"/etc/cas/cas.db\" [api] api_listen = \"0.0.0.0:8081\" enclave_listen = \"0.0.0.0:18765\"","title":"Example Configuration File"},{"location":"CAS_config/#database","text":"CAS stores its data in an SQLite database, the location of which is determined in by the database.path option of the configuration: database.path = \"/etc/cas/database.sqlite\" The database is always encrypted to protected the confidentiality of the contained secrets. The database encryption key is stored as cipher text in a separate file with the same filename, but ending with .key-store . Note Note that the key used to encrypt the database encryption key is derived using SGX' seal key derivation feature. Therefore, CAS can only decrypt the database encryption key and access the database if it is executed on the same machine which has created the .key-store file, or the key is explicitly made available to a CAS on another machine with CAS' Backup feature. In particular, that means if the machine hosting CAS breaks, e.g. due to hardware failure, or is not available anymore, for example, because it was a cloud machine, you'll access to all data stored by CAS is lost!","title":"Database"},{"location":"CAS_config/#security-in-debug-mode","text":"In debug mode, SGX generally does not protect your data as the debug interface exposes the enclave's memory. That means one can easily extract the database encryption key from a CAS running in SGX debug mode with our scone-gdb debugger. However, you might not be aware of this. Therefore, to counteract a false sense of security (and help debugging) CAS will dump its database encryption key anytime it does not run in production mode to the console and into a file next to the database file ending with .key . To reiterate: this is not a security vulnerability. Any slightly invested attacker could easily extract the key with a debugger.","title":"Security in Debug Mode"},{"location":"CAS_config/#interfaces","text":"At the moment CAS offers two interfaces: A client REST API with which clients can upload, upgrade and query sessions, and an enclave interface with which enclaves communicate with CAS. These configuration options will determine on which network interfaces and ports CAS will listen for incoming connection. Interfaces are defined with values satisfying std::net::SocketAddr 's FromStr implementation . The client API is configured via the api_listen option while the enclave interface is configured through the enclave_listen option: [api] enclave_listen = \"0.0.0.0:18765\" api_listen = \"0.0.0.0:8080\" Note A configuration with an IP address of 0.0.0.0 will make the CAS listen on any available network interface.","title":"Interfaces"},{"location":"CAS_config/#owner-configuration","text":"After its initial start, a new CAS instance remains in an unprovisioned state. In this state, its functionality is severely limited until someone claims ownership and provides a configuration. Since the owner configuration contains security-sensitive information, it is necessary to attest the CAS beforehand. The configuration will be stored encrypted as part of the CAS database. The SCONE CLI is the primary tool to manage the CAS' owner configuration: Initial provisioning: scone cas provision --config-file <...> Later configuration update: scone cas update-config --config-file <...> Please refer to the SCONE CLI documentation for a full description. The configuration file is usually provided in TOML format.","title":"Owner Configuration"},{"location":"CAS_config/#example-configuration-file_1","text":"The following file can be used for a development environment: [api_identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" [ias] spid = \"32C2D32CD4F1C873625BBE65D973A702\" linkable_quotes = true sp_key = \"1c8b569cffd94a85ad11342bfad5a3ad\" base_uri = \"https://api.trustedservices.intel.com/sgx/dev/attestation/v3\" [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\"","title":"Example Configuration File"},{"location":"CAS_config/#api-identity","text":"Contents of the TLS end-entity server and client certificate presented by CAS can be configured in the api.identity section: [api.identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" This section is not mandatory. If not specified, default values listed above will be used. In case the section is provided in the configuration file, it must contain at least common_name field.","title":"API Identity"},{"location":"CAS_config/#ias-client","text":"The IAS client enables CAS to verify enclave EPID quotes using the Intel SGX Attestation Service (IAS) . EPID quotes will be used for remote attestation of program enclaves, SCONE Quoting Enclaves, and the CAS itself. At the moment, IAS client credentials are required for normal CAS operation. Its configuration consists of the Service Provider ID (SPID), linkability setting and authentication information. spid has to be a 32 character or 16 byte long hex string. linkable_quotes is either true or false . Both are provided to you when you enroll with Intel's IAS. base_url is the url endpoint at which the client will query Intel's attestation service. This setting is optional and will point towards the development environment endpoint of IAS by default. You have to adapt this setting if you want to use an IAS production identity. IAS is access-controlled. CAS supports Rev4 and Rev5 IAS authentication (choose either one, not both): You can specify your Rev4 X.509 certificate and private key through the ias.identity option. Refer to the section about private keys information regarding private key encoding. If you're using a Rev5 subscription key to authenticate your account, specify it under the sp_key option. [ias] spid = \"2EC5BE64E242BCC9E4670D49E5C7091E\" linkable_quotes = true base_url = \"https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3\" identity = \"\"\" -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- \"\"\"","title":"IAS Client"},{"location":"CAS_config/#intel-provisioning-certification-service-for-ecdsa-attestation-client-dcap","text":"The Intel Provisioning Certification Service for ECDSA Attestation Client enables CAS to verify DCAP quotes created by the Intel Quoting Enclave. DCAP quotes can be used as an alternative to EPID quotes for remote attestation of program enclaves. Configuring a DCAP client credential is optional. The configuration encompasses a subscription key. This key is provided to you when you enroll with Intel's Provisioning Certification Service. [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\"","title":"Intel Provisioning Certification Service for ECDSA Attestation Client (DCAP)"},{"location":"CAS_config/#private-keys","text":"We support PKCS#8 encoded private keys. By default, openssl will encode elliptic curve keys in the incompatible format specified in SEC 1: Elliptic Curve Cryptography . You can use the openssl cli to convert these keys into PKCS#8 : openssl pkcs8 -in pkey.pem -topk8 --nocrypt -out pkey.pkcs8.pem PKCS#1-encoded private keys, e.g. key with ------BEGIN RSA PRIVATE KEY----- , can be converted into PKCS#8 with the following command: openssl pkcs8 -topk8 -inform PEM -outform PEM -nocrypt -in pkcs1.key -out pkcs8.key","title":"Private Keys"},{"location":"CAS_config_5.0.0/","text":"Configuration Due to the ability to securely operate CAS remotely, its configuration is split into two independent parts: An availability configuration, supplied by the CAS operator at startup, and an owner configuration, provisioned by the CAS owner at runtime. Availability Configuration The availability configuration encompasses file paths and network interfaces - everything that is necessary in order to start a CAS instance. The configuration is read from a TOML -formatted file. This part of the configuration is not security-sensitive. It does not take part in remote attestation, is stored unencrypted, and can be changed by the CAS operator at any time. Example Configuration File [database] path = \"/etc/cas/cas.db\" [api] api_listen = \"0.0.0.0:8081\" enclave_listen = \"0.0.0.0:18765\" Database CAS stores its data in an SQLite database, the location of which is determined in by the database.path option of the configuration: database.path = \"/etc/cas/database.sqlite\" The database is always encrypted to protected the confidentiality of the contained secrets. The database encryption key is stored as cipher text in a separate file with the same filename, but ending with .key-store . Note Note that the key used to encrypt the database encryption key is derived using SGX' seal key derivation feature. Therefore, CAS can only decrypt the database encryption key and access the database if it is executed on the same machine which has created the .key-store file, or the key is explicitly made available to a CAS on another machine with CAS' Backup feature. In particular, that means if the machine hosting CAS breaks, e.g. due to hardware failure, or is not available anymore, for example, because it was a cloud machine, you'll access to all data stored by CAS is lost! Security in Debug Mode In debug mode, SGX generally does not protect your data as the debug interface exposes the enclave's memory. That means one can easily extract the database encryption key from a CAS running in SGX debug mode with our scone-gdb debugger. However, you might not be aware of this. Therefore, to counteract a false sense of security (and help debugging) CAS will dump its database encryption key anytime it does not run in production mode to the console and into a file next to the database file ending with .key . To reiterate: this is not a security vulnerability. Any slightly invested attacker could easily extract the key with a debugger. Interfaces At the moment CAS offers two interfaces: A client REST API with which clients can upload, upgrade and query sessions, and an enclave interface with which enclaves communicate with CAS. These configuration options will determine on which network interfaces and ports CAS will listen for incoming connection. Interfaces are defined with values satisfying std::net::SocketAddr 's FromStr implementation . The client API is configured via the api_listen option while the enclave interface is configured through the enclave_listen option: [api] enclave_listen = \"0.0.0.0:18765\" api_listen = \"0.0.0.0:8080\" Note A configuration with an IP address of 0.0.0.0 will make the CAS listen on any available network interface. Owner Configuration After its initial start, a new CAS instance remains in an unprovisioned state. In this state, its functionality is severely limited until someone claims ownership and provides a configuration. Since the owner configuration contains security-sensitive information, it is necessary to attest the CAS beforehand. The configuration will be stored encrypted as part of the CAS database. The SCONE CLI is the primary tool to manage the CAS' owner configuration: Initial provisioning: scone cas provision --config-file <...> Later configuration update: scone cas update-config --config-file <...> Please refer to the SCONE CLI documentation for a full description. The configuration file is usually provided in TOML format. Example Configuration File The following file can be used for a development environment: [api_identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" [ias] spid = \"32C2D32CD4F1C873625BBE65D973A702\" linkable_quotes = true sp_key = \"1c8b569cffd94a85ad11342bfad5a3ad\" base_uri = \"https://api.trustedservices.intel.com/sgx/dev/attestation/v3\" [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\" API Identity Contents of the TLS end-entity server and client certificate presented by CAS can be configured in the api.identity section: [api.identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" This section is not mandatory. If not specified, default values listed above will be used. In case the section is provided in the configuration file, it must contain at least common_name field. IAS Client The IAS client enables CAS to verify enclave EPID quotes using the Intel SGX Attestation Service (IAS) . EPID quotes will be used for remote attestation of program enclaves, SCONE Quoting Enclaves, and the CAS itself. At the moment, IAS client credentials are required for normal CAS operation. Its configuration consists of the Service Provider ID (SPID), linkability setting and authentication information. spid has to be a 32 character or 16 byte long hex string. linkable_quotes is either true or false . Both are provided to you when you enroll with Intel's IAS. base_url is the url endpoint at which the client will query Intel's attestation service. This setting is optional and will point towards the development environment endpoint of IAS by default. You have to adapt this setting if you want to use an IAS production identity. IAS is access-controlled. CAS supports Rev4 and Rev5 IAS authentication (choose either one, not both): You can specify your Rev4 X.509 certificate and private key through the ias.identity option. Refer to the section about private keys information regarding private key encoding. If you're using a Rev5 subscription key to authenticate your account, specify it under the sp_key option. [ias] spid = \"2EC5BE64E242BCC9E4670D49E5C7091E\" linkable_quotes = true base_url = \"https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3\" identity = \"\"\" -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- \"\"\" Intel Provisioning Certification Service for ECDSA Attestation Client (DCAP) The Intel Provisioning Certification Service for ECDSA Attestation Client enables CAS to verify DCAP quotes created by the Intel Quoting Enclave. DCAP quotes can be used as an alternative to EPID quotes for remote attestation of program enclaves. Configuring a DCAP client credential is optional. The configuration encompasses a subscription key. This key is provided to you when you enroll with Intel's Provisioning Certification Service. [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\" Private Keys We support PKCS#8 encoded private keys. By default, openssl will encode elliptic curve keys in the incompatible format specified in SEC 1: Elliptic Curve Cryptography . You can use the openssl cli to convert these keys into PKCS#8 : openssl pkcs8 -in pkey.pem -topk8 --nocrypt -out pkey.pkcs8.pem PKCS#1-encoded private keys, e.g. key with ------BEGIN RSA PRIVATE KEY----- , can be converted into PKCS#8 with the following command: openssl pkcs8 -topk8 -inform PEM -outform PEM -nocrypt -in pkcs1.key -out pkcs8.key","title":"Configuration"},{"location":"CAS_config_5.0.0/#configuration","text":"Due to the ability to securely operate CAS remotely, its configuration is split into two independent parts: An availability configuration, supplied by the CAS operator at startup, and an owner configuration, provisioned by the CAS owner at runtime.","title":"Configuration"},{"location":"CAS_config_5.0.0/#availability-configuration","text":"The availability configuration encompasses file paths and network interfaces - everything that is necessary in order to start a CAS instance. The configuration is read from a TOML -formatted file. This part of the configuration is not security-sensitive. It does not take part in remote attestation, is stored unencrypted, and can be changed by the CAS operator at any time.","title":"Availability Configuration"},{"location":"CAS_config_5.0.0/#example-configuration-file","text":"[database] path = \"/etc/cas/cas.db\" [api] api_listen = \"0.0.0.0:8081\" enclave_listen = \"0.0.0.0:18765\"","title":"Example Configuration File"},{"location":"CAS_config_5.0.0/#database","text":"CAS stores its data in an SQLite database, the location of which is determined in by the database.path option of the configuration: database.path = \"/etc/cas/database.sqlite\" The database is always encrypted to protected the confidentiality of the contained secrets. The database encryption key is stored as cipher text in a separate file with the same filename, but ending with .key-store . Note Note that the key used to encrypt the database encryption key is derived using SGX' seal key derivation feature. Therefore, CAS can only decrypt the database encryption key and access the database if it is executed on the same machine which has created the .key-store file, or the key is explicitly made available to a CAS on another machine with CAS' Backup feature. In particular, that means if the machine hosting CAS breaks, e.g. due to hardware failure, or is not available anymore, for example, because it was a cloud machine, you'll access to all data stored by CAS is lost!","title":"Database"},{"location":"CAS_config_5.0.0/#security-in-debug-mode","text":"In debug mode, SGX generally does not protect your data as the debug interface exposes the enclave's memory. That means one can easily extract the database encryption key from a CAS running in SGX debug mode with our scone-gdb debugger. However, you might not be aware of this. Therefore, to counteract a false sense of security (and help debugging) CAS will dump its database encryption key anytime it does not run in production mode to the console and into a file next to the database file ending with .key . To reiterate: this is not a security vulnerability. Any slightly invested attacker could easily extract the key with a debugger.","title":"Security in Debug Mode"},{"location":"CAS_config_5.0.0/#interfaces","text":"At the moment CAS offers two interfaces: A client REST API with which clients can upload, upgrade and query sessions, and an enclave interface with which enclaves communicate with CAS. These configuration options will determine on which network interfaces and ports CAS will listen for incoming connection. Interfaces are defined with values satisfying std::net::SocketAddr 's FromStr implementation . The client API is configured via the api_listen option while the enclave interface is configured through the enclave_listen option: [api] enclave_listen = \"0.0.0.0:18765\" api_listen = \"0.0.0.0:8080\" Note A configuration with an IP address of 0.0.0.0 will make the CAS listen on any available network interface.","title":"Interfaces"},{"location":"CAS_config_5.0.0/#owner-configuration","text":"After its initial start, a new CAS instance remains in an unprovisioned state. In this state, its functionality is severely limited until someone claims ownership and provides a configuration. Since the owner configuration contains security-sensitive information, it is necessary to attest the CAS beforehand. The configuration will be stored encrypted as part of the CAS database. The SCONE CLI is the primary tool to manage the CAS' owner configuration: Initial provisioning: scone cas provision --config-file <...> Later configuration update: scone cas update-config --config-file <...> Please refer to the SCONE CLI documentation for a full description. The configuration file is usually provided in TOML format.","title":"Owner Configuration"},{"location":"CAS_config_5.0.0/#example-configuration-file_1","text":"The following file can be used for a development environment: [api_identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" [ias] spid = \"32C2D32CD4F1C873625BBE65D973A702\" linkable_quotes = true sp_key = \"1c8b569cffd94a85ad11342bfad5a3ad\" base_uri = \"https://api.trustedservices.intel.com/sgx/dev/attestation/v3\" [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\"","title":"Example Configuration File"},{"location":"CAS_config_5.0.0/#api-identity","text":"Contents of the TLS end-entity server and client certificate presented by CAS can be configured in the api.identity section: [api.identity] common_name = \"cas\" alt_names = [\"cas\", \"localhost\"] country_name = \"DE\" org_name = \"scontain UG\" state_or_province = \"Saxony\" locality = \"Dresden\" email = \"info@scontain.com\" This section is not mandatory. If not specified, default values listed above will be used. In case the section is provided in the configuration file, it must contain at least common_name field.","title":"API Identity"},{"location":"CAS_config_5.0.0/#ias-client","text":"The IAS client enables CAS to verify enclave EPID quotes using the Intel SGX Attestation Service (IAS) . EPID quotes will be used for remote attestation of program enclaves, SCONE Quoting Enclaves, and the CAS itself. At the moment, IAS client credentials are required for normal CAS operation. Its configuration consists of the Service Provider ID (SPID), linkability setting and authentication information. spid has to be a 32 character or 16 byte long hex string. linkable_quotes is either true or false . Both are provided to you when you enroll with Intel's IAS. base_url is the url endpoint at which the client will query Intel's attestation service. This setting is optional and will point towards the development environment endpoint of IAS by default. You have to adapt this setting if you want to use an IAS production identity. IAS is access-controlled. CAS supports Rev4 and Rev5 IAS authentication (choose either one, not both): You can specify your Rev4 X.509 certificate and private key through the ias.identity option. Refer to the section about private keys information regarding private key encoding. If you're using a Rev5 subscription key to authenticate your account, specify it under the sp_key option. [ias] spid = \"2EC5BE64E242BCC9E4670D49E5C7091E\" linkable_quotes = true base_url = \"https://test-as.sgx.trustedservices.intel.com:443/attestation/sgx/v3\" identity = \"\"\" -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- \"\"\"","title":"IAS Client"},{"location":"CAS_config_5.0.0/#intel-provisioning-certification-service-for-ecdsa-attestation-client-dcap","text":"The Intel Provisioning Certification Service for ECDSA Attestation Client enables CAS to verify DCAP quotes created by the Intel Quoting Enclave. DCAP quotes can be used as an alternative to EPID quotes for remote attestation of program enclaves. Configuring a DCAP client credential is optional. The configuration encompasses a subscription key. This key is provided to you when you enroll with Intel's Provisioning Certification Service. [dcap] subscription_key = \"aecd5ebb682346028d60c36131eb2d92\"","title":"Intel Provisioning Certification Service for ECDSA Attestation Client (DCAP)"},{"location":"CAS_config_5.0.0/#private-keys","text":"We support PKCS#8 encoded private keys. By default, openssl will encode elliptic curve keys in the incompatible format specified in SEC 1: Elliptic Curve Cryptography . You can use the openssl cli to convert these keys into PKCS#8 : openssl pkcs8 -in pkey.pem -topk8 --nocrypt -out pkey.pkcs8.pem PKCS#1-encoded private keys, e.g. key with ------BEGIN RSA PRIVATE KEY----- , can be converted into PKCS#8 with the following command: openssl pkcs8 -topk8 -inform PEM -outform PEM -nocrypt -in pkcs1.key -out pkcs8.key","title":"Private Keys"},{"location":"CAS_for_development/","text":"CAS for Development We explain how to start a SCONE CAS instance for development on your local machine. This CAS instance runs inside a debug enclave, i.e., do not use this in production . For setting up a production mode CAS, send us an email. For running CAS in a Kubernetes Cluster, please set up CAS with helm . Public CAS for development We have setup a public CAS instance at domain scone-cas.cf for testing and development. This instance runs in pre-release mode, i.e., do not use this instance for production . Pulling CAS Image To start CAS, you first pull CAS Docker image to your local registry. To be able to do so, please ask us via email for access to CAS. We will give you access to a private Docker repository and we will send you the name of the private repository. For this tutorial, please set the environment variable CAS to the name of the Docker repository - typically this might look something like this: export CAS = registry.scontain.com:5050/sconecuratedimages/services:cas Pull the CAS image like this: docker pull $CAS If this fails, ensure that you are logged into docker (via docker login ) and that you set environment variable CAS properly. Determining the SGX device Depending on the platform, the SGX device is named /dev/isgx or /dev/sgx . To write generic software, you could use the bash function determine_sgx_device . It sets environment variable SGXDEVICE to the device that needs to be mounted. Starting and Stopping CAS The easiest way to start CAS is to use a simple Docker compose file. To do so, create a new directory for the Docker compose file: mkdir -p CAS cd CAS Create a compose file that exposes the ports of CAS to the host: determine_sgx_device cat > docker-compose.yml <<EOF version: '3.2' services: cas: command: sh -c \"SCONE_HEAP=1G cas -c /etc/cas/cas.toml\" environment: - SCONE_LOG=7 - SCONE_MODE=HW image: $CAS volumes: - \"$SGXDEVICE:$SGXDEVICE\" ports: - target: 8081 published: 8081 protocol: tcp mode: host - target: 18765 published: 18765 protocol: tcp mode: host EOF Now start CAS in the background as follows: docker-compose up -d cas By executing docker-compose logs cas you will see the output of CAS. You can check if CAS is still running by executing: docker-compose up -d cas This will result in an output like cas_cas_1 is up-to-date You can stop CAS by executing: docker-compose stop cas","title":"Starting CAS"},{"location":"CAS_for_development/#cas-for-development","text":"We explain how to start a SCONE CAS instance for development on your local machine. This CAS instance runs inside a debug enclave, i.e., do not use this in production . For setting up a production mode CAS, send us an email. For running CAS in a Kubernetes Cluster, please set up CAS with helm . Public CAS for development We have setup a public CAS instance at domain scone-cas.cf for testing and development. This instance runs in pre-release mode, i.e., do not use this instance for production .","title":"CAS for Development"},{"location":"CAS_for_development/#pulling-cas-image","text":"To start CAS, you first pull CAS Docker image to your local registry. To be able to do so, please ask us via email for access to CAS. We will give you access to a private Docker repository and we will send you the name of the private repository. For this tutorial, please set the environment variable CAS to the name of the Docker repository - typically this might look something like this: export CAS = registry.scontain.com:5050/sconecuratedimages/services:cas Pull the CAS image like this: docker pull $CAS If this fails, ensure that you are logged into docker (via docker login ) and that you set environment variable CAS properly.","title":"Pulling CAS Image"},{"location":"CAS_for_development/#determining-the-sgx-device","text":"Depending on the platform, the SGX device is named /dev/isgx or /dev/sgx . To write generic software, you could use the bash function determine_sgx_device . It sets environment variable SGXDEVICE to the device that needs to be mounted.","title":"Determining the SGX device"},{"location":"CAS_for_development/#starting-and-stopping-cas","text":"The easiest way to start CAS is to use a simple Docker compose file. To do so, create a new directory for the Docker compose file: mkdir -p CAS cd CAS Create a compose file that exposes the ports of CAS to the host: determine_sgx_device cat > docker-compose.yml <<EOF version: '3.2' services: cas: command: sh -c \"SCONE_HEAP=1G cas -c /etc/cas/cas.toml\" environment: - SCONE_LOG=7 - SCONE_MODE=HW image: $CAS volumes: - \"$SGXDEVICE:$SGXDEVICE\" ports: - target: 8081 published: 8081 protocol: tcp mode: host - target: 18765 published: 18765 protocol: tcp mode: host EOF Now start CAS in the background as follows: docker-compose up -d cas By executing docker-compose logs cas you will see the output of CAS. You can check if CAS is still running by executing: docker-compose up -d cas This will result in an output like cas_cas_1 is up-to-date You can stop CAS by executing: docker-compose stop cas","title":"Starting and Stopping CAS"},{"location":"CAS_session_lang_0_2/","text":"SCONE Session Language (v0.2) The SCONE session language is used to create session descriptions that entail all security-relevant details of a SCONE application. A SCONE application can consist of one or more session descriptions . Each session description defines a set of services that are part of the application, secrets that are securely stored and passed to the services, images that define which regions of a container (image) are encrypted or authenticated , volumes which are like docker volumes but encrypted, and access policy that defines who can read or modify the session description. The session language is a subset of YAML , i.e., a session description is valid YAML. It is similar to and takes its bearing from docker-compose files . As a session description is typically stored in a file, we use session file as a somewhat interchangeable synonym for session description. We use the terms session description and session policy (or just policy ) interchangeably. Session Description Hash Each session description has a unique hash value, in short just session hash . This value is used to reference the session in multiple situations. For example, when a session shall be updated the hash of the predecessor session has to be provided to ensure continuity, i.e., no lost-updates. The session hash is deterministically calculated from the session description, such that any modification to the description would yield a different hash. Moreover, the calculation is cryptographically sound in the sense that a malicious operator can not come up with a modified session description that has the same session hash, i.e., it is Preimage-Attack resistant. The exact procedure of how the session hash is obtained is an implementation detail. Session Description Structure There are a number of top-level keys in a session file. Version This document describes version 0.2. version : \"0.2\" Note Without a version field, version 0.1 is assumed, make sure to include the field in all session descriptions in order to use version 0.2. Session Name Every session description has to provide a session name , or short just name . The session name is used to reference a session over the course of its lifetime: while a session hash identifies a unique, not mutable description of a session, the session description references by a session name can be changed by the session's owner. Thus, the session name is the primary property with which a session is referenced outside of session management. As sessions will be routeable in a future version of CAS and session names are the primary way of referencing a session, they have to obey certain restrictions regarding the allowed character set. To prevent issues with future use of session names within URIs their characters have to be drawn from the \"Unreserved Characters\" in URIs as defined by RFC3986 . Further, we set a somewhat arbitrary restriction of a maximal length of 128 characters. In PCRE the match clause matching valid session names would be [A-Za-z0-9_\\-.~]{1,128} . name : my-testing-session Providing a session name is mandatory . Predecessor Session If a session description is an update for a previously existing session, i.e. the session's name is already in use, it has to include the previous session's hash to detect and prevent lost updates. predecessor : e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 Note The value has to be 32 hex encoded bytes. When creating a session, no predecessor session exists and hence no predecessor key is permitted. Service Description Services of a SCONE application are described below the services top-level key. A service in a SCONE session is a SCONE program with a specific software state and configuration. For now, there can be an unlimited number of instances of a service. In the current version, the service description specifies the service configuration and which software (code) is allowed to access it. A service configuration consists of the command to be executed, any environment variables and the process working directory from which path resolution of relative paths starts. These properties may contain secrets and would be prone to inspection and manipulation by adversaries. services : - name : my-test-service command : test --arg=value environment : VAR_1 : value pwd : / name To be able to identify a service, every service has to have a name key. As the name will be part of the routeable session identification in the future, its character set and length is restricted in the same way as the session name is. name : my-test-service Providing a service name is mandatory . command The command of a service is a sequence of the program's name plus any command-line arguments that have to be provided for it to deliver the expected service. Note The first element of the command, the program name, should match the actual program's file name. Although not enforced, in the future automatic scheduling will not function properly if the provided program name does not match. command : test --arg=value The command can be specified as a list, as well. This is helpful in situations where the automatic whitespace splitting would rip arguments apart. command : [ \"test\" , \"--my_name\" , \"Franz Gregor\" ] Note Command arguments have to be C-String compatible, i.e., they are not allowed to contain a NULL byte. environment The environment variables provide a process with values that are needed to provide the expected service. As we cannot trust pre-existing environment variables, only the environment specified in the session policy will be available to the SCONE program. environment : VAR_1 : value Note Variable names and values have to be C-String compatible, i.e. they are not allowed to contain the NULL byte. Moreover, variable names are not allowed to contain the equal (=) sign. Environment variables that are consumed during enclave creation (e.g., SCONE_HEAP ), or used by the SCONE runtime (e.g., SCONE_CAS_ADDR ) should not be included in the list of environment variables for the service as they will have no effect. Instead, they should be provided directly during program invocation. Note that the value of SCONE_HEAP affects mrenclave and via mrenclave , a session description can also limit the set of permitted SCONE_HEAP values. pwd The process working directory (pwd) , or current working directory (cwd), is the directory from which relative paths are resolved. For example, a program writing a private key to private.key with pwd of /encrypted/ would resolve actually write to /encrypted/private.key . Changing the pwd to /plaintext/ before the writing would lead it to write the private key to /plaintext/private.key instead. To prevent this kind of manipulation the pwd of a SCONE service has to be specified explicitly in the session description. pwd : /home/user/scone/encrypted Note Please note that: - The specified directory has to exist in the environment the SCONE program will be executed in. If it is not found, the program cannot start. - The provided value has to be C-String compatible, i.e. it is not allowed to contain the NULL byte. - The allowed character may be further restricted by the used file system. Image A service can optionally use an image, which has to be specified as part of the session (see Section Images ). services : - name : my_service image_name : my_image Images must be specified if their volumes or secret injection files should be used by a service. Attestation Attestation is the process of verifying a remote SCONE program's integrity ensuring it has not been (maliciously) manipulated. SCONE programs have to engage into attestation with CAS to be allowed to access their service's configuration and secrets. That is, only SCONE programs that can show (using attestation) that they satisfy a certain attestation policy are provided with arguments, environment variables, and secrets. Measurement based Attestation Measurement-based attestation gives access to service secrets based on the enclave measurement value, abbreviated as mrenclave . An enclave measurement value uniquely identifies its initial state. The measurement value of a SCONE program can be determined by running it either with SCONE_VERSION or SCONE_HASH environment variables set. For example, if you have an executable go-webserver you can determine its measurement value by executing: $ SCONE_HASH = 1 ./go-webserver d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 Any modifications to the executable, malicious or not, will void access capabilities. A service can define a list of mrenclaves that are allowed to get access to the configuration as follows: mrenclaves : [ d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 ] For example, you can define mrenclave s for different permitted HEAP_SIZE s. Also, to permit a smooth transition to new service versions, you can first add the mrenclaves of the new version, upgrade the services, and then remove the mrenclave s of the old service version. Note The enclave measurement value ( mrenclave ) has to be a sequence of 32 hex encoded bytes. mrenclave is mandatory . Secrets Secrets are defined in the secrets section of the session. Each secret is uniquely identified with a name. The values of secrets are either generated by CAS or are explicitly given in the session. The former has the advantage that the secret value can be ensured to be never known by any human. Secret Kinds In most cases, the kind of a secret has to be specified as well. This is necessary for CAS to know how to generate its value or how to interpret the value provided in the session, in the case of explicit secrets . We support these four kinds of secrets: ascii , binary , service leaf and intermediate certificates . ASCII Secrets An ASCII secret is a string compromised of ASCII characters guaranteed not to contain NULL bytes. Explicit Secret Values In case of an explicit secret, the value can contain all characters that can be put into a string in YAML, for example: secrets : - name : my_explicit_ascii_secret kind : ascii value : \"secret-value\" Note that explicit secret values can contain any character that can be represented in YAML. In particular, newline and escape sequences can be put into such a secret. The only exception to this rule is the NULL byte that can not be part of an ascii secret. Be sure that your application can handle these edge-cases. Generated Secret Values If no value field is specified, CAS will generate a secret value on-demand. The generated string will only contain printable ascii characters. In this case, you may specify the length of the string to generate with the size field. If no size is given, a default length of 20 will be used. secrets : - name : my_generate_ascii_secret kind : ascii size : 12 Binary Secrets binary secrets behave similar to ascii secrets except they can contain NULL bytes, generated values are not restricted to printable ascii characters, and explicit secret values have to be specified encoded in hex. secrets : - name : my_explicit_binary_secret kind : binary value : DEADBEEF Certificates CAS can generate X509v3 certificates as well. In fact, CAS provides a whole certificate PKI. There are two kinds of X509 certificates that can be generated: leaf and intermediate certificates . Their main difference is in their capability to sign other certificates. Leaf certificates are indicated with x509 and intermediate certificates with x509-ca . secrets : - name : my_leaf_certificate kind : x509 - name : my_intermediate_certificate kind : x509-ca Note that certificates have to be generated by CAS - they cannot be specified by the user. Certificates differ from other secrets in terms of access. Please see the Secret Access chapter for the details. Session Certificate Each certificate generated in a session is signed by the session certificate. The session certificate can be made explicitly available with the session-ca secret kind. This, in particular, allows exporting the session CA certificate to other sessions that can then be used to verify that incoming TLS connection are under the control of the session. secrets : - name : exporting-session-ca-cert kind : session-ca export : - another-session Note that, while normal certificates ( x509 , or x509-ca ) expose also their key, session certificates do only expose their certificate. Certificates differ from other secrets in terms of access. Please see the Secret Access chapter for the details. Secret Access Secrets are injected via standard means (program arguments, environment, and configuration files) into a service. Secret values are not directly included in a service's configuration as this would bound the service to specific secrets. Instead placeholder variables are put that are replaced during runtime in the service. These placeholders are of the form $$SCONE::secret_name$$ . For example, to inject a secret into a service's program arguments its command field in the session description could look like this: [ ... ] command : service_name --private-key $$SCONE::service_private_key$$ --non-confidential-argument 1234 --non-confidential-flag [ ... ] In the above example, the command 's program arguments will be presented to the service with $$SCONE::service_private_key$$ replaced by the actual value of service_private_key . To inject secrets into a service's environment the session description could contain this: [ ... ] environment : FILE_ENCRYPTION_KEY : \"$$SCONE::MY_SERVICES_ENCRYPTION_SECRET$$\" [ ... ] Note Secrets referred to ( service_private_key and MY_SERVICES_ENCRYPTION_SECRET in the above example) have to be either defined in the secrests section of the session description or imported from another session. Certificate Values Certificate secrets may be composed of multiple values. For once, there is obviously the certificate itself, then their is the private key, and lastly, they might have their certificate chain attached. These values are available with suffixes to the secret name in the placeholder. Consider a certificate with name my_cert , the following placeholders are available for secret consumption: * $$SCONE::my_cert.crt$$ delivers the certificate in PEM-encoding, * $$SCONE::my_cert.key$$ delivers the certificate's private key in PEM-encoding, and * $$SCONE::my_cert.chain$$ delivers the certificate's chain in PEM-encoding. Note that only the .crt suffix - i.e., the certificate in PEM-encoding - is guaranteed to exist. The key, for example, is not available for session ca certificates. And the chain is not present on imported certificates. Secret Injection Files A secret injection file is a file in the file system that will be updated with secrets received by the runtime from CAS. They use the same secret placeholders that can also be used in program arguments and environment variables. For example, imagine a service configuration file containing a password. Simply writing the password into the file in cleartext would leak it - that is not an option. Using SCONE's filesystem shield to encrypt the file complicates the setup, and - if distributed as part of an image - would require users to change the encryption keys in order to protect their individual passwords. Instead, a secret injection file allows to specify a placeholder for the password, which will be dynamically replaced at runtime through the means of secret injection: /etc/mysql/my.cnf : [client] user=mysqluser password=$$SCONE::mysqlpass$$ The path to this configuration file must be specified as part of an image (see Images ). The corresponding session would look like this: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - /etc/mysql/my.cnf Alternatively, the file's content may be specified within the session: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ The content specified in the session file can contain multiline strings. This way, entire configuration files can be embedded in the session description, even if no secret injection is required. Producing valid multiline strings in YAML can be challenging - https://yaml-multiline.info/ can be of great help to find the desired syntax. Secret injection files are prepared during SCONE runtime initialization. If the file content is not provided in the session, the file at the specified path is opened, potentially through SCONE's filesystem shield, and read into memory. The secret injection is applied and the resulting file is put into SCONE's in-memory file system at the specified path. Any application requests regarding this file are served from this in-memory file system. Thus, modifications to secret injection files are not propagated into the file system and are not persistent across program invocations. Secret Sharing Session owners can decide to share their secrets with other parties to enable collaboration. For example, database operators could use TLS to implement access control to databases. They would define an access certificate, configure the database to only allow connections from said certificate and export it to the database client: name : database_operator secrets : - name : database_access_certificate kind : x509 export : database_client The secret owner might also specify multiple receiving sessions at once: name : database_operator secrets : - name : database_access_certificate kind : x509 export : - database_client - another_client Furthermore, the export might be restricted to certain instances of the importing session. For more details, see the concrete format by which other sessions can be referenced in Referencing Other Sessions . The database client, on the other hand, would import it and could use it in their session as if it was their own certificate: name : database_client secrets : - name : database_access_certificate kind : x509 # optional import : database_operator On the importing side, the kind of a secret can be optionally defined to ensure imported secrets match a specific form, but this is not strictly necessary. In very specific cases, secrets may also be made public (exported to any session without authentication ) - this may be useful when, for example, defining certificate hashes: name : policy_checker secrets : - name : policy_checker_certificate_hash kind : ascii value : \"ce29906ee68a580410f0d41c67984ff7b384310e84dadf2b07c21252aa01fe1f\" export_public : true This hash can be used in another session's access control policy through secret substitution (see Access Control ): name : checked_session secrets : - name : policy_checker_certificate_hash import : policy_checker access_policy : read : - \"$$SCONE::policy_checker_certificate_hash\" Warning By using export_public: true , the whole world will be able to see the secret value. Make sure this is your intention. Secret Migration When uploading a new session which has a predecessor session, secret migration takes place. In short: secret values which were generated as part of the old session will be kept when the new session defines a secret with the same name and compatible kind. Example old session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : binary value : DEADBEEF Example new session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : x509 In the given example, the value of my_generated_ascii_secret will be kept, as it stays an ASCII secret of the same size, whereas foobar will be freshly generated, since its kind changed. Secret migration also takes place when using a different session description language version (e.g. 0.1 -> 0.2)! For details, see the session secrets migration documentation . Volumes Similar to Docker, the volumes keyword allows to specify file system regions that can be mounted into the file system in addition to the regions specified by the main file system protection file of a service. Each volume has an arbitrary but unique name . In order to grant services access to a volume, it first has to be included in an image (see section Images ). Subsequently, the image can be specified for a service (see section Service Description ), and all image-defined volumes will be mounted automatically. Note The volume name is not the volume's mount point. The latter is defined as part of an image . Each volume can have a file system protection file key ( fspf_key ) and file system protection file tag ( fspf_tag ): volumes : - name : my_database fspf_key : f843051d21afa9e52a5b54a708a8032bc49581e982696a81393b8da4a32d00b8 fspf_tag : 8d8fbe332fb9c893020be791ccd3e8a8 fspf_key is the key used to encrypt volume.fspf (file system protection file for the volume) and fspf_tag describes the initial state of the volume. On each volume update (e.g., creation of a file in the region or update of an existing file), the SCONE runtime will send a new fspf_tag to CAS to ensure integrity and freshness of the volume state. If neither fspf_key nor fspf_tag are specified, a volume will be automatically initialized during the first use: volumes : - name : my_database The volume will contain a single encrypted region, and a new key will be generated by CAS to encrypt volume.fspf . Once initialized, CAS and the runtime will work together to track the updates of the volume, similar to a regular volume. This ensures that a volume can be initialized only once. Use of automatically initialized volumes ensures that the key for the file system is only visible inside of CAS and the application(s) that get access to this volume, i.e., no system administrator will ever see the volume key. Volumes may be exported to other sessions, which implies authorizing the other sessions to decrypt and read existing files or encrypt and store new files: volumes : - name : my_database export : another-session or volumes : - name : my_database export : - another-session - foobar:668e9aaba22c7631bbcc89b627d77e53539bcaade9e7c2c08242f56aab272088 The concrete format by which other sessions can be referenced is described in section Referencing Other Sessions . Similarly, volumes may be imported from another session (in which case fspf key/tag and export list must be omitted): volumes : - name : my_database import : the_exporting_session Images The images keyword allows specifying images usable by services (see section Service Description ). Each image has an arbitrary but unique name : images : - name : my_image Images may define access to volumes , by referencing a previously declared volume's name and giving it a mount point ( path ): volumes : - name : my_database images : - name : my_image volumes : - name : my_database path : /media/database Note The given path must already exist in the filesystem, e.g., through a mounted docker volume. The information provided as part of the session description are only used to encrypt and authenticate all of the volume's files. The CAS does not actually store the encrypted files. Images may also contain secret injection files, a way to inject secrets into configuration files: images : - name : proxy_image injection_files : - /etc/nginx/nginx.conf - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ For details, refer to section Secret Injection Files . Access Control Any operation on a session description requires permission. If the entity requesting a certain operation is not explicitly permitted to perform said operation, the request will fail. access_policy keyword allows to specify lists of entities that are allowed to perform the following operations: read : permit to read the policy - without the secrets update : permit to update the policy. Note that entities listed here must also be present under read . Granting permission to a certain entity to perform one of these operation involves adding their public certificate to the list of authorized entities. This certificate shall be used when establishing connection to CAS (see API Documentation, Authentication section ). TLS ensures that the client is in possession of the corresponding private key. When using the scone CLI, the user certificate can be shown by running scone self show . Besides public certificates, the following values can be used: SHA256 hash of a public certificate in PEM format. Note that CAS supports only LF ( \\n ) line endings. Please make sure your certificate file adheres to this requirement. Otherwise, you will not be able to perform the operation. When using the scone CLI, the hash can be shown by running scone self show . The hash can also be calculated by the sha256sum tool: $ sha256sum cert.pem 1809fafa119b97db77a43562c5241b3db33d21a85516e35ebe0a19bf0e3d29ee cert.pem CREATOR keyword: permit access to the creator of the policy: this is the public key of the TLS client certificate used when creating this session ANY keyword: permit access to any entity. If ANY is specified, there must be no other entries in the list for this operation NONE keyword: deny all requests for a particular operation. If NONE is specified, there must be no other entries in the list for this operation $$SCONE::secret-name$$ will dynamically use the value of a secret with the given name ( secret-name ) at permission evaluation time. The replaced value must be either CREATOR (ascii), a SHA256 hash (ascii) or certificate as defined above. It is possible to use explicit secrets, generated secrets, and imported secrets. When referencing X.509 certificates, the trailing .crt after the secret name can be specified, but may also be omitted. If the mentioned secret does not exist, cannot be read, or has an incompatible value, it will be ignored. By default, the access policy is defined as follows: access_policy : read : - CREATOR update : - CREATOR If the session description does not overwrite some of the operations, default values are used. Example policy: access_policy : read : - CREATOR - 1809fafa119b97db77a43562c5241b3db33d21a85516e35ebe0a19bf0e3d29ee - | -----BEGIN CERTIFICATE----- MIIFwTCCA6mgAwIBAgIUCF1MVJJ78BIf4WmTE24aAX7NlHowDQYJKoZIhvcNAQEL BQAwcDELMAkGA1UEBhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9y dGxhbmQxFTATBgNVBAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYD VQQDDA93d3cuZXhhbXBsZS5jb20wHhcNMTkwNzIyMTU1NTExWhcNMTkwODIyMTU1 NTExWjBwMQswCQYDVQQGEwJVUzEPMA0GA1UECAwGT3JlZ29uMREwDwYDVQQHDAhQ b3J0bGFuZDEVMBMGA1UECgwMQ29tcGFueSBOYW1lMQwwCgYDVQQLDANPcmcxGDAW BgNVBAMMD3d3dy5leGFtcGxlLmNvbTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCC AgoCggIBALVbVIrBAlzDOztWs9hZr5kvYoUwq/hL7zaMrYKBLQJZFNbhmMaUsW7A Fzj87dzP3xIf4c2r3IGJSukv7hJpaJ2Ykv80i3C7EiFgaDV/+JP9d/GjsvcW20zH mtJcBIkdkqPt1epOtxsMyJGZL+34DoWOqgY7up6nCirr+MeUxYJ/dWBFD1j0iuHl Y+rEMsv4xFBndgLmMQNlcMyXtBgPls4EgnDfnjICqIYMHt6PG+kwoR4tbs+v2Gsl vqldxI7efErZh+kKtjtFxt6qzrypUs9bYgH3tsaUE0xYeK/A2llylJzPOv6vkCqg vPOJETcZyoeH46niITdPssYr4yPQOxn/a7WS+7Mn2y6o5z4Q+DkB96lzUyvVJnwO aorzec0PaB/qqYrHqVfftMu4thMwHGB8CrGUiq/ImHPWkfobyVcMYJ0/LaLSDHFj 1hN36VkzWqQcCM6ymhjx9Lpfzzxna5910jE86zb1cMnD/eAAd90jpJvGJN43Hw40 MIvjYBunOy9P3ah0kgCk7gW0oKlYHxugv8pZVHMwU1HFIdwYvlGd09XHFDyj9tul eX8zaVwaNeLUrMdJN5Ct1HX16RpnpaIMwwExzXgsZ01BQcfIcGWGbvBfH2C86klt SuG7M6kxk4XgIIlwTSGk7qJlfd4s8PD1fVJNKvJZwXXoQBy4hCrTAgMBAAGjUzBR MB0GA1UdDgQWBBRSUKop9QDGmSdLCfzWlBIF5ClNVDAfBgNVHSMEGDAWgBRSUKop 9QDGmSdLCfzWlBIF5ClNVDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUA A4ICAQAny4QmzvCza0TQUuh7YIxtkBjJoyTSDGmr5W/4AWKsoZJM9QxzWeIAXhWD UPH2TfwcNX/ovZiSDQaGduRL67Wk8lbp1MvjACMEtLRjsbnYtGFdhit0fR97B5Y6 d06Ka/NXgPTJorXx8WSWUp0qaAQcgvhfgF0vnOSB5CbP5RSYE5TuLu6gh+iQTrBI Syl+9UaopkbQDRsg+XRfie+kUxQgldUAFvFmu6sM6FTbw0KGkrsOajwpF/Fu5hSV Ucov4Lzrrxkok5FzWPkVtMalLZ4Du+ZUYG//10WZg+HdrIwx3m2wxrFIkZaMKxv4 ZkIMsb6DUPUZqy8qZpMzIqvDzx3iYEWWfBOCJWBjs8/V1mAuUu6TBCKAJpvfX6bU hNrCbnrpuxuCCPJnj9sXkBDvl5rcyfshTtKl3NoBrRRDuUHWsJWzsKvBQtwN46vF CbF0aXOozihtmmcMpFFeDIj6p/5qlaJtslegtfv2zoztc3e2ituOjqFQ/I5pplvo p8EGwCI1xTGF0BTatcSV1+lLNeONhhAtwliV13nPSH1o4yxoZ+xZTZq4+9ylw7dq yV3BQM11U6OyAPE1G6EX0PgFvLm25sGTJq9TKXs9yWPRit9vHcOCXSGn8osn4SMg Puqpk+3M9xR8XDPJiBjkxcSnt9+EDNwpthTzgUEoyM6dY8nvWA== -----END CERTIFICATE----- - \"$$SCONE::remote_validation_service$$\" update : - NONE This policy will allow read requests from the creator, a user whose public certificate has hash 1809fafa119b97db77a43562c5241b3db33d21a85516e35ebe0a19bf0e3d29ee , a user whose certificate is specified in the session description and a user whose credential is taken from the secret named remote_validation_service (which may be imported from another session). No one is allowed to update the session description. Referencing Other Sessions In some cases it may be necessary to reference other sessions, e.g. when exporting or importing volumes or secrets . This is possible using the given session ID format: <session name>[:<session hash>] Access to/from remote sessions will be verified according to this ID. The hash part is optional. Examples: my_cloud_service my_cloud_service:408c03d53e8689062dff5fa21866c173fd482351df47767371556bec395241c6","title":"Policy Language 0.2"},{"location":"CAS_session_lang_0_2/#scone-session-language-v02","text":"The SCONE session language is used to create session descriptions that entail all security-relevant details of a SCONE application. A SCONE application can consist of one or more session descriptions . Each session description defines a set of services that are part of the application, secrets that are securely stored and passed to the services, images that define which regions of a container (image) are encrypted or authenticated , volumes which are like docker volumes but encrypted, and access policy that defines who can read or modify the session description. The session language is a subset of YAML , i.e., a session description is valid YAML. It is similar to and takes its bearing from docker-compose files . As a session description is typically stored in a file, we use session file as a somewhat interchangeable synonym for session description. We use the terms session description and session policy (or just policy ) interchangeably.","title":"SCONE Session Language (v0.2)"},{"location":"CAS_session_lang_0_2/#session-description-hash","text":"Each session description has a unique hash value, in short just session hash . This value is used to reference the session in multiple situations. For example, when a session shall be updated the hash of the predecessor session has to be provided to ensure continuity, i.e., no lost-updates. The session hash is deterministically calculated from the session description, such that any modification to the description would yield a different hash. Moreover, the calculation is cryptographically sound in the sense that a malicious operator can not come up with a modified session description that has the same session hash, i.e., it is Preimage-Attack resistant. The exact procedure of how the session hash is obtained is an implementation detail.","title":"Session Description Hash"},{"location":"CAS_session_lang_0_2/#session-description-structure","text":"There are a number of top-level keys in a session file.","title":"Session Description Structure"},{"location":"CAS_session_lang_0_2/#version","text":"This document describes version 0.2. version : \"0.2\" Note Without a version field, version 0.1 is assumed, make sure to include the field in all session descriptions in order to use version 0.2.","title":"Version"},{"location":"CAS_session_lang_0_2/#session-name","text":"Every session description has to provide a session name , or short just name . The session name is used to reference a session over the course of its lifetime: while a session hash identifies a unique, not mutable description of a session, the session description references by a session name can be changed by the session's owner. Thus, the session name is the primary property with which a session is referenced outside of session management. As sessions will be routeable in a future version of CAS and session names are the primary way of referencing a session, they have to obey certain restrictions regarding the allowed character set. To prevent issues with future use of session names within URIs their characters have to be drawn from the \"Unreserved Characters\" in URIs as defined by RFC3986 . Further, we set a somewhat arbitrary restriction of a maximal length of 128 characters. In PCRE the match clause matching valid session names would be [A-Za-z0-9_\\-.~]{1,128} . name : my-testing-session Providing a session name is mandatory .","title":"Session Name"},{"location":"CAS_session_lang_0_2/#predecessor-session","text":"If a session description is an update for a previously existing session, i.e. the session's name is already in use, it has to include the previous session's hash to detect and prevent lost updates. predecessor : e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 Note The value has to be 32 hex encoded bytes. When creating a session, no predecessor session exists and hence no predecessor key is permitted.","title":"Predecessor Session"},{"location":"CAS_session_lang_0_2/#service-description","text":"Services of a SCONE application are described below the services top-level key. A service in a SCONE session is a SCONE program with a specific software state and configuration. For now, there can be an unlimited number of instances of a service. In the current version, the service description specifies the service configuration and which software (code) is allowed to access it. A service configuration consists of the command to be executed, any environment variables and the process working directory from which path resolution of relative paths starts. These properties may contain secrets and would be prone to inspection and manipulation by adversaries. services : - name : my-test-service command : test --arg=value environment : VAR_1 : value pwd : /","title":"Service Description"},{"location":"CAS_session_lang_0_2/#name","text":"To be able to identify a service, every service has to have a name key. As the name will be part of the routeable session identification in the future, its character set and length is restricted in the same way as the session name is. name : my-test-service Providing a service name is mandatory .","title":"name"},{"location":"CAS_session_lang_0_2/#command","text":"The command of a service is a sequence of the program's name plus any command-line arguments that have to be provided for it to deliver the expected service. Note The first element of the command, the program name, should match the actual program's file name. Although not enforced, in the future automatic scheduling will not function properly if the provided program name does not match. command : test --arg=value The command can be specified as a list, as well. This is helpful in situations where the automatic whitespace splitting would rip arguments apart. command : [ \"test\" , \"--my_name\" , \"Franz Gregor\" ] Note Command arguments have to be C-String compatible, i.e., they are not allowed to contain a NULL byte.","title":"command"},{"location":"CAS_session_lang_0_2/#environment","text":"The environment variables provide a process with values that are needed to provide the expected service. As we cannot trust pre-existing environment variables, only the environment specified in the session policy will be available to the SCONE program. environment : VAR_1 : value Note Variable names and values have to be C-String compatible, i.e. they are not allowed to contain the NULL byte. Moreover, variable names are not allowed to contain the equal (=) sign. Environment variables that are consumed during enclave creation (e.g., SCONE_HEAP ), or used by the SCONE runtime (e.g., SCONE_CAS_ADDR ) should not be included in the list of environment variables for the service as they will have no effect. Instead, they should be provided directly during program invocation. Note that the value of SCONE_HEAP affects mrenclave and via mrenclave , a session description can also limit the set of permitted SCONE_HEAP values.","title":"environment"},{"location":"CAS_session_lang_0_2/#pwd","text":"The process working directory (pwd) , or current working directory (cwd), is the directory from which relative paths are resolved. For example, a program writing a private key to private.key with pwd of /encrypted/ would resolve actually write to /encrypted/private.key . Changing the pwd to /plaintext/ before the writing would lead it to write the private key to /plaintext/private.key instead. To prevent this kind of manipulation the pwd of a SCONE service has to be specified explicitly in the session description. pwd : /home/user/scone/encrypted Note Please note that: - The specified directory has to exist in the environment the SCONE program will be executed in. If it is not found, the program cannot start. - The provided value has to be C-String compatible, i.e. it is not allowed to contain the NULL byte. - The allowed character may be further restricted by the used file system.","title":"pwd"},{"location":"CAS_session_lang_0_2/#image","text":"A service can optionally use an image, which has to be specified as part of the session (see Section Images ). services : - name : my_service image_name : my_image Images must be specified if their volumes or secret injection files should be used by a service.","title":"Image"},{"location":"CAS_session_lang_0_2/#attestation","text":"Attestation is the process of verifying a remote SCONE program's integrity ensuring it has not been (maliciously) manipulated. SCONE programs have to engage into attestation with CAS to be allowed to access their service's configuration and secrets. That is, only SCONE programs that can show (using attestation) that they satisfy a certain attestation policy are provided with arguments, environment variables, and secrets.","title":"Attestation"},{"location":"CAS_session_lang_0_2/#measurement-based-attestation","text":"Measurement-based attestation gives access to service secrets based on the enclave measurement value, abbreviated as mrenclave . An enclave measurement value uniquely identifies its initial state. The measurement value of a SCONE program can be determined by running it either with SCONE_VERSION or SCONE_HASH environment variables set. For example, if you have an executable go-webserver you can determine its measurement value by executing: $ SCONE_HASH = 1 ./go-webserver d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 Any modifications to the executable, malicious or not, will void access capabilities. A service can define a list of mrenclaves that are allowed to get access to the configuration as follows: mrenclaves : [ d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 ] For example, you can define mrenclave s for different permitted HEAP_SIZE s. Also, to permit a smooth transition to new service versions, you can first add the mrenclaves of the new version, upgrade the services, and then remove the mrenclave s of the old service version. Note The enclave measurement value ( mrenclave ) has to be a sequence of 32 hex encoded bytes. mrenclave is mandatory .","title":"Measurement based Attestation"},{"location":"CAS_session_lang_0_2/#secrets","text":"Secrets are defined in the secrets section of the session. Each secret is uniquely identified with a name. The values of secrets are either generated by CAS or are explicitly given in the session. The former has the advantage that the secret value can be ensured to be never known by any human.","title":"Secrets"},{"location":"CAS_session_lang_0_2/#secret-kinds","text":"In most cases, the kind of a secret has to be specified as well. This is necessary for CAS to know how to generate its value or how to interpret the value provided in the session, in the case of explicit secrets . We support these four kinds of secrets: ascii , binary , service leaf and intermediate certificates .","title":"Secret Kinds"},{"location":"CAS_session_lang_0_2/#ascii-secrets","text":"An ASCII secret is a string compromised of ASCII characters guaranteed not to contain NULL bytes.","title":"ASCII Secrets"},{"location":"CAS_session_lang_0_2/#explicit-secret-values","text":"In case of an explicit secret, the value can contain all characters that can be put into a string in YAML, for example: secrets : - name : my_explicit_ascii_secret kind : ascii value : \"secret-value\" Note that explicit secret values can contain any character that can be represented in YAML. In particular, newline and escape sequences can be put into such a secret. The only exception to this rule is the NULL byte that can not be part of an ascii secret. Be sure that your application can handle these edge-cases.","title":"Explicit Secret Values"},{"location":"CAS_session_lang_0_2/#generated-secret-values","text":"If no value field is specified, CAS will generate a secret value on-demand. The generated string will only contain printable ascii characters. In this case, you may specify the length of the string to generate with the size field. If no size is given, a default length of 20 will be used. secrets : - name : my_generate_ascii_secret kind : ascii size : 12","title":"Generated Secret Values"},{"location":"CAS_session_lang_0_2/#binary-secrets","text":"binary secrets behave similar to ascii secrets except they can contain NULL bytes, generated values are not restricted to printable ascii characters, and explicit secret values have to be specified encoded in hex. secrets : - name : my_explicit_binary_secret kind : binary value : DEADBEEF","title":"Binary Secrets"},{"location":"CAS_session_lang_0_2/#certificates","text":"CAS can generate X509v3 certificates as well. In fact, CAS provides a whole certificate PKI. There are two kinds of X509 certificates that can be generated: leaf and intermediate certificates . Their main difference is in their capability to sign other certificates. Leaf certificates are indicated with x509 and intermediate certificates with x509-ca . secrets : - name : my_leaf_certificate kind : x509 - name : my_intermediate_certificate kind : x509-ca Note that certificates have to be generated by CAS - they cannot be specified by the user. Certificates differ from other secrets in terms of access. Please see the Secret Access chapter for the details.","title":"Certificates"},{"location":"CAS_session_lang_0_2/#session-certificate","text":"Each certificate generated in a session is signed by the session certificate. The session certificate can be made explicitly available with the session-ca secret kind. This, in particular, allows exporting the session CA certificate to other sessions that can then be used to verify that incoming TLS connection are under the control of the session. secrets : - name : exporting-session-ca-cert kind : session-ca export : - another-session Note that, while normal certificates ( x509 , or x509-ca ) expose also their key, session certificates do only expose their certificate. Certificates differ from other secrets in terms of access. Please see the Secret Access chapter for the details.","title":"Session Certificate"},{"location":"CAS_session_lang_0_2/#secret-access","text":"Secrets are injected via standard means (program arguments, environment, and configuration files) into a service. Secret values are not directly included in a service's configuration as this would bound the service to specific secrets. Instead placeholder variables are put that are replaced during runtime in the service. These placeholders are of the form $$SCONE::secret_name$$ . For example, to inject a secret into a service's program arguments its command field in the session description could look like this: [ ... ] command : service_name --private-key $$SCONE::service_private_key$$ --non-confidential-argument 1234 --non-confidential-flag [ ... ] In the above example, the command 's program arguments will be presented to the service with $$SCONE::service_private_key$$ replaced by the actual value of service_private_key . To inject secrets into a service's environment the session description could contain this: [ ... ] environment : FILE_ENCRYPTION_KEY : \"$$SCONE::MY_SERVICES_ENCRYPTION_SECRET$$\" [ ... ] Note Secrets referred to ( service_private_key and MY_SERVICES_ENCRYPTION_SECRET in the above example) have to be either defined in the secrests section of the session description or imported from another session.","title":"Secret Access"},{"location":"CAS_session_lang_0_2/#certificate-values","text":"Certificate secrets may be composed of multiple values. For once, there is obviously the certificate itself, then their is the private key, and lastly, they might have their certificate chain attached. These values are available with suffixes to the secret name in the placeholder. Consider a certificate with name my_cert , the following placeholders are available for secret consumption: * $$SCONE::my_cert.crt$$ delivers the certificate in PEM-encoding, * $$SCONE::my_cert.key$$ delivers the certificate's private key in PEM-encoding, and * $$SCONE::my_cert.chain$$ delivers the certificate's chain in PEM-encoding. Note that only the .crt suffix - i.e., the certificate in PEM-encoding - is guaranteed to exist. The key, for example, is not available for session ca certificates. And the chain is not present on imported certificates.","title":"Certificate Values"},{"location":"CAS_session_lang_0_2/#secret-injection-files","text":"A secret injection file is a file in the file system that will be updated with secrets received by the runtime from CAS. They use the same secret placeholders that can also be used in program arguments and environment variables. For example, imagine a service configuration file containing a password. Simply writing the password into the file in cleartext would leak it - that is not an option. Using SCONE's filesystem shield to encrypt the file complicates the setup, and - if distributed as part of an image - would require users to change the encryption keys in order to protect their individual passwords. Instead, a secret injection file allows to specify a placeholder for the password, which will be dynamically replaced at runtime through the means of secret injection: /etc/mysql/my.cnf : [client] user=mysqluser password=$$SCONE::mysqlpass$$ The path to this configuration file must be specified as part of an image (see Images ). The corresponding session would look like this: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - /etc/mysql/my.cnf Alternatively, the file's content may be specified within the session: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ The content specified in the session file can contain multiline strings. This way, entire configuration files can be embedded in the session description, even if no secret injection is required. Producing valid multiline strings in YAML can be challenging - https://yaml-multiline.info/ can be of great help to find the desired syntax. Secret injection files are prepared during SCONE runtime initialization. If the file content is not provided in the session, the file at the specified path is opened, potentially through SCONE's filesystem shield, and read into memory. The secret injection is applied and the resulting file is put into SCONE's in-memory file system at the specified path. Any application requests regarding this file are served from this in-memory file system. Thus, modifications to secret injection files are not propagated into the file system and are not persistent across program invocations.","title":"Secret Injection Files"},{"location":"CAS_session_lang_0_2/#secret-sharing","text":"Session owners can decide to share their secrets with other parties to enable collaboration. For example, database operators could use TLS to implement access control to databases. They would define an access certificate, configure the database to only allow connections from said certificate and export it to the database client: name : database_operator secrets : - name : database_access_certificate kind : x509 export : database_client The secret owner might also specify multiple receiving sessions at once: name : database_operator secrets : - name : database_access_certificate kind : x509 export : - database_client - another_client Furthermore, the export might be restricted to certain instances of the importing session. For more details, see the concrete format by which other sessions can be referenced in Referencing Other Sessions . The database client, on the other hand, would import it and could use it in their session as if it was their own certificate: name : database_client secrets : - name : database_access_certificate kind : x509 # optional import : database_operator On the importing side, the kind of a secret can be optionally defined to ensure imported secrets match a specific form, but this is not strictly necessary. In very specific cases, secrets may also be made public (exported to any session without authentication ) - this may be useful when, for example, defining certificate hashes: name : policy_checker secrets : - name : policy_checker_certificate_hash kind : ascii value : \"ce29906ee68a580410f0d41c67984ff7b384310e84dadf2b07c21252aa01fe1f\" export_public : true This hash can be used in another session's access control policy through secret substitution (see Access Control ): name : checked_session secrets : - name : policy_checker_certificate_hash import : policy_checker access_policy : read : - \"$$SCONE::policy_checker_certificate_hash\" Warning By using export_public: true , the whole world will be able to see the secret value. Make sure this is your intention.","title":"Secret Sharing"},{"location":"CAS_session_lang_0_2/#secret-migration","text":"When uploading a new session which has a predecessor session, secret migration takes place. In short: secret values which were generated as part of the old session will be kept when the new session defines a secret with the same name and compatible kind. Example old session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : binary value : DEADBEEF Example new session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : x509 In the given example, the value of my_generated_ascii_secret will be kept, as it stays an ASCII secret of the same size, whereas foobar will be freshly generated, since its kind changed. Secret migration also takes place when using a different session description language version (e.g. 0.1 -> 0.2)! For details, see the session secrets migration documentation .","title":"Secret Migration"},{"location":"CAS_session_lang_0_2/#volumes","text":"Similar to Docker, the volumes keyword allows to specify file system regions that can be mounted into the file system in addition to the regions specified by the main file system protection file of a service. Each volume has an arbitrary but unique name . In order to grant services access to a volume, it first has to be included in an image (see section Images ). Subsequently, the image can be specified for a service (see section Service Description ), and all image-defined volumes will be mounted automatically. Note The volume name is not the volume's mount point. The latter is defined as part of an image . Each volume can have a file system protection file key ( fspf_key ) and file system protection file tag ( fspf_tag ): volumes : - name : my_database fspf_key : f843051d21afa9e52a5b54a708a8032bc49581e982696a81393b8da4a32d00b8 fspf_tag : 8d8fbe332fb9c893020be791ccd3e8a8 fspf_key is the key used to encrypt volume.fspf (file system protection file for the volume) and fspf_tag describes the initial state of the volume. On each volume update (e.g., creation of a file in the region or update of an existing file), the SCONE runtime will send a new fspf_tag to CAS to ensure integrity and freshness of the volume state. If neither fspf_key nor fspf_tag are specified, a volume will be automatically initialized during the first use: volumes : - name : my_database The volume will contain a single encrypted region, and a new key will be generated by CAS to encrypt volume.fspf . Once initialized, CAS and the runtime will work together to track the updates of the volume, similar to a regular volume. This ensures that a volume can be initialized only once. Use of automatically initialized volumes ensures that the key for the file system is only visible inside of CAS and the application(s) that get access to this volume, i.e., no system administrator will ever see the volume key. Volumes may be exported to other sessions, which implies authorizing the other sessions to decrypt and read existing files or encrypt and store new files: volumes : - name : my_database export : another-session or volumes : - name : my_database export : - another-session - foobar:668e9aaba22c7631bbcc89b627d77e53539bcaade9e7c2c08242f56aab272088 The concrete format by which other sessions can be referenced is described in section Referencing Other Sessions . Similarly, volumes may be imported from another session (in which case fspf key/tag and export list must be omitted): volumes : - name : my_database import : the_exporting_session","title":"Volumes"},{"location":"CAS_session_lang_0_2/#images","text":"The images keyword allows specifying images usable by services (see section Service Description ). Each image has an arbitrary but unique name : images : - name : my_image Images may define access to volumes , by referencing a previously declared volume's name and giving it a mount point ( path ): volumes : - name : my_database images : - name : my_image volumes : - name : my_database path : /media/database Note The given path must already exist in the filesystem, e.g., through a mounted docker volume. The information provided as part of the session description are only used to encrypt and authenticate all of the volume's files. The CAS does not actually store the encrypted files. Images may also contain secret injection files, a way to inject secrets into configuration files: images : - name : proxy_image injection_files : - /etc/nginx/nginx.conf - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ For details, refer to section Secret Injection Files .","title":"Images"},{"location":"CAS_session_lang_0_2/#access-control","text":"Any operation on a session description requires permission. If the entity requesting a certain operation is not explicitly permitted to perform said operation, the request will fail. access_policy keyword allows to specify lists of entities that are allowed to perform the following operations: read : permit to read the policy - without the secrets update : permit to update the policy. Note that entities listed here must also be present under read . Granting permission to a certain entity to perform one of these operation involves adding their public certificate to the list of authorized entities. This certificate shall be used when establishing connection to CAS (see API Documentation, Authentication section ). TLS ensures that the client is in possession of the corresponding private key. When using the scone CLI, the user certificate can be shown by running scone self show . Besides public certificates, the following values can be used: SHA256 hash of a public certificate in PEM format. Note that CAS supports only LF ( \\n ) line endings. Please make sure your certificate file adheres to this requirement. Otherwise, you will not be able to perform the operation. When using the scone CLI, the hash can be shown by running scone self show . The hash can also be calculated by the sha256sum tool: $ sha256sum cert.pem 1809fafa119b97db77a43562c5241b3db33d21a85516e35ebe0a19bf0e3d29ee cert.pem CREATOR keyword: permit access to the creator of the policy: this is the public key of the TLS client certificate used when creating this session ANY keyword: permit access to any entity. If ANY is specified, there must be no other entries in the list for this operation NONE keyword: deny all requests for a particular operation. If NONE is specified, there must be no other entries in the list for this operation $$SCONE::secret-name$$ will dynamically use the value of a secret with the given name ( secret-name ) at permission evaluation time. The replaced value must be either CREATOR (ascii), a SHA256 hash (ascii) or certificate as defined above. It is possible to use explicit secrets, generated secrets, and imported secrets. When referencing X.509 certificates, the trailing .crt after the secret name can be specified, but may also be omitted. If the mentioned secret does not exist, cannot be read, or has an incompatible value, it will be ignored. By default, the access policy is defined as follows: access_policy : read : - CREATOR update : - CREATOR If the session description does not overwrite some of the operations, default values are used. Example policy: access_policy : read : - CREATOR - 1809fafa119b97db77a43562c5241b3db33d21a85516e35ebe0a19bf0e3d29ee - | -----BEGIN CERTIFICATE----- MIIFwTCCA6mgAwIBAgIUCF1MVJJ78BIf4WmTE24aAX7NlHowDQYJKoZIhvcNAQEL BQAwcDELMAkGA1UEBhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9y dGxhbmQxFTATBgNVBAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYD VQQDDA93d3cuZXhhbXBsZS5jb20wHhcNMTkwNzIyMTU1NTExWhcNMTkwODIyMTU1 NTExWjBwMQswCQYDVQQGEwJVUzEPMA0GA1UECAwGT3JlZ29uMREwDwYDVQQHDAhQ b3J0bGFuZDEVMBMGA1UECgwMQ29tcGFueSBOYW1lMQwwCgYDVQQLDANPcmcxGDAW BgNVBAMMD3d3dy5leGFtcGxlLmNvbTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCC AgoCggIBALVbVIrBAlzDOztWs9hZr5kvYoUwq/hL7zaMrYKBLQJZFNbhmMaUsW7A Fzj87dzP3xIf4c2r3IGJSukv7hJpaJ2Ykv80i3C7EiFgaDV/+JP9d/GjsvcW20zH mtJcBIkdkqPt1epOtxsMyJGZL+34DoWOqgY7up6nCirr+MeUxYJ/dWBFD1j0iuHl Y+rEMsv4xFBndgLmMQNlcMyXtBgPls4EgnDfnjICqIYMHt6PG+kwoR4tbs+v2Gsl vqldxI7efErZh+kKtjtFxt6qzrypUs9bYgH3tsaUE0xYeK/A2llylJzPOv6vkCqg vPOJETcZyoeH46niITdPssYr4yPQOxn/a7WS+7Mn2y6o5z4Q+DkB96lzUyvVJnwO aorzec0PaB/qqYrHqVfftMu4thMwHGB8CrGUiq/ImHPWkfobyVcMYJ0/LaLSDHFj 1hN36VkzWqQcCM6ymhjx9Lpfzzxna5910jE86zb1cMnD/eAAd90jpJvGJN43Hw40 MIvjYBunOy9P3ah0kgCk7gW0oKlYHxugv8pZVHMwU1HFIdwYvlGd09XHFDyj9tul eX8zaVwaNeLUrMdJN5Ct1HX16RpnpaIMwwExzXgsZ01BQcfIcGWGbvBfH2C86klt SuG7M6kxk4XgIIlwTSGk7qJlfd4s8PD1fVJNKvJZwXXoQBy4hCrTAgMBAAGjUzBR MB0GA1UdDgQWBBRSUKop9QDGmSdLCfzWlBIF5ClNVDAfBgNVHSMEGDAWgBRSUKop 9QDGmSdLCfzWlBIF5ClNVDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUA A4ICAQAny4QmzvCza0TQUuh7YIxtkBjJoyTSDGmr5W/4AWKsoZJM9QxzWeIAXhWD UPH2TfwcNX/ovZiSDQaGduRL67Wk8lbp1MvjACMEtLRjsbnYtGFdhit0fR97B5Y6 d06Ka/NXgPTJorXx8WSWUp0qaAQcgvhfgF0vnOSB5CbP5RSYE5TuLu6gh+iQTrBI Syl+9UaopkbQDRsg+XRfie+kUxQgldUAFvFmu6sM6FTbw0KGkrsOajwpF/Fu5hSV Ucov4Lzrrxkok5FzWPkVtMalLZ4Du+ZUYG//10WZg+HdrIwx3m2wxrFIkZaMKxv4 ZkIMsb6DUPUZqy8qZpMzIqvDzx3iYEWWfBOCJWBjs8/V1mAuUu6TBCKAJpvfX6bU hNrCbnrpuxuCCPJnj9sXkBDvl5rcyfshTtKl3NoBrRRDuUHWsJWzsKvBQtwN46vF CbF0aXOozihtmmcMpFFeDIj6p/5qlaJtslegtfv2zoztc3e2ituOjqFQ/I5pplvo p8EGwCI1xTGF0BTatcSV1+lLNeONhhAtwliV13nPSH1o4yxoZ+xZTZq4+9ylw7dq yV3BQM11U6OyAPE1G6EX0PgFvLm25sGTJq9TKXs9yWPRit9vHcOCXSGn8osn4SMg Puqpk+3M9xR8XDPJiBjkxcSnt9+EDNwpthTzgUEoyM6dY8nvWA== -----END CERTIFICATE----- - \"$$SCONE::remote_validation_service$$\" update : - NONE This policy will allow read requests from the creator, a user whose public certificate has hash 1809fafa119b97db77a43562c5241b3db33d21a85516e35ebe0a19bf0e3d29ee , a user whose certificate is specified in the session description and a user whose credential is taken from the secret named remote_validation_service (which may be imported from another session). No one is allowed to update the session description.","title":"Access Control"},{"location":"CAS_session_lang_0_2/#referencing-other-sessions","text":"In some cases it may be necessary to reference other sessions, e.g. when exporting or importing volumes or secrets . This is possible using the given session ID format: <session name>[:<session hash>] Access to/from remote sessions will be verified according to this ID. The hash part is optional. Examples: my_cloud_service my_cloud_service:408c03d53e8689062dff5fa21866c173fd482351df47767371556bec395241c6","title":"Referencing Other Sessions"},{"location":"CAS_session_lang_0_3/","text":"SCONE Session Language (v0.3) The SCONE session language is used to create session descriptions that entail all security-relevant details of a SCONE application. A SCONE application can consist of one or more session descriptions . Each session description defines a set of services that are part of the application, secrets that are securely stored and passed to the services, images that define which regions of a container (image) are encrypted or authenticated , volumes which are like docker volumes but encrypted, and access policy that defines who can read or modify the session description. The session language is a subset of YAML , i.e., a session description is valid YAML. It is similar to and takes its bearing from docker-compose files . As a session description is typically stored in a file, we use session file as a somewhat interchangeable synonym for session description. We use the terms session description and session policy (or just policy ) interchangeably. Changes since version 0.2 The format by which other sessions are referenced for export/import purposes has changed (see Referencing Other Sessions ) Secrets can now be exported/imported to/from sessions located on another CAS Certificates can now be specified as explicit values for x509 secrets. x509 secrets now require a private-key secret (unless imported or given an explicit value). This allows more fine-grained export rules (exporting only the certificate, but not the private key) and issuing multiple certificates using the same key x509 secrets have new parameters: private_key , common_name , endpoint , dns , issuer , valid_for Access Control Policies now use certificate keys instead of entire certificates - certificate hashes have been replaced by certificate key hashes Session Description Hash Each session description has a unique hash value, in short just session hash . This value is used to reference the session in multiple situations. For example, when a session shall be updated the hash of the predecessor session has to be provided to ensure continuity, i.e., no lost-updates. The session hash is deterministically calculated from the session description, such that any modification to the description would yield a different hash. Moreover, the calculation is cryptographically sound in the sense that a malicious operator can not come up with a modified session description that has the same session hash, i.e., it is Preimage-Attack resistant. The exact procedure of how the session hash is obtained is an implementation detail. Session Description Structure There are a number of top-level keys in a session file. Version This document describes version 0.3. version : \"0.3\" Note Without a version field, version 0.1 is assumed, make sure to include the field in all session descriptions in order to use version 0.3. Session Name Every session description has to provide a session name , or short just name . The session name is used to reference a session over the course of its lifetime: while a session hash identifies a unique, not mutable description of a session, the session description references by a session name can be changed by the session's owner. Thus, the session name is the primary property with which a session is referenced outside of session management. As sessions will be routeable in a future version of CAS and session names are the primary way of referencing a session, they have to obey certain restrictions regarding the allowed character set. To prevent issues with future use of session names within URIs their characters have to be drawn from the \"Unreserved Characters\" in URIs as defined by RFC3986 . Further, we set a somewhat arbitrary restriction of a maximal length of 128 characters. In PCRE the match clause matching valid session names would be [A-Za-z0-9_\\-.~]{1,128} . name : my-testing-session Providing a session name is mandatory . Predecessor Session If a session description is an update for a previously existing session, i.e. the session's name is already in use, it has to include the previous session's hash to detect and prevent lost updates. predecessor : e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 Note The value has to be 32 hex encoded bytes. When creating a session, no predecessor session exists and hence no predecessor key is permitted. Service Description Services of a SCONE application are described below the services top-level key. A service in a SCONE session is a SCONE program with a specific software state and configuration. For now, there can be an unlimited number of instances of a service. In the current version, the service description specifies the service configuration and which software (code) is allowed to access it. A service configuration consists of the command to be executed, any environment variables and the process working directory from which path resolution of relative paths starts. These properties may contain secrets and would be prone to inspection and manipulation by adversaries. services : - name : my-test-service command : test --arg=value environment : VAR_1 : value pwd : / name To be able to identify a service, every service has to have a name key. As the name will be part of the routeable session identification in the future, its character set and length is restricted in the same way as the session name is. name : my-test-service Providing a service name is mandatory . command The command of a service is a sequence of the program's name plus any command-line arguments that have to be provided for it to deliver the expected service. Note The first element of the command, the program name, should match the actual program's file name. Although not enforced, in the future automatic scheduling will not function properly if the provided program name does not match. command : test --arg=value The command is split at whitespace characters. If spaces should be preserved for an argument, either quote the argument (with single or double quotes): command : mysql -e \"show databases;\" or use a list instead: command : [ \"mysql\" , \"-e\" , \"show databases;\" ] Note Command arguments have to be C-String compatible, i.e., they are not allowed to contain a NULL byte. environment The environment variables provide a process with values that are needed to provide the expected service. As we cannot trust pre-existing environment variables, only the environment specified in the session policy will be available to the SCONE program. environment : VAR_1 : value Note Variable names and values have to be C-String compatible, i.e. they are not allowed to contain the NULL byte. Moreover, variable names are not allowed to contain the equal (=) sign. Environment variables that are consumed during enclave creation (e.g., SCONE_HEAP ), or used by the SCONE runtime (e.g., SCONE_CAS_ADDR ) should not be included in the list of environment variables for the service as they will have no effect. Instead, they should be provided directly during program invocation. Note that the value of SCONE_HEAP affects mrenclave and via mrenclave , a session description can also limit the set of permitted SCONE_HEAP values. pwd The process working directory (pwd) , or current working directory (cwd), is the directory from which relative paths are resolved. For example, a program writing a private key to private.key with pwd of /encrypted/ would resolve actually write to /encrypted/private.key . Changing the pwd to /plaintext/ before the writing would lead it to write the private key to /plaintext/private.key instead. To prevent this kind of manipulation the pwd of a SCONE service has to be specified explicitly in the session description. pwd : /home/user/scone/encrypted Note Please note that: - The specified directory has to exist in the environment the SCONE program will be executed in. If it is not found, the program cannot start. - The provided value has to be C-String compatible, i.e. it is not allowed to contain the NULL byte. - The allowed character may be further restricted by the used file system. Image A service can optionally use an image, which has to be specified as part of the session (see Section Images ). services : - name : my_service image_name : my_image Images must be specified if their volumes or secret injection files should be used by a service. Attestation Attestation is the process of verifying a remote SCONE program's integrity ensuring it has not been (maliciously) manipulated. SCONE programs have to engage into attestation with CAS to be allowed to access their service's configuration and secrets. That is, only SCONE programs that can show (using attestation) that they satisfy a certain attestation policy are provided with arguments, environment variables, and secrets. The service-level attestation configuration depends on the session's attestation security settings . If attestation has been disabled for the whole session, then the following sections do not apply. Measurement-based Attestation Measurement-based attestation gives access to service secrets based on the enclave measurement value, abbreviated as mrenclave . An enclave measurement value uniquely identifies its initial state. The measurement value of a SCONE program can be determined by running it either with SCONE_VERSION or SCONE_HASH environment variables set. For example, if you have an executable go-webserver you can determine its measurement value by executing: $ SCONE_HASH = 1 ./go-webserver d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 Any modifications to the executable, malicious or not, will void access capabilities. A service can define a list of mrenclaves that are allowed to get access to the configuration as follows: mrenclaves : [ d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 ] For example, you can define mrenclave s for different permitted HEAP_SIZE s. Also, to permit a smooth transition to new service versions, you can first add the mrenclaves of the new version, upgrade the services, and then remove the mrenclave s of the old service version. Note The enclave measurement value ( mrenclave ) has to be a sequence of 32 hex encoded bytes. mrenclave is mandatory . Platform-based Attestation Platform-based attestation gives access to services that are deployed on a specific platform. This is not sufficiently secure on its own to prevent program manipulation! Thus, platform-based attestation should always be combined with the measurement-based attestation. The platform identity is the public key of the SCONE Quoting Enclave (SCONE QE), which is part of the SCONE Local Attestation Service (LAS) . The SCONE QE will have a unique public key on each platform. Platforms that are allowed to get access to the configuration can be specified as follows: platforms : [ 23653F9C4F55E92625752EA1883384531F28679D3E708EDF3BD535A36AEC6B2F ] Platform-based attestation requires the SCONE attestation scheme, which is used by default. Note The platform identity has to be a sequence of 32 hex-encoded bytes (i.e., 64 characters). Secrets Secrets are defined in the secrets section of the session. Each secret is uniquely identified with a name. The values of secrets are either generated by CAS or are explicitly given in the session. The former has the advantage that the secret value can be ensured to be never known by any human. Secret Kinds In most cases, the kind of a secret has to be specified as well. This is necessary for CAS to know how to generate its value or how to interpret the value provided in the session, in the case of explicit secrets . We support these four kinds of secrets: ascii , binary , service leaf and intermediate certificates . ASCII Secrets An ASCII secret is a string compromised of ASCII characters guaranteed not to contain NULL bytes. Explicit Secret Values In case of an explicit secret, the value can contain all characters that can be put into a string in YAML, for example: secrets : - name : my_explicit_ascii_secret kind : ascii value : \"secret-value\" Note that explicit secret values can contain any character that can be represented in YAML. In particular, newline and escape sequences can be put into such a secret. The only exception to this rule is the NULL byte that can not be part of an ascii secret. Be sure that your application can handle these edge-cases. Generated Secret Values If no value field is specified, CAS will generate a secret value on-demand. The generated string will only contain printable ascii characters. In this case, you may specify the length of the string to generate with the size field. If no size is given, a default length of 20 will be used. secrets : - name : my_generate_ascii_secret kind : ascii size : 12 Binary Secrets binary secrets behave similar to ascii secrets except they can contain NULL bytes, generated values are not restricted to printable ascii characters, and explicit secret values have to be specified encoded in hex. secrets : - name : my_generated_binary_secret kind : binary size : 32 - name : my_explicit_binary_secret kind : binary value : DEADBEEF Private Keys Private keys are required when generating X.509 certificates (refer to the next section). Possession of a certificate's private key is necessary in order to sign other certificates or use them as e.g. TLS server or client certificates. Defining an automatically generated private key is simple: secrets : - name : my_tls_server_private_key kind : private-key Generated private keys use the NIST P-256 elliptic curve. Alternatively, a PEM-encoded PKCS#8 P-256 EC private key may also be specified explicitly: secrets : - name : my_tls_server_private_key kind : private-key value : | -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- Private keys are accessible in PEM-encoded PKCS#8 format. Certificates CAS is able to generate and manage X.509v3 certificates, making it easy to construct a Public Key Infrastructure (PKI). There are two kinds of X.509 certificates that CAS supports: End-entity and Certification Authority (CA) certificates. Their main difference is in their capability to sign other certificates. End-entity certificates are indicated by kind x509 , CA certificates by x509-ca . Additional parameters can be specified for certificates: private_key : Unless imported or given an explicit value, each certificate requires a private key, referencing another private-key secret. issuer : Optional reference to another x509-ca certificate which signs this certificate. If omitted, generated certificates will be self-signed. Note that access to the issuer's private key is required for signing unless an explicit certificate value is specified. common_name : An optional certificate common name. If omitted, the first dns name (see below) will be used as the common name. If no DNS name has been specified, the secret's name will be used as the common name. endpoint : Optional, must be either server or client when specified. When specified, an Extended Key Usage (EKU) extension will be added to the generated certificate. If omitted, no EKU extension will be added. Only applicable to end-entity certificates. dns : Optional DNS name or list of DNS names that will be included in the Subject Alternative Name (SAN) extension of the generated certificate. If omitted, no SAN extension will be added. Only applicable to end-entity certificates. Cannot be used when the certificate has a client endpoint. valid_for : Optional duration (e.g.: 1 year or 59d ) for which generated certificates are valid. Certificates will automatically be re-generated before they expire. If omitted, generated certificates stay valid for a virtually unlimited amount of time. Not applicable to imported certificates or certificates with an explicit value . Example: secrets : - name : my_ca_certificate kind : x509-ca private_key : my_ca_private_key common_name : \"My own CA\" - name : my_server_certificate kind : x509 private_key : my_server_certificate_private_key issuer : my_ca_certificate common_name : \"example.com\" valid_for : 90 days endpoint : server dns : - example.com - db.example.com - \"*.api.example.com\" By default, certificates will be generated by CAS, but they can also be given an explicit PEM-encoded value instead: secrets : - name : my_server_certificate kind : x509 value : | -----BEGIN CERTIFICATE----- MIIB0TCCAXigAwIBAgIUX4hfQWl+PKcrmOFW8phCO1vtQpUwCgYIKoZIzj0EAwIw HzEdMBsGA1UEAwwUVGVzdCBJbnRlcm1lZGlhdGUgQ0EwIBcNMjAwNTA2MTQ1NTAx WhgPMzAxOTA5MDcxNDU1MDFaMBYxFDASBgNVBAMMC2V4YW1wbGUuY29tMFkwEwYH KoZIzj0CAQYIKoZIzj0DAQcDQgAE1gqmACizRH9ENwun/rkmqoCjxf6NPJNHTpYg y8D5UOy5HNZXSi6ZNNL1x89d6UZfrYfDrf/PwH5LOhrgfkm42aOBmDCBlTAdBgNV HQ4EFgQUw3LfMbqwEJJRr5v7itg4A5jR0ngwHwYDVR0jBBgwFoAUHgrspKzSFC8r 0/ygrsQIGA+Ft6MwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMB BggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMBYGA1UdEQQPMA2CC2V4YW1wbGUuY29t MAoGCCqGSM49BAMCA0cAMEQCIAC7fWVRBG3mBzfyKf/WidYVL9IcAOdYsElgHTKb 5h5HAiBlXklXq2lXo3srdQNJ4I8QJwWMYLIZUUw7fmWKOaireg== -----END CERTIFICATE----- Certificates differ from other secrets in terms of access. Please see the Secret Access chapter for the details. Certificate Verification Failure If the verification of the generated certificate fails, this is often an indication that the common_name or the dns names ( Subject Alternative Name ) are not matching the DNS name of the service that use this certificate. Secret Access Secrets are injected via standard means (program arguments, environment, and configuration files) into a service. Secret values are not directly included in a service's configuration as this would bound the service to specific secrets. Instead placeholder variables are put that are replaced during runtime in the service. These placeholders are of the form $$SCONE::secret_name$$ . For example, to inject a secret into a service's program arguments its command field in the session description could look like this: [ ... ] command : service_name --private-key $$SCONE::service_private_key$$ --non-confidential-argument 1234 --non-confidential-flag [ ... ] In the above example, the command 's program arguments will be presented to the service with $$SCONE::service_private_key$$ replaced by the actual value of service_private_key . To inject secrets into a service's environment the session description could contain this: [ ... ] environment : FILE_ENCRYPTION_KEY : \"$$SCONE::MY_SERVICES_ENCRYPTION_SECRET$$\" [ ... ] Note Secrets referred to ( service_private_key and MY_SERVICES_ENCRYPTION_SECRET in the above example) have to be either defined in the secrets section of the session description or imported from another session. Binary Secret Values By default, when using $$SCONE::my_secret$$ , binary secrets are injected in binary form. When working with environment variables or command line arguments, a hexadecimal representation may be preferable: $$SCONE::my_secret.hex$$ Certificate Values Certificate secrets may be composed of multiple values: First off the certificate itself, optionally (if accessible) their private key, and lastly they may have an attached certificate chain. These values are available with suffixes to the secret name in the placeholder. Consider a certificate with name my_cert , the following placeholders are available for secret consumption: * $$SCONE::my_cert.crt$$ delivers the certificate in PEM-encoding, * $$SCONE::my_cert.key$$ delivers the certificate's private key in PEM-encoding, and * $$SCONE::my_cert.chain$$ delivers the certificate's chain in PEM-encoding, with the root CA certificate first and the certificate itself last (included). Root CA certificate and topmost intermediate CA certificates may be omitted if they are not known or the certificate is self-signed. Note that the .key suffix is not guaranteed to exist, it is for instance not available when importing certificates from another session which does not grant access to the certificates' private keys. Secret Injection Files A secret injection file is a file in the file system that will be updated with secrets received by the runtime from CAS. They use the same secret placeholders that can also be used in program arguments and environment variables. For example, imagine a service configuration file containing a password. Simply writing the password into the file in cleartext would leak it - that is not an option. Using SCONE's filesystem shield to encrypt the file complicates the setup, and - if distributed as part of an image - would require users to change the encryption keys in order to protect their individual passwords. Instead, a secret injection file allows to specify a placeholder for the password, which will be dynamically replaced at runtime through the means of secret injection: /etc/mysql/my.cnf : [client] user=mysqluser password=$$SCONE::mysqlpass$$ The path to this configuration file must be specified as part of an image (see Images ). The corresponding session would look like this: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - /etc/mysql/my.cnf Alternatively, the file's content may be specified within the session: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ The content specified in the session file can contain multiline strings. This way, entire configuration files can be embedded in the session description, even if no secret injection is required. Producing valid multiline strings in YAML can be challenging - https://yaml-multiline.info/ can be of great help to find the desired syntax. Secret injection files are prepared during SCONE runtime initialization. If the file content is not provided in the session, the file at the specified path is opened, potentially through SCONE's filesystem shield, and read into memory. The secret injection is applied and the resulting file is put into SCONE's in-memory file system at the specified path. Any application requests regarding this file are served from this in-memory file system. Thus, modifications to secret injection files are not propagated into the file system and are not persistent across program invocations. Secret Sharing Session owners can decide to share their secrets with other parties to enable collaboration. For example, database operators could use TLS to implement access control to databases. They would define an access certificate, configure the database to only allow connections from said certificate and export it to the database client: name : database_operator secrets : - name : database_access_key kind : private-key export : session : database_client - name : database_access_certificate kind : x509 private_key : database_access_key export : session : database_client The secret owner might also specify multiple receiving sessions at once: name : database_operator secrets : - name : database_access_key kind : private-key export : - session : database_client - session : another_client - name : database_access_certificate kind : x509 private_key : database_access_key export : - session : database_client - session : another_client Furthermore, the export might be restricted to certain instances of the importing session. For more details, see the concrete SCONE ID format by which other sessions can be referenced in Referencing Other Sessions . The database client, on the other hand, would import it and could use it in their session as if it was their own certificate: name : database_client secrets : - name : database_access_certificate kind : x509 # optional import : session : database_operator secret : database_access_certificate On the importing side, the kind of a secret can be optionally defined to ensure imported secrets match a specific form, but this is not strictly necessary. When importing certificates, the associated private-key does not have to be imported explicitly - it will be available automatically if the exporting session exports the private key as well. In very specific cases, secrets may also be made public (exported to anyone without authentication ) - this may be useful when, for example, defining certificate key hashes: name : policy_checker secrets : - name : certificate_hash kind : ascii value : \"4sEY84YhUKT7Q7m4qUj2pmQMxFvZK5XpDyJ3QR6mETVRDkyren\" export_public : true This hash can be used in another session's access control policy through secret substitution (see Access Control ): name : checked_session secrets : - name : policy_checker_certificate_hash import : session : policy_checker secret : certificate_hash access_policy : read : - \"$$SCONE::policy_checker_certificate_hash\" Warning By using export_public: true , the whole world will be able to see the secret value. Make sure this is your intention. These secrets may also be queried through the /v1/values CAS REST API endpoint. Secret Migration When uploading a new session which has a predecessor session, secret migration takes place. In short: ASCII and binary secret values which were generated as part of the old session will be kept when the new session defines a secret with the same name and compatible kind. Example old session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : binary value : DEADBEEF Example new session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : private-key In the given example, the value of my_generated_ascii_secret will be kept, as it stays an ASCII secret of the same size, whereas foobar will be freshly generated, since its kind changed. Note Generated private keys and certificates will never be kept, they will always be re-generated when updating a session. Secret migration also takes place when using a different session description language version (e.g. 0.2 -> 0.3)! For details, see the session secrets migration documentation . Volumes Similar to Docker, the volumes keyword allows to specify file system regions that can be mounted into the file system in addition to the regions specified by the main file system protection file of a service. Each volume has an arbitrary but unique name . In order to grant services access to a volume, it first has to be included in an image (see section Images ). Subsequently, the image can be specified for a service (see section Service Description ), and all image-defined volumes will be mounted automatically. Note The volume name is not the volume's mount point. The latter is defined as part of an image . Each volume can have a file system protection file key ( fspf_key ) and file system protection file tag ( fspf_tag ): volumes : - name : my_database fspf_key : f843051d21afa9e52a5b54a708a8032bc49581e982696a81393b8da4a32d00b8 fspf_tag : 8d8fbe332fb9c893020be791ccd3e8a8 fspf_key is the key used to encrypt volume.fspf (file system protection file for the volume) and fspf_tag describes the initial state of the volume. On each volume update (e.g., creation of a file in the region or update of an existing file), the SCONE runtime will send a new fspf_tag to CAS to ensure integrity and freshness of the volume state. If neither fspf_key nor fspf_tag are specified, a volume will be automatically initialized during the first use: volumes : - name : my_database The volume will contain a single encrypted region, and a new key will be generated by CAS to encrypt volume.fspf . Once initialized, CAS and the runtime will work together to track the updates of the volume, similar to a regular volume. This ensures that a volume can be initialized only once. Use of automatically initialized volumes ensures that the key for the file system is only visible inside of CAS and the application(s) that get access to this volume, i.e., no system administrator will ever see the volume key. Volume Sharing Volumes may be exported to other sessions, which implies authorizing the other sessions to decrypt and read existing files or encrypt and store new files: volumes : - name : my_database export : session : another-session or volumes : - name : my_database export : - session : another-session - session : foobar session_hash : 668e9aaba22c7631bbcc89b627d77e53539bcaade9e7c2c08242f56aab272088 The concrete SCONE ID format by which other sessions can be referenced is described in section Referencing Other Sessions . Note that volumes can only be exported to local sessions on the same CAS, not to sessions on a remote CAS. Similarly, volumes may be imported from another session (in which case fspf key/tag and export list must be omitted): volumes : - name : their_database import : session : the_exporting_session volume : my_database When exporting a volume, it is possible to restrict which kind of modifications an importing session is allowed to perform. Section Image Volumes demonstrates how volume access can be restricted for local services; the same is also possible for volume exports by using the update_policy key: volumes : - name : my_database export : - to : session : foobar session_hash : 668e9aaba22c7631bbcc89b627d77e53539bcaade9e7c2c08242f56aab272088 update_policy : ephemeral Note that the SCONE ID ( session and session_hash ) needed to be moved to a new to: section! update_policy can have any of the following values: ephemeral : Importing sessions may only use the ephemeral policy. This implies that they are not allowed to alter the volume's state on the CAS (please read Image Volumes for further explanation). Any tag policy specified by the importing session is ignored. rollback_protected : This is the default setting if update_policy is omitted. Importing sessions can use either a rollback_protected or an ephemeral policy at their discretion. no_rollback_protection : Importing sessions are allowed to use a rollback_protected , ephemeral or no_rollback_protection policy at their discretion. This option is dangerous, as it also allows third parties to perform rollbacks unnoticed. It should not be necessary for most use cases. Images The images keyword allows specifying images usable by services (see section Service Description ). Each image has an arbitrary but unique name : images : - name : my_image Image Volumes Images may define access to volumes , by referencing a previously declared volume's name and giving it a mount point ( path ): volumes : - name : my_database images : - name : my_image volumes : - name : my_database path : /media/database Note The given path must already exist in the filesystem, e.g., through a mounted docker volume. The information provided as part of the session description are only used to encrypt and authenticate all of the volume's files. The CAS does not actually store the encrypted files. By default, volumes are rollback-protected: Attempting to restore an old volume state and letting a service read files from or write files to this old state will be detected and prevented. In specific cases, however, this may not be the desired behavior. The update_policy key can be used to change it: volumes : - name : my_database images : - name : my_image volumes : - name : my_database path : /media/database update_policy : no_rollback_protection The following values are possible as a volume update_policy policy: rollback_protected : This is the default setting if update_policy is omitted. Services are allowed to alter the volume state, any rollback will be detected and lead to an error upon access. This ensures a coherent, linear history of volume changes. ephemeral : Any changes done to the volume state will be ignored by CAS. This can be useful when a volume is supposed to remain in a specific state - an initialization service could use an image with volume update_policy: rollback_protected to prepare the volume, and all other services can use update_policy: ephemeral in order to ignore further modifications. Note that this does not prevent services from modifying files and directories in the volume's path! This will still work as long as the service is running. Once it is restarted, however, CAS will detect the modification and refuse any access to the modified volume. Therefore, if you expect services to make changes to the volume, it is a good idea to back up the initialized volume, and restore this backup once the service is stopped. This ensures that new instances of the service are able to access the volume in its expected state. no_rollback_protection : Services are allowed to alter the volume's state and rollback prevention checks are disabled. This option is dangerous, as it also allows third parties to perform rollbacks unnoticed. It should not be necessary for most use cases. Note When using an imported volume, the exporting session may have applied restrictions on the volume's update_policy . Attempting to use an unauthorized update_policy will lead to errors upon volume access. For details, please refer to section Volume Sharing . Secret Injection Files Images may also contain secret injection files, a way to inject secrets into configuration files: images : - name : proxy_image injection_files : - /etc/nginx/nginx.conf - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ For details, refer to section Secret Injection Files . Access Control Any operation on a session description requires permission. If the entity requesting a certain operation is not explicitly permitted to perform said operation, the request will fail. The access_policy keyword allows specifying lists of entities that are allowed to perform the following operations: read : permit to read the policy - without the secrets update : permit to update the policy. Note that entities listed here must also be present under read . Granting permission to a certain entity to perform one of these operations involves adding their client certificate public key to the list of authorized entities. A certificate with this key shall be used when establishing a connection to CAS (see API Documentation, Authentication section ). TLS ensures that the client is in possession of the corresponding private key. CAS uses key-based authentication instead of whole certificate authentication to ensure that certificates can be renewed without problems - otherwise, users could be locked out of the session when their certificate expires. When using the scone CLI, the user certificate can be shown by running scone self show . Beside public certificates, the following values can be used: Public key hash of a certificate. This hash can be calculated by using the SCONE CLI: scone self show shows the hash of the CLI client identity, scone cert show-key-hash \"path_to_certificate_file_in_pem_format\" shows the key hash for any certificate file. A valid hash looks similar to: 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 CREATOR keyword: permit access to the creator of the policy: this is the public key of the TLS client certificate used when creating this session ANY keyword: permit access to any entity. If ANY is specified, there must be no other entries in the list for this operation NONE keyword: deny all requests for a particular operation. If NONE is specified, there must be no other entries in the list for this operation $$SCONE::secret-name$$ will dynamically use the value of a secret with the given name ( secret-name ) at permission evaluation time. The replaced value must be either CREATOR (ascii), a certificate key hash (ascii) or certificate as defined above. It is possible to use explicit secrets, generated secrets, and imported secrets. When referencing X.509 certificates, the trailing .crt after the secret name can be specified, but may also be omitted. If the mentioned secret does not exist, cannot be read, or has an incompatible value, it will be ignored, but an error message will be shown on unsuccessful authentication attempts. By default, the access policy is defined as follows: access_policy : read : - CREATOR update : - CREATOR Default values will be used for operations not explicitly specified in a session description. Example policy: access_policy : read : - CREATOR - 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 - | -----BEGIN CERTIFICATE----- MIIFwTCCA6mgAwIBAgIUCF1MVJJ78BIf4WmTE24aAX7NlHowDQYJKoZIhvcNAQEL BQAwcDELMAkGA1UEBhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9y dGxhbmQxFTATBgNVBAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYD VQQDDA93d3cuZXhhbXBsZS5jb20wHhcNMTkwNzIyMTU1NTExWhcNMTkwODIyMTU1 NTExWjBwMQswCQYDVQQGEwJVUzEPMA0GA1UECAwGT3JlZ29uMREwDwYDVQQHDAhQ b3J0bGFuZDEVMBMGA1UECgwMQ29tcGFueSBOYW1lMQwwCgYDVQQLDANPcmcxGDAW BgNVBAMMD3d3dy5leGFtcGxlLmNvbTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCC AgoCggIBALVbVIrBAlzDOztWs9hZr5kvYoUwq/hL7zaMrYKBLQJZFNbhmMaUsW7A Fzj87dzP3xIf4c2r3IGJSukv7hJpaJ2Ykv80i3C7EiFgaDV/+JP9d/GjsvcW20zH mtJcBIkdkqPt1epOtxsMyJGZL+34DoWOqgY7up6nCirr+MeUxYJ/dWBFD1j0iuHl Y+rEMsv4xFBndgLmMQNlcMyXtBgPls4EgnDfnjICqIYMHt6PG+kwoR4tbs+v2Gsl vqldxI7efErZh+kKtjtFxt6qzrypUs9bYgH3tsaUE0xYeK/A2llylJzPOv6vkCqg vPOJETcZyoeH46niITdPssYr4yPQOxn/a7WS+7Mn2y6o5z4Q+DkB96lzUyvVJnwO aorzec0PaB/qqYrHqVfftMu4thMwHGB8CrGUiq/ImHPWkfobyVcMYJ0/LaLSDHFj 1hN36VkzWqQcCM6ymhjx9Lpfzzxna5910jE86zb1cMnD/eAAd90jpJvGJN43Hw40 MIvjYBunOy9P3ah0kgCk7gW0oKlYHxugv8pZVHMwU1HFIdwYvlGd09XHFDyj9tul eX8zaVwaNeLUrMdJN5Ct1HX16RpnpaIMwwExzXgsZ01BQcfIcGWGbvBfH2C86klt SuG7M6kxk4XgIIlwTSGk7qJlfd4s8PD1fVJNKvJZwXXoQBy4hCrTAgMBAAGjUzBR MB0GA1UdDgQWBBRSUKop9QDGmSdLCfzWlBIF5ClNVDAfBgNVHSMEGDAWgBRSUKop 9QDGmSdLCfzWlBIF5ClNVDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUA A4ICAQAny4QmzvCza0TQUuh7YIxtkBjJoyTSDGmr5W/4AWKsoZJM9QxzWeIAXhWD UPH2TfwcNX/ovZiSDQaGduRL67Wk8lbp1MvjACMEtLRjsbnYtGFdhit0fR97B5Y6 d06Ka/NXgPTJorXx8WSWUp0qaAQcgvhfgF0vnOSB5CbP5RSYE5TuLu6gh+iQTrBI Syl+9UaopkbQDRsg+XRfie+kUxQgldUAFvFmu6sM6FTbw0KGkrsOajwpF/Fu5hSV Ucov4Lzrrxkok5FzWPkVtMalLZ4Du+ZUYG//10WZg+HdrIwx3m2wxrFIkZaMKxv4 ZkIMsb6DUPUZqy8qZpMzIqvDzx3iYEWWfBOCJWBjs8/V1mAuUu6TBCKAJpvfX6bU hNrCbnrpuxuCCPJnj9sXkBDvl5rcyfshTtKl3NoBrRRDuUHWsJWzsKvBQtwN46vF CbF0aXOozihtmmcMpFFeDIj6p/5qlaJtslegtfv2zoztc3e2ituOjqFQ/I5pplvo p8EGwCI1xTGF0BTatcSV1+lLNeONhhAtwliV13nPSH1o4yxoZ+xZTZq4+9ylw7dq yV3BQM11U6OyAPE1G6EX0PgFvLm25sGTJq9TKXs9yWPRit9vHcOCXSGn8osn4SMg Puqpk+3M9xR8XDPJiBjkxcSnt9+EDNwpthTzgUEoyM6dY8nvWA== -----END CERTIFICATE----- - \"$$SCONE::remote_validation_service$$\" update : - NONE This policy will allow read requests from the creator, a user whose TLS client certificate has a public key hash 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 , a user whose TLS client certificate has a public key similar to the one of the certificate specified in the session description and a user whose credential is taken from the secret named remote_validation_service (which may be imported from another session). No one is allowed to update the session description. Attestation Security The services' security directly depends on CAS' ability to verify that a service process has not been manipulated (referred to as attestation), as described in the service attestation section . CAS ships with secure defaults, and only allows hardware-attested services running on trustworthy platforms in secure production-mode enclaves. Tolerating hardware attestation issues Sometimes it may be necessary to relax the secure defaults, for instance, when: Services should run in a testing environment for debugging purposes A trade-off is being made between security and performance, like enabling Hyper-Threading (which opens an Information Disclosure Side Channel) A platform with known security vulnerabilities is used in a testing environment Example: security : attestation : tolerate : [ debug-mode , hyperthreading ] tolerate is an array which can hold the following variants: debug-mode enclaves allow introspection with a debugger. This disables enclave protection, all secrets can be extracted. hyperthreading : Enabled Hyper-Threading opens a Microarchitectural Data Sampling Information Disclosure Side Channel on Intel SGX platforms. If the performance penalty can be tolerated, disabling Hyper-Threading instead of ignoring the issue may be viable. See https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00233.html insecure-igpu : An enabled integrated GPU could be exploited to disclose information on Intel SGX platforms. If not required for operation, disable the IGPU instead of tolerating this issue. See https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00219.html software-hardening-needed : The service program needs to be re-compiled with different compiler settings to protect against exploits. This is a category for a collection of Intel SGX advisories, which need to be specified separately (see below). insecure-configuration : The platform needs to be reconfigured to protect against certain kinds of attacks. This is a category for a collection of Intel SGX advisories, which need to be specified separately (see below). outdated-tcb : The Trusted Computing Base (TCB) of the platform, i.e. firmware or microcode, needs to be updated due to security flaws. This is a category for a collection of Intel SGX advisories, which need to be specified separately (see below). When running a service on a platform that is affected by one of the above problems, attestation will fail with an error, unless they have been specified in the tolerate list. The error message will contain instructions on how to change attestation security settings in order to ignore the specific problems. Note Instead of ignoring platform security issues, solving them should always be preferred. Often, this involves updating the platform's firmware or CPU microcode. Sometimes, it can be necessary to change BIOS/UEFI settings or recompile the service with different settings. The error message as well as linked Intel advisories contain more details. Warning Displayed attestation error messages are not authenticated, and may be forged by a MitM attacker. It is advisable to critically review the suggested configuration changes as well as Intel advisories. Intel will release advisories when becoming aware of new security issues with SGX-enabled processors. Some of the problems listed in these advisories are encountered more frequently, in which case SCONE/CAS provide shortcuts to ignore them (like hyperthreading ). Others may be more severe or very recent. In order to ignore them, they and their associated category ( software-hardening-needed / insecure-configuration / outdated-tcb ) need to be specified explicitly. Example: security : attestation : tolerate : [ outdated-tcb ] ignore_advisories : [ INTEL-SA-00161 , INTEL-SA-00270 ] When in debug-mode , all advisories can be ignored by using a wildcard, simplifying the workflow in testing environments: security : attestation : tolerate : [ debug-mode , outdated-tcb ] ignore_advisories : \"*\" As this silently ignores newly released platform advisories too, wildcards cannot be used in (non-debug) production mode. Trusted SCONE Quoting Enclaves By default, the system uses the SCONE attestation scheme. This scheme is built around a separate quoting enclave (QE), into which trust has to be established. This is done automatically using EPID-based attestation for a set of known trustworthy QE measurements embedded into the CAS software. If EPID-based attestation is not available, or you want to use custom QEs, the QE public keys can be manually specified as trustworthy: security : attestation : trusted_scone_qe_pubkeys : [ \"E37F149AE30896E314A2859874C5A9C7803FB3187B99F5D08E526B1C0396507C\" ] The QE public key can be found in the log output of the Local Attestation Service (LAS). Specified keys are an additional trust anchor, and do not replace the trusted built-in QE measurements. To trust the given keys exclusively, additionally enable platform-based attestation for individual services. Disabling attestation Attestation can be completely disabled: security : attestation : mode : none # Default mode is 'hardware' This may be useful in testing environments where no hardware-based attestation is available. Warning Disabling attestation implies that there is no service authentication being performed at all . Everybody with access to CAS can access all secrets of all of this session's services. Referencing Other Sessions In some cases it may be necessary to reference other sessions, e.g. when exporting or importing volumes or secrets . This is possible by using a SCONE ID. The following sections describe the format and content of this ID. Local CAS Sessions Referencing another session on the same CAS is simple: session: <session name> - e.g. session: my-database Note that, depending on the context, you may also have to supply a secret, volume or service name, e.g.: session : my-database secret : db-encryption-key By using this simple form, you trust the session's owner, and accept that their session configuration may change over time. If you want to trust a specific session configuration instead, you have to additionally specify a session hash : session : <session name> session_hash : <session hash-key> e.g.: session : my-database session_hash : a51c14dd7029d2ef54a50a9e26efcdd37c4971b5b62cb6d244c9216f80b6eadf Exporting/Importing elements to/from the other session will be prevented by the CAS when the currently active session does not match the specified hash. Note that, in this case, your own session may cease working when importing secrets or volumes from another session whose hash has changed, requiring you to update the session. Sessions on another CAS Referencing sessions on another CAS requires more information - the remote CAS' address and a key to authenticate it: cas_url : <CAS address> cas_key : <CAS key hash> session : <session name> e.g.: cas_url : cas.example.com cas_key : 46YyxrywJ8PFRruWX8YLxa9q4axxYJgTbA81tv7NBcJfn43DQt session : company-storage The cas_key is used for authentication and ensures that exports/imports will only be performed to/from the correct CAS. It is of utmost importance to specify a verified, correct key. When a CAS was attested using the CLI, you can query its verified key by using scone cas show-identification -c . Similarly, when attesting a CAS, you can supply the key received from another session's owner by using scone cas attest -c <CAS key hash> . By using cas_key , you trust the remote CAS' owner and allow them to perform software updates without further confirmation. Most of the time, this is an adequate solution. In specific cases, when trusting the remote CAS' owner is not an option, it may be necessary to pin a specific CAS software revision instead. This may be done by using the cas_software_key instead of a cas_key : cas_url : <CAS address> cas_software_key : <CAS key hash> session : <session name> e.g.: cas_url : cas.example.com cas_software_key : 3AC5RSbL73aVVf98m2UMTN2BFsQv6eQufi5BGBxrG8awxP4ygQ session : company-storage In this case, exporting/importing elements to/from the other session will be prevented by the local CAS when the software of the remote CAS changes. The same limitations as specified for the usage of a session_hash apply here as well. The software key can be retrieved or specified on the CLI by using scone cas show-identification -s and scone cas attest -s <CAS software key hash> respectively. URL short form Instead of using YAML syntax, you can also fit all SCONE ID fields on a single line, forming a SCONE URL. For instance, cas_url : cas.example.com cas_software_key : 3AC5RSbL73aVVf98m2UMTN2BFsQv6eQufi5BGBxrG8awxP4ygQ session : company-storage can also be written as cas.example.com/cas_software_key=3AC5RSbL73aVVf98m2UMTN2BFsQv6eQufi5BGBxrG8awxP4ygQ,session=company-storage This may be useful when referencing the same CAS multiple times, as the CAS URL and keys can be extracted into a common variable, example: export : - $OTHER_CAS,session=A - $OTHER_CAS,session=B - $OTHER_CAS,session=C For replacing the variable with a concrete value, please refer to the SCONE CLI documentation. The same format can also be used when referencing sessions on the local CAS. Simply omit the host and keys (but notice the leading slash / ): /session=company-storage,session_hash=a51c14dd7029d2ef54a50a9e26efcdd37c4971b5b62cb6d244c9216f80b6eadf","title":"Policy Language 0.3"},{"location":"CAS_session_lang_0_3/#scone-session-language-v03","text":"The SCONE session language is used to create session descriptions that entail all security-relevant details of a SCONE application. A SCONE application can consist of one or more session descriptions . Each session description defines a set of services that are part of the application, secrets that are securely stored and passed to the services, images that define which regions of a container (image) are encrypted or authenticated , volumes which are like docker volumes but encrypted, and access policy that defines who can read or modify the session description. The session language is a subset of YAML , i.e., a session description is valid YAML. It is similar to and takes its bearing from docker-compose files . As a session description is typically stored in a file, we use session file as a somewhat interchangeable synonym for session description. We use the terms session description and session policy (or just policy ) interchangeably.","title":"SCONE Session Language (v0.3)"},{"location":"CAS_session_lang_0_3/#changes-since-version-02","text":"The format by which other sessions are referenced for export/import purposes has changed (see Referencing Other Sessions ) Secrets can now be exported/imported to/from sessions located on another CAS Certificates can now be specified as explicit values for x509 secrets. x509 secrets now require a private-key secret (unless imported or given an explicit value). This allows more fine-grained export rules (exporting only the certificate, but not the private key) and issuing multiple certificates using the same key x509 secrets have new parameters: private_key , common_name , endpoint , dns , issuer , valid_for Access Control Policies now use certificate keys instead of entire certificates - certificate hashes have been replaced by certificate key hashes","title":"Changes since version 0.2"},{"location":"CAS_session_lang_0_3/#session-description-hash","text":"Each session description has a unique hash value, in short just session hash . This value is used to reference the session in multiple situations. For example, when a session shall be updated the hash of the predecessor session has to be provided to ensure continuity, i.e., no lost-updates. The session hash is deterministically calculated from the session description, such that any modification to the description would yield a different hash. Moreover, the calculation is cryptographically sound in the sense that a malicious operator can not come up with a modified session description that has the same session hash, i.e., it is Preimage-Attack resistant. The exact procedure of how the session hash is obtained is an implementation detail.","title":"Session Description Hash"},{"location":"CAS_session_lang_0_3/#session-description-structure","text":"There are a number of top-level keys in a session file.","title":"Session Description Structure"},{"location":"CAS_session_lang_0_3/#version","text":"This document describes version 0.3. version : \"0.3\" Note Without a version field, version 0.1 is assumed, make sure to include the field in all session descriptions in order to use version 0.3.","title":"Version"},{"location":"CAS_session_lang_0_3/#session-name","text":"Every session description has to provide a session name , or short just name . The session name is used to reference a session over the course of its lifetime: while a session hash identifies a unique, not mutable description of a session, the session description references by a session name can be changed by the session's owner. Thus, the session name is the primary property with which a session is referenced outside of session management. As sessions will be routeable in a future version of CAS and session names are the primary way of referencing a session, they have to obey certain restrictions regarding the allowed character set. To prevent issues with future use of session names within URIs their characters have to be drawn from the \"Unreserved Characters\" in URIs as defined by RFC3986 . Further, we set a somewhat arbitrary restriction of a maximal length of 128 characters. In PCRE the match clause matching valid session names would be [A-Za-z0-9_\\-.~]{1,128} . name : my-testing-session Providing a session name is mandatory .","title":"Session Name"},{"location":"CAS_session_lang_0_3/#predecessor-session","text":"If a session description is an update for a previously existing session, i.e. the session's name is already in use, it has to include the previous session's hash to detect and prevent lost updates. predecessor : e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 Note The value has to be 32 hex encoded bytes. When creating a session, no predecessor session exists and hence no predecessor key is permitted.","title":"Predecessor Session"},{"location":"CAS_session_lang_0_3/#service-description","text":"Services of a SCONE application are described below the services top-level key. A service in a SCONE session is a SCONE program with a specific software state and configuration. For now, there can be an unlimited number of instances of a service. In the current version, the service description specifies the service configuration and which software (code) is allowed to access it. A service configuration consists of the command to be executed, any environment variables and the process working directory from which path resolution of relative paths starts. These properties may contain secrets and would be prone to inspection and manipulation by adversaries. services : - name : my-test-service command : test --arg=value environment : VAR_1 : value pwd : /","title":"Service Description"},{"location":"CAS_session_lang_0_3/#name","text":"To be able to identify a service, every service has to have a name key. As the name will be part of the routeable session identification in the future, its character set and length is restricted in the same way as the session name is. name : my-test-service Providing a service name is mandatory .","title":"name"},{"location":"CAS_session_lang_0_3/#command","text":"The command of a service is a sequence of the program's name plus any command-line arguments that have to be provided for it to deliver the expected service. Note The first element of the command, the program name, should match the actual program's file name. Although not enforced, in the future automatic scheduling will not function properly if the provided program name does not match. command : test --arg=value The command is split at whitespace characters. If spaces should be preserved for an argument, either quote the argument (with single or double quotes): command : mysql -e \"show databases;\" or use a list instead: command : [ \"mysql\" , \"-e\" , \"show databases;\" ] Note Command arguments have to be C-String compatible, i.e., they are not allowed to contain a NULL byte.","title":"command"},{"location":"CAS_session_lang_0_3/#environment","text":"The environment variables provide a process with values that are needed to provide the expected service. As we cannot trust pre-existing environment variables, only the environment specified in the session policy will be available to the SCONE program. environment : VAR_1 : value Note Variable names and values have to be C-String compatible, i.e. they are not allowed to contain the NULL byte. Moreover, variable names are not allowed to contain the equal (=) sign. Environment variables that are consumed during enclave creation (e.g., SCONE_HEAP ), or used by the SCONE runtime (e.g., SCONE_CAS_ADDR ) should not be included in the list of environment variables for the service as they will have no effect. Instead, they should be provided directly during program invocation. Note that the value of SCONE_HEAP affects mrenclave and via mrenclave , a session description can also limit the set of permitted SCONE_HEAP values.","title":"environment"},{"location":"CAS_session_lang_0_3/#pwd","text":"The process working directory (pwd) , or current working directory (cwd), is the directory from which relative paths are resolved. For example, a program writing a private key to private.key with pwd of /encrypted/ would resolve actually write to /encrypted/private.key . Changing the pwd to /plaintext/ before the writing would lead it to write the private key to /plaintext/private.key instead. To prevent this kind of manipulation the pwd of a SCONE service has to be specified explicitly in the session description. pwd : /home/user/scone/encrypted Note Please note that: - The specified directory has to exist in the environment the SCONE program will be executed in. If it is not found, the program cannot start. - The provided value has to be C-String compatible, i.e. it is not allowed to contain the NULL byte. - The allowed character may be further restricted by the used file system.","title":"pwd"},{"location":"CAS_session_lang_0_3/#image","text":"A service can optionally use an image, which has to be specified as part of the session (see Section Images ). services : - name : my_service image_name : my_image Images must be specified if their volumes or secret injection files should be used by a service.","title":"Image"},{"location":"CAS_session_lang_0_3/#attestation","text":"Attestation is the process of verifying a remote SCONE program's integrity ensuring it has not been (maliciously) manipulated. SCONE programs have to engage into attestation with CAS to be allowed to access their service's configuration and secrets. That is, only SCONE programs that can show (using attestation) that they satisfy a certain attestation policy are provided with arguments, environment variables, and secrets. The service-level attestation configuration depends on the session's attestation security settings . If attestation has been disabled for the whole session, then the following sections do not apply.","title":"Attestation"},{"location":"CAS_session_lang_0_3/#measurement-based-attestation","text":"Measurement-based attestation gives access to service secrets based on the enclave measurement value, abbreviated as mrenclave . An enclave measurement value uniquely identifies its initial state. The measurement value of a SCONE program can be determined by running it either with SCONE_VERSION or SCONE_HASH environment variables set. For example, if you have an executable go-webserver you can determine its measurement value by executing: $ SCONE_HASH = 1 ./go-webserver d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 Any modifications to the executable, malicious or not, will void access capabilities. A service can define a list of mrenclaves that are allowed to get access to the configuration as follows: mrenclaves : [ d08b7351eeb8d5c0653d32794218f183ac9c6fa3923b67052752f78a3559de61 ] For example, you can define mrenclave s for different permitted HEAP_SIZE s. Also, to permit a smooth transition to new service versions, you can first add the mrenclaves of the new version, upgrade the services, and then remove the mrenclave s of the old service version. Note The enclave measurement value ( mrenclave ) has to be a sequence of 32 hex encoded bytes. mrenclave is mandatory .","title":"Measurement-based Attestation"},{"location":"CAS_session_lang_0_3/#platform-based-attestation","text":"Platform-based attestation gives access to services that are deployed on a specific platform. This is not sufficiently secure on its own to prevent program manipulation! Thus, platform-based attestation should always be combined with the measurement-based attestation. The platform identity is the public key of the SCONE Quoting Enclave (SCONE QE), which is part of the SCONE Local Attestation Service (LAS) . The SCONE QE will have a unique public key on each platform. Platforms that are allowed to get access to the configuration can be specified as follows: platforms : [ 23653F9C4F55E92625752EA1883384531F28679D3E708EDF3BD535A36AEC6B2F ] Platform-based attestation requires the SCONE attestation scheme, which is used by default. Note The platform identity has to be a sequence of 32 hex-encoded bytes (i.e., 64 characters).","title":"Platform-based Attestation"},{"location":"CAS_session_lang_0_3/#secrets","text":"Secrets are defined in the secrets section of the session. Each secret is uniquely identified with a name. The values of secrets are either generated by CAS or are explicitly given in the session. The former has the advantage that the secret value can be ensured to be never known by any human.","title":"Secrets"},{"location":"CAS_session_lang_0_3/#secret-kinds","text":"In most cases, the kind of a secret has to be specified as well. This is necessary for CAS to know how to generate its value or how to interpret the value provided in the session, in the case of explicit secrets . We support these four kinds of secrets: ascii , binary , service leaf and intermediate certificates .","title":"Secret Kinds"},{"location":"CAS_session_lang_0_3/#ascii-secrets","text":"An ASCII secret is a string compromised of ASCII characters guaranteed not to contain NULL bytes.","title":"ASCII Secrets"},{"location":"CAS_session_lang_0_3/#explicit-secret-values","text":"In case of an explicit secret, the value can contain all characters that can be put into a string in YAML, for example: secrets : - name : my_explicit_ascii_secret kind : ascii value : \"secret-value\" Note that explicit secret values can contain any character that can be represented in YAML. In particular, newline and escape sequences can be put into such a secret. The only exception to this rule is the NULL byte that can not be part of an ascii secret. Be sure that your application can handle these edge-cases.","title":"Explicit Secret Values"},{"location":"CAS_session_lang_0_3/#generated-secret-values","text":"If no value field is specified, CAS will generate a secret value on-demand. The generated string will only contain printable ascii characters. In this case, you may specify the length of the string to generate with the size field. If no size is given, a default length of 20 will be used. secrets : - name : my_generate_ascii_secret kind : ascii size : 12","title":"Generated Secret Values"},{"location":"CAS_session_lang_0_3/#binary-secrets","text":"binary secrets behave similar to ascii secrets except they can contain NULL bytes, generated values are not restricted to printable ascii characters, and explicit secret values have to be specified encoded in hex. secrets : - name : my_generated_binary_secret kind : binary size : 32 - name : my_explicit_binary_secret kind : binary value : DEADBEEF","title":"Binary Secrets"},{"location":"CAS_session_lang_0_3/#private-keys","text":"Private keys are required when generating X.509 certificates (refer to the next section). Possession of a certificate's private key is necessary in order to sign other certificates or use them as e.g. TLS server or client certificates. Defining an automatically generated private key is simple: secrets : - name : my_tls_server_private_key kind : private-key Generated private keys use the NIST P-256 elliptic curve. Alternatively, a PEM-encoded PKCS#8 P-256 EC private key may also be specified explicitly: secrets : - name : my_tls_server_private_key kind : private-key value : | -----BEGIN PRIVATE KEY----- ... -----END PRIVATE KEY----- Private keys are accessible in PEM-encoded PKCS#8 format.","title":"Private Keys"},{"location":"CAS_session_lang_0_3/#certificates","text":"CAS is able to generate and manage X.509v3 certificates, making it easy to construct a Public Key Infrastructure (PKI). There are two kinds of X.509 certificates that CAS supports: End-entity and Certification Authority (CA) certificates. Their main difference is in their capability to sign other certificates. End-entity certificates are indicated by kind x509 , CA certificates by x509-ca . Additional parameters can be specified for certificates: private_key : Unless imported or given an explicit value, each certificate requires a private key, referencing another private-key secret. issuer : Optional reference to another x509-ca certificate which signs this certificate. If omitted, generated certificates will be self-signed. Note that access to the issuer's private key is required for signing unless an explicit certificate value is specified. common_name : An optional certificate common name. If omitted, the first dns name (see below) will be used as the common name. If no DNS name has been specified, the secret's name will be used as the common name. endpoint : Optional, must be either server or client when specified. When specified, an Extended Key Usage (EKU) extension will be added to the generated certificate. If omitted, no EKU extension will be added. Only applicable to end-entity certificates. dns : Optional DNS name or list of DNS names that will be included in the Subject Alternative Name (SAN) extension of the generated certificate. If omitted, no SAN extension will be added. Only applicable to end-entity certificates. Cannot be used when the certificate has a client endpoint. valid_for : Optional duration (e.g.: 1 year or 59d ) for which generated certificates are valid. Certificates will automatically be re-generated before they expire. If omitted, generated certificates stay valid for a virtually unlimited amount of time. Not applicable to imported certificates or certificates with an explicit value . Example: secrets : - name : my_ca_certificate kind : x509-ca private_key : my_ca_private_key common_name : \"My own CA\" - name : my_server_certificate kind : x509 private_key : my_server_certificate_private_key issuer : my_ca_certificate common_name : \"example.com\" valid_for : 90 days endpoint : server dns : - example.com - db.example.com - \"*.api.example.com\" By default, certificates will be generated by CAS, but they can also be given an explicit PEM-encoded value instead: secrets : - name : my_server_certificate kind : x509 value : | -----BEGIN CERTIFICATE----- MIIB0TCCAXigAwIBAgIUX4hfQWl+PKcrmOFW8phCO1vtQpUwCgYIKoZIzj0EAwIw HzEdMBsGA1UEAwwUVGVzdCBJbnRlcm1lZGlhdGUgQ0EwIBcNMjAwNTA2MTQ1NTAx WhgPMzAxOTA5MDcxNDU1MDFaMBYxFDASBgNVBAMMC2V4YW1wbGUuY29tMFkwEwYH KoZIzj0CAQYIKoZIzj0DAQcDQgAE1gqmACizRH9ENwun/rkmqoCjxf6NPJNHTpYg y8D5UOy5HNZXSi6ZNNL1x89d6UZfrYfDrf/PwH5LOhrgfkm42aOBmDCBlTAdBgNV HQ4EFgQUw3LfMbqwEJJRr5v7itg4A5jR0ngwHwYDVR0jBBgwFoAUHgrspKzSFC8r 0/ygrsQIGA+Ft6MwDgYDVR0PAQH/BAQDAgWgMB0GA1UdJQQWMBQGCCsGAQUFBwMB BggrBgEFBQcDAjAMBgNVHRMBAf8EAjAAMBYGA1UdEQQPMA2CC2V4YW1wbGUuY29t MAoGCCqGSM49BAMCA0cAMEQCIAC7fWVRBG3mBzfyKf/WidYVL9IcAOdYsElgHTKb 5h5HAiBlXklXq2lXo3srdQNJ4I8QJwWMYLIZUUw7fmWKOaireg== -----END CERTIFICATE----- Certificates differ from other secrets in terms of access. Please see the Secret Access chapter for the details. Certificate Verification Failure If the verification of the generated certificate fails, this is often an indication that the common_name or the dns names ( Subject Alternative Name ) are not matching the DNS name of the service that use this certificate.","title":"Certificates"},{"location":"CAS_session_lang_0_3/#secret-access","text":"Secrets are injected via standard means (program arguments, environment, and configuration files) into a service. Secret values are not directly included in a service's configuration as this would bound the service to specific secrets. Instead placeholder variables are put that are replaced during runtime in the service. These placeholders are of the form $$SCONE::secret_name$$ . For example, to inject a secret into a service's program arguments its command field in the session description could look like this: [ ... ] command : service_name --private-key $$SCONE::service_private_key$$ --non-confidential-argument 1234 --non-confidential-flag [ ... ] In the above example, the command 's program arguments will be presented to the service with $$SCONE::service_private_key$$ replaced by the actual value of service_private_key . To inject secrets into a service's environment the session description could contain this: [ ... ] environment : FILE_ENCRYPTION_KEY : \"$$SCONE::MY_SERVICES_ENCRYPTION_SECRET$$\" [ ... ] Note Secrets referred to ( service_private_key and MY_SERVICES_ENCRYPTION_SECRET in the above example) have to be either defined in the secrets section of the session description or imported from another session.","title":"Secret Access"},{"location":"CAS_session_lang_0_3/#binary-secret-values","text":"By default, when using $$SCONE::my_secret$$ , binary secrets are injected in binary form. When working with environment variables or command line arguments, a hexadecimal representation may be preferable: $$SCONE::my_secret.hex$$","title":"Binary Secret Values"},{"location":"CAS_session_lang_0_3/#certificate-values","text":"Certificate secrets may be composed of multiple values: First off the certificate itself, optionally (if accessible) their private key, and lastly they may have an attached certificate chain. These values are available with suffixes to the secret name in the placeholder. Consider a certificate with name my_cert , the following placeholders are available for secret consumption: * $$SCONE::my_cert.crt$$ delivers the certificate in PEM-encoding, * $$SCONE::my_cert.key$$ delivers the certificate's private key in PEM-encoding, and * $$SCONE::my_cert.chain$$ delivers the certificate's chain in PEM-encoding, with the root CA certificate first and the certificate itself last (included). Root CA certificate and topmost intermediate CA certificates may be omitted if they are not known or the certificate is self-signed. Note that the .key suffix is not guaranteed to exist, it is for instance not available when importing certificates from another session which does not grant access to the certificates' private keys.","title":"Certificate Values"},{"location":"CAS_session_lang_0_3/#secret-injection-files","text":"A secret injection file is a file in the file system that will be updated with secrets received by the runtime from CAS. They use the same secret placeholders that can also be used in program arguments and environment variables. For example, imagine a service configuration file containing a password. Simply writing the password into the file in cleartext would leak it - that is not an option. Using SCONE's filesystem shield to encrypt the file complicates the setup, and - if distributed as part of an image - would require users to change the encryption keys in order to protect their individual passwords. Instead, a secret injection file allows to specify a placeholder for the password, which will be dynamically replaced at runtime through the means of secret injection: /etc/mysql/my.cnf : [client] user=mysqluser password=$$SCONE::mysqlpass$$ The path to this configuration file must be specified as part of an image (see Images ). The corresponding session would look like this: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - /etc/mysql/my.cnf Alternatively, the file's content may be specified within the session: services : - name : my_database_client image_name : my_db_client_image images : - name : my_db_client_image injection_files : - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ The content specified in the session file can contain multiline strings. This way, entire configuration files can be embedded in the session description, even if no secret injection is required. Producing valid multiline strings in YAML can be challenging - https://yaml-multiline.info/ can be of great help to find the desired syntax. Secret injection files are prepared during SCONE runtime initialization. If the file content is not provided in the session, the file at the specified path is opened, potentially through SCONE's filesystem shield, and read into memory. The secret injection is applied and the resulting file is put into SCONE's in-memory file system at the specified path. Any application requests regarding this file are served from this in-memory file system. Thus, modifications to secret injection files are not propagated into the file system and are not persistent across program invocations.","title":"Secret Injection Files"},{"location":"CAS_session_lang_0_3/#secret-sharing","text":"Session owners can decide to share their secrets with other parties to enable collaboration. For example, database operators could use TLS to implement access control to databases. They would define an access certificate, configure the database to only allow connections from said certificate and export it to the database client: name : database_operator secrets : - name : database_access_key kind : private-key export : session : database_client - name : database_access_certificate kind : x509 private_key : database_access_key export : session : database_client The secret owner might also specify multiple receiving sessions at once: name : database_operator secrets : - name : database_access_key kind : private-key export : - session : database_client - session : another_client - name : database_access_certificate kind : x509 private_key : database_access_key export : - session : database_client - session : another_client Furthermore, the export might be restricted to certain instances of the importing session. For more details, see the concrete SCONE ID format by which other sessions can be referenced in Referencing Other Sessions . The database client, on the other hand, would import it and could use it in their session as if it was their own certificate: name : database_client secrets : - name : database_access_certificate kind : x509 # optional import : session : database_operator secret : database_access_certificate On the importing side, the kind of a secret can be optionally defined to ensure imported secrets match a specific form, but this is not strictly necessary. When importing certificates, the associated private-key does not have to be imported explicitly - it will be available automatically if the exporting session exports the private key as well. In very specific cases, secrets may also be made public (exported to anyone without authentication ) - this may be useful when, for example, defining certificate key hashes: name : policy_checker secrets : - name : certificate_hash kind : ascii value : \"4sEY84YhUKT7Q7m4qUj2pmQMxFvZK5XpDyJ3QR6mETVRDkyren\" export_public : true This hash can be used in another session's access control policy through secret substitution (see Access Control ): name : checked_session secrets : - name : policy_checker_certificate_hash import : session : policy_checker secret : certificate_hash access_policy : read : - \"$$SCONE::policy_checker_certificate_hash\" Warning By using export_public: true , the whole world will be able to see the secret value. Make sure this is your intention. These secrets may also be queried through the /v1/values CAS REST API endpoint.","title":"Secret Sharing"},{"location":"CAS_session_lang_0_3/#secret-migration","text":"When uploading a new session which has a predecessor session, secret migration takes place. In short: ASCII and binary secret values which were generated as part of the old session will be kept when the new session defines a secret with the same name and compatible kind. Example old session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : binary value : DEADBEEF Example new session: secrets : - name : my_generated_ascii_secret kind : ascii size : 12 - name : foobar kind : private-key In the given example, the value of my_generated_ascii_secret will be kept, as it stays an ASCII secret of the same size, whereas foobar will be freshly generated, since its kind changed. Note Generated private keys and certificates will never be kept, they will always be re-generated when updating a session. Secret migration also takes place when using a different session description language version (e.g. 0.2 -> 0.3)! For details, see the session secrets migration documentation .","title":"Secret Migration"},{"location":"CAS_session_lang_0_3/#volumes","text":"Similar to Docker, the volumes keyword allows to specify file system regions that can be mounted into the file system in addition to the regions specified by the main file system protection file of a service. Each volume has an arbitrary but unique name . In order to grant services access to a volume, it first has to be included in an image (see section Images ). Subsequently, the image can be specified for a service (see section Service Description ), and all image-defined volumes will be mounted automatically. Note The volume name is not the volume's mount point. The latter is defined as part of an image . Each volume can have a file system protection file key ( fspf_key ) and file system protection file tag ( fspf_tag ): volumes : - name : my_database fspf_key : f843051d21afa9e52a5b54a708a8032bc49581e982696a81393b8da4a32d00b8 fspf_tag : 8d8fbe332fb9c893020be791ccd3e8a8 fspf_key is the key used to encrypt volume.fspf (file system protection file for the volume) and fspf_tag describes the initial state of the volume. On each volume update (e.g., creation of a file in the region or update of an existing file), the SCONE runtime will send a new fspf_tag to CAS to ensure integrity and freshness of the volume state. If neither fspf_key nor fspf_tag are specified, a volume will be automatically initialized during the first use: volumes : - name : my_database The volume will contain a single encrypted region, and a new key will be generated by CAS to encrypt volume.fspf . Once initialized, CAS and the runtime will work together to track the updates of the volume, similar to a regular volume. This ensures that a volume can be initialized only once. Use of automatically initialized volumes ensures that the key for the file system is only visible inside of CAS and the application(s) that get access to this volume, i.e., no system administrator will ever see the volume key.","title":"Volumes"},{"location":"CAS_session_lang_0_3/#volume-sharing","text":"Volumes may be exported to other sessions, which implies authorizing the other sessions to decrypt and read existing files or encrypt and store new files: volumes : - name : my_database export : session : another-session or volumes : - name : my_database export : - session : another-session - session : foobar session_hash : 668e9aaba22c7631bbcc89b627d77e53539bcaade9e7c2c08242f56aab272088 The concrete SCONE ID format by which other sessions can be referenced is described in section Referencing Other Sessions . Note that volumes can only be exported to local sessions on the same CAS, not to sessions on a remote CAS. Similarly, volumes may be imported from another session (in which case fspf key/tag and export list must be omitted): volumes : - name : their_database import : session : the_exporting_session volume : my_database When exporting a volume, it is possible to restrict which kind of modifications an importing session is allowed to perform. Section Image Volumes demonstrates how volume access can be restricted for local services; the same is also possible for volume exports by using the update_policy key: volumes : - name : my_database export : - to : session : foobar session_hash : 668e9aaba22c7631bbcc89b627d77e53539bcaade9e7c2c08242f56aab272088 update_policy : ephemeral Note that the SCONE ID ( session and session_hash ) needed to be moved to a new to: section! update_policy can have any of the following values: ephemeral : Importing sessions may only use the ephemeral policy. This implies that they are not allowed to alter the volume's state on the CAS (please read Image Volumes for further explanation). Any tag policy specified by the importing session is ignored. rollback_protected : This is the default setting if update_policy is omitted. Importing sessions can use either a rollback_protected or an ephemeral policy at their discretion. no_rollback_protection : Importing sessions are allowed to use a rollback_protected , ephemeral or no_rollback_protection policy at their discretion. This option is dangerous, as it also allows third parties to perform rollbacks unnoticed. It should not be necessary for most use cases.","title":"Volume Sharing"},{"location":"CAS_session_lang_0_3/#images","text":"The images keyword allows specifying images usable by services (see section Service Description ). Each image has an arbitrary but unique name : images : - name : my_image","title":"Images"},{"location":"CAS_session_lang_0_3/#image-volumes","text":"Images may define access to volumes , by referencing a previously declared volume's name and giving it a mount point ( path ): volumes : - name : my_database images : - name : my_image volumes : - name : my_database path : /media/database Note The given path must already exist in the filesystem, e.g., through a mounted docker volume. The information provided as part of the session description are only used to encrypt and authenticate all of the volume's files. The CAS does not actually store the encrypted files. By default, volumes are rollback-protected: Attempting to restore an old volume state and letting a service read files from or write files to this old state will be detected and prevented. In specific cases, however, this may not be the desired behavior. The update_policy key can be used to change it: volumes : - name : my_database images : - name : my_image volumes : - name : my_database path : /media/database update_policy : no_rollback_protection The following values are possible as a volume update_policy policy: rollback_protected : This is the default setting if update_policy is omitted. Services are allowed to alter the volume state, any rollback will be detected and lead to an error upon access. This ensures a coherent, linear history of volume changes. ephemeral : Any changes done to the volume state will be ignored by CAS. This can be useful when a volume is supposed to remain in a specific state - an initialization service could use an image with volume update_policy: rollback_protected to prepare the volume, and all other services can use update_policy: ephemeral in order to ignore further modifications. Note that this does not prevent services from modifying files and directories in the volume's path! This will still work as long as the service is running. Once it is restarted, however, CAS will detect the modification and refuse any access to the modified volume. Therefore, if you expect services to make changes to the volume, it is a good idea to back up the initialized volume, and restore this backup once the service is stopped. This ensures that new instances of the service are able to access the volume in its expected state. no_rollback_protection : Services are allowed to alter the volume's state and rollback prevention checks are disabled. This option is dangerous, as it also allows third parties to perform rollbacks unnoticed. It should not be necessary for most use cases. Note When using an imported volume, the exporting session may have applied restrictions on the volume's update_policy . Attempting to use an unauthorized update_policy will lead to errors upon volume access. For details, please refer to section Volume Sharing .","title":"Image Volumes"},{"location":"CAS_session_lang_0_3/#secret-injection-files_1","text":"Images may also contain secret injection files, a way to inject secrets into configuration files: images : - name : proxy_image injection_files : - /etc/nginx/nginx.conf - path : /etc/mysql/my.cnf content : | [client] user=mysqluser password=$$SCONE::mysqlpass$$ For details, refer to section Secret Injection Files .","title":"Secret Injection Files"},{"location":"CAS_session_lang_0_3/#access-control","text":"Any operation on a session description requires permission. If the entity requesting a certain operation is not explicitly permitted to perform said operation, the request will fail. The access_policy keyword allows specifying lists of entities that are allowed to perform the following operations: read : permit to read the policy - without the secrets update : permit to update the policy. Note that entities listed here must also be present under read . Granting permission to a certain entity to perform one of these operations involves adding their client certificate public key to the list of authorized entities. A certificate with this key shall be used when establishing a connection to CAS (see API Documentation, Authentication section ). TLS ensures that the client is in possession of the corresponding private key. CAS uses key-based authentication instead of whole certificate authentication to ensure that certificates can be renewed without problems - otherwise, users could be locked out of the session when their certificate expires. When using the scone CLI, the user certificate can be shown by running scone self show . Beside public certificates, the following values can be used: Public key hash of a certificate. This hash can be calculated by using the SCONE CLI: scone self show shows the hash of the CLI client identity, scone cert show-key-hash \"path_to_certificate_file_in_pem_format\" shows the key hash for any certificate file. A valid hash looks similar to: 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 CREATOR keyword: permit access to the creator of the policy: this is the public key of the TLS client certificate used when creating this session ANY keyword: permit access to any entity. If ANY is specified, there must be no other entries in the list for this operation NONE keyword: deny all requests for a particular operation. If NONE is specified, there must be no other entries in the list for this operation $$SCONE::secret-name$$ will dynamically use the value of a secret with the given name ( secret-name ) at permission evaluation time. The replaced value must be either CREATOR (ascii), a certificate key hash (ascii) or certificate as defined above. It is possible to use explicit secrets, generated secrets, and imported secrets. When referencing X.509 certificates, the trailing .crt after the secret name can be specified, but may also be omitted. If the mentioned secret does not exist, cannot be read, or has an incompatible value, it will be ignored, but an error message will be shown on unsuccessful authentication attempts. By default, the access policy is defined as follows: access_policy : read : - CREATOR update : - CREATOR Default values will be used for operations not explicitly specified in a session description. Example policy: access_policy : read : - CREATOR - 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 - | -----BEGIN CERTIFICATE----- MIIFwTCCA6mgAwIBAgIUCF1MVJJ78BIf4WmTE24aAX7NlHowDQYJKoZIhvcNAQEL BQAwcDELMAkGA1UEBhMCVVMxDzANBgNVBAgMBk9yZWdvbjERMA8GA1UEBwwIUG9y dGxhbmQxFTATBgNVBAoMDENvbXBhbnkgTmFtZTEMMAoGA1UECwwDT3JnMRgwFgYD VQQDDA93d3cuZXhhbXBsZS5jb20wHhcNMTkwNzIyMTU1NTExWhcNMTkwODIyMTU1 NTExWjBwMQswCQYDVQQGEwJVUzEPMA0GA1UECAwGT3JlZ29uMREwDwYDVQQHDAhQ b3J0bGFuZDEVMBMGA1UECgwMQ29tcGFueSBOYW1lMQwwCgYDVQQLDANPcmcxGDAW BgNVBAMMD3d3dy5leGFtcGxlLmNvbTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCC AgoCggIBALVbVIrBAlzDOztWs9hZr5kvYoUwq/hL7zaMrYKBLQJZFNbhmMaUsW7A Fzj87dzP3xIf4c2r3IGJSukv7hJpaJ2Ykv80i3C7EiFgaDV/+JP9d/GjsvcW20zH mtJcBIkdkqPt1epOtxsMyJGZL+34DoWOqgY7up6nCirr+MeUxYJ/dWBFD1j0iuHl Y+rEMsv4xFBndgLmMQNlcMyXtBgPls4EgnDfnjICqIYMHt6PG+kwoR4tbs+v2Gsl vqldxI7efErZh+kKtjtFxt6qzrypUs9bYgH3tsaUE0xYeK/A2llylJzPOv6vkCqg vPOJETcZyoeH46niITdPssYr4yPQOxn/a7WS+7Mn2y6o5z4Q+DkB96lzUyvVJnwO aorzec0PaB/qqYrHqVfftMu4thMwHGB8CrGUiq/ImHPWkfobyVcMYJ0/LaLSDHFj 1hN36VkzWqQcCM6ymhjx9Lpfzzxna5910jE86zb1cMnD/eAAd90jpJvGJN43Hw40 MIvjYBunOy9P3ah0kgCk7gW0oKlYHxugv8pZVHMwU1HFIdwYvlGd09XHFDyj9tul eX8zaVwaNeLUrMdJN5Ct1HX16RpnpaIMwwExzXgsZ01BQcfIcGWGbvBfH2C86klt SuG7M6kxk4XgIIlwTSGk7qJlfd4s8PD1fVJNKvJZwXXoQBy4hCrTAgMBAAGjUzBR MB0GA1UdDgQWBBRSUKop9QDGmSdLCfzWlBIF5ClNVDAfBgNVHSMEGDAWgBRSUKop 9QDGmSdLCfzWlBIF5ClNVDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUA A4ICAQAny4QmzvCza0TQUuh7YIxtkBjJoyTSDGmr5W/4AWKsoZJM9QxzWeIAXhWD UPH2TfwcNX/ovZiSDQaGduRL67Wk8lbp1MvjACMEtLRjsbnYtGFdhit0fR97B5Y6 d06Ka/NXgPTJorXx8WSWUp0qaAQcgvhfgF0vnOSB5CbP5RSYE5TuLu6gh+iQTrBI Syl+9UaopkbQDRsg+XRfie+kUxQgldUAFvFmu6sM6FTbw0KGkrsOajwpF/Fu5hSV Ucov4Lzrrxkok5FzWPkVtMalLZ4Du+ZUYG//10WZg+HdrIwx3m2wxrFIkZaMKxv4 ZkIMsb6DUPUZqy8qZpMzIqvDzx3iYEWWfBOCJWBjs8/V1mAuUu6TBCKAJpvfX6bU hNrCbnrpuxuCCPJnj9sXkBDvl5rcyfshTtKl3NoBrRRDuUHWsJWzsKvBQtwN46vF CbF0aXOozihtmmcMpFFeDIj6p/5qlaJtslegtfv2zoztc3e2ituOjqFQ/I5pplvo p8EGwCI1xTGF0BTatcSV1+lLNeONhhAtwliV13nPSH1o4yxoZ+xZTZq4+9ylw7dq yV3BQM11U6OyAPE1G6EX0PgFvLm25sGTJq9TKXs9yWPRit9vHcOCXSGn8osn4SMg Puqpk+3M9xR8XDPJiBjkxcSnt9+EDNwpthTzgUEoyM6dY8nvWA== -----END CERTIFICATE----- - \"$$SCONE::remote_validation_service$$\" update : - NONE This policy will allow read requests from the creator, a user whose TLS client certificate has a public key hash 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 , a user whose TLS client certificate has a public key similar to the one of the certificate specified in the session description and a user whose credential is taken from the secret named remote_validation_service (which may be imported from another session). No one is allowed to update the session description.","title":"Access Control"},{"location":"CAS_session_lang_0_3/#attestation-security","text":"The services' security directly depends on CAS' ability to verify that a service process has not been manipulated (referred to as attestation), as described in the service attestation section . CAS ships with secure defaults, and only allows hardware-attested services running on trustworthy platforms in secure production-mode enclaves.","title":"Attestation Security"},{"location":"CAS_session_lang_0_3/#tolerating-hardware-attestation-issues","text":"Sometimes it may be necessary to relax the secure defaults, for instance, when: Services should run in a testing environment for debugging purposes A trade-off is being made between security and performance, like enabling Hyper-Threading (which opens an Information Disclosure Side Channel) A platform with known security vulnerabilities is used in a testing environment Example: security : attestation : tolerate : [ debug-mode , hyperthreading ] tolerate is an array which can hold the following variants: debug-mode enclaves allow introspection with a debugger. This disables enclave protection, all secrets can be extracted. hyperthreading : Enabled Hyper-Threading opens a Microarchitectural Data Sampling Information Disclosure Side Channel on Intel SGX platforms. If the performance penalty can be tolerated, disabling Hyper-Threading instead of ignoring the issue may be viable. See https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00233.html insecure-igpu : An enabled integrated GPU could be exploited to disclose information on Intel SGX platforms. If not required for operation, disable the IGPU instead of tolerating this issue. See https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00219.html software-hardening-needed : The service program needs to be re-compiled with different compiler settings to protect against exploits. This is a category for a collection of Intel SGX advisories, which need to be specified separately (see below). insecure-configuration : The platform needs to be reconfigured to protect against certain kinds of attacks. This is a category for a collection of Intel SGX advisories, which need to be specified separately (see below). outdated-tcb : The Trusted Computing Base (TCB) of the platform, i.e. firmware or microcode, needs to be updated due to security flaws. This is a category for a collection of Intel SGX advisories, which need to be specified separately (see below). When running a service on a platform that is affected by one of the above problems, attestation will fail with an error, unless they have been specified in the tolerate list. The error message will contain instructions on how to change attestation security settings in order to ignore the specific problems. Note Instead of ignoring platform security issues, solving them should always be preferred. Often, this involves updating the platform's firmware or CPU microcode. Sometimes, it can be necessary to change BIOS/UEFI settings or recompile the service with different settings. The error message as well as linked Intel advisories contain more details. Warning Displayed attestation error messages are not authenticated, and may be forged by a MitM attacker. It is advisable to critically review the suggested configuration changes as well as Intel advisories. Intel will release advisories when becoming aware of new security issues with SGX-enabled processors. Some of the problems listed in these advisories are encountered more frequently, in which case SCONE/CAS provide shortcuts to ignore them (like hyperthreading ). Others may be more severe or very recent. In order to ignore them, they and their associated category ( software-hardening-needed / insecure-configuration / outdated-tcb ) need to be specified explicitly. Example: security : attestation : tolerate : [ outdated-tcb ] ignore_advisories : [ INTEL-SA-00161 , INTEL-SA-00270 ] When in debug-mode , all advisories can be ignored by using a wildcard, simplifying the workflow in testing environments: security : attestation : tolerate : [ debug-mode , outdated-tcb ] ignore_advisories : \"*\" As this silently ignores newly released platform advisories too, wildcards cannot be used in (non-debug) production mode.","title":"Tolerating hardware attestation issues"},{"location":"CAS_session_lang_0_3/#trusted-scone-quoting-enclaves","text":"By default, the system uses the SCONE attestation scheme. This scheme is built around a separate quoting enclave (QE), into which trust has to be established. This is done automatically using EPID-based attestation for a set of known trustworthy QE measurements embedded into the CAS software. If EPID-based attestation is not available, or you want to use custom QEs, the QE public keys can be manually specified as trustworthy: security : attestation : trusted_scone_qe_pubkeys : [ \"E37F149AE30896E314A2859874C5A9C7803FB3187B99F5D08E526B1C0396507C\" ] The QE public key can be found in the log output of the Local Attestation Service (LAS). Specified keys are an additional trust anchor, and do not replace the trusted built-in QE measurements. To trust the given keys exclusively, additionally enable platform-based attestation for individual services.","title":"Trusted SCONE Quoting Enclaves"},{"location":"CAS_session_lang_0_3/#disabling-attestation","text":"Attestation can be completely disabled: security : attestation : mode : none # Default mode is 'hardware' This may be useful in testing environments where no hardware-based attestation is available. Warning Disabling attestation implies that there is no service authentication being performed at all . Everybody with access to CAS can access all secrets of all of this session's services.","title":"Disabling attestation"},{"location":"CAS_session_lang_0_3/#referencing-other-sessions","text":"In some cases it may be necessary to reference other sessions, e.g. when exporting or importing volumes or secrets . This is possible by using a SCONE ID. The following sections describe the format and content of this ID.","title":"Referencing Other Sessions"},{"location":"CAS_session_lang_0_3/#local-cas-sessions","text":"Referencing another session on the same CAS is simple: session: <session name> - e.g. session: my-database Note that, depending on the context, you may also have to supply a secret, volume or service name, e.g.: session : my-database secret : db-encryption-key By using this simple form, you trust the session's owner, and accept that their session configuration may change over time. If you want to trust a specific session configuration instead, you have to additionally specify a session hash : session : <session name> session_hash : <session hash-key> e.g.: session : my-database session_hash : a51c14dd7029d2ef54a50a9e26efcdd37c4971b5b62cb6d244c9216f80b6eadf Exporting/Importing elements to/from the other session will be prevented by the CAS when the currently active session does not match the specified hash. Note that, in this case, your own session may cease working when importing secrets or volumes from another session whose hash has changed, requiring you to update the session.","title":"Local CAS Sessions"},{"location":"CAS_session_lang_0_3/#sessions-on-another-cas","text":"Referencing sessions on another CAS requires more information - the remote CAS' address and a key to authenticate it: cas_url : <CAS address> cas_key : <CAS key hash> session : <session name> e.g.: cas_url : cas.example.com cas_key : 46YyxrywJ8PFRruWX8YLxa9q4axxYJgTbA81tv7NBcJfn43DQt session : company-storage The cas_key is used for authentication and ensures that exports/imports will only be performed to/from the correct CAS. It is of utmost importance to specify a verified, correct key. When a CAS was attested using the CLI, you can query its verified key by using scone cas show-identification -c . Similarly, when attesting a CAS, you can supply the key received from another session's owner by using scone cas attest -c <CAS key hash> . By using cas_key , you trust the remote CAS' owner and allow them to perform software updates without further confirmation. Most of the time, this is an adequate solution. In specific cases, when trusting the remote CAS' owner is not an option, it may be necessary to pin a specific CAS software revision instead. This may be done by using the cas_software_key instead of a cas_key : cas_url : <CAS address> cas_software_key : <CAS key hash> session : <session name> e.g.: cas_url : cas.example.com cas_software_key : 3AC5RSbL73aVVf98m2UMTN2BFsQv6eQufi5BGBxrG8awxP4ygQ session : company-storage In this case, exporting/importing elements to/from the other session will be prevented by the local CAS when the software of the remote CAS changes. The same limitations as specified for the usage of a session_hash apply here as well. The software key can be retrieved or specified on the CLI by using scone cas show-identification -s and scone cas attest -s <CAS software key hash> respectively.","title":"Sessions on another CAS"},{"location":"CAS_session_lang_0_3/#url-short-form","text":"Instead of using YAML syntax, you can also fit all SCONE ID fields on a single line, forming a SCONE URL. For instance, cas_url : cas.example.com cas_software_key : 3AC5RSbL73aVVf98m2UMTN2BFsQv6eQufi5BGBxrG8awxP4ygQ session : company-storage can also be written as cas.example.com/cas_software_key=3AC5RSbL73aVVf98m2UMTN2BFsQv6eQufi5BGBxrG8awxP4ygQ,session=company-storage This may be useful when referencing the same CAS multiple times, as the CAS URL and keys can be extracted into a common variable, example: export : - $OTHER_CAS,session=A - $OTHER_CAS,session=B - $OTHER_CAS,session=C For replacing the variable with a concrete value, please refer to the SCONE CLI documentation. The same format can also be used when referencing sessions on the local CAS. Simply omit the host and keys (but notice the leading slash / ): /session=company-storage,session_hash=a51c14dd7029d2ef54a50a9e26efcdd37c4971b5b62cb6d244c9216f80b6eadf","title":"URL short form"},{"location":"EncryptedWordCount/","text":"Encrypted Python Programs and Encrypted Input With the help of a simple wordcount Python program, we show how to execute encrypted Python code inside of an SGX enclave. In this example, we also show how to encrypt an input file. The Python engine itself also runs inside the same enclave. We put both the Python code as well as the input file of the wordcount in the same encrypted filesystem. Typically, we would put the Python code in the encrypted filesystem of the image and the encrypted input and output files in one or more encrypted volumes mapped into the container. This Python image that we use is not intended for production usage The Python libraries in the Python base image **are not encrypted* . Moreover, the Python engine runs inside of a debug enclave. Contact us , if you need a production-ready Python engine with encrypted Python libraries.* You require access to the images Just register a free account on gitlab.scontain.com . Getting the Code After you gotten access to the repos, you can get the code via: git clone https://github.com/scontain/EncryptedWordCount.git and then enter directory EncryptedWordCount cd EncryptedWordCount Files The Python source code and the input file is stored in directory native-files/ . The wordcount.py code is just some very simple Python code to count the frequency of words in a given file: #!/usr/bin/python import sys file = open ( sys . argv [ 1 ], \"r+\" ) wordcount = {} for word in file . read () . split (): if word not in wordcount : wordcount [ word ] = 1 else : wordcount [ word ] += 1 for k , v in wordcount . items (): print ( k , v ) Creating Image with Encrypted wordcount The repository includes a script create_image.sh that encrypts the files in directory native-files/ : the files are integrity , confidentiality and rollback protected attests that CAS is a proper CAS running inside an enclave and then stores the TLS certificate of the CAS in the local file system generates a docker image containing a Python engine which runs inside of an enclave pushes a security policy to CAS via TLS (checking the certificate of CAS) that includes (a) the encryption key of the encrypted files, and (b) limits access to the generated image access to the security policy is controlled via a self-generated key pair Perform all of the above steps by executing the following shell script: ./create_image.sh This creates an image encryptedwordcount stored in the local registry and a session in a SCONE Configuration and Attestation Service (CAS) running on a remote site. We assume in this demo that the creation of the wordcount image is performed on a trusted host. The execution of the wordcount can be performed on an untrusted host. For simplicity, we execute on the same host. Running the wordcount image First, read some environment variables set by ./create_image.sh source myenv and then start the wordcount.py with docker-compose : docker-compose up You will see quite some log output ending with ... python_1 | scone 1 python_1 | region. 3 python_1 | Hence, 1 python_1 | {ephemeral 1 python_1 | without 1 python_1 | command 1 python_1 | File 1 python_1 | mechanisms. 1 python_1 | region: 1 python_1 | the 19 python_1 | path. 1 python_1 | document. 1 encryptedwordcount_python_1 exited with code 0 Stop the compose file with control-C. You can run it again by executing docker-compose up again. Cleanup Cleanup afterwards by executing: ./cleanup.sh before starting it with ./create_image.sh and docker-compose up again. Note: Please read Hello World tutorial to learn more about the technical details.","title":"Encrypted Code and Input"},{"location":"EncryptedWordCount/#encrypted-python-programs-and-encrypted-input","text":"With the help of a simple wordcount Python program, we show how to execute encrypted Python code inside of an SGX enclave. In this example, we also show how to encrypt an input file. The Python engine itself also runs inside the same enclave. We put both the Python code as well as the input file of the wordcount in the same encrypted filesystem. Typically, we would put the Python code in the encrypted filesystem of the image and the encrypted input and output files in one or more encrypted volumes mapped into the container. This Python image that we use is not intended for production usage The Python libraries in the Python base image **are not encrypted* . Moreover, the Python engine runs inside of a debug enclave. Contact us , if you need a production-ready Python engine with encrypted Python libraries.* You require access to the images Just register a free account on gitlab.scontain.com .","title":"Encrypted Python Programs and Encrypted Input"},{"location":"EncryptedWordCount/#getting-the-code","text":"After you gotten access to the repos, you can get the code via: git clone https://github.com/scontain/EncryptedWordCount.git and then enter directory EncryptedWordCount cd EncryptedWordCount","title":"Getting the Code"},{"location":"EncryptedWordCount/#files","text":"The Python source code and the input file is stored in directory native-files/ . The wordcount.py code is just some very simple Python code to count the frequency of words in a given file: #!/usr/bin/python import sys file = open ( sys . argv [ 1 ], \"r+\" ) wordcount = {} for word in file . read () . split (): if word not in wordcount : wordcount [ word ] = 1 else : wordcount [ word ] += 1 for k , v in wordcount . items (): print ( k , v )","title":"Files"},{"location":"EncryptedWordCount/#creating-image-with-encrypted-wordcount","text":"The repository includes a script create_image.sh that encrypts the files in directory native-files/ : the files are integrity , confidentiality and rollback protected attests that CAS is a proper CAS running inside an enclave and then stores the TLS certificate of the CAS in the local file system generates a docker image containing a Python engine which runs inside of an enclave pushes a security policy to CAS via TLS (checking the certificate of CAS) that includes (a) the encryption key of the encrypted files, and (b) limits access to the generated image access to the security policy is controlled via a self-generated key pair Perform all of the above steps by executing the following shell script: ./create_image.sh This creates an image encryptedwordcount stored in the local registry and a session in a SCONE Configuration and Attestation Service (CAS) running on a remote site. We assume in this demo that the creation of the wordcount image is performed on a trusted host. The execution of the wordcount can be performed on an untrusted host. For simplicity, we execute on the same host.","title":"Creating Image with Encrypted wordcount"},{"location":"EncryptedWordCount/#running-the-wordcount-image","text":"First, read some environment variables set by ./create_image.sh source myenv and then start the wordcount.py with docker-compose : docker-compose up You will see quite some log output ending with ... python_1 | scone 1 python_1 | region. 3 python_1 | Hence, 1 python_1 | {ephemeral 1 python_1 | without 1 python_1 | command 1 python_1 | File 1 python_1 | mechanisms. 1 python_1 | region: 1 python_1 | the 19 python_1 | path. 1 python_1 | document. 1 encryptedwordcount_python_1 exited with code 0 Stop the compose file with control-C. You can run it again by executing docker-compose up again.","title":"Running the wordcount image"},{"location":"EncryptedWordCount/#cleanup","text":"Cleanup afterwards by executing: ./cleanup.sh before starting it with ./create_image.sh and docker-compose up again. Note: Please read Hello World tutorial to learn more about the technical details.","title":"Cleanup"},{"location":"Fortran/","text":"Fortran SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler scone gfortran (a.k.a. scone-gfortran , a.k.a. gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran. Image Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode . Help If you need some help, just execute in the container: $ scone gfortran --help Usage: x86_64-linux-musl-gfortran [ options ] file... Options: ... Let's try a simple Fortran program. cat > gcd.f << EOF * euclid.f (FORTRAN 77) * Find greatest common divisor using the Euclidean algorithm PROGRAM EUCLID PRINT *, 'A?' READ *, NA IF (NA.LE.0) THEN PRINT *, 'A must be a positive integer.' STOP END IF PRINT *, 'B?' READ *, NB IF (NB.LE.0) THEN PRINT *, 'B must be a positive integer.' STOP END IF PRINT *, 'The GCD of', NA, ' and', NB, ' is', NGCD(NA, NB), '.' STOP END FUNCTION NGCD(NA, NB) IA = NA IB = NB 1 IF (IB.NE.0) THEN ITEMP = IA IA = IB IB = MOD(ITEMP, IB) GOTO 1 END IF NGCD = IA RETURN END EOF We compile the program with scone gfortran (a.k.a. scone-gfortran ): scone gfortran gcd.f -o gcd We can now run this program as follows: SCONE_VERSION = 1 ./gcd << EOF 10 15 EOF The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: e9c60984724c6d5ffc8cc8a6ba4377910e63c8534ef24b87d0727e712809ba50 A? B? The GCD of 10 and 15 is 5 . Debugging You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs. Screencast","title":"Fortran"},{"location":"Fortran/#fortran","text":"SCONE supports Fortran with the help of cross-compilation. This page focuses on the SCONE gfortran cross compiler scone gfortran (a.k.a. scone-gfortran , a.k.a. gfortran ). This cross compiler is based on gfortran and hence, the command line options are the same as those of gfortran.","title":"Fortran"},{"location":"Fortran/#image","text":"Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode .","title":"Image"},{"location":"Fortran/#help","text":"If you need some help, just execute in the container: $ scone gfortran --help Usage: x86_64-linux-musl-gfortran [ options ] file... Options: ... Let's try a simple Fortran program. cat > gcd.f << EOF * euclid.f (FORTRAN 77) * Find greatest common divisor using the Euclidean algorithm PROGRAM EUCLID PRINT *, 'A?' READ *, NA IF (NA.LE.0) THEN PRINT *, 'A must be a positive integer.' STOP END IF PRINT *, 'B?' READ *, NB IF (NB.LE.0) THEN PRINT *, 'B must be a positive integer.' STOP END IF PRINT *, 'The GCD of', NA, ' and', NB, ' is', NGCD(NA, NB), '.' STOP END FUNCTION NGCD(NA, NB) IA = NA IB = NB 1 IF (IB.NE.0) THEN ITEMP = IA IA = IB IB = MOD(ITEMP, IB) GOTO 1 END IF NGCD = IA RETURN END EOF We compile the program with scone gfortran (a.k.a. scone-gfortran ): scone gfortran gcd.f -o gcd We can now run this program as follows: SCONE_VERSION = 1 ./gcd << EOF 10 15 EOF The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: e9c60984724c6d5ffc8cc8a6ba4377910e63c8534ef24b87d0727e712809ba50 A? B? The GCD of 10 and 15 is 5 .","title":"Help"},{"location":"Fortran/#debugging","text":"You can use scone-gdb to debug your applications when running inside of an enclave. For some more details on how to use the debugger, please read how to debug GO programs.","title":"Debugging"},{"location":"Fortran/#screencast","text":"","title":"Screencast"},{"location":"GO/","text":"GO SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image registry.scontain.com:5050/sconecuratedimages/crosscompilers . Example Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode . Lets consider a simple GO program (take from a GO tutorial ): cat > web-srv.go << EOF package main import ( \"os\" \"fmt\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hi there, I love %s!\\n\", r.URL.Path[1:]) if r.URL.Path[1:] == \"EXIT\" { os.Exit(0) } } func main() { http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } EOF You can cross-compile this program as follows: SCONE_HEAP = 1G scone-gccgo web-srv.go -O3 -o web-srv-go -g You can start the compiled program (and enable some debug messages) as follows: SCONE_VERSION = 1 ./web-srv-go & The output should look as follows: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: dea1dadce2884bbfd642c10f436c1d02db7ac0f4e4f3abe5d2fde031056405dd You can now connect to port 8080, for example, with curl : curl localhost:8080/SCONE The output should be as follows: Hi there, I love SCONE! You can terminate the server with curl localhost:8080/EXIT This will output the following text: curl: (52) Empty reply from server Building Dependencies Building larger applications that include external dependencies can be difficult when using scone-gccgo alone. To simplify the building of complex applications, we recommend the use of the go command. First, install go inside a registry.scontain.com:5050/sconecuratedimages/crosscompilers container as follows: $ apk update $ apk add go You can then build your dependencies with the help of go and the SCONE go crosscompiler: $ go build -compiler gccgo -buildmode=exe Note you need to specify gccgo not scone-gccgo : gccgo is an alias of scone-gccgo . For a more detailed example, please read how we compile groupcache . Debugging SCONE supports debugging of programs running inside of an enclave with the help of gdb. Debugging inside of a container Standard containers have not sufficient rights to use the debugger. Hence, you must start a container with SYS_PTRACE capability. For example: docker run --cap-add SYS_PTRACE -it -p 8080 :8080 -v \" $PWD \" /EXAMPLE:/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/crosscompilers Handling Illegal instructions Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. scone-gdb ./web-srv-go This will produce the following output: GNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git Copyright (C) 2017 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"x86_64-linux-gnu\". Type \"show configuration\" for configuration details. For bug reporting instructions, please see: <http://www.gnu.org/software/gdb/bugs/>. Find the GDB manual and other documentation resources online at: <http://www.gnu.org/software/gdb/documentation/>. For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\"... Source directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd Setting environment variable \"LD_PRELOAD\" to null value. Reading symbols from ./web-srv-go...done. [SCONE] Initializing... If your program contains some illegal instructions, you need to ask the debugger to forward the signals, that these illegal instructions cause, to the program via handle SIGILL nostop pass : # (gdb) handle SIGILL nostop pass This will produce the following output: Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction (gdb) Since we do not patch the CPUID instructions in this run, run you will see something like this: Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [Thread debugging using libthread_db enabled] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\". [SCONE] Enclave base: 1000000000 [SCONE] Loaded debug symbols [New Thread 0x7f1786d26700 (LWP 105)] [New Thread 0x7f1786525700 (LWP 106)] [New Thread 0x7f1785d24700 (LWP 107)] [New Thread 0x7f1785523700 (LWP 108)] [New Thread 0x7f1787502700 (LWP 109)] [New Thread 0x7f17874fa700 (LWP 110)] [New Thread 0x7f17874f2700 (LWP 111)] [New Thread 0x7f17874ea700 (LWP 112)] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 8 \"web-srv-go\" received signal SIGILL, Illegal instruction. You could interrupt this execution via control c: ^C Thread 1 \"web-srv-go\" received signal SIGINT, Interrupt. 0x00007f17870f69dd in pthread_join ( threadid = 139739022911232 , thread_return = 0x7ffe1c807928 ) at pthread_join.c:90 90 pthread_join.c: No such file or directory. ( gdb ) where #0 0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90 #1 0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764 ( gdb ) cont Continuing. Breakpoints scone-gdb support breakpoints. Say, we want to get control in the debugger whenever a request is being processed by the handler. We would set a breakpoint at function main.handler as follows: scone-gdb ./web-srv-go ... [ SCONE ] Initializing... ( gdb ) handle SIGILL nostop pass Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction ( gdb ) break main.handler Function \"main.handler\" not defined. Make breakpoint pending on future shared library load? ( y or [ n ]) y Breakpoint 1 ( main.handler ) pending. ( gdb ) run Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\" . [ SCONE ] Enclave base: 1000000000 [ SCONE ] Loaded debug symbols [ New Thread 0x7fb6cad32700 ( LWP 243 )] [ New Thread 0x7fb6ca531700 ( LWP 244 )] [ New Thread 0x7fb6c9d30700 ( LWP 245 )] [ New Thread 0x7fb6c952f700 ( LWP 246 )] [ New Thread 0x7fb6cb50e700 ( LWP 247 )] [ New Thread 0x7fb6cb506700 ( LWP 248 )] [ New Thread 0x7fb6cb4fe700 ( LWP 249 )] [ New Thread 0x7fb6cb4f6700 ( LWP 250 )] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Note that at the time when we are setting the breakpoint, the symbols of the code running inside of the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on. We are now sending a request with the help of curl from a different window. This triggers the breakpoint: [ Switching to Thread 0x7fb6cb506700 ( LWP 248 )] Thread 7 \"web-srv-go\" hit Breakpoint 1 , main.handler ( w = ..., r = 0x100909e300 ) at web-srv.go:8 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) n 9 fmt.Fprintf ( w, \"Hi there, I love %s!\" , r.URL.Path [ 1 : ]) ( gdb ) n 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) c Continuing. Screencast","title":"GO"},{"location":"GO/#go","text":"SCONE supports cross-compiling GO programs to run these inside of SGX enclaves. The GO cross-compiler is part of image registry.scontain.com:5050/sconecuratedimages/crosscompilers .","title":"GO"},{"location":"GO/#example","text":"Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device : docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode . Lets consider a simple GO program (take from a GO tutorial ): cat > web-srv.go << EOF package main import ( \"os\" \"fmt\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hi there, I love %s!\\n\", r.URL.Path[1:]) if r.URL.Path[1:] == \"EXIT\" { os.Exit(0) } } func main() { http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } EOF You can cross-compile this program as follows: SCONE_HEAP = 1G scone-gccgo web-srv.go -O3 -o web-srv-go -g You can start the compiled program (and enable some debug messages) as follows: SCONE_VERSION = 1 ./web-srv-go & The output should look as follows: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb (Wed May 30 15:17:05 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: dea1dadce2884bbfd642c10f436c1d02db7ac0f4e4f3abe5d2fde031056405dd You can now connect to port 8080, for example, with curl : curl localhost:8080/SCONE The output should be as follows: Hi there, I love SCONE! You can terminate the server with curl localhost:8080/EXIT This will output the following text: curl: (52) Empty reply from server","title":"Example"},{"location":"GO/#building-dependencies","text":"Building larger applications that include external dependencies can be difficult when using scone-gccgo alone. To simplify the building of complex applications, we recommend the use of the go command. First, install go inside a registry.scontain.com:5050/sconecuratedimages/crosscompilers container as follows: $ apk update $ apk add go You can then build your dependencies with the help of go and the SCONE go crosscompiler: $ go build -compiler gccgo -buildmode=exe Note you need to specify gccgo not scone-gccgo : gccgo is an alias of scone-gccgo . For a more detailed example, please read how we compile groupcache .","title":"Building Dependencies"},{"location":"GO/#debugging","text":"SCONE supports debugging of programs running inside of an enclave with the help of gdb.","title":"Debugging"},{"location":"GO/#debugging-inside-of-a-container","text":"Standard containers have not sufficient rights to use the debugger. Hence, you must start a container with SYS_PTRACE capability. For example: docker run --cap-add SYS_PTRACE -it -p 8080 :8080 -v \" $PWD \" /EXAMPLE:/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/crosscompilers","title":"Debugging inside of a container"},{"location":"GO/#handling-illegal-instructions","text":"Some instructions, like CPUID, are not permitted inside of enclaves. For some of these instructions, like CPUID, we provide an automatic emulation. However, we recommend not to use any illegal instructions inside of enclaves despite having an automatic emulation of these instructions. For example, we provide static replacements of the CPUID instruction. scone-gdb ./web-srv-go This will produce the following output: GNU gdb (Ubuntu 7.12.50.20170314-0ubuntu1.1) 7.12.50.20170314-git Copyright (C) 2017 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"x86_64-linux-gnu\". Type \"show configuration\" for configuration details. For bug reporting instructions, please see: <http://www.gnu.org/software/gdb/bugs/>. Find the GDB manual and other documentation resources online at: <http://www.gnu.org/software/gdb/documentation/>. For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\"... Source directories searched: /opt/scone/scone-gdb/gdb-sgxmusl-plugin:$cdir:$cwd Setting environment variable \"LD_PRELOAD\" to null value. Reading symbols from ./web-srv-go...done. [SCONE] Initializing... If your program contains some illegal instructions, you need to ask the debugger to forward the signals, that these illegal instructions cause, to the program via handle SIGILL nostop pass : # (gdb) handle SIGILL nostop pass This will produce the following output: Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction (gdb) Since we do not patch the CPUID instructions in this run, run you will see something like this: Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [Thread debugging using libthread_db enabled] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\". [SCONE] Enclave base: 1000000000 [SCONE] Loaded debug symbols [New Thread 0x7f1786d26700 (LWP 105)] [New Thread 0x7f1786525700 (LWP 106)] [New Thread 0x7f1785d24700 (LWP 107)] [New Thread 0x7f1785523700 (LWP 108)] [New Thread 0x7f1787502700 (LWP 109)] [New Thread 0x7f17874fa700 (LWP 110)] [New Thread 0x7f17874f2700 (LWP 111)] [New Thread 0x7f17874ea700 (LWP 112)] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 8 \"web-srv-go\" received signal SIGILL, Illegal instruction. You could interrupt this execution via control c: ^C Thread 1 \"web-srv-go\" received signal SIGINT, Interrupt. 0x00007f17870f69dd in pthread_join ( threadid = 139739022911232 , thread_return = 0x7ffe1c807928 ) at pthread_join.c:90 90 pthread_join.c: No such file or directory. ( gdb ) where #0 0x00007f17870f69dd in pthread_join (threadid=139739022911232, thread_return=0x7ffe1c807928) at pthread_join.c:90 #1 0x0000002000004053 in main (argc=1, argv=0x7ffe1c807c18, envp=0x7ffe1c807c28) at ./tools/starter-exec.c:764 ( gdb ) cont Continuing.","title":"Handling Illegal instructions"},{"location":"GO/#breakpoints","text":"scone-gdb support breakpoints. Say, we want to get control in the debugger whenever a request is being processed by the handler. We would set a breakpoint at function main.handler as follows: scone-gdb ./web-srv-go ... [ SCONE ] Initializing... ( gdb ) handle SIGILL nostop pass Signal Stop Print Pass to program Description SIGILL No Yes Yes Illegal instruction ( gdb ) break main.handler Function \"main.handler\" not defined. Make breakpoint pending on future shared library load? ( y or [ n ]) y Breakpoint 1 ( main.handler ) pending. ( gdb ) run Starting program: /usr/src/myapp/web-srv-go warning: Error disabling address space randomization: Operation not permitted [ Thread debugging using libthread_db enabled ] Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\" . [ SCONE ] Enclave base: 1000000000 [ SCONE ] Loaded debug symbols [ New Thread 0x7fb6cad32700 ( LWP 243 )] [ New Thread 0x7fb6ca531700 ( LWP 244 )] [ New Thread 0x7fb6c9d30700 ( LWP 245 )] [ New Thread 0x7fb6c952f700 ( LWP 246 )] [ New Thread 0x7fb6cb50e700 ( LWP 247 )] [ New Thread 0x7fb6cb506700 ( LWP 248 )] [ New Thread 0x7fb6cb4fe700 ( LWP 249 )] [ New Thread 0x7fb6cb4f6700 ( LWP 250 )] Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Thread 6 \"web-srv-go\" received signal SIGILL, Illegal instruction. Note that at the time when we are setting the breakpoint, the symbols of the code running inside of the enclave are not yet known. Hence, we just let gdb know that the symbol will be defined later on. We are now sending a request with the help of curl from a different window. This triggers the breakpoint: [ Switching to Thread 0x7fb6cb506700 ( LWP 248 )] Thread 7 \"web-srv-go\" hit Breakpoint 1 , main.handler ( w = ..., r = 0x100909e300 ) at web-srv.go:8 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) n 9 fmt.Fprintf ( w, \"Hi there, I love %s!\" , r.URL.Path [ 1 : ]) ( gdb ) n 8 func handler ( w http.ResponseWriter, r *http.Request ) { ( gdb ) c Continuing.","title":"Breakpoints"},{"location":"GO/#screencast","text":"","title":"Screencast"},{"location":"Java/","text":"Java SCONE supports Java: we maintain an image containing OpenJDK8. Ensure that you have the newest version of this image cached: docker pull registry.scontain.com:5050/sconecuratedimages/apps:8-jdk-alpine We also provide images: registry.scontain.com:5050/sconecuratedimages/apps:11-jdk-alpine registry.scontain.com:5050/sconecuratedimages/apps:15-jdk-alpine Determine which SGX device to mount with function determine_sgx_device and run image: determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:8-jdk-alpine In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode . Example Let us start with a simple hello world example: cat > HelloWorld.java << EOF public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); } } EOF We can compile as follows: javac HelloWorld.java and run inside of an enclave as follows: java HelloWorld This might produce some warnings (since we disable access to the proc filesystem ): / # java HelloWorld Picked up JAVA_TOOL_OPTIONS: -Xmx256m OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World We can run Java with some additional log messages to see what accesses are blocked: SCONE_VERSION = 1 SCONE_LOG = 7 java HelloWorld which results in the following output export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_HEAP=2147483648 export SCONE_STACK=81920 export SCONE_LOG=7 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) # export SCONE_DISABLE_EDMM=yes export SCONE_MPROTECT=yes Revision: 9a3643062da22bc2eed5812d94b5363e0f81422d (Tue Feb 5 23:32:58 2019 +0000) Branch: master Configure options: --enable-shared --enable-debug --prefix=/home/ubuntu/dlequoc/java/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: cf60b6d0cf3f154ada58aea485444eddb857f37a62c13b768f232022560e8754 Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World This correctly prints Hello World despite some few warnings that the heap is exhausted. Screencast Environment variables SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM, container or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . You can increase the heap to ensure that JVM does not run out of heap memory: SCONE_LOG = 7 SCONE_HEAP = 12G java HelloWorld will result in the following output: Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World Example: Zookeeper The above image runs Zookeeper without any modification. Our Zookeeper image is available here: registry.scontain.com:5050/sconecuratedimages/apps:zookeeper-alpine . Zookeeper inside of an enclave runs zk-smoketest without any issues.","title":"Java"},{"location":"Java/#java","text":"SCONE supports Java: we maintain an image containing OpenJDK8. Ensure that you have the newest version of this image cached: docker pull registry.scontain.com:5050/sconecuratedimages/apps:8-jdk-alpine We also provide images: registry.scontain.com:5050/sconecuratedimages/apps:11-jdk-alpine registry.scontain.com:5050/sconecuratedimages/apps:15-jdk-alpine Determine which SGX device to mount with function determine_sgx_device and run image: determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:8-jdk-alpine In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode .","title":"Java"},{"location":"Java/#example","text":"Let us start with a simple hello world example: cat > HelloWorld.java << EOF public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); } } EOF We can compile as follows: javac HelloWorld.java and run inside of an enclave as follows: java HelloWorld This might produce some warnings (since we disable access to the proc filesystem ): / # java HelloWorld Picked up JAVA_TOOL_OPTIONS: -Xmx256m OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World We can run Java with some additional log messages to see what accesses are blocked: SCONE_VERSION = 1 SCONE_LOG = 7 java HelloWorld which results in the following output export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_HEAP=2147483648 export SCONE_STACK=81920 export SCONE_LOG=7 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) # export SCONE_DISABLE_EDMM=yes export SCONE_MPROTECT=yes Revision: 9a3643062da22bc2eed5812d94b5363e0f81422d (Tue Feb 5 23:32:58 2019 +0000) Branch: master Configure options: --enable-shared --enable-debug --prefix=/home/ubuntu/dlequoc/java/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: cf60b6d0cf3f154ada58aea485444eddb857f37a62c13b768f232022560e8754 Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [SCONE|WARN] src/mman/anon.c:128:mmap_anon(): Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World This correctly prints Hello World despite some few warnings that the heap is exhausted.","title":"Example"},{"location":"Java/#screencast","text":"","title":"Screencast"},{"location":"Java/#environment-variables","text":"SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM, container or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . You can increase the heap to ensure that JVM does not run out of heap memory: SCONE_LOG = 7 SCONE_HEAP = 12G java HelloWorld will result in the following output: Picked up JAVA_TOOL_OPTIONS: -Xmx256m [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/mountinfo is not supported [SCONE|WARN] src/scone-shielding/proc_fs.c:324:_proc_fs_open(): open: /proc/self/maps is not supported OpenJDK 64-Bit Server VM warning: Can't detect initial thread stack location - find_vma failed Hello World","title":"Environment variables"},{"location":"Java/#example-zookeeper","text":"The above image runs Zookeeper without any modification. Our Zookeeper image is available here: registry.scontain.com:5050/sconecuratedimages/apps:zookeeper-alpine . Zookeeper inside of an enclave runs zk-smoketest without any issues.","title":"Example: Zookeeper"},{"location":"LASIntro/","text":"LAS for Development We explain how to start a Local Attestation Service (LAS) instance for development. LAS is need to perform a local attestation (i.e., this creates a quote that can be verified by CAS). Note that this LAS runs inside a debug enclave, i.e., do not use this LAS instance in production . For setting up a production mode LAS, send us an email. For running LAS in a Kubernetes Cluster, please set up LAS with helm . Pulling LAS Image To start LAS, you first pull LAS to your local registry. To do so, please set the environment variable LAS to the image repository that we given you access to. The standard LAS image name is defined as follows: export LAS = registry.scontain.com:5050/sconecuratedimages/services:las Pull the image from Docker hub like this: docker pull $LAS If this fails, ensure that you are logged into docker (via docker login ) and that we granted you access to that image. Determining the SGX device Depending on the platform, the SGX device is named /dev/isgx or /dev/sgx . To write generic software, you could use the bash function determine_sgx_device . It sets environment variable SGXDEVICE to the device that needs to be mounted. Starting and Stopping LAS The easiest way to start LAS is to use a simple Docker compose file. Please create a separate directory for that: mkdir -p LAS cd LAS Create the following compose file: determine_sgx_device cat > docker-compose.yml <<EOF version: '3.2' services: las: image: registry.scontain.com:5050/sconecuratedimages/services:las devices: - \"$SGXDEVICE:$SGXDEVICE\" ports: - target: 18766 published: 18766 protocol: tcp mode: host EOF Now start LAS in the background as follows: docker-compose up -d las By executing docker-compose logs las you will see the output of LAS. You can check if LAS is still running by executing: docker-compose up -d las This will result in an output like las_las_1 is up-to-date You can stop LAS by executing: docker-compose stop","title":"Starting LAS"},{"location":"LASIntro/#las-for-development","text":"We explain how to start a Local Attestation Service (LAS) instance for development. LAS is need to perform a local attestation (i.e., this creates a quote that can be verified by CAS). Note that this LAS runs inside a debug enclave, i.e., do not use this LAS instance in production . For setting up a production mode LAS, send us an email. For running LAS in a Kubernetes Cluster, please set up LAS with helm .","title":"LAS for Development"},{"location":"LASIntro/#pulling-las-image","text":"To start LAS, you first pull LAS to your local registry. To do so, please set the environment variable LAS to the image repository that we given you access to. The standard LAS image name is defined as follows: export LAS = registry.scontain.com:5050/sconecuratedimages/services:las Pull the image from Docker hub like this: docker pull $LAS If this fails, ensure that you are logged into docker (via docker login ) and that we granted you access to that image.","title":"Pulling LAS Image"},{"location":"LASIntro/#determining-the-sgx-device","text":"Depending on the platform, the SGX device is named /dev/isgx or /dev/sgx . To write generic software, you could use the bash function determine_sgx_device . It sets environment variable SGXDEVICE to the device that needs to be mounted.","title":"Determining the SGX device"},{"location":"LASIntro/#starting-and-stopping-las","text":"The easiest way to start LAS is to use a simple Docker compose file. Please create a separate directory for that: mkdir -p LAS cd LAS Create the following compose file: determine_sgx_device cat > docker-compose.yml <<EOF version: '3.2' services: las: image: registry.scontain.com:5050/sconecuratedimages/services:las devices: - \"$SGXDEVICE:$SGXDEVICE\" ports: - target: 18766 published: 18766 protocol: tcp mode: host EOF Now start LAS in the background as follows: docker-compose up -d las By executing docker-compose logs las you will see the output of LAS. You can check if LAS is still running by executing: docker-compose up -d las This will result in an output like las_las_1 is up-to-date You can stop LAS by executing: docker-compose stop","title":"Starting and Stopping LAS"},{"location":"MicrosoftAzureIntegrations/","text":"Microsoft Azure Integrations We showcase the integrations to Microsoft Azure built in SCONE platform. You can clone this example as follows: git clone https://github.com/scontain/scone-azure-integration.git Summary In this sample, a Flask REST API (built with SCONE) running on a Confidential Azure Kubernetes Service cluster is attested by a Microsoft Azure Attestation provider, allowing secrets stored in an Azure Key Vault instance and in the SCONE CAS (Configuration and Attestation Service) to be securely delivered to the attested enclave instance. Demo Prerequisites To run this tutorial, you need an Azure subscription and the following resources: Register an application to Azure Active Directory To fetch tokens, you must provide the Application (client) ID , the Directory (tenant) ID and the credentials ( Client secret ) of your application. Please note that credentials can be either client secrets or certificates, and the SCONE platform supports both. For the sake of this demo, however, we are assuming a client secret. Please refer to the official Azure documentation to see how to register applications and create client secrets . Finally, export the credentials to the environment: export AZURE_TENANT_ID = \"...\" export AZURE_CLIENT_ID = \"...\" export AZURE_CLIENT_SECRET = \"...\" Create an Azure Key Vault instance Please refer to the official Azure documentation to see how to create an Azure Key Vault instance . This tutorial assumes that the AKV has to secrets: A secret: a string of your choice. The content of this secret will be retrieved via the REST API at the end. In this example, we're using a13f688c788626380ae1209e31e664891fc19dcdd0cfa29c27ca7e6e16b83a95 . A certificate. This certificate, as well as its private key, will be used by our REST API when setting up an HTTPS server. Set up the certificate Common Name to CN=rest-api.scone.sample . Please note that the certificate can either be signed by one of the integrated authorities or be self-signed. Export the name of such secrets to the environment, as well as the AKV URL, e.g. : export AKV_VAULT = \"https://scone-sample.vault.azure.net/\" export AKV_SECRET_NAME = \"sample-secret\" export AKV_CERT_SECRET_NAME = \"flask\" Finally, refer to the official Azure documentation to give AKV access the application you created in the previous example . Create Microsoft Azure Attestation provider This step is required only if you want to explore custom policies. Please refer to the official Azure documentation to see how to create an MAA instance and submit custom policies . If you are not interested in custom policies, you can rely on one of the shared MAA instances provided by Microsoft, e.g. : export MAA_PROVIDER = \"https://sharedcus.cus.attest.azure.net\" Create a Confidential Azure Kubernetes Service cluster This step is required only if you want to run the demo in a Kubernetes cluster using the included Helm charts. Please refer to the official Azure documentation to see how to create a confidential AKS cluster . If you are not interested in Kubernetes, you can rely on the included docker-compose manifests in deploy/compose . Please note that you must run the demo in an Azure Confidential Computing VM , otherwise MAA won't attest your enclaves. Running Setup Export the appropriate image names and SGX device: source environment Install SCONE Attestation Services: CAS and LAS. We expose CAS to the internet through an Azure Load Balancer and an external IP address ( --set service.type=LoadBalancer ). If you want to specify a specific static IP address, you have to create it in the appropriate resource group and add --set service.loadBalancerIP=$STATIC_IP . Please refer to the Azure documentation to see how to create static IP addresses to use with AKS . Please also note that the services rely on the SGX Device Plugin ( --set useSGXDevPlugin=azure ) and AESM ( --set externalAesmd.enabled=true ) provided by Azure. helm install cas deploy/helm/cas \\ --set service.type = LoadBalancer \\ --set useSGXDevPlugin = azure helm install las deploy/helm/las \\ --set useSGXDevPlugin = azure Submit policies First, retrieve the CAS public IP (please note that it may take a while until the IP is available): export SCONE_CAS_ADDR = $( kubectl get svc cas --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Specify the name of the CAS namespace and of the session we are going to create. export CAS_NAMESPACE = azure-integration- $RANDOM$RANDOM$RANDOM export PYTHON_SESSION_NAME = demo Start a local SCONE CLI container to submit policies to our CAS. Pass the environment variables to the local container, so the CLI can use them to populate the session template ( session.template.yml ): docker run -d --rm --network host \\ -v $PWD :/templates \\ -e SCONE_CAS_ADDR = $SCONE_CAS_ADDR \\ -e CAS_NAMESPACE = $CAS_NAMESPACE \\ -e PYTHON_SESSION_NAME = $PYTHON_SESSION_NAME \\ -e AZURE_TENANT_ID = $AZURE_TENANT_ID \\ -e AZURE_CLIENT_ID = $AZURE_CLIENT_ID \\ -e AZURE_CLIENT_SECRET = $AZURE_CLIENT_SECRET \\ -e AKV_VAULT = $AKV_VAULT \\ -e AKV_SECRET_NAME = $AKV_SECRET_NAME \\ -e AKV_CERT_SECRET_NAME = $AKV_CERT_SECRET_NAME \\ -e MAA_PROVIDER = $MAA_PROVIDER \\ --name scone-cli \\ --entrypoint sh \\ $CLI_IMAGE \\ -c \"sleep 7200\" docker exec -it scone-cli bash Inside of the CLI container, run: ./templates/attest-cas.sh Create a CAS namespace: scone session create --use-env /templates/namespace.template.yml Submit the templates by running the following command. The option --use-env will allow the CLI to use the environment to replace variables inside of the session template. scone session create --use-env /templates/session.template.yml Start application Run the application without attestation: the application will crash because there are no certificates in the expected locations. helm install api-no-attestation deploy/helm/rest-api-sample \\ --set service.type = LoadBalancer \\ --set useSGXDevPlugin = azure Now, run the application with attestation. The REST API should start once it gets attested and the appropriate secret and certificates\u2014retrieved from AKV\u2014are transparently and securely injected into the enclave's filesystem and environment. export SCONE_CONFIG_ID = $CAS_NAMESPACE / $PYTHON_SESSION_NAME /rest-api helm install api deploy/helm/rest-api-sample \\ --set service.type = LoadBalancer \\ --set useSGXDevPlugin = azure \\ --set scone.cas = $SCONE_CAS_ADDR \\ --set scone.configId = $SCONE_CONFIG_ID Retrieve the public IP address for our REST API: export API_ADDR = $( kubectl get svc api-rest-api-sample --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Access the REST API using cURL. Please note that you must use the Common Name as specified in the certificate that you created in AKV. curl https://rest-api.scone.sample:4996/secret --resolve rest-api.scone.sample:4996: $API_ADDR If you created a self-signed certificate in AKV, you need to pass -k flag to cURL. The expected output looks like: { \"access_timestamp\" : 1615855461.7656054 , \"secret\" : \"a13f688c788626380ae1209e31e664891fc19dcdd0cfa29c27ca7e6e16b83a95\" } Limitations Some MAA instances run inside of enclaves. We currently do not support attestation the MAA instance itself. AAD tokens have a maximum expiration time of 24 hours. We currently do not support the renewal of AAD tokens at runtime. They are renewed upon every enclave startup only.","title":"AKS Integration Tutorial"},{"location":"MicrosoftAzureIntegrations/#microsoft-azure-integrations","text":"We showcase the integrations to Microsoft Azure built in SCONE platform. You can clone this example as follows: git clone https://github.com/scontain/scone-azure-integration.git","title":"Microsoft Azure Integrations"},{"location":"MicrosoftAzureIntegrations/#summary","text":"In this sample, a Flask REST API (built with SCONE) running on a Confidential Azure Kubernetes Service cluster is attested by a Microsoft Azure Attestation provider, allowing secrets stored in an Azure Key Vault instance and in the SCONE CAS (Configuration and Attestation Service) to be securely delivered to the attested enclave instance.","title":"Summary"},{"location":"MicrosoftAzureIntegrations/#demo","text":"","title":"Demo"},{"location":"MicrosoftAzureIntegrations/#prerequisites","text":"To run this tutorial, you need an Azure subscription and the following resources:","title":"Prerequisites"},{"location":"MicrosoftAzureIntegrations/#register-an-application-to-azure-active-directory","text":"To fetch tokens, you must provide the Application (client) ID , the Directory (tenant) ID and the credentials ( Client secret ) of your application. Please note that credentials can be either client secrets or certificates, and the SCONE platform supports both. For the sake of this demo, however, we are assuming a client secret. Please refer to the official Azure documentation to see how to register applications and create client secrets . Finally, export the credentials to the environment: export AZURE_TENANT_ID = \"...\" export AZURE_CLIENT_ID = \"...\" export AZURE_CLIENT_SECRET = \"...\"","title":"Register an application to Azure Active Directory"},{"location":"MicrosoftAzureIntegrations/#create-an-azure-key-vault-instance","text":"Please refer to the official Azure documentation to see how to create an Azure Key Vault instance . This tutorial assumes that the AKV has to secrets: A secret: a string of your choice. The content of this secret will be retrieved via the REST API at the end. In this example, we're using a13f688c788626380ae1209e31e664891fc19dcdd0cfa29c27ca7e6e16b83a95 . A certificate. This certificate, as well as its private key, will be used by our REST API when setting up an HTTPS server. Set up the certificate Common Name to CN=rest-api.scone.sample . Please note that the certificate can either be signed by one of the integrated authorities or be self-signed. Export the name of such secrets to the environment, as well as the AKV URL, e.g. : export AKV_VAULT = \"https://scone-sample.vault.azure.net/\" export AKV_SECRET_NAME = \"sample-secret\" export AKV_CERT_SECRET_NAME = \"flask\" Finally, refer to the official Azure documentation to give AKV access the application you created in the previous example .","title":"Create an Azure Key Vault instance"},{"location":"MicrosoftAzureIntegrations/#create-microsoft-azure-attestation-provider","text":"This step is required only if you want to explore custom policies. Please refer to the official Azure documentation to see how to create an MAA instance and submit custom policies . If you are not interested in custom policies, you can rely on one of the shared MAA instances provided by Microsoft, e.g. : export MAA_PROVIDER = \"https://sharedcus.cus.attest.azure.net\"","title":"Create Microsoft Azure Attestation provider"},{"location":"MicrosoftAzureIntegrations/#create-a-confidential-azure-kubernetes-service-cluster","text":"This step is required only if you want to run the demo in a Kubernetes cluster using the included Helm charts. Please refer to the official Azure documentation to see how to create a confidential AKS cluster . If you are not interested in Kubernetes, you can rely on the included docker-compose manifests in deploy/compose . Please note that you must run the demo in an Azure Confidential Computing VM , otherwise MAA won't attest your enclaves.","title":"Create a Confidential Azure Kubernetes Service cluster"},{"location":"MicrosoftAzureIntegrations/#running","text":"","title":"Running"},{"location":"MicrosoftAzureIntegrations/#setup","text":"Export the appropriate image names and SGX device: source environment Install SCONE Attestation Services: CAS and LAS. We expose CAS to the internet through an Azure Load Balancer and an external IP address ( --set service.type=LoadBalancer ). If you want to specify a specific static IP address, you have to create it in the appropriate resource group and add --set service.loadBalancerIP=$STATIC_IP . Please refer to the Azure documentation to see how to create static IP addresses to use with AKS . Please also note that the services rely on the SGX Device Plugin ( --set useSGXDevPlugin=azure ) and AESM ( --set externalAesmd.enabled=true ) provided by Azure. helm install cas deploy/helm/cas \\ --set service.type = LoadBalancer \\ --set useSGXDevPlugin = azure helm install las deploy/helm/las \\ --set useSGXDevPlugin = azure","title":"Setup"},{"location":"MicrosoftAzureIntegrations/#submit-policies","text":"First, retrieve the CAS public IP (please note that it may take a while until the IP is available): export SCONE_CAS_ADDR = $( kubectl get svc cas --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Specify the name of the CAS namespace and of the session we are going to create. export CAS_NAMESPACE = azure-integration- $RANDOM$RANDOM$RANDOM export PYTHON_SESSION_NAME = demo Start a local SCONE CLI container to submit policies to our CAS. Pass the environment variables to the local container, so the CLI can use them to populate the session template ( session.template.yml ): docker run -d --rm --network host \\ -v $PWD :/templates \\ -e SCONE_CAS_ADDR = $SCONE_CAS_ADDR \\ -e CAS_NAMESPACE = $CAS_NAMESPACE \\ -e PYTHON_SESSION_NAME = $PYTHON_SESSION_NAME \\ -e AZURE_TENANT_ID = $AZURE_TENANT_ID \\ -e AZURE_CLIENT_ID = $AZURE_CLIENT_ID \\ -e AZURE_CLIENT_SECRET = $AZURE_CLIENT_SECRET \\ -e AKV_VAULT = $AKV_VAULT \\ -e AKV_SECRET_NAME = $AKV_SECRET_NAME \\ -e AKV_CERT_SECRET_NAME = $AKV_CERT_SECRET_NAME \\ -e MAA_PROVIDER = $MAA_PROVIDER \\ --name scone-cli \\ --entrypoint sh \\ $CLI_IMAGE \\ -c \"sleep 7200\" docker exec -it scone-cli bash Inside of the CLI container, run: ./templates/attest-cas.sh Create a CAS namespace: scone session create --use-env /templates/namespace.template.yml Submit the templates by running the following command. The option --use-env will allow the CLI to use the environment to replace variables inside of the session template. scone session create --use-env /templates/session.template.yml","title":"Submit policies"},{"location":"MicrosoftAzureIntegrations/#start-application","text":"Run the application without attestation: the application will crash because there are no certificates in the expected locations. helm install api-no-attestation deploy/helm/rest-api-sample \\ --set service.type = LoadBalancer \\ --set useSGXDevPlugin = azure Now, run the application with attestation. The REST API should start once it gets attested and the appropriate secret and certificates\u2014retrieved from AKV\u2014are transparently and securely injected into the enclave's filesystem and environment. export SCONE_CONFIG_ID = $CAS_NAMESPACE / $PYTHON_SESSION_NAME /rest-api helm install api deploy/helm/rest-api-sample \\ --set service.type = LoadBalancer \\ --set useSGXDevPlugin = azure \\ --set scone.cas = $SCONE_CAS_ADDR \\ --set scone.configId = $SCONE_CONFIG_ID Retrieve the public IP address for our REST API: export API_ADDR = $( kubectl get svc api-rest-api-sample --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Access the REST API using cURL. Please note that you must use the Common Name as specified in the certificate that you created in AKV. curl https://rest-api.scone.sample:4996/secret --resolve rest-api.scone.sample:4996: $API_ADDR If you created a self-signed certificate in AKV, you need to pass -k flag to cURL. The expected output looks like: { \"access_timestamp\" : 1615855461.7656054 , \"secret\" : \"a13f688c788626380ae1209e31e664891fc19dcdd0cfa29c27ca7e6e16b83a95\" }","title":"Start application"},{"location":"MicrosoftAzureIntegrations/#limitations","text":"Some MAA instances run inside of enclaves. We currently do not support attestation the MAA instance itself. AAD tokens have a maximum expiration time of 24 hours. We currently do not support the renewal of AAD tokens at runtime. They are renewed upon every enclave startup only.","title":"Limitations"},{"location":"MrEnclave/","text":"Determining MrEnclave An enclave is identified by a hash value which is called MrEnclave . This hash is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave . To determine MrEnclave , we provide a simple way to determine MrEnclave on the developer site via environment variable SCONE_HASH=1 . Example: MrEnclave of Python Let us determine MrEnclave of our python interpreter. We start the container and then set environment variable SCONE_HASH=1 to ask SCONE to print MrEnclave and then terminate and SCONE_ALPINE=1 to ensure that the application is indeed started with SCONE. Note When setting SCONE_HASH=1 the program is not executed - only MrEnclave is printed on stdout.** > docker run -it registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 sh $ SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541 Now, let us change the heap size via environment variable SCONE_HEAP by asking for a 2GB heap: $ SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python aa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465 By default, SCONE does not permit to load dynamic libraries after startup. By setting SCONE_ALLOW_DLOPEN=1 , we permit to load dynamic libraries during runtime. This changes MrEnclave : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Using debug output As an alternative to SCONE_HASH=1 is to print MrEnclave via debug messages by setting SCONE_VERSION=1 : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_VERSION = 1 SCONE_ALPINE = 1 /usr/local/bin/python export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( protected ) export SCONE_MPROTECT = no Revision: b6a40e091e2adb253f019401723d2a734e887a74 ( Fri Jan 26 07 :44:44 2018 +0100 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Enclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Python 2 .7.14 ( default, Jan 10 2018 , 05 :35:30 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information.","title":"MrEnclave"},{"location":"MrEnclave/#determining-mrenclave","text":"An enclave is identified by a hash value which is called MrEnclave . This hash is determined by content of the pages of an enclave and the access rights. In particular, the means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave . To determine MrEnclave , we provide a simple way to determine MrEnclave on the developer site via environment variable SCONE_HASH=1 .","title":"Determining MrEnclave"},{"location":"MrEnclave/#example-mrenclave-of-python","text":"Let us determine MrEnclave of our python interpreter. We start the container and then set environment variable SCONE_HASH=1 to ask SCONE to print MrEnclave and then terminate and SCONE_ALPINE=1 to ensure that the application is indeed started with SCONE. Note When setting SCONE_HASH=1 the program is not executed - only MrEnclave is printed on stdout.** > docker run -it registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 sh $ SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 5430b3c0ab0e8a24ea4481e6022704cdbbcff68f6457eb0cdeaecfd734fec541 Now, let us change the heap size via environment variable SCONE_HEAP by asking for a 2GB heap: $ SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python aa25d6e1863819fca72f4f3315131ba4a438d1e1643c030190ca665215912465 By default, SCONE does not permit to load dynamic libraries after startup. By setting SCONE_ALLOW_DLOPEN=1 , we permit to load dynamic libraries during runtime. This changes MrEnclave : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_HASH = 1 SCONE_ALPINE = 1 /usr/local/bin/python 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508","title":"Example: MrEnclave of Python"},{"location":"MrEnclave/#using-debug-output","text":"As an alternative to SCONE_HASH=1 is to print MrEnclave via debug messages by setting SCONE_VERSION=1 : $ SCONE_ALLOW_DLOPEN = 1 SCONE_HEAP = 2G SCONE_VERSION = 1 SCONE_ALPINE = 1 /usr/local/bin/python export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( protected ) export SCONE_MPROTECT = no Revision: b6a40e091e2adb253f019401723d2a734e887a74 ( Fri Jan 26 07 :44:44 2018 +0100 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Enclave hash: 9c56db536e046a5fb84a5c482ce86e6592071dff75dc0e3eb27d701cf2c40508 Python 2 .7.14 ( default, Jan 10 2018 , 05 :35:30 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information.","title":"Using debug output"},{"location":"Nodejs/","text":"Node We provide a Node 10.14 image (and other versions) that run inside of an enclave: docker pull registry.scontain.com:5050/sconecuratedimages/apps:node-10.14-alpine Example Let's look at a little hello world program. First, we need to start a node container. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker run -it $MOUNT_SGXDEVICE -p3000:3000 registry.scontain.com:5050/sconecuratedimages/apps:node-10.14-alpine sh In case you have no sgx driver installed, the programs will then run in SIM mode , i.e., the SCONE software runs but in native mode and not inside an enclave. Do not add npm The new node container images have npm already installed. If one would add nodejs-npm with apk , the native version will be installed and the SCONE version is replaced. # DO NOT EXECUTE apk add --no-cache nodejs-npm # Ensure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB: export SCONE_HEAP=1G We create a new application myapp : mkdir myapp cd myapp cat > package.json << EOF { \"name\": \"myapp\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"app.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } EOF We install express with the help of npm : npm install express --save Let's store the hello world code: cat > app.js << EOF var express = require('express'); var app = express(); app.get('/', function (req, res) { res.send('Hello World!'); }); app.listen(3000, function () { console.log('Example app listening on port 3000!'); }); EOF We can run this application inside of an enclave with node . We can also enable some debug messages by setting environment variable SCONE_VERSION=1 to print that we run inside of an enclave: SCONE_VERSION = 1 node app.js This results in an output like this: export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 4294967296 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: e349ed6e4821f0cbfe895413c616409848216173 ( Wed Feb 28 19 :28:04 2018 +0100 ) Branch: master Configure options: --enable-shared --enable-debug --prefix = /builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb Example app listening on port 3000 ! You can now try to send a request to myapp from another shell in the container. Assuming that you did not start a new container in meantime, execute in another shell of your host: docker exec -it $( docker ps -l -q ) sh Inside of the container, first install curl and then query myapp : apk add --no-cache curl curl localhost:3000/ This results in an output like this: Hello World!/ # Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. errno ENOSYS SCONE does not yet support the fork system call (- this will happen later this year). If you spawn processes, there will be some error message like: npm ERR! spawn ENOSYS Environment variables SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . Environment variable SCONE_VERSION=1 prints debug messages - to show that the program runs inside of an enclave. SCONE_MODE=hw enforce that program runs in hardware enclave. By default, we set SCONE_MODE=auto which uses hardware enclave if available and software emulation otherwise. Dockerfile The above example, you could more easily put the following text in a Dockerfile: FROM registry.scontain.com:5050/sconecuratedimages/apps:node-10.14-alpine ENV SCONE_HEAP = 1G EXPOSE 3000 RUN apk add --no-cache nodejs-npm \\ && mkdir myapp \\ && cd myapp \\ && echo \"{\" > package.json \\ && echo '\"name\": \"myapp\",' >> package.json \\ && echo '\"version\": \"1.0.0\",' >> package.json \\ && echo '\"description\": \"\",' >> package.json \\ && echo '\"main\": \"app.js\",' >> package.json \\ && echo '\"scripts\":' >> package.json { \\ && echo ' \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"' >> package.json \\ && echo '},' >> package.json \\ && echo '\"author\": \"\",' >> package.json \\ && echo '\"license\": \"ISC\"' >> package.json \\ && echo '}' >> package.json \\ && npm install express --save \\ && echo \"var express = require('express');\" > app.js \\ && echo \"var app = express();\" >> app.js \\ && echo \"app.get('/', function (req, res) {\" >> app.js \\ && echo \" res.send('Hello World!');\" >> app.js \\ && echo \"});\" >> app.js \\ && echo \"app.listen(3000, function () {\" >> app.js \\ && echo \" console.log('Example app listening on port 3000!');\" >> app.js \\ && echo \"});\" >> app.js CMD SCONE_VERSION = 1 node /myapp/app.js Now create an image myapp as follows: docker build -t myapp . You can run a container of this image as a daemon as follows: docker run -d -p 3000 :3000 myapp You can now query myapp as follows: curl localhost:3000 This results in an output like this: Hello World! Attestation and Secret Provisioning We describe how to attest a node application and to provision secrets to the node app here .","title":"Node"},{"location":"Nodejs/#node","text":"We provide a Node 10.14 image (and other versions) that run inside of an enclave: docker pull registry.scontain.com:5050/sconecuratedimages/apps:node-10.14-alpine","title":"Node"},{"location":"Nodejs/#example","text":"Let's look at a little hello world program. First, we need to start a node container. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker run -it $MOUNT_SGXDEVICE -p3000:3000 registry.scontain.com:5050/sconecuratedimages/apps:node-10.14-alpine sh In case you have no sgx driver installed, the programs will then run in SIM mode , i.e., the SCONE software runs but in native mode and not inside an enclave. Do not add npm The new node container images have npm already installed. If one would add nodejs-npm with apk , the native version will be installed and the SCONE version is replaced. # DO NOT EXECUTE apk add --no-cache nodejs-npm # Ensure we can run even in a resource-constrainted VM by setting the maximum heap size to a reasonable value of 1GB: export SCONE_HEAP=1G We create a new application myapp : mkdir myapp cd myapp cat > package.json << EOF { \"name\": \"myapp\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"app.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\" } EOF We install express with the help of npm : npm install express --save Let's store the hello world code: cat > app.js << EOF var express = require('express'); var app = express(); app.get('/', function (req, res) { res.send('Hello World!'); }); app.listen(3000, function () { console.log('Example app listening on port 3000!'); }); EOF We can run this application inside of an enclave with node . We can also enable some debug messages by setting environment variable SCONE_VERSION=1 to print that we run inside of an enclave: SCONE_VERSION = 1 node app.js This results in an output like this: export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 4294967296 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: e349ed6e4821f0cbfe895413c616409848216173 ( Wed Feb 28 19 :28:04 2018 +0100 ) Branch: master Configure options: --enable-shared --enable-debug --prefix = /builds/scone/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 28cf4f0953ba54af02b9d042fa2ec88a832d749ae4e5395cabd50369e72a5dcb Example app listening on port 3000 ! You can now try to send a request to myapp from another shell in the container. Assuming that you did not start a new container in meantime, execute in another shell of your host: docker exec -it $( docker ps -l -q ) sh Inside of the container, first install curl and then query myapp : apk add --no-cache curl curl localhost:3000/ This results in an output like this: Hello World!/ # Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. errno ENOSYS SCONE does not yet support the fork system call (- this will happen later this year). If you spawn processes, there will be some error message like: npm ERR! spawn ENOSYS","title":"Example"},{"location":"Nodejs/#environment-variables","text":"SGXv1 cannot dynamically increase the memory of an enclave. Hence, we have to determine the maximum heap (and stack) size at program start: you can increase the heap by setting environment variable SCONE_HEAP , e.g., SCONE_HEAP=8G . In case you run out of memory inside the enclave, increase the heap size. In case your program gets killed by the OS, you might have selected a too large heap that is not supported by your VM or your host. Similarily, you can increase the stack size of threads running inside of enclaves by setting environment variable SCONE_STACK . Environment variable SCONE_VERSION=1 prints debug messages - to show that the program runs inside of an enclave. SCONE_MODE=hw enforce that program runs in hardware enclave. By default, we set SCONE_MODE=auto which uses hardware enclave if available and software emulation otherwise.","title":"Environment variables"},{"location":"Nodejs/#dockerfile","text":"The above example, you could more easily put the following text in a Dockerfile: FROM registry.scontain.com:5050/sconecuratedimages/apps:node-10.14-alpine ENV SCONE_HEAP = 1G EXPOSE 3000 RUN apk add --no-cache nodejs-npm \\ && mkdir myapp \\ && cd myapp \\ && echo \"{\" > package.json \\ && echo '\"name\": \"myapp\",' >> package.json \\ && echo '\"version\": \"1.0.0\",' >> package.json \\ && echo '\"description\": \"\",' >> package.json \\ && echo '\"main\": \"app.js\",' >> package.json \\ && echo '\"scripts\":' >> package.json { \\ && echo ' \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"' >> package.json \\ && echo '},' >> package.json \\ && echo '\"author\": \"\",' >> package.json \\ && echo '\"license\": \"ISC\"' >> package.json \\ && echo '}' >> package.json \\ && npm install express --save \\ && echo \"var express = require('express');\" > app.js \\ && echo \"var app = express();\" >> app.js \\ && echo \"app.get('/', function (req, res) {\" >> app.js \\ && echo \" res.send('Hello World!');\" >> app.js \\ && echo \"});\" >> app.js \\ && echo \"app.listen(3000, function () {\" >> app.js \\ && echo \" console.log('Example app listening on port 3000!');\" >> app.js \\ && echo \"});\" >> app.js CMD SCONE_VERSION = 1 node /myapp/app.js Now create an image myapp as follows: docker build -t myapp . You can run a container of this image as a daemon as follows: docker run -d -p 3000 :3000 myapp You can now query myapp as follows: curl localhost:3000 This results in an output like this: Hello World!","title":"Dockerfile"},{"location":"Nodejs/#attestation-and-secret-provisioning","text":"We describe how to attest a node application and to provision secrets to the node app here .","title":"Attestation and Secret Provisioning"},{"location":"Python/","text":"Python SCONE supports running Python programs inside of SGX enclaves. We maintain Docker images for various Python versions / Python engines like: Python 3.5 : registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5 (community edition) We have most of the current Python versions in repo registry.scontain.com:5050/sconecuratedimages/python like for example Python 3.9 : registry.scontain.com:5050/sconecuratedimages/python:3.9.0-alpine3.12-scone5.1 This Python repo requires a subscription. PyPy for SCONE PyPySCONE's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is PyPy inside an enclave faster than native Python. Workflow SCONE supports the typical Docker workflow to create Docker images that contain the Python engine as well as the Python program. SCONE supports the encryption of the Python programs to ensure both the confidentiality as well as the integrity of the programs. A typical workflow might look like this: We at scontain.com maintain the SCONE-Python image and push this to registry.scontain.com:5050 . This image can be used by authorized application developers to add encrypted Python programs and encrypted libraries. The SCONE runtime of the Python engine needs to get access to the encryption key to be able to decrypt transparently the Python scripts for the Python engine. To do so, the application developer defines a security policy that ensures that only the Python engine that executes the application of the application provider gets access to this encryption key. When the Python engine starts completely inside of a SGX enclave. The SCONE runtime transparently attests the Python engine as well as the filesystem : Only if both the Python engine has the expected MrEnclave as well as the filesystem state is exactly as expected, the SCONE runtime gets the encryption key from the SCONE CAS (Configuration and Attestation Service). The application developer therefore adds the expected MrEnclave and the initial filesystem state in form of a security policy to SCONE CAS. Complex Workflows SCONE supports more complex workflows in which the user can also specify encrypted volumes for input as well as output data. We explain a more complex example in the context of our blender use case . Image Getting access to Python Images You need access to a private SCONTAIN hub repository registry.scontain.com:5050/sconecuratedimages/apps to be able to evaluate our Python images. Please register a free account at gitlab.scontain.com . Currently, we provide a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine. You can pull this image as follows: docker pull registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6 Python Interpreter To run the Python interpreter inside an enclave in interactive mode, first start the container: docker run --rm -it $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6 sh The execute python inside of the container: SCONE_HEAP = 256M SCONE_VERSION = 1 python Since we set SCONE_VERSION=1 , we get the following outputs 1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 268435456 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab ( Thu Aug 16 16 :45:05 2018 +0200 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /mnt/ssd/franz/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: f129bbd19627367c03e2980c0f04a32809a7aae1d795a75220d9054daf537b30 Python 2 .7.13 ( default, Jun 1 2018 , 13 :20:58 ) [ GCC 7 .3.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. The meaning of protected versus unprotected library is explained in the faq . Running an application Say, you have a Python application called myapp.py in your current directory. To execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. To run Python inside of an enclave, you can set the environment variable SCONE_MODE=HW and SCONE_ALPINE=1 . To issue some debug messages that show that we are running inside an enclave, set SCONE_VERSION=1 In general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of MRENCLAVE , i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set SCONE_ALLOW_DLOPEN=2 . For operations, the environment variables are set by the CAS and you must set SCONE_ALLOW_DLOPEN either to SCONE_ALLOW_DLOPEN=1 to enable loading of dynamic libraries or must not define SCONE_ALLOW_DLOPEN . Python applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable SCONE_HEAP and the stack size via STACK_SIZE at startup. Python program exits Python does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like, SCONE_HEAP=256M to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.** Note that you can set the environment variable of a process - in our case python - running inside a container with docker option -e : docker run --rm $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp -e SCONE_HEAP = 256M -e SCONE_MODE = HW -e SCONE_ALLOW_DLOPEN = 2 -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6 python myapp.py Will produce an output like export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 ... NumPy Let's see how we can install some extra packages that your python program might need. Let us focus on NumPy first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a Dockerfile . First, we determine which SGX device to mount with function determine_sgx_device and start the SCONE Python image: determine_sgx_device docker run $MOUNT_SGXDEVICE -it --rm registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6 sh This is a minimal image and you need to add some packages to be able to install packages that compile external code: apk add --no-cache bats libbsd openssl musl-dev build-base We then install numpy inside of the container with the help of pip : pip install numpy == 1 .14.5 This results in an output like Collecting numpy == 1 .14.5 Downloading https://files.pythonhosted.org/packages/d5/6e/f00492653d0fdf6497a181a1c1d46bbea5a2383e7faf4c8ca6d6f3d2581d/numpy-1.14.5.zip ( 4 .9MB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 .9MB 375kB/s Installing collected packages: numpy Running setup.py install for numpy ... done Successfully installed numpy-1.14.5 Ok, let's try to execute some examples with NumPy . Let's run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages: SCONE_HEAP = 256M SCONE_VERSION = 1 python during startup this issues the following messages export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 10000000000 export SCONE_STACK = 0 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes export SCONE_ALLOW_DLOPEN2 = yes Revision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Python 2 .7.14 ( default, Dec 19 2017 , 22 :29:22 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Now, we can import numpy and execute some commands: >>> import numpy as np >>> a = np.arange ( 15 ) .reshape ( 3 , 5 ) >>> a array ([[ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ]]) >>> a.shape ( 3 , 5 ) >>> a.ndim 2 >>> a.dtype.name 'int64' >>> a.itemsize 8 >>> type ( a ) < type 'numpy.ndarray' > >>> Cairo Let's look at another popular library: the cairo graphics library. cairo is written in C and has Python bindings provided by package pycairo . In this case, we need to install the C-library first: In Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows: apk add --no-cache cairo-dev cairo fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ( 1 /55 ) Installing expat-dev ( 2 .2.5-r0 ) ... ( 55 /55 ) Installing cairo-dev ( 1 .14.10-r0 ) Executing busybox-1.27.2-r6.trigger Executing glib-2.54.2-r0.trigger No schema files found: doing nothing. OK: 297 MiB in 112 packages $ Now we can install the Python bindings of cairo with pip : pip install pycairo Collecting pycairo Downloading pycairo-1.15.4.tar.gz ( 178kB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 184kB 1 .7MB/s Building wheels for collected packages: pycairo Running setup.py bdist_wheel for pycairo ... done Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f Successfully built pycairo Installing collected packages: pycairo Successfully installed pycairo-1.15.4 We can now start Python again via SCONE_HEAP = 256M SCONE_VERSION = 1 python before we execute some cairo examples: >>> import cairo >>> import math >>> WIDTH, HEIGHT = 256 , 256 >>> >>> surface = cairo.ImageSurface ( cairo.FORMAT_ARGB32, WIDTH, HEIGHT ) >>> ctx = cairo.Context ( surface ) >>> ctx.scale ( WIDTH, HEIGHT ) # Normalizing the canvas >>> >>> pat = cairo.LinearGradient ( 0 .0, 0 .0, 0 .0, 1 .0 ) >>> pat.add_color_stop_rgba ( 1 , 0 .7, 0 , 0 , 0 .5 ) # First stop, 50% opacity >>> pat.add_color_stop_rgba ( 0 , 0 .9, 0 .7, 0 .2, 1 ) # Last stop, 100% opacity >>> >>> ctx.rectangle ( 0 , 0 , 1 , 1 ) # Rectangle(x0, y0, x1, y1) >>> ctx.set_source ( pat ) >>> ctx.fill () >>> ctx.translate ( 0 .1, 0 .1 ) # Changing the current transformation matrix >>> >>> ctx.move_to ( 0 , 0 ) >>> # Arc(cx, cy, radius, start_angle, stop_angle) ... ctx.arc ( 0 .2, 0 .1, 0 .1, -math.pi/2, 0 ) >>> ctx.line_to ( 0 .5, 0 .1 ) # Line to (x,y) >>> # Curve(x1, y1, x2, y2, x3, y3) ... ctx.curve_to ( 0 .5, 0 .2, 0 .5, 0 .4, 0 .2, 0 .8 ) >>> ctx.close_path () >>> >>> ctx.set_source_rgb ( 0 .3, 0 .2, 0 .5 ) # Solid color >>> ctx.set_line_width ( 0 .02 ) >>> ctx.stroke () >>> surface.write_to_png ( \"example.png\" ) # Output to PNG >>> exit () This generates a file example.png in the working directory. Example Let's look at another example: We use pip to install a Python chess library. Then we run Python inside of an enclave and import the chess library. We use the Scholar's mate example from https://pypi.python.org/pypi/python-chess Limitation We do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner. Screencast The other environment variables are explained below. Also read Section Environment Variables for further details. \u21a9","title":"Python"},{"location":"Python/#python","text":"SCONE supports running Python programs inside of SGX enclaves. We maintain Docker images for various Python versions / Python engines like: Python 3.5 : registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5 (community edition) We have most of the current Python versions in repo registry.scontain.com:5050/sconecuratedimages/python like for example Python 3.9 : registry.scontain.com:5050/sconecuratedimages/python:3.9.0-alpine3.12-scone5.1 This Python repo requires a subscription. PyPy for SCONE PyPySCONE's speed is close to PyPy (\"just in time Python\") and in almost all SpeedCenter benchmarks is PyPy inside an enclave faster than native Python.","title":"Python"},{"location":"Python/#workflow","text":"SCONE supports the typical Docker workflow to create Docker images that contain the Python engine as well as the Python program. SCONE supports the encryption of the Python programs to ensure both the confidentiality as well as the integrity of the programs. A typical workflow might look like this: We at scontain.com maintain the SCONE-Python image and push this to registry.scontain.com:5050 . This image can be used by authorized application developers to add encrypted Python programs and encrypted libraries. The SCONE runtime of the Python engine needs to get access to the encryption key to be able to decrypt transparently the Python scripts for the Python engine. To do so, the application developer defines a security policy that ensures that only the Python engine that executes the application of the application provider gets access to this encryption key. When the Python engine starts completely inside of a SGX enclave. The SCONE runtime transparently attests the Python engine as well as the filesystem : Only if both the Python engine has the expected MrEnclave as well as the filesystem state is exactly as expected, the SCONE runtime gets the encryption key from the SCONE CAS (Configuration and Attestation Service). The application developer therefore adds the expected MrEnclave and the initial filesystem state in form of a security policy to SCONE CAS.","title":"Workflow"},{"location":"Python/#complex-workflows","text":"SCONE supports more complex workflows in which the user can also specify encrypted volumes for input as well as output data. We explain a more complex example in the context of our blender use case .","title":"Complex Workflows"},{"location":"Python/#image","text":"Getting access to Python Images You need access to a private SCONTAIN hub repository registry.scontain.com:5050/sconecuratedimages/apps to be able to evaluate our Python images. Please register a free account at gitlab.scontain.com . Currently, we provide a simple Python 2.7 image that is based on the standard Python image python:2.7-alpine. You can pull this image as follows: docker pull registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6","title":"Image"},{"location":"Python/#python-interpreter","text":"To run the Python interpreter inside an enclave in interactive mode, first start the container: docker run --rm -it $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6 sh The execute python inside of the container: SCONE_HEAP = 256M SCONE_VERSION = 1 python Since we set SCONE_VERSION=1 , we get the following outputs 1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 268435456 export SCONE_STACK = 4194304 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab ( Thu Aug 16 16 :45:05 2018 +0200 ) Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /mnt/ssd/franz/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: f129bbd19627367c03e2980c0f04a32809a7aae1d795a75220d9054daf537b30 Python 2 .7.13 ( default, Jun 1 2018 , 13 :20:58 ) [ GCC 7 .3.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Potential error messages: Could not create enclave: Error opening SGX device Your machine / container does not support SGX. Set mode to automatic via SCONE_MODE=AUTO : in AUTO mode, SCONE will use SGX enclaves when available and emulation mode otherwise. Killed Your machine / container has most likely too little memory: the Linux OOM (Out Of Memory) killer, terminated your program. Try to reduce memory size by reducing environment variable SCONE_HEAP appropriately. The meaning of protected versus unprotected library is explained in the faq .","title":"Python Interpreter"},{"location":"Python/#running-an-application","text":"Say, you have a Python application called myapp.py in your current directory. To execute this with Pyhton 2.7 inside an enclave, you need to set some environment variables. To run Python inside of an enclave, you can set the environment variable SCONE_MODE=HW and SCONE_ALPINE=1 . To issue some debug messages that show that we are running inside an enclave, set SCONE_VERSION=1 In general, we only permit the loading of dynamic libraries during the startup of a program - these libraries are part of MRENCLAVE , i.e., the hash of the enclave. To enable the loading of dynamic libraries after startup (and without requiring the authentication of this library via the file shield), one can set SCONE_ALLOW_DLOPEN=2 . For operations, the environment variables are set by the CAS and you must set SCONE_ALLOW_DLOPEN either to SCONE_ALLOW_DLOPEN=1 to enable loading of dynamic libraries or must not define SCONE_ALLOW_DLOPEN . Python applications often require large heaps and large stacks. The current SGX CPUs (SGXv1) do not permit to increase the size of enclaves dynamically. This implies that enclaves might run out of memory if the initial enclave size was set to small. Selecting large enclave size by default would result in long startup times for all programs. SCONE permits to set the heap size via environment variable SCONE_HEAP and the stack size via STACK_SIZE at startup. Python program exits Python does not always deal gracefully with out of memory situations: often, it terminates with some misleading error message if Python runs out of heap or stack memory. Please try to give python sufficient stack and heap size if this happens. We recommend to start with a large heap, like, SCONE_HEAP=256M to ensure that Python has sufficient heap. If your program runs without any problem with a large heap, you can try to reduce the heap size to speedup program startup times.** Note that you can set the environment variable of a process - in our case python - running inside a container with docker option -e : docker run --rm $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp -e SCONE_HEAP = 256M -e SCONE_MODE = HW -e SCONE_ALLOW_DLOPEN = 2 -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6 python myapp.py Will produce an output like export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 ...","title":"Running an application"},{"location":"Python/#numpy","text":"Let's see how we can install some extra packages that your python program might need. Let us focus on NumPy first, a very popular package for scientific computing. Note that the following steps you would typically perform as part of a Dockerfile . First, we determine which SGX device to mount with function determine_sgx_device and start the SCONE Python image: determine_sgx_device docker run $MOUNT_SGXDEVICE -it --rm registry.scontain.com:5050/sconecuratedimages/apps:python-2-alpine3.6 sh This is a minimal image and you need to add some packages to be able to install packages that compile external code: apk add --no-cache bats libbsd openssl musl-dev build-base We then install numpy inside of the container with the help of pip : pip install numpy == 1 .14.5 This results in an output like Collecting numpy == 1 .14.5 Downloading https://files.pythonhosted.org/packages/d5/6e/f00492653d0fdf6497a181a1c1d46bbea5a2383e7faf4c8ca6d6f3d2581d/numpy-1.14.5.zip ( 4 .9MB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 .9MB 375kB/s Installing collected packages: numpy Running setup.py install for numpy ... done Successfully installed numpy-1.14.5 Ok, let's try to execute some examples with NumPy . Let's run Python inside an enclave, give it plenty of heap memory and ask SCONE to print some debug messages: SCONE_HEAP = 256M SCONE_VERSION = 1 python during startup this issues the following messages export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 10000000000 export SCONE_STACK = 0 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes export SCONE_ALLOW_DLOPEN2 = yes Revision: 7950fbd1a699ba15f9382ebaefc3ce0d4090801f Branch: master ( dirty ) Configure options: --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Python 2 .7.14 ( default, Dec 19 2017 , 22 :29:22 ) [ GCC 6 .4.0 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. >>> Now, we can import numpy and execute some commands: >>> import numpy as np >>> a = np.arange ( 15 ) .reshape ( 3 , 5 ) >>> a array ([[ 0 , 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 , 9 ] , [ 10 , 11 , 12 , 13 , 14 ]]) >>> a.shape ( 3 , 5 ) >>> a.ndim 2 >>> a.dtype.name 'int64' >>> a.itemsize 8 >>> type ( a ) < type 'numpy.ndarray' > >>>","title":"NumPy"},{"location":"Python/#cairo","text":"Let's look at another popular library: the cairo graphics library. cairo is written in C and has Python bindings provided by package pycairo . In this case, we need to install the C-library first: In Alpine Linux - which is the basis of the SCONE Python image - we can install cairo as follows: apk add --no-cache cairo-dev cairo fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz ( 1 /55 ) Installing expat-dev ( 2 .2.5-r0 ) ... ( 55 /55 ) Installing cairo-dev ( 1 .14.10-r0 ) Executing busybox-1.27.2-r6.trigger Executing glib-2.54.2-r0.trigger No schema files found: doing nothing. OK: 297 MiB in 112 packages $ Now we can install the Python bindings of cairo with pip : pip install pycairo Collecting pycairo Downloading pycairo-1.15.4.tar.gz ( 178kB ) 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 184kB 1 .7MB/s Building wheels for collected packages: pycairo Running setup.py bdist_wheel for pycairo ... done Stored in directory: /root/.cache/pip/wheels/99/a6/16/79c5186b0ead4be059ce3102496b1ff776776b31da8e51af8f Successfully built pycairo Installing collected packages: pycairo Successfully installed pycairo-1.15.4 We can now start Python again via SCONE_HEAP = 256M SCONE_VERSION = 1 python before we execute some cairo examples: >>> import cairo >>> import math >>> WIDTH, HEIGHT = 256 , 256 >>> >>> surface = cairo.ImageSurface ( cairo.FORMAT_ARGB32, WIDTH, HEIGHT ) >>> ctx = cairo.Context ( surface ) >>> ctx.scale ( WIDTH, HEIGHT ) # Normalizing the canvas >>> >>> pat = cairo.LinearGradient ( 0 .0, 0 .0, 0 .0, 1 .0 ) >>> pat.add_color_stop_rgba ( 1 , 0 .7, 0 , 0 , 0 .5 ) # First stop, 50% opacity >>> pat.add_color_stop_rgba ( 0 , 0 .9, 0 .7, 0 .2, 1 ) # Last stop, 100% opacity >>> >>> ctx.rectangle ( 0 , 0 , 1 , 1 ) # Rectangle(x0, y0, x1, y1) >>> ctx.set_source ( pat ) >>> ctx.fill () >>> ctx.translate ( 0 .1, 0 .1 ) # Changing the current transformation matrix >>> >>> ctx.move_to ( 0 , 0 ) >>> # Arc(cx, cy, radius, start_angle, stop_angle) ... ctx.arc ( 0 .2, 0 .1, 0 .1, -math.pi/2, 0 ) >>> ctx.line_to ( 0 .5, 0 .1 ) # Line to (x,y) >>> # Curve(x1, y1, x2, y2, x3, y3) ... ctx.curve_to ( 0 .5, 0 .2, 0 .5, 0 .4, 0 .2, 0 .8 ) >>> ctx.close_path () >>> >>> ctx.set_source_rgb ( 0 .3, 0 .2, 0 .5 ) # Solid color >>> ctx.set_line_width ( 0 .02 ) >>> ctx.stroke () >>> surface.write_to_png ( \"example.png\" ) # Output to PNG >>> exit () This generates a file example.png in the working directory.","title":"Cairo"},{"location":"Python/#example","text":"Let's look at another example: We use pip to install a Python chess library. Then we run Python inside of an enclave and import the chess library. We use the Scholar's mate example from https://pypi.python.org/pypi/python-chess","title":"Example"},{"location":"Python/#limitation","text":"We do not yet support fork, i.e., you spawn new processes from within your Python programs. We are currently working on removing this limitation of SCONE. Until then, we expect you to have an external spawner.","title":"Limitation"},{"location":"Python/#screencast","text":"The other environment variables are explained below. Also read Section Environment Variables for further details. \u21a9","title":"Screencast"},{"location":"R/","text":"R We just added experimental support for R. For now, Rscript only support a single script as argument: no expressions or other arguments supported. Image You can pull this image as follows: docker pull registry.scontain.com:5050/sconecuratedimages/apps:R Running R First, we determine which SGX device to mount with function determine_sgx_device . To start R, just execute: determine_sgx_device docker run $MOUNT_SGXDEVICE -it --rm registry.scontain.com:5050/sconecuratedimages/apps:R This will output something like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=4194304 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab (Thu Aug 16 16:45:05 2018 +0200) Branch: new-docker-images-cf (dirty) Configure options: --enable-shared --enable-debug --prefix=/home/christof/GIT/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 71e730d77fcae6fd37b80cd8669f2d75b8e58dbba80afa48929ae817bf263bb0 Warning message: failed to set alternate signal stack R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) Example Execute some first R program (taken from www.rexamples.com ): a <- 42 A <- a * 2 # R is case sensitive print ( a ) cat ( A , \"\\n\" ) # \"84\" is concatenated with \"\\n\" if ( A > a ) # true, 84 > 42 { cat ( A , \">\" , a , \"\\n\" ) } This will result in an output: 84 > 42 Example 2 A somewhat more complex example from www.rexamples.com : #utility functions readinteger <- function () { n <- readline ( prompt = \"Enter an integer: \" ) if ( ! grepl ( \"^[0-9]+$\" , n )) { return ( readinteger ()) } return ( as.integer ( n )) } # real program start here num <- round ( runif ( 1 ) * 100 , digits = 0 ) guess <- -1 cat ( \"Guess a number between 0 and 100.\\n\" ) while ( guess != num ) { guess <- readinteger () if ( guess == num ) { cat ( \"Congratulations,\" , num , \"is right.\\n\" ) } else if ( guess < num ) { cat ( \"It's bigger!\\n\" ) } else if ( guess > num ) { cat ( \"It's smaller!\\n\" ) } } This will result in an otherput as follows: Enter an integer: 50 It's bigger! Enter an integer: 75 It's bigger! Enter an integer: 87 It's smaller! Enter an integer: 82 It's smaller! Enter an integer: 78 It's bigger! Enter an integer: 80 It's bigger! Enter an integer: 81 Congratulations, 81 is right. Screencast","title":"R"},{"location":"R/#r","text":"We just added experimental support for R. For now, Rscript only support a single script as argument: no expressions or other arguments supported.","title":"R"},{"location":"R/#image","text":"You can pull this image as follows: docker pull registry.scontain.com:5050/sconecuratedimages/apps:R","title":"Image"},{"location":"R/#running-r","text":"First, we determine which SGX device to mount with function determine_sgx_device . To start R, just execute: determine_sgx_device docker run $MOUNT_SGXDEVICE -it --rm registry.scontain.com:5050/sconecuratedimages/apps:R This will output something like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=1073741824 export SCONE_STACK=4194304 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no Revision: d0afc0f23819476cbc7d944a20e91d79fcb6f9ab (Thu Aug 16 16:45:05 2018 +0200) Branch: new-docker-images-cf (dirty) Configure options: --enable-shared --enable-debug --prefix=/home/christof/GIT/subtree-scone/built/cross-compiler/x86_64-linux-musl Enclave hash: 71e730d77fcae6fd37b80cd8669f2d75b8e58dbba80afa48929ae817bf263bb0 Warning message: failed to set alternate signal stack R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit)","title":"Running R"},{"location":"R/#example","text":"Execute some first R program (taken from www.rexamples.com ): a <- 42 A <- a * 2 # R is case sensitive print ( a ) cat ( A , \"\\n\" ) # \"84\" is concatenated with \"\\n\" if ( A > a ) # true, 84 > 42 { cat ( A , \">\" , a , \"\\n\" ) } This will result in an output: 84 > 42","title":"Example"},{"location":"R/#example-2","text":"A somewhat more complex example from www.rexamples.com : #utility functions readinteger <- function () { n <- readline ( prompt = \"Enter an integer: \" ) if ( ! grepl ( \"^[0-9]+$\" , n )) { return ( readinteger ()) } return ( as.integer ( n )) } # real program start here num <- round ( runif ( 1 ) * 100 , digits = 0 ) guess <- -1 cat ( \"Guess a number between 0 and 100.\\n\" ) while ( guess != num ) { guess <- readinteger () if ( guess == num ) { cat ( \"Congratulations,\" , num , \"is right.\\n\" ) } else if ( guess < num ) { cat ( \"It's bigger!\\n\" ) } else if ( guess > num ) { cat ( \"It's smaller!\\n\" ) } } This will result in an otherput as follows: Enter an integer: 50 It's bigger! Enter an integer: 75 It's bigger! Enter an integer: 87 It's smaller! Enter an integer: 82 It's smaller! Enter an integer: 78 It's bigger! Enter an integer: 80 It's bigger! Enter an integer: 81 Congratulations, 81 is right.","title":"Example 2"},{"location":"R/#screencast","text":"","title":"Screencast"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/","text":"Running Java Applications in Scone with CAS-Policy Requirements The following steps assume that you have a running CAS container and a running LAS server and know its addresses. Some of the latest containers for CAS and LAS are available at docker pull registry.scontain.com:5050/sconecuratedimages/services:cas docker pull registry.scontain.com:5050/sconecuratedimages/kubernetes:las but the service repository requires a commercial subscription. You can use our public CAS instance at domain scone-cas.cf instead. Make sure the sgx driver and docker is installed and running correctly by doing the java hello world sample. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-8-alpine or, in case the container does not start the bash command automatically: docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-8-alpine /bin/sh For Java 11 or Java 15 instead of Java 8: docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-11-alpine /bin/sh docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-15-alpine /bin/sh We provide a quick hello-world sample in Java cat > HelloWorld.java << EOF import java.util.Map; public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); Map<String, String> env = System.getenv(); for (String envName : env.keySet()) { System.out.format(\"%s=%s%n\", envName, env.get(envName)); } } } EOF Compile it, by executing: javac HelloWorld.java Expected output: # javac HelloWorld.java Picked up JAVA_TOOL_OPTIONS: -Xmx256m # You need to set some environment variables: export SCONE_CAS_ADDR = scone-cas.cf # we use the public CAS service export SCONE_LAS_ADDR = 127 .0.0.1 # we must run a local LAS service export SCONE_VERSION = 1 # show the SCONE version export SCONE_LOG = 7 # maximum LOG level java HelloWorld Expected output after 2-3 minutes with a NUC and 8GB memory. Note that if you have a CPU that supports EDMM (Enclave dynamic memory management), the startup times will be much quicker. / # export SCONE_VERSION = 1 / # export SCONE_LOG = 7 / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG =/ etc / sgx - musl . conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version : 1.1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 : 29 : 26 2020 + 0000 ) Branch: 6 ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash : 7954791612 f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . Picked up JAVA_TOOL_OPTIONS : - Xmx256m [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / mountinfo is not supported [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / maps is not supported OpenJDK 64 - Bit Server VM warning : Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . Hello World PATH =/ usr / local / sbin :/ usr / local / bin :/ usr / sbin :/ usr / bin :/ sbin :/ bin :/ usr / lib / jvm / java - 1.8 - openjdk / jre / bin :/ usr / lib / jvm / java - 1.8 - openjdk / bin SCONE_VERSION = 1 SCONE_HEAP = 4 G SCONE_LOG = 7 SCONE_CAS_ADDR = scone - cas . cf JAVA_HOME =/ usr / lib / jvm / java - 1.8 - openjdk TERM = xterm LANG = C . UTF - 8 SCONE_ALPINE = 1 SCONE_ALLOW_DLOPEN = 2 HOSTNAME = 86 fd2f848e41 SCONE_MPROTECT = 1 SCONE_LAS_ADDR = 127.0.0.1 LD_LIBRARY_PATH =/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 / server :/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 :/ usr / lib / jvm / java - 1.8 - openjdk / jre /../ lib / amd64 JAVA_TOOL_OPTIONS =- Xmx256m PWD =/ HOME =/ root SHLVL = 2 / # Posting a SCONE-Policy to configure the execution inside of Enclave Using the CAS container to provision an execution inside of an enclave with environment variables etc. which were previously set. Therefore, the SCONE-Policy for Java executions must include the following parameters LD_LIBRARY_PATH and if necessary JAVA_TOOL_OPTIONS and the CLASSPATH of the required Java libraries: First, similar to the posting sessions tutorial , we create a yaml file for java 1.8 openjdk cat > sessionJavaHelloWorld.yml <<EOF name: java services: - name: hello mrenclaves: [7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb] # MRENCLAVE = Enclave hash: from execution with SCONE_VERSION=1 command: \"java HelloWorld\" pwd: / environment: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64\" JAVA_TOOL_OPTIONS: \"-Xmx256m\" CLASSPATH: \"/\" # path to directory contained your code TMP_SECRET_VAR: \"This is a protected secret distributed by Scone CAS!\" EOF We create client certificate and key material to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @sessionJavaHelloWorld.yml -X POST https:// $SCONE_CAS_ADDR :8081/session Expected output (please ignore the 'not-supported' fields): Created Session [ id = b11ac009c564f6e0c8c2b61d7b4774e89f896e83ec09ef3f919bd420aa3ab252, name = 'not-supported' , status = 'not-supported' ] If the output is not giving out an idea the session was not created. One possible reasons can be formatting errors like not using quotation marks (\"java Helloworld\") or single quotation marks ('java HelloWorld') If we have set up the SCONE_CAS_ADDR and SCONE_LAS_ADDR, we export the environment variable according to the policy's name \"java\" and the service \"hello\" export SCONE_CONFIG_ID = \"java/hello\" export SCONE_CAS_ADDR = $YOUR_CAS_ADDR export SCONE_LAS_ADDR = $YOUR_LAS_ADDR export SCONE_VERSION = 1 export SCONE_LOG = 7 Now if we start the command \"java HelloWorld\" again, we see the following output. / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version: 1 .1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 :29:26 2020 +0000 ) Branch: 6ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash: 7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | INFO ] src/shielding/crypto.c:222:crypto_get_identity () : Generated enclave certificate [ SCONE | INFO ] src/shielding/eai_attestor.c:174:eai_attestor_init () : Created TLS context to communicate with CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:288:eai_attestor_attest () : Sending Attestation Request to LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:311:eai_attestor_attest () : Got Quote from LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:324:eai_attestor_attest () : Sending enclave hello message to CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:349:eai_attestor_attest () : Successfully attested enclave via CAS [ SCONE | DEBUG ] src/process/init.c:639:__scone_prepare_secure_config () : Sending configuration request to CAS! [ SCONE | DEBUG ] src/process/init.c:666:__scone_prepare_secure_config () : Received configuration from CAS! [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. Picked up JAVA_TOOL_OPTIONS: -Xmx256m [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/mountinfo is not supported [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/maps is not supported OpenJDK 64 -Bit Server VM warning: Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World TMP_SECRET_VAR = This is a protected secret distributed by Scone CAS! CLASSPATH = / JAVA_TOOL_OPTIONS = -Xmx256m LD_LIBRARY_PATH = /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64 / # Note that the content of TMP_SECRET_VAR came from the Scone policy which was posted earlier to the CAS. And we see the following logfiles in the containers of CAS (in case you are running a local copy): ubuntu@kmaster:~$ docker logs cas_cas_1 [ 2020 -05-08T11:32:02Z DEBUG rustls::server::hs ] decided upon suite SupportedCipherSuite { suite: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, kx: ECDHE, bulk: AES_256_GCM, hash: SHA384, sign: ECDSA, enc_key_len: 32 , fixed_iv_len: 4 , explicit_nonce_len: 8 , hkdf_algorithm: Algorithm ( Algorithm ( SHA384 )) } [ 2020 -05-08T11:32:02Z DEBUG rustls::server::tls12 ] Session saved [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 518 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::EnclaveHello from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Fresh got message: EnclaveMessage::EnclaveHello [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Processing enclave hello message [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is valid, it has not been tampared with. [ 2020 -05-08T11:32:02Z WARN cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : CAS is running in debug mode, simulation mode or w/o SCONE runtime: It will accept quotes from SCONE quoting enclaves running in debug mode [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Hash of channel certificate: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976, SGX reportdata: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976 [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Secure channel terminates in enclave: Hash of channel certifacte matches SGX reportdata [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is signed by known and trusted quoting enclave: true [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Trust established into enclave integrity. Sending attestation complete message [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Complete to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 16 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::ConfigRequest from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Attested got message: EnclaveMessage::ConfigRequest [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Received configuration request for config id javax2/hello [ 2020 -05-08T11:32:02Z DEBUG cas_backend::backend ] Got request for service: hello in session: javax2. Attificate is: SCONE ( SCONEAttificate { identifier: \"791f84651890514eefe32ecfe54a75d95475636f61180a6ca1765cbbee0253dd\" , certificate: \"-----BEGIN CERTIFICATE-----\\nMIIBZjCB6qADAgECAhEA9vVqCQoOsEgtQ7Lo09QqNjAMBggqhkjOPQQDAwUAMA0x\\nCzAJBgNVBAYTAkRFMCAXDTE4MDkxNzE5MzAwMFoYDzIwNTAwMTAxMDEwMDAwWjAA\\nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAEoavE7JGhE5YqrBV979f8NTxUzGURrDE2\\naU3vuZukWkioO8g+3Ec+YaVMu0N3CWy0hnwKD4nyCr7criLlXhvRaxQdJdp6d+4b\\nMJXGtrgvcF4I1ZAb17FJOHoUD9Q5IVKSoxcwFTATBgNVHREEDDAKgghlbmNsYXZl\\nADAMBggqhkjOPQQDAwUAA2kAMGYCMQCusCCDg8EKrrP75FL5kUShjoG28dWsAMVN\\nDfv2InlX5ZCATrZ3YwSl8+MfV1Key0sCMQDMaA+jVF+bqr82C9iIYZhr4vXCWUhI\\njYVnKAo+/GP+pwwvNE7XZwPTXbEPdRoyrbw=\\n-----END CERTIFICATE-----\\n\" , report: SgxReportBody { cpusvn: \"02020000000000000000000000000000\" , miscselect: \"00000000\" , attributes: SgxAttributes { flags: 7 , xfrm: 27 , } , mrenclave: \"7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb\" , mrsigner: \"11c4e150b76c2b145f7fadb6c30455e1046b9e6fbb75b49c6e13341ad8acc5bd\" , isvprodid: 0 , isvsvn: 0 , reportdata: \"ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976\" , } , scone_qe_pub_key: a470cda5245af18ef5d8d3e1b30982a3bc74848376ca4fd03237c0f342e97239, } , ) [ 2020 -05-08T11:32:02Z INFO cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Provisioned configuration javax2/hello [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Config to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z WARN cas::api::enclave ] Failure while handling enclave connection: DisplayChain ( Error ( Msg ( \"Connection closed\" ) , State { next_error: None, backtrace: InternalBacktrace { backtrace: None } })) And we see the following logfiles in the containers of LAS: ubuntu@kmaster:~$ docker logs las_las_1 jhi [ 8 ] : --> jhi start jhi [ 8 ] : <-- jhi start aesm_service [ 11 ] : [ ADMIN ] White List update requested aesm_service [ 11 ] : The server sock is 0x55c64a935e30 aesm_service [ 11 ] : [ ADMIN ] White list update request successful for Version: 73 [ 10000 :INFO@04.05.2020/11:01:42 ] APP: Creating LAS target information message. aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning initiated aesm_service [ 11 ] : The Request ID is f6131707e3714d359f33dca65dc72308 aesm_service [ 11 ] : The Request ID is 23c495fac0b94c8984546fbddc0a75f3 aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning successful [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has MRENCLAVE D9A05D04E07CD75F4251D41A4D7F0FCA63BF392A19BEDE5C69A5BB60A94E376F [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has public key A470CDA5245AF18EF5D8D3E1B30982A3BC74848376CA4FD03237C0F342E97239 [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: LAS is listening on 0 .0.0.0:18766 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 2 , size: 1116 [ 01000 :WARNING@04.05.2020/11:40:47 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@04.05.2020/11:40:47 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection! ...multiple log entries [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 01000 :WARNING@08.05.2020/11:38:27 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection! Common errors Missing LD_LIBRARY_PATH If the system is started without the default LD_LIBRARY_PATH and the SCONE-Policy is missing its environment variable the output will be: [SCONE|ERROR] src/syscall/execve.c:108:syscall_SYS_execve(): execve is only supported after a vfork (vfork is not active!) No error information Error: trying to exec /usr/lib/jvm/java-1.8-openjdk/jre/bin/java. Check if file exists and permissions are set correctly. The solution is settings the LD_LIBRARY_PATH according to the posting session example given above: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/. Author: Hendrik","title":"Java with remote attestation"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#running-java-applications-in-scone-with-cas-policy","text":"","title":"Running Java Applications in Scone with CAS-Policy"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#requirements","text":"The following steps assume that you have a running CAS container and a running LAS server and know its addresses. Some of the latest containers for CAS and LAS are available at docker pull registry.scontain.com:5050/sconecuratedimages/services:cas docker pull registry.scontain.com:5050/sconecuratedimages/kubernetes:las but the service repository requires a commercial subscription. You can use our public CAS instance at domain scone-cas.cf instead. Make sure the sgx driver and docker is installed and running correctly by doing the java hello world sample. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-8-alpine or, in case the container does not start the bash command automatically: docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-8-alpine /bin/sh For Java 11 or Java 15 instead of Java 8: docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-11-alpine /bin/sh docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/apps:openjdk-15-alpine /bin/sh We provide a quick hello-world sample in Java cat > HelloWorld.java << EOF import java.util.Map; public class HelloWorld { public static void main(String[] args) { System.out.println(\"Hello World\"); Map<String, String> env = System.getenv(); for (String envName : env.keySet()) { System.out.format(\"%s=%s%n\", envName, env.get(envName)); } } } EOF Compile it, by executing: javac HelloWorld.java Expected output: # javac HelloWorld.java Picked up JAVA_TOOL_OPTIONS: -Xmx256m # You need to set some environment variables: export SCONE_CAS_ADDR = scone-cas.cf # we use the public CAS service export SCONE_LAS_ADDR = 127 .0.0.1 # we must run a local LAS service export SCONE_VERSION = 1 # show the SCONE version export SCONE_LOG = 7 # maximum LOG level java HelloWorld Expected output after 2-3 minutes with a NUC and 8GB memory. Note that if you have a CPU that supports EDMM (Enclave dynamic memory management), the startup times will be much quicker. / # export SCONE_VERSION = 1 / # export SCONE_LOG = 7 / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG =/ etc / sgx - musl . conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version : 1.1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 : 29 : 26 2020 + 0000 ) Branch: 6 ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash : 7954791612 f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . [ SCONE | WARN ] src / syscall / syscall . c : 698 : __scone_syscall_unshielded (): system call : SYS_membarrier , number 324 is not implemented . Picked up JAVA_TOOL_OPTIONS : - Xmx256m [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / mountinfo is not supported [ SCONE | WARN ] src / shielding / proc_fs . c : 368 : _proc_fs_open (): open : / proc / self / maps is not supported OpenJDK 64 - Bit Server VM warning : Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . [ SCONE | WARN ] src / syscall / anon . c : 144 : mmap_anon (): Protected heap memory exhausted ! Set SCONE_HEAP environment variable to increase it . Hello World PATH =/ usr / local / sbin :/ usr / local / bin :/ usr / sbin :/ usr / bin :/ sbin :/ bin :/ usr / lib / jvm / java - 1.8 - openjdk / jre / bin :/ usr / lib / jvm / java - 1.8 - openjdk / bin SCONE_VERSION = 1 SCONE_HEAP = 4 G SCONE_LOG = 7 SCONE_CAS_ADDR = scone - cas . cf JAVA_HOME =/ usr / lib / jvm / java - 1.8 - openjdk TERM = xterm LANG = C . UTF - 8 SCONE_ALPINE = 1 SCONE_ALLOW_DLOPEN = 2 HOSTNAME = 86 fd2f848e41 SCONE_MPROTECT = 1 SCONE_LAS_ADDR = 127.0.0.1 LD_LIBRARY_PATH =/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 / server :/ usr / lib / jvm / java - 1.8 - openjdk / jre / lib / amd64 :/ usr / lib / jvm / java - 1.8 - openjdk / jre /../ lib / amd64 JAVA_TOOL_OPTIONS =- Xmx256m PWD =/ HOME =/ root SHLVL = 2 / #","title":"Requirements"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#posting-a-scone-policy-to-configure-the-execution-inside-of-enclave","text":"Using the CAS container to provision an execution inside of an enclave with environment variables etc. which were previously set. Therefore, the SCONE-Policy for Java executions must include the following parameters LD_LIBRARY_PATH and if necessary JAVA_TOOL_OPTIONS and the CLASSPATH of the required Java libraries: First, similar to the posting sessions tutorial , we create a yaml file for java 1.8 openjdk cat > sessionJavaHelloWorld.yml <<EOF name: java services: - name: hello mrenclaves: [7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb] # MRENCLAVE = Enclave hash: from execution with SCONE_VERSION=1 command: \"java HelloWorld\" pwd: / environment: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64\" JAVA_TOOL_OPTIONS: \"-Xmx256m\" CLASSPATH: \"/\" # path to directory contained your code TMP_SECRET_VAR: \"This is a protected secret distributed by Scone CAS!\" EOF We create client certificate and key material to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @sessionJavaHelloWorld.yml -X POST https:// $SCONE_CAS_ADDR :8081/session Expected output (please ignore the 'not-supported' fields): Created Session [ id = b11ac009c564f6e0c8c2b61d7b4774e89f896e83ec09ef3f919bd420aa3ab252, name = 'not-supported' , status = 'not-supported' ] If the output is not giving out an idea the session was not created. One possible reasons can be formatting errors like not using quotation marks (\"java Helloworld\") or single quotation marks ('java HelloWorld') If we have set up the SCONE_CAS_ADDR and SCONE_LAS_ADDR, we export the environment variable according to the policy's name \"java\" and the service \"hello\" export SCONE_CONFIG_ID = \"java/hello\" export SCONE_CAS_ADDR = $YOUR_CAS_ADDR export SCONE_LAS_ADDR = $YOUR_LAS_ADDR export SCONE_VERSION = 1 export SCONE_LOG = 7 Now if we start the command \"java HelloWorld\" again, we see the following output. / # java HelloWorld export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_LOG = 7 export SCONE_HEAP = 4294967296 export SCONE_STACK = 2097152 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes musl version: 1 .1.24 Revision: efb2bdadba60c120f36864a1d675c4c4ca35ed69 ( Tue Apr 28 06 :29:26 2020 +0000 ) Branch: 6ab648c20350b71cfeb8468001e8ebd78779870a Enclave hash: 7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb [ SCONE | INFO ] src/shielding/crypto.c:222:crypto_get_identity () : Generated enclave certificate [ SCONE | INFO ] src/shielding/eai_attestor.c:174:eai_attestor_init () : Created TLS context to communicate with CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:288:eai_attestor_attest () : Sending Attestation Request to LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:311:eai_attestor_attest () : Got Quote from LAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:324:eai_attestor_attest () : Sending enclave hello message to CAS [ SCONE | DEBUG ] src/shielding/eai_attestor.c:349:eai_attestor_attest () : Successfully attested enclave via CAS [ SCONE | DEBUG ] src/process/init.c:639:__scone_prepare_secure_config () : Sending configuration request to CAS! [ SCONE | DEBUG ] src/process/init.c:666:__scone_prepare_secure_config () : Received configuration from CAS! [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. [ SCONE | WARN ] src/syscall/syscall.c:698:__scone_syscall_unshielded () : system call: SYS_membarrier, number 324 is not implemented. Picked up JAVA_TOOL_OPTIONS: -Xmx256m [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/mountinfo is not supported [ SCONE | WARN ] src/shielding/proc_fs.c:368:_proc_fs_open () : open: /proc/self/maps is not supported OpenJDK 64 -Bit Server VM warning: Can ' t detect primordial thread stack location - find_vma failed [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. [ SCONE | WARN ] src/syscall/anon.c:144:mmap_anon () : Protected heap memory exhausted! Set SCONE_HEAP environment variable to increase it. Hello World TMP_SECRET_VAR = This is a protected secret distributed by Scone CAS! CLASSPATH = / JAVA_TOOL_OPTIONS = -Xmx256m LD_LIBRARY_PATH = /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64 / # Note that the content of TMP_SECRET_VAR came from the Scone policy which was posted earlier to the CAS. And we see the following logfiles in the containers of CAS (in case you are running a local copy): ubuntu@kmaster:~$ docker logs cas_cas_1 [ 2020 -05-08T11:32:02Z DEBUG rustls::server::hs ] decided upon suite SupportedCipherSuite { suite: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, kx: ECDHE, bulk: AES_256_GCM, hash: SHA384, sign: ECDSA, enc_key_len: 32 , fixed_iv_len: 4 , explicit_nonce_len: 8 , hkdf_algorithm: Algorithm ( Algorithm ( SHA384 )) } [ 2020 -05-08T11:32:02Z DEBUG rustls::server::tls12 ] Session saved [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 518 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::EnclaveHello from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Fresh got message: EnclaveMessage::EnclaveHello [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Processing enclave hello message [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is valid, it has not been tampared with. [ 2020 -05-08T11:32:02Z WARN cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : CAS is running in debug mode, simulation mode or w/o SCONE runtime: It will accept quotes from SCONE quoting enclaves running in debug mode [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Hash of channel certificate: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976, SGX reportdata: ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976 [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Secure channel terminates in enclave: Hash of channel certifacte matches SGX reportdata [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : SconeQuote is signed by known and trusted quoting enclave: true [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Trust established into enclave integrity. Sending attestation complete message [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Complete to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Received length of next message [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Next message will be 16 bytes long [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Protobuf message received completely, interpreting it... [ 2020 -05-08T11:32:02Z DEBUG eai ] Got message EnclaveMessage::ConfigRequest from enclave [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : In current state: Attested got message: EnclaveMessage::ConfigRequest [ 2020 -05-08T11:32:02Z DEBUG cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Received configuration request for config id javax2/hello [ 2020 -05-08T11:32:02Z DEBUG cas_backend::backend ] Got request for service: hello in session: javax2. Attificate is: SCONE ( SCONEAttificate { identifier: \"791f84651890514eefe32ecfe54a75d95475636f61180a6ca1765cbbee0253dd\" , certificate: \"-----BEGIN CERTIFICATE-----\\nMIIBZjCB6qADAgECAhEA9vVqCQoOsEgtQ7Lo09QqNjAMBggqhkjOPQQDAwUAMA0x\\nCzAJBgNVBAYTAkRFMCAXDTE4MDkxNzE5MzAwMFoYDzIwNTAwMTAxMDEwMDAwWjAA\\nMHYwEAYHKoZIzj0CAQYFK4EEACIDYgAEoavE7JGhE5YqrBV979f8NTxUzGURrDE2\\naU3vuZukWkioO8g+3Ec+YaVMu0N3CWy0hnwKD4nyCr7criLlXhvRaxQdJdp6d+4b\\nMJXGtrgvcF4I1ZAb17FJOHoUD9Q5IVKSoxcwFTATBgNVHREEDDAKgghlbmNsYXZl\\nADAMBggqhkjOPQQDAwUAA2kAMGYCMQCusCCDg8EKrrP75FL5kUShjoG28dWsAMVN\\nDfv2InlX5ZCATrZ3YwSl8+MfV1Key0sCMQDMaA+jVF+bqr82C9iIYZhr4vXCWUhI\\njYVnKAo+/GP+pwwvNE7XZwPTXbEPdRoyrbw=\\n-----END CERTIFICATE-----\\n\" , report: SgxReportBody { cpusvn: \"02020000000000000000000000000000\" , miscselect: \"00000000\" , attributes: SgxAttributes { flags: 7 , xfrm: 27 , } , mrenclave: \"7954791612f147832807e1290604c11a6cefb6ff12fb97a6b51530be5004aecb\" , mrsigner: \"11c4e150b76c2b145f7fadb6c30455e1046b9e6fbb75b49c6e13341ad8acc5bd\" , isvprodid: 0 , isvsvn: 0 , reportdata: \"ec4f96d3faba2fdbbed2c7e1140f7df066e03440943eae7e9ab439c4262c075c74a8199b2b230a7760580c7a4799af34649e94a854709b6b08c61d6d7abac976\" , } , scone_qe_pub_key: a470cda5245af18ef5d8d3e1b30982a3bc74848376ca4fd03237c0f342e97239, } , ) [ 2020 -05-08T11:32:02Z INFO cas::api::enclave::connection ] V4 ( 192 .168.0.101:58584 ) : Provisioned configuration javax2/hello [ 2020 -05-08T11:32:02Z DEBUG eai ] Sending message Config to enclave [ 2020 -05-08T11:32:02Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z DEBUG eai::convert ] Waiting for more data before decoding protobuf message [ 2020 -05-08T11:34:50Z WARN cas::api::enclave ] Failure while handling enclave connection: DisplayChain ( Error ( Msg ( \"Connection closed\" ) , State { next_error: None, backtrace: InternalBacktrace { backtrace: None } })) And we see the following logfiles in the containers of LAS: ubuntu@kmaster:~$ docker logs las_las_1 jhi [ 8 ] : --> jhi start jhi [ 8 ] : <-- jhi start aesm_service [ 11 ] : [ ADMIN ] White List update requested aesm_service [ 11 ] : The server sock is 0x55c64a935e30 aesm_service [ 11 ] : [ ADMIN ] White list update request successful for Version: 73 [ 10000 :INFO@04.05.2020/11:01:42 ] APP: Creating LAS target information message. aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning initiated aesm_service [ 11 ] : The Request ID is f6131707e3714d359f33dca65dc72308 aesm_service [ 11 ] : The Request ID is 23c495fac0b94c8984546fbddc0a75f3 aesm_service [ 11 ] : [ ADMIN ] EPID Provisioning successful [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has MRENCLAVE D9A05D04E07CD75F4251D41A4D7F0FCA63BF392A19BEDE5C69A5BB60A94E376F [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: SCONE QE has public key A470CDA5245AF18EF5D8D3E1B30982A3BC74848376CA4FD03237C0F342E97239 [ 10000 :INFO@04.05.2020/11:01:44 ] STARTER: LAS is listening on 0 .0.0.0:18766 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@04.05.2020/11:40:46 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 2 , size: 1116 [ 01000 :WARNING@04.05.2020/11:40:47 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@04.05.2020/11:40:47 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection! ...multiple log entries [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Received attestation request [ 10000 :INFO@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Sent quote success: 1 , type: 3 , size: 480 [ 01000 :WARNING@08.05.2020/11:38:27 ] RECV_FUNC: Connection closed while reading message length. [ 01000 :WARNING@08.05.2020/11:38:27 ] CONNECTION_HANDLER: Unable to decode received message. Terminating connection!","title":"Posting a SCONE-Policy to configure the execution inside of Enclave"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#common-errors","text":"","title":"Common errors"},{"location":"Running_Java_Applications_in_Scone_with_remote_attestation/#missing-ld_library_path","text":"If the system is started without the default LD_LIBRARY_PATH and the SCONE-Policy is missing its environment variable the output will be: [SCONE|ERROR] src/syscall/execve.c:108:syscall_SYS_execve(): execve is only supported after a vfork (vfork is not active!) No error information Error: trying to exec /usr/lib/jvm/java-1.8-openjdk/jre/bin/java. Check if file exists and permissions are set correctly. The solution is settings the LD_LIBRARY_PATH according to the posting session example given above: LD_LIBRARY_PATH: \"/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/. Author: Hendrik","title":"Missing LD_LIBRARY_PATH"},{"location":"Rust/","text":"Rust SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves. To build Rust applications, we provide variants of the rustc and cargo command line utilities as part of image registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu : scone-rustc / scone rustc You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu ): > docker run -it registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu $ scone rustc --version rustc 1 .38.0 ( 625451e37 2019 -09-23 ) Let's try a simple hello world program. $ mkdir ~/projects $ cd ~/projects $ mkdir hello_world $ cd hello_world Let's try our rust program: $ cat > main.rs << EOF fn main() { println!(\"Hello, world!\"); } EOF Let's compile the program for running inside of enclaves: $ scone rustc main.rs --target = x86_64-scone-linux-musl $ ls main main.rs Let's run main inside an enclave and print some debug information: $ SCONE_MODE = HW SCONE_VERSION = 1 ./main export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = no export SCONE_ALLOW_DLOPEN2 = no Hello, world! scone-cargo and scone cargo : You can build projects with scone cargo : $ scone cargo build --target = x86_64-scone-linux-musl Alternatively, you can use scone-cargo if, for example, you need a command without a space. scone cargo , as well as, scone rustc has access to the SCONE-compiled rust standard library and the target file. --target=x86_64-scone-linux-musl instructs it to use our target file - essentially triggering a cross-compiler build. Due to the cross-compilation, crates that depend on compiled C libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate. The following is an example of how an executable with openssl can be compiled: $ OPENSSL_LIB_DIR = /libressl-2.4.5 OPENSSL_INCLUDE_DIR = /libressl-2.4.5/include/ OPENSSL_STATIC = 1 PKG_CONFIG_ALLOW_CROSS = 1 scone-cargo build --target = scone In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.","title":"Rust"},{"location":"Rust/#rust","text":"SCONE supports the Rust programming language. Rust combines speed and strong type safety and it is hence our language of choice for new applications that need to run inside of enclaves. To build Rust applications, we provide variants of the rustc and cargo command line utilities as part of image registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu :","title":"Rust"},{"location":"Rust/#scone-rustc-scone-rustc","text":"You can compile Rust programs but links against the SCONE libc instead of a standard libc. To print the version of Rust execute (inside container registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu ): > docker run -it registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu $ scone rustc --version rustc 1 .38.0 ( 625451e37 2019 -09-23 ) Let's try a simple hello world program. $ mkdir ~/projects $ cd ~/projects $ mkdir hello_world $ cd hello_world Let's try our rust program: $ cat > main.rs << EOF fn main() { println!(\"Hello, world!\"); } EOF Let's compile the program for running inside of enclaves: $ scone rustc main.rs --target = x86_64-scone-linux-musl $ ls main main.rs Let's run main inside an enclave and print some debug information: $ SCONE_MODE = HW SCONE_VERSION = 1 ./main export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = no export SCONE_ALLOW_DLOPEN2 = no Hello, world!","title":"scone-rustc /  scone rustc"},{"location":"Rust/#scone-cargo-and-scone-cargo","text":"You can build projects with scone cargo : $ scone cargo build --target = x86_64-scone-linux-musl Alternatively, you can use scone-cargo if, for example, you need a command without a space. scone cargo , as well as, scone rustc has access to the SCONE-compiled rust standard library and the target file. --target=x86_64-scone-linux-musl instructs it to use our target file - essentially triggering a cross-compiler build. Due to the cross-compilation, crates that depend on compiled C libraries, such as openssl or error-chain, do not work out of the box. Cargo will not use the system installed libraries because it wrongly assumes that they do not fit the target architecture. To solve this issue, one has to either provide the compiled libraries or deactivate the crate. The following is an example of how an executable with openssl can be compiled: $ OPENSSL_LIB_DIR = /libressl-2.4.5 OPENSSL_INCLUDE_DIR = /libressl-2.4.5/include/ OPENSSL_STATIC = 1 PKG_CONFIG_ALLOW_CROSS = 1 scone-cargo build --target = scone In the case of error-chain, one can just deactivate its optional backtrace feature that actually requires a precompiled library.","title":"scone-cargo and scone cargo:"},{"location":"SCONE_CLI/","text":"SCONE CLI We maintain a single unified command line interface (CLI) scone that helps to to start and stop secure containers as well as secure applications. scone also provides functionality to install and monitor SCONE hosts. Use of docker CLI In many cases you can just use the docker CLI to run SCONE containers. This is particularly useful if you want to run some SCONE containers on your local machine. This removes, for example, the need to setup ssh . The scone command is structured similar as the docker CLI or the infinit CLI: One needs to specify an object (like host ) and a command (like install ) and some options. For some commands, some of the options are actually not optional but mandatory. You need to have access to SCONE container images You require permissions to be able to run the examples given in this section. Just register a free account on gitlab.scontain.com . To use the scone CLI , you need to start it in a container. Assuming that you have a docker engine installed, you try the following examples by running the following container: > docker run -it registry.scontain.com:5050/sconecuratedimages/sconecli Help scone has a built in help. To get a list of all objects , just type: $ scone --help To get a list of all commands for a given object (like host), execute: $ scone host --help To get a list of all options for a given object and command (e.g., host install) and some examples, just execute: $ scone host install --help bash auto-completion If you are using bash as your shell, scone supports auto-completion. This means that instead you can use the TAB key to see the options. For example, $ scone <TAB> will show all available objects. If you have already specified an object, auto-completion helps you to list all commands: $ scone host <TAB> If you also specified an command, it will provide you with a list of options (that you have not specified yet): $ scone host install <TAB> Of course, it also supports auto-completion: $ scone host install -n<TAB> will result in $ scone host install -name","title":"SCONE CLI"},{"location":"SCONE_CLI/#scone-cli","text":"We maintain a single unified command line interface (CLI) scone that helps to to start and stop secure containers as well as secure applications. scone also provides functionality to install and monitor SCONE hosts. Use of docker CLI In many cases you can just use the docker CLI to run SCONE containers. This is particularly useful if you want to run some SCONE containers on your local machine. This removes, for example, the need to setup ssh . The scone command is structured similar as the docker CLI or the infinit CLI: One needs to specify an object (like host ) and a command (like install ) and some options. For some commands, some of the options are actually not optional but mandatory. You need to have access to SCONE container images You require permissions to be able to run the examples given in this section. Just register a free account on gitlab.scontain.com . To use the scone CLI , you need to start it in a container. Assuming that you have a docker engine installed, you try the following examples by running the following container: > docker run -it registry.scontain.com:5050/sconecuratedimages/sconecli","title":"SCONE CLI"},{"location":"SCONE_CLI/#help","text":"scone has a built in help. To get a list of all objects , just type: $ scone --help To get a list of all commands for a given object (like host), execute: $ scone host --help To get a list of all options for a given object and command (e.g., host install) and some examples, just execute: $ scone host install --help","title":"Help"},{"location":"SCONE_CLI/#bash-auto-completion","text":"If you are using bash as your shell, scone supports auto-completion. This means that instead you can use the TAB key to see the options. For example, $ scone <TAB> will show all available objects. If you have already specified an object, auto-completion helps you to list all commands: $ scone host <TAB> If you also specified an command, it will provide you with a list of options (that you have not specified yet): $ scone host install <TAB> Of course, it also supports auto-completion: $ scone host install -n<TAB> will result in $ scone host install -name","title":"bash auto-completion"},{"location":"SCONE_Curated_Images/","text":"SCONE Curated Images We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub: Private images: 1 Image Name Description registry.scontain.com:5050/sconecuratedimages/crosscompilers a container image with all the SCONE crosscompilers. registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime a container image that can run dynamically linked applications inside of an enclave. registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 a container image including a python interpreter running inside of an enclave. registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 a container image including a python interpreter running inside of an enclave. registry.scontain.com:5050/sconecuratedimages/apps:mongodb-alpine MongoDB container image. registry.scontain.com:5050/sconecuratedimages/apps:scone-vault-latest Vault 0.8.1 container image. registry.scontain.com:5050/sconecuratedimages/apps:memcached-alpine Memcached container image. registry.scontain.com:5050/sconecuratedimages/apps:node-8.9-alpine a container image for node running inside an enclave. registry.scontain.com:5050/sconecuratedimages/apps:nginx-1.13-alpine a container image for nginx running inside an enclave. registry.scontain.com:5050/sconecuratedimages/apps:8-jdk-alpine a container image for Java applications running inside an enclave. Please send us an email if you need a curated image of another application or a different/newer version of an application. Most of the time, we will be able to provide you an image on short notice. Login in Access to some SCONE images is restricted. First, register a free account on gitlab.scontain.com . Second, log into to our Scontain registry via: docker login registry.scontain.com:5050 before you will be able to pull any of the private curated images. Scone Compilers To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer. Dynamically-Linked Binaries Even if you have no SGX CPU extension / no SGX driver installed on your computer, you can use a standard gcc compiler - as long as the requirements mentioned in SGX ToolChain are satisfied. docker pull registry.scontain.com:5050/sconecuratedimages/muslgcc Note that the binaries generated with the above image are just native binaries, i.e., they run outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library. To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image: docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime Statically-Linked Binaries To generate statically-linked secure binaries you need a cross compiler. You can pull this image from Docker hub (you need to be granted access rights for that): docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers Scone Hello World You can pull the following (private) image. This image only runs in hardware mode: docker pull registry.scontain.com:5050/sconecuratedimages/helloworld You can run on the standard Docker engine - if you have the SGX driver installed. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device > docker run $MOUNT_SGXDEVICE registry.scontain.com:5050/sconecuratedimages/helloworld Hello World If you do not have the SGX driver installed, you get an error message: > docker run $MOUNT_SGXDEVICE registry.scontain.com:5050/sconecuratedimages/helloworld docker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device \"/dev/isgx\" : no such file or directory. In this case, install the SGX driver . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled. Screencast Just register a free account on gitlab.scontain.com . \u21a9","title":"SCONE Curated Images"},{"location":"SCONE_Curated_Images/#scone-curated-images","text":"We provide a set of curated SCONE container images on a (partially private) repositories on Docker hub: Private images: 1 Image Name Description registry.scontain.com:5050/sconecuratedimages/crosscompilers a container image with all the SCONE crosscompilers. registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime a container image that can run dynamically linked applications inside of an enclave. registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 a container image including a python interpreter running inside of an enclave. registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 a container image including a python interpreter running inside of an enclave. registry.scontain.com:5050/sconecuratedimages/apps:mongodb-alpine MongoDB container image. registry.scontain.com:5050/sconecuratedimages/apps:scone-vault-latest Vault 0.8.1 container image. registry.scontain.com:5050/sconecuratedimages/apps:memcached-alpine Memcached container image. registry.scontain.com:5050/sconecuratedimages/apps:node-8.9-alpine a container image for node running inside an enclave. registry.scontain.com:5050/sconecuratedimages/apps:nginx-1.13-alpine a container image for nginx running inside an enclave. registry.scontain.com:5050/sconecuratedimages/apps:8-jdk-alpine a container image for Java applications running inside an enclave. Please send us an email if you need a curated image of another application or a different/newer version of an application. Most of the time, we will be able to provide you an image on short notice.","title":"SCONE Curated Images"},{"location":"SCONE_Curated_Images/#login-in","text":"Access to some SCONE images is restricted. First, register a free account on gitlab.scontain.com . Second, log into to our Scontain registry via: docker login registry.scontain.com:5050 before you will be able to pull any of the private curated images.","title":"Login in"},{"location":"SCONE_Curated_Images/#scone-compilers","text":"To run a local copy of the SCONE (cross-)compilers, just pull the appropriate image on your computer.","title":"Scone Compilers"},{"location":"SCONE_Curated_Images/#dynamically-linked-binaries","text":"Even if you have no SGX CPU extension / no SGX driver installed on your computer, you can use a standard gcc compiler - as long as the requirements mentioned in SGX ToolChain are satisfied. docker pull registry.scontain.com:5050/sconecuratedimages/muslgcc Note that the binaries generated with the above image are just native binaries, i.e., they run outside of enclaves . To be able to run the binary inside of an enclave, you need to have installed the SCONE runtime library. To run a dynamically-linked binary, one needs a special runtime environment. We provide this in form of a (private) container image: docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime","title":"Dynamically-Linked Binaries"},{"location":"SCONE_Curated_Images/#statically-linked-binaries","text":"To generate statically-linked secure binaries you need a cross compiler. You can pull this image from Docker hub (you need to be granted access rights for that): docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers","title":"Statically-Linked Binaries"},{"location":"SCONE_Curated_Images/#scone-hello-world","text":"You can pull the following (private) image. This image only runs in hardware mode: docker pull registry.scontain.com:5050/sconecuratedimages/helloworld You can run on the standard Docker engine - if you have the SGX driver installed. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device > docker run $MOUNT_SGXDEVICE registry.scontain.com:5050/sconecuratedimages/helloworld Hello World If you do not have the SGX driver installed, you get an error message: > docker run $MOUNT_SGXDEVICE registry.scontain.com:5050/sconecuratedimages/helloworld docker: Error response from daemon: linux runtime spec devices: error gathering device information while adding custom device \"/dev/isgx\" : no such file or directory. In this case, install the SGX driver . This installation will fail in case you disabled SGX in the BIOS or your CPU is not SGX-enabled.","title":"Scone Hello World"},{"location":"SCONE_Curated_Images/#screencast","text":"Just register a free account on gitlab.scontain.com . \u21a9","title":"Screencast"},{"location":"SCONE_Dockerfile/","text":"Dockerfile We show how to generate a first secure container image with the help of a Dockerfile. Install the tutorial Clone the tutorial: git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git Access to SCONE Curated Images Right now, access to the curated images is still restricted. Please, send email to info@scontain.com to request access. Generate HelloAgain image (dynamically-linked) We first generate a hello again container image with a dynamically-linked secure program: cd SCONE_TUTORIAL/DLDockerFile The Dockerfile to generate the new image looks like this: FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime RUN mkdir /hello COPY dyn_hello_again /hello/ CMD SCONE_MODE = HW SCONE_ALPINE = 1 SCONE_VERSION = 1 /hello/dyn_hello_again This assumes that we already generated the dynamically linked binary with an appropriately configured gcc. We generate this with the provided gcc image: docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/muslgcc gcc hello_again.c -o dyn_hello_again We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials): ./generate.sh Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device . You can run this program inside of enclave (with the output of debug messages): determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/helloworld:dynamic export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello Again This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment. Screencast Generate HelloAgain image (statically-linked) We generate a hello again container image. > cd SCONE_TUTORIAL/DockerFile The Dockerfile is quite straight forward: FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers MAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\" RUN mkdir /hello COPY hello_again.c /hello/ RUN cd /hello && scone-gcc hello_again.c -o again CMD [ \"/hello/again\" ] You can either execute all step manually (see below) or you can just execute > docker login ./generate.sh and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub. We define the image name and tag that we want to generate: export TAG = \"again\" export FULLTAG = \"registry.scontain.com:5050/sconecuratedimages/helloworld: $TAG \" We build the image: > docker build --pull -t $FULLTAG . > docker run $MOUNT_SGXDEVICE -it $FULLTAG We push it to docker hub (will fail unless you have the right to push $FULLTAG ): > docker push $FULLTAG Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"myrepository/helloAgain\" Screencast","title":"SCONE Dockerfile"},{"location":"SCONE_Dockerfile/#dockerfile","text":"We show how to generate a first secure container image with the help of a Dockerfile.","title":"Dockerfile"},{"location":"SCONE_Dockerfile/#install-the-tutorial","text":"Clone the tutorial: git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git","title":"Install the tutorial"},{"location":"SCONE_Dockerfile/#access-to-scone-curated-images","text":"Right now, access to the curated images is still restricted. Please, send email to info@scontain.com to request access.","title":"Access to SCONE Curated Images"},{"location":"SCONE_Dockerfile/#generate-helloagain-image-dynamically-linked","text":"We first generate a hello again container image with a dynamically-linked secure program: cd SCONE_TUTORIAL/DLDockerFile The Dockerfile to generate the new image looks like this: FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime RUN mkdir /hello COPY dyn_hello_again /hello/ CMD SCONE_MODE = HW SCONE_ALPINE = 1 SCONE_VERSION = 1 /hello/dyn_hello_again This assumes that we already generated the dynamically linked binary with an appropriately configured gcc. We generate this with the provided gcc image: docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/muslgcc gcc hello_again.c -o dyn_hello_again We provide a little script that generates the image and pushes it to Docker hub (which should fail since you should not have the credentials): ./generate.sh Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device . You can run this program inside of enclave (with the output of debug messages): determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/helloworld:dynamic export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello Again This image is nicely small (only 11MB) since it only contains the runtime environment and no development environment.","title":"Generate HelloAgain image (dynamically-linked)"},{"location":"SCONE_Dockerfile/#screencast","text":"","title":"Screencast"},{"location":"SCONE_Dockerfile/#generate-helloagain-image-statically-linked","text":"We generate a hello again container image. > cd SCONE_TUTORIAL/DockerFile The Dockerfile is quite straight forward: FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers MAINTAINER Christof Fetzer \"christof.fetzer@gmail.com\" RUN mkdir /hello COPY hello_again.c /hello/ RUN cd /hello && scone-gcc hello_again.c -o again CMD [ \"/hello/again\" ] You can either execute all step manually (see below) or you can just execute > docker login ./generate.sh and watch the outputs. The push of the image should fail since you should not have the access rights to push the image to Docker hub. We define the image name and tag that we want to generate: export TAG = \"again\" export FULLTAG = \"registry.scontain.com:5050/sconecuratedimages/helloworld: $TAG \" We build the image: > docker build --pull -t $FULLTAG . > docker run $MOUNT_SGXDEVICE -it $FULLTAG We push it to docker hub (will fail unless you have the right to push $FULLTAG ): > docker push $FULLTAG Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"myrepository/helloAgain\"","title":"Generate HelloAgain image (statically-linked)"},{"location":"SCONE_Dockerfile/#screencast_1","text":"","title":"Screencast"},{"location":"SCONE_ENV/","text":"SCONE Environment Variables To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service. Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters. SCONE Configuration File The location of the SCONE configuration file can be controlled with an environment variable: SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened, the default configuration file located in /etc/sgx-musl.conf is read instead. If the default configuration file cannot be read either, the program exits with an error message. Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves. The configuration file can define most of the behaviors that one can define via environment variables. However, the SCONE_... environment variables have higher priority than the settings from the config file. Format of SCONE Configuration File The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: Command Option(s) Description Q n defines the number of queues used to execute system calls n [number of queues] sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the SCONE_QUEUES environment variable H s defines the heap size in bytes s [heap size in bytes] sets the size of heap, equivalent to setting SCONE_HEAP environment variable P N determines the backoff behavior of the queues N [spin number] equivalent to setting SCONE_SSPINS environment variable L S determines the backoff behavior of the queues S [sleep time] equivalent to setting SCONE_SSLEEP environment variable; s C Q R sthread serving system calls outside of an enclave C [core-no] if non-negative number: pin this sthread to core C ; if a negative number, do not pin this thread Q [queue-no] in [0..n] ; this sthreads serves this queue R [realtime] always set this to 0 e C Q R ethread running inside of enclave and executes application threads (which we call lthreads) C [core-no] non-negative number: pin to this core; negative number: no pinning ot this thread Q [queue-no] in [0..n]: this sthreads serves this queue R [realtime] always set this to 0 The number of sthreads is automatically increased if more sthreads are needed to process system calls. An sthread will block if it does not have any work left to do. Hence, we recommend to start exactly one sthread per ethread . ethreads will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one ethread per hyper-thread. Unless you want to limit the the number of CPU resources an enclave should use, the following is a good generic configuration file: $ sudo tee /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF Run Mode SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode. Value Description SCONE_MODE=HW run program inside of enclave. If program fails to create an enclave, it will fail. SCONE_MODE=SIM run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode. SCONE_MODE=AUTO run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode Memory-Related Environment Variables SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave. The default heap size is 64MB and is used if SCONE_HEAP is not set. Value Description SCONE_HEAP=s the requested heap size in bytes SCONE_HEAP=sK the requested heap size in KiloBytes SCONE_HEAP=sM the requested heap size in MegaBytes SCONE_HEAP=sG the requested heap size in GigaBytes SCONE_STACK : size of the stack allocated to threads spawned in the enclave. The default stack size is 64 KBytes. Please increase this if your expect your threads to require more than 64KBytes. For programs like MariaDB, MongoDB, Python, or node, we increase the stack size to 4 MBytes . Value Description SCONE_STACK=s the requested stack size in bytes SCONE_STACK=sK the requested stack size in KiloBytes SCONE_STACK=sM the requested stack size in MegaBytes Debug SCONE_VERSION if defined, SCONE will print the values of some of the SCONE environment variables during startup. SCONE_LOG set to value between 0 (no) and 7 (debug) to see more or less messages on stderr from the SCONE platform. Messages could be warnings if certain functions are not (yet) implemented inside of an enclave. Example output for SCONE_VERSION=1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_LOG = 6 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes Revision: e37dda73a6db435973f2e00347bd4cf462e4e027 ( Sat Aug 25 22 :59:30 2018 +0200 ) Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Dynamic library support: SCONE_ALLOW_DLOPEN=\"1\" : if defined, permit to load protected libraries after startup: These libraries must be authenticated or encrypted to be able to load these. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of MRENCLAVE . The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup. Note that SCONE_ALLOW_DLOPEN=\"1\" must be set in the policy of an attested application to have an effect. SCONE_ALLOW_DLOPEN=\"2\" : must be used for debugging only : this value enables loading of unprotected dynamic libraries after startup, i.e., libraries that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should either set SCONE_ALLOW_DLOPEN=\"1\" or you should not define SCONE_ALLOW_DLOPEN . Never use SCONE_ALLOW_DLOPEN=2 in production mode . Performance tuning variables SCONE_QUEUES : number of systemcall queues to be used. SCONE_SLOTS : systemcalls queue length: must be larger than the maximum number of lthreads . SCONE_SIGPIPE : if set to 1 , SIGPIPE signals are delivered to the application SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. SCONE_SSLEEP : determines how fast we increase the backoff. Safety SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker. Dynamic link loader The dynamic link loader is part of image registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime ( see tutorial ). SCONE_LD_DEBUG : print the dynamically loaded libraries and their SHA-256 hashes LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries. LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself. SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave. Fork Support We support fork since release 4.2. It is, however, by default disabled: when a process executes fork , the call will fail by default. Ony if the environment variable SCONE_FORK is set to 1, fork will be executed.","title":"environment variables"},{"location":"SCONE_ENV/#scone-environment-variables","text":"To simplify development and debugging, SCONE supports a range of environment variables to control its behavior. These environment variables are mainly used for development and debugging. In operational settings, the configuration would be provided in a secure fashion via the SCONE configuration and attestation service. Also, the performance of SCONE-based applications can be tuned by selecting the appropriate configuration for an application. We have tool support to automatically tune these parameters.","title":"SCONE Environment Variables"},{"location":"SCONE_ENV/#scone-configuration-file","text":"The location of the SCONE configuration file can be controlled with an environment variable: SCONE_CONFIG : if defined, this determines the path of SCONE configuration file. If this environment variable is not defined or the file cannot be opened, the default configuration file located in /etc/sgx-musl.conf is read instead. If the default configuration file cannot be read either, the program exits with an error message. Changing the location of the configuration file is, for example, useful in the context of testing or when you run your application outside of a container when you want to run different applications with different configurations inside of enclaves. The configuration file can define most of the behaviors that one can define via environment variables. However, the SCONE_... environment variables have higher priority than the settings from the config file.","title":"SCONE Configuration File"},{"location":"SCONE_ENV/#format-of-scone-configuration-file","text":"The format for the configuration file: on each line, there is a statement beginning with a single-character command code, and up to three numbers. The possible commands currently are: Command Option(s) Description Q n defines the number of queues used to execute system calls n [number of queues] sets the number of syscall-return queue pairs to allocate. This is equivalent to setting the SCONE_QUEUES environment variable H s defines the heap size in bytes s [heap size in bytes] sets the size of heap, equivalent to setting SCONE_HEAP environment variable P N determines the backoff behavior of the queues N [spin number] equivalent to setting SCONE_SSPINS environment variable L S determines the backoff behavior of the queues S [sleep time] equivalent to setting SCONE_SSLEEP environment variable; s C Q R sthread serving system calls outside of an enclave C [core-no] if non-negative number: pin this sthread to core C ; if a negative number, do not pin this thread Q [queue-no] in [0..n] ; this sthreads serves this queue R [realtime] always set this to 0 e C Q R ethread running inside of enclave and executes application threads (which we call lthreads) C [core-no] non-negative number: pin to this core; negative number: no pinning ot this thread Q [queue-no] in [0..n]: this sthreads serves this queue R [realtime] always set this to 0 The number of sthreads is automatically increased if more sthreads are needed to process system calls. An sthread will block if it does not have any work left to do. Hence, we recommend to start exactly one sthread per ethread . ethreads will leave the enclave it there is no more work for them to do. Hence, it makes sense to start one ehthread per core. In some situations, it might even make sense to start one ethread per hyper-thread. Unless you want to limit the the number of CPU resources an enclave should use, the following is a good generic configuration file: $ sudo tee /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF","title":"Format of SCONE Configuration File"},{"location":"SCONE_ENV/#run-mode","text":"SCONE_MODE : defines if application should run inside of an enclave or outside in simulation mode. Value Description SCONE_MODE=HW run program inside of enclave. If program fails to create an enclave, it will fail. SCONE_MODE=SIM run program outside of enclave. All SCONE related software runs but just outside of the enclave. This is not 100% compatible with hardware mode since, for example, some instructions are permitted in native mode that are not permitted in hardware mode. SCONE_MODE=AUTO run program inside of enclave if SGX is available. Otherwise, run in simulation mode. AUTO is the default mode","title":"Run Mode"},{"location":"SCONE_ENV/#memory-related-environment-variables","text":"SCONE_HEAP : size of the heap allocated for the program during the startup of the enclave. The default heap size is 64MB and is used if SCONE_HEAP is not set. Value Description SCONE_HEAP=s the requested heap size in bytes SCONE_HEAP=sK the requested heap size in KiloBytes SCONE_HEAP=sM the requested heap size in MegaBytes SCONE_HEAP=sG the requested heap size in GigaBytes SCONE_STACK : size of the stack allocated to threads spawned in the enclave. The default stack size is 64 KBytes. Please increase this if your expect your threads to require more than 64KBytes. For programs like MariaDB, MongoDB, Python, or node, we increase the stack size to 4 MBytes . Value Description SCONE_STACK=s the requested stack size in bytes SCONE_STACK=sK the requested stack size in KiloBytes SCONE_STACK=sM the requested stack size in MegaBytes","title":"Memory-Related Environment Variables"},{"location":"SCONE_ENV/#debug","text":"SCONE_VERSION if defined, SCONE will print the values of some of the SCONE environment variables during startup. SCONE_LOG set to value between 0 (no) and 7 (debug) to see more or less messages on stderr from the SCONE platform. Messages could be warnings if certain functions are not (yet) implemented inside of an enclave. Example output for SCONE_VERSION=1 : export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 2147483648 export SCONE_STACK = 81920 export SCONE_LOG = 6 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_ESPINS = 10000 export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_VARYS = no export SCONE_ALLOW_DLOPEN = yes ( unprotected ) export SCONE_MPROTECT = yes Revision: e37dda73a6db435973f2e00347bd4cf462e4e027 ( Sat Aug 25 22 :59:30 2018 +0200 ) Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl","title":"Debug"},{"location":"SCONE_ENV/#dynamic-library-support","text":"SCONE_ALLOW_DLOPEN=\"1\" : if defined, permit to load protected libraries after startup: These libraries must be authenticated or encrypted to be able to load these. Note that all libraries that are loaded during startup are measured and contribute to the hash of the enclave, i.e., they are part of MRENCLAVE . The libraries loaded during startup could reside in a file region that is not protect or that is authenticated. These libraries must not be in an encrypted region since the encryption keys are not yet known during startup. Note that SCONE_ALLOW_DLOPEN=\"1\" must be set in the policy of an attested application to have an effect. SCONE_ALLOW_DLOPEN=\"2\" : must be used for debugging only : this value enables loading of unprotected dynamic libraries after startup, i.e., libraries that are neither authenticated nor encrypted. For example, Python programs might dynamically load modules after startup. Our approach to enforce the integrity of these dynamic libraries with the help of a file protection shield, i.e., you should either set SCONE_ALLOW_DLOPEN=\"1\" or you should not define SCONE_ALLOW_DLOPEN . Never use SCONE_ALLOW_DLOPEN=2 in production mode .","title":"Dynamic library support:"},{"location":"SCONE_ENV/#performance-tuning-variables","text":"SCONE_QUEUES : number of systemcall queues to be used. SCONE_SLOTS : systemcalls queue length: must be larger than the maximum number of lthreads . SCONE_SIGPIPE : if set to 1 , SIGPIPE signals are delivered to the application SCONE_SSPINS=N : In case an Ethread does not have any lthread to execute, an Ethread first pauses for up to N times to wait for more work to show up. After that, it sleeps for up to N times. Each time increasing the sleep time. SCONE_SSLEEP : determines how fast we increase the backoff.","title":"Performance tuning variables"},{"location":"SCONE_ENV/#safety","text":"SCONE_SGXBOUNDS : must be defined to enable bounds checking. Furthermore, you need to compile your program with our SGX boundschecker.","title":"Safety"},{"location":"SCONE_ENV/#dynamic-link-loader","text":"The dynamic link loader is part of image registry.scontain.com:5050/sconecuratedimages/crosscompilers:runtime ( see tutorial ). SCONE_LD_DEBUG : print the dynamically loaded libraries and their SHA-256 hashes LD_LIBRARY_PATH : you can control from where the dynamic link loader looks for shared libraries. LD_PRELOAD : you can instruct the dynamic link loader to load libraries before loading the libraries specified by the program itself. SCONE_ALPINE=1 : run dynamically-linked program inside of an enclave.","title":"Dynamic link loader"},{"location":"SCONE_ENV/#fork-support","text":"We support fork since release 4.2. It is, however, by default disabled: when a process executes fork , the call will fail by default. Ony if the environment variable SCONE_FORK is set to 1, fork will be executed.","title":"Fork Support"},{"location":"SCONE_Fileshield/","text":"SCONE File Protection This section describes the low-level SCONE file encryption interface to encrypt the root filesystem of an image. In many cases, one might want to use a SCONE volume instead: a SCONE volume is typically mapped on a Kubernetes volume with access modes ReadWriteOnce or ReadOnlyMany a SCONE volume is automatically encrypted, i.e., no need to perform the fspf operations described in this section a SCONE volume - as a Kubernetes volume - exists only once and SCONE CAS tracks its confidentiality and integrity including freshness. Concepts SCONE supports the transparent encryption and/or authentication of files of the root file system of a container / image. By transparent , we mean that there are no application code changes needed to support this. The underlying idea of SCONE file protection is that a user specifies that each file is either : authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity, encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or not-protected , i.e. SCONE reads and write the files without any extra protection mechanisms. For example, you might use not-protected if your application already encrypts its files or if you need direct access to devices. Marking all files individually as either authenticated , encrypted , or not-protected would not be very practical. Hence, we support to partition the filesystem into regions : regions do not overlap and each file belongs to exactly one region. A region is defined by a path. For example, region / is the root region and you could, for example, specify that all files in region / must be authenticated. You can define a second region, for example, region /data/db and that this region is encrypted. Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file /etc/db.conf would belong, in this case, to region / and file /data/db/table.db would belong to region /data/db . SCONE supports ephemeral regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted. When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. All files in an ephemeral region are lost when the application exits. All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a kernel region. For each region, you need to specify if the region is either ephemeral or kernel . Each region belongs to one of the following six classes: { ephemeral | kernel } X { not-protected | authenticated | encrypted } Example Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted. Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory volume ) and we will later mount this in the container as /data . The original (non-encrypted) files are stored in directory data-original . > mkdir -p volume > mkdir -p data-original Let's write some files in the data-original directory: cat > data-original/hello.txt << EOF Hello World EOF cat > data-original/world.py << EOF f = open('/data/hello.txt', 'r') print str(f.read()) EOF Let's check that volume is empty and we print the hash values of the two files in data-original : > ls volume > shasum data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c data-original/world.py Now, we start the SCONE crosscompiler in a container to create the encrypted volume: > docker run -it -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" registry.scontain.com:5050/sconecuratedimages/crosscompilers File System Protection File All the metadata required for checking the consistency of the files is stored in a file system protection file , or, short fspf . SCONE supports multiple *fspf*s. Let's start with a simple example with a single fspf . The fspf file is created via command scone fspf create and let us name this file fspf.pb . We execute the following commands inside the container (as indicated by the $ prompt): $ cd /data $ scone fspf create fspf.pb Created empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423 We can now split the file system in regions , a region is a subtree. You can add regions to a fspf with the help of command scone fspf addr . Each region has exactly one of the following properties: authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option --authenticated . encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option --encrypted . not-protected : files are neither authenticated nor encrypted. Specify command line option --not-protected . File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead. Say for now, that by default we do not protect files and we want to read files and write back changed files to the file system. To do so, we define that the root tree is --kernel as well as --not-protected : $ scone fspf addr fspf.pb / --kernel / --not-protected Added region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84 You need to specify root / In case you define more than one region, you always need to define a root region \"/\". Let us add another region /data that should be encrypted and persisted. To encrypt the files, we specify option --encrypted . We specify option --kernel followed by a path (here, also /data ) to request that files in this region are written to the kernel file system into directory /data . $ scone fspf addr fspf.pb /data --encrypted --kernel /data Added region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7 The encryption key for a file is chosen at random and stored in fspf.pb . We use the Intel random number generator RdRand to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option --key-length 16 and --key-length 24 when creating a region with command scone fspf addr . Now, that we defined the regions, i.e., / and /data , we can add files to region /data . Let's just add all files in /data-original , encrypt these and write the encrypted files to /data . Note, the first /data argument specifies the protection region that determines the protection policy. The second, specifies where the encrypted files will be stored. That is, the command iterates over and reads the existing files in /data-original and encrypts them. The encrypted file content is written into the directory /data while the protection metadata of the individual files is added to the fsfp.pb file. $ scone fspf addf fspf.pb /data /data-original /data Added files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63 Note that if the /data region would have been only authenticated and not encrypted, the tool does not need to write out any (encrypted) files. It will only add the file names and the checksums (tags) of the files located in /data-original to the fspf.pb file. Thus, you could drop the last argument in this case. Coming back to the above example, we can now compare the hash values of the original files and the encrypted files: $ shasum /data/* 87fd97468024e3d2864516ff5840e15d9615340d /data/fspf.pb 31732914910f4a08b9832c442074b0932915476c /data/hello.txt 8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c /data/world.py $ shasum /data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a /data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c /data-original/world.py The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb $ scone fspf encrypt fspf.pb > /data-original/keytag We store the random encryption key as well as the tag of file fspf.pb in file /data-original/keytag . We introduce a very simple program that reads the two files: $ cat > example.c << EOF #include <stdio.h> #include <stdlib.h> void printfile(const char* fn) { FILE *fp = fopen(fn, \"r\"); char c; while((c=fgetc(fp))!=EOF ){ printf ( \"%c\" ,c ) ; } fclose ( fp ) ; } int main () { printfile ( \"/data/hello.txt\" ) ; printfile ( \"/data/world.py\" ) ; } EOF Let's crosscompile this program: scone gcc example.c -o example Executing this program results in an output like this: $./example R??C? q?z??E?? | \u042e? } \u00fc ?o $? ?!rga??\u0387* ` ?????????Gw? We need to activate the file system shield via environment variables by setting the location of the file system protection file (in SCONE_FSPF ), the encryption key of the file (in SCONE_FSPF_KEY ) and the tag of the fspf (in SCONE_FSPF_TAG ). We can extract the encryption key as well as the tag of fspf.pb from file /data-original/keytag : $ export SCONE_FSPF_KEY = $( cat /data-original/keytag | awk '{print $11}' ) $ export SCONE_FSPF_TAG = $( cat /data-original/keytag | awk '{print $9}' ) $ export SCONE_FSPF = /data/fspf.pb We can now execute this program again: $ ./example Hello World f = open ( '/data/hello.txt' , 'r' ) print str ( f.read ()) Variables SCONE_FSPF_KEY , SCONE_FSPF_TAG and SCONE_FSPF should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please use CAS . Python Let's try a similar approach for Python. In the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file: docker run -it -v \" $PWD /volume:/data\" registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 bash The files /data/world.py and /data/hello.txt are encrypted: $ cat /data/world.py ? = ??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y??>f? Let's activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /data/fspf.pb We can now run the encrypted world.py program with the the Python interpreter: SCONE_HEAP = 100000000 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python /data/world.py export SCONE_QUEUES = 1 ... Hello World Protecting the Root Region Note that in the above example, Python will not be permitted to load dynamic libraries outside of the protected directory /data : a dynamic library must reside in either an authenticated or an encrypted region. To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries. Let us show how to authenticate all files in region / $ scone fspf addr fspf.pb / --kernel / --authenticated We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region / . Adding files from an existing container image We show how to add a subset of the files of container image registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 to our root region. To do so, we ensure that we have the newest images: > docker pull registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 > docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers How can we add all files in a container to the fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to /var/run/docker.sock : > docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" registry.scontain.com:5050/sconecuratedimages/crosscompilers Let us ensure that Docker is installed in this container: apt-get update apt-get install -y docker.io Now, we want to add all files of some target container. In our example, this is an instance of image registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 . We ensure that we pulled the latest image before we start the container: CONTAINER_ID = ` docker run -d registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 printf OK ` We can now copy all files from this container into a new directory rootvol : $ cd $ mkdir -p rootvol $ docker cp $CONTAINER_ID :/ ./rootvol Now that we have a copy of the files, we should not forget to garbage collect this container: docker rm $CONTAINER_ID Let's remove some directories that we do not want our program to access, like for example, /dev : $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++ Now, we create a root fspf : $ scone fspf create fspf.pb $ scone fspf addr fspf.pb / --kernel / --authenticated $ scone fspf addf fspf.pb / ./rootvol / $ scone fspf encrypt fspf.pb > keytag We can now create a new container image with this file system protection file using this Dockerfile $ cat > Dockerfile << EOF FROM registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 COPY fspf.pb / EOF $ docker build -t registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6-authenticated . We can run a container as follows: $ docker run -it registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh Let us activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb Let's run python with authenticated file system: SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python Checking the File System Shield Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in side of a python container: > docker run -i registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh $ cat > helloworld-manual.py << EOF print \"Hello World\" EOF When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file. $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld-manual.py ( fails ) We can, however, add a new file via programs that have access to the key of the fspf . We can, for example, write a python program to add a new python program to the file system. By default, we disable that the root fspf is updated. We can enable updates by setting environment variable SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable SCONE_FSPF_MUTABLE=1 ). $ SCONE_HEAP = 1000000000 SCONE_FSPF_MUTABLE = 1 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python << PYTHON f = open('helloworld.py', 'w') f.write('print \"Hello World\"\\n') f.close() PYTHON ``` The tag of the file system protection file is now changed. We can determine the new TAG with the help of command scone fspf show : $ export SCONE_FSPF_TAG = $( scone fspf show --tag /fspf.pb ) Now, we can run the new helloworld.py : $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld.py ... Hello World Extended Example To learn how to use multiple file system protection files, please have a look at the following screencast. Below is the script that is executed in the screencast: docker run -it -v $PWD :/mnt registry.scontain.com:5050/sconecuratedimages/crosscompilers mkdir -p /example mkdir -p /mnt/authenticated/ mkdir -p /mnt/encrypted/ cd /example mkdir -p .original scone fspf create fspf.pb # add protection regions scone fspf addr fspf.pb / -e --ephemeral scone fspf addr fspf.pb /mnt/authenticated -a --kernel /mnt/authenticated scone fspf addr fspf.pb /mnt/encrypted -e --kernel /mnt/encrypted # add files # enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated scone fspf addf fspf.pb /mnt/authenticated ./original # enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted # the client will write the encrypted files to ./mnt/encrypted scone fspf addf fspf.pb /mnt/encrypted ./original ./mnt/encrypted KEYTAG = ` scone fspf encrypt fspf.pb ` export SCONE_FSPF_KEY = ` echo $KEYTAG | awk '{print $11}' ` export SCONE_FSPF_TAG = ` echo $KEYTAG | awk '{print $9}' ` echo \"SCONE_FSPF_KEY= ${ SCONE_FSPF_KEY } ; SCONE_FSPF_TAG= ${ SCONE_FSPF_TAG } \" cat > example.c << EOF #include <stdio.h> int main() { FILE *fp = fopen(\"/mnt/authenticated/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); fp = fopen(\"/mnt/encrypted/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); } EOF scone gcc example.c -o sgxex cat > /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF SCONE_FSPF = fspf.pb ./sgxex cat /mnt/authenticated/hello cat /mnt/encrypted/hello cat > cat.c << EOF #include <stdio.h> int main() { char buf[80]; FILE *fp = fopen(\"/mnt/authenticated/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); fp = fopen(\"/mnt/encrypted/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); } EOF scone gcc cat.c -o native_cat ./native_cat scone gcc cat.c -o sgxcat SCONE_FSPF = fspf.pb ./sgxcat","title":"scone fspf"},{"location":"SCONE_Fileshield/#scone-file-protection","text":"This section describes the low-level SCONE file encryption interface to encrypt the root filesystem of an image. In many cases, one might want to use a SCONE volume instead: a SCONE volume is typically mapped on a Kubernetes volume with access modes ReadWriteOnce or ReadOnlyMany a SCONE volume is automatically encrypted, i.e., no need to perform the fspf operations described in this section a SCONE volume - as a Kubernetes volume - exists only once and SCONE CAS tracks its confidentiality and integrity including freshness.","title":"SCONE File Protection"},{"location":"SCONE_Fileshield/#concepts","text":"SCONE supports the transparent encryption and/or authentication of files of the root file system of a container / image. By transparent , we mean that there are no application code changes needed to support this. The underlying idea of SCONE file protection is that a user specifies that each file is either : authenticated , i.e., SCONE checks that the content was not modified by some unauthorized entity, encrypted , i.e., the confidentiality is protected by encryption. Encrypted files are always authenticated, or not-protected , i.e. SCONE reads and write the files without any extra protection mechanisms. For example, you might use not-protected if your application already encrypts its files or if you need direct access to devices. Marking all files individually as either authenticated , encrypted , or not-protected would not be very practical. Hence, we support to partition the filesystem into regions : regions do not overlap and each file belongs to exactly one region. A region is defined by a path. For example, region / is the root region and you could, for example, specify that all files in region / must be authenticated. You can define a second region, for example, region /data/db and that this region is encrypted. Each file belongs to exactly one region: it belongs to the region that has the longest common path prefix with this file. For example, file /etc/db.conf would belong, in this case, to region / and file /data/db/table.db would belong to region /data/db . SCONE supports ephemeral regions: files are stored in main memory outside of the enclave. Since the main memory is not protected, we recommend that an ephemeral regions is either authenticated or encrypted. When a program starts, all its ephemeral regions are empty. The only way to add files to an ephemeral region is by the application writing to this region. All files in an ephemeral region are lost when the application exits. All files that need to be persistent should be stored in a non-ephemeral region instead. We refer to this as a kernel region. For each region, you need to specify if the region is either ephemeral or kernel . Each region belongs to one of the following six classes: { ephemeral | kernel } X { not-protected | authenticated | encrypted }","title":"Concepts"},{"location":"SCONE_Fileshield/#example","text":"Sometimes, we might only need to protect the files that are passed to a container via some volume. In this case, it would be sufficient that the volume is either authenticated or encrypted. Let us demonstrate this via a simple example in which we pass an encrypted volume to a container. We create this encrypted volume in our local filesystem (in directory volume ) and we will later mount this in the container as /data . The original (non-encrypted) files are stored in directory data-original . > mkdir -p volume > mkdir -p data-original Let's write some files in the data-original directory: cat > data-original/hello.txt << EOF Hello World EOF cat > data-original/world.py << EOF f = open('/data/hello.txt', 'r') print str(f.read()) EOF Let's check that volume is empty and we print the hash values of the two files in data-original : > ls volume > shasum data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c data-original/world.py Now, we start the SCONE crosscompiler in a container to create the encrypted volume: > docker run -it -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" registry.scontain.com:5050/sconecuratedimages/crosscompilers","title":"Example"},{"location":"SCONE_Fileshield/#file-system-protection-file","text":"All the metadata required for checking the consistency of the files is stored in a file system protection file , or, short fspf . SCONE supports multiple *fspf*s. Let's start with a simple example with a single fspf . The fspf file is created via command scone fspf create and let us name this file fspf.pb . We execute the following commands inside the container (as indicated by the $ prompt): $ cd /data $ scone fspf create fspf.pb Created empty file system protection file in fspf.pb. AES-GCM tag: 0e3da7ad62f5bc7c7bb08c67b16f2423 We can now split the file system in regions , a region is a subtree. You can add regions to a fspf with the help of command scone fspf addr . Each region has exactly one of the following properties: authenticated : the integrity of files is checked, i.e., any unauthorized modification of this file is detected and results in a reading error inside of the enclave. Specify command line option --authenticated . encrypted : the confidentiality and integrity of files is protected, i.e., encrypted always implies that the files are also authenticated. Specify command line option --encrypted . not-protected : files are neither authenticated nor encrypted. Specify command line option --not-protected . File system changes of containers are typically ephemeral in the sense that file updates are lost when a container terminates. When specifying option --ephemeral , files in this region are not written to disk, the are written to an in memory file system instead. Say for now, that by default we do not protect files and we want to read files and write back changed files to the file system. To do so, we define that the root tree is --kernel as well as --not-protected : $ scone fspf addr fspf.pb / --kernel / --not-protected Added region / to file system protection file fspf.pb new AES-GCM tag: dd961af10b5aaa5cb1044c35a3f42c84 You need to specify root / In case you define more than one region, you always need to define a root region \"/\". Let us add another region /data that should be encrypted and persisted. To encrypt the files, we specify option --encrypted . We specify option --kernel followed by a path (here, also /data ) to request that files in this region are written to the kernel file system into directory /data . $ scone fspf addr fspf.pb /data --encrypted --kernel /data Added region /data to file system protection file fspf.pb new AES-GCM tag: 8481369d3ffdd9b6aeb30d044bf5c1c7 The encryption key for a file is chosen at random and stored in fspf.pb . We use the Intel random number generator RdRand to generate the key. The default key length of a region is 32 bytes. Alternatives are key length of 16 and 24 bytes. These can be selected via option --key-length 16 and --key-length 24 when creating a region with command scone fspf addr . Now, that we defined the regions, i.e., / and /data , we can add files to region /data . Let's just add all files in /data-original , encrypt these and write the encrypted files to /data . Note, the first /data argument specifies the protection region that determines the protection policy. The second, specifies where the encrypted files will be stored. That is, the command iterates over and reads the existing files in /data-original and encrypts them. The encrypted file content is written into the directory /data while the protection metadata of the individual files is added to the fsfp.pb file. $ scone fspf addf fspf.pb /data /data-original /data Added files to file system protection file fspf.pb new AES-GCM tag: 39a268166e628cf76e3fca80aa2d4f63 Note that if the /data region would have been only authenticated and not encrypted, the tool does not need to write out any (encrypted) files. It will only add the file names and the checksums (tags) of the files located in /data-original to the fspf.pb file. Thus, you could drop the last argument in this case. Coming back to the above example, we can now compare the hash values of the original files and the encrypted files: $ shasum /data/* 87fd97468024e3d2864516ff5840e15d9615340d /data/fspf.pb 31732914910f4a08b9832c442074b0932915476c /data/hello.txt 8d07f3f576785c373a5e70e8dbcfa8ee06ca6d0c /data/world.py $ shasum /data-original/* 648a6a6ffffdaa0badb23b8baf90b6168dd16b3a /data-original/hello.txt deda99d44e880ea8f2250f45c5c20c15d568d84c /data-original/world.py The fspf itself is not yet encrypted. We encrypt this file via command scone fspf encrypt fspf.pb $ scone fspf encrypt fspf.pb > /data-original/keytag We store the random encryption key as well as the tag of file fspf.pb in file /data-original/keytag . We introduce a very simple program that reads the two files: $ cat > example.c << EOF #include <stdio.h> #include <stdlib.h> void printfile(const char* fn) { FILE *fp = fopen(fn, \"r\"); char c; while((c=fgetc(fp))!=EOF ){ printf ( \"%c\" ,c ) ; } fclose ( fp ) ; } int main () { printfile ( \"/data/hello.txt\" ) ; printfile ( \"/data/world.py\" ) ; } EOF Let's crosscompile this program: scone gcc example.c -o example Executing this program results in an output like this: $./example R??C? q?z??E?? | \u042e? } \u00fc ?o $? ?!rga??\u0387* ` ?????????Gw? We need to activate the file system shield via environment variables by setting the location of the file system protection file (in SCONE_FSPF ), the encryption key of the file (in SCONE_FSPF_KEY ) and the tag of the fspf (in SCONE_FSPF_TAG ). We can extract the encryption key as well as the tag of fspf.pb from file /data-original/keytag : $ export SCONE_FSPF_KEY = $( cat /data-original/keytag | awk '{print $11}' ) $ export SCONE_FSPF_TAG = $( cat /data-original/keytag | awk '{print $9}' ) $ export SCONE_FSPF = /data/fspf.pb We can now execute this program again: $ ./example Hello World f = open ( '/data/hello.txt' , 'r' ) print str ( f.read ()) Variables SCONE_FSPF_KEY , SCONE_FSPF_TAG and SCONE_FSPF should only be set manually for debugging since they cannot securely be passed in this way to programs running inside enclaves. To securely pass environment variables, please use CAS .","title":"File System Protection File"},{"location":"SCONE_Fileshield/#python","text":"Let's try a similar approach for Python. In the above example, we encrypted a Python program. Let's try to execute this encrypted program that accesses an encrypted file: docker run -it -v \" $PWD /volume:/data\" registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 bash The files /data/world.py and /data/hello.txt are encrypted: $ cat /data/world.py ? = ??J??0?6+?Q?nKd?*N,??.?G???????R?cO?t?y??>f? Let's activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /data/fspf.pb We can now run the encrypted world.py program with the the Python interpreter: SCONE_HEAP = 100000000 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python /data/world.py export SCONE_QUEUES = 1 ... Hello World","title":"Python"},{"location":"SCONE_Fileshield/#protecting-the-root-region","text":"Note that in the above example, Python will not be permitted to load dynamic libraries outside of the protected directory /data : a dynamic library must reside in either an authenticated or an encrypted region. To deal with this, we must define one or more authenticated or encrypted file regions that contain the dynamic libraries. Let us show how to authenticate all files in region / $ scone fspf addr fspf.pb / --kernel / --authenticated We need to add all files that our application might access. Often, these files in the root region might be defined in some container image. Let's see how we can add these files to our region / .","title":"Protecting the Root Region"},{"location":"SCONE_Fileshield/#adding-files-from-an-existing-container-image","text":"We show how to add a subset of the files of container image registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 to our root region. To do so, we ensure that we have the newest images: > docker pull registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 > docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers How can we add all files in a container to the fspf ? One way to do so requires to run Docker inside of a Docker container. To be able to do so, we need to permit our outermost docker container to have access to /var/run/docker.sock : > docker run -it -v /var/run/docker.sock:/var/run/docker.sock -v \" $PWD /volume:/data\" -v \" $PWD /data-original:/data-original\" registry.scontain.com:5050/sconecuratedimages/crosscompilers Let us ensure that Docker is installed in this container: apt-get update apt-get install -y docker.io Now, we want to add all files of some target container. In our example, this is an instance of image registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 . We ensure that we pulled the latest image before we start the container: CONTAINER_ID = ` docker run -d registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 printf OK ` We can now copy all files from this container into a new directory rootvol : $ cd $ mkdir -p rootvol $ docker cp $CONTAINER_ID :/ ./rootvol Now that we have a copy of the files, we should not forget to garbage collect this container: docker rm $CONTAINER_ID Let's remove some directories that we do not want our program to access, like for example, /dev : $ rm -rf rootvol/dev rootvol/proc rootvol/bin rootvol/media rootvol/mnt rootvol/usr/share/X11 rootvol/usr/share/terminfo rootvol/optrootvol/usr/include/c++/ rootvol/usr/lib/tcl8.6 rootvol/usr/lib/gcc rootvol/opt rootvol/sys rootvol/usr/include/c++ Now, we create a root fspf : $ scone fspf create fspf.pb $ scone fspf addr fspf.pb / --kernel / --authenticated $ scone fspf addf fspf.pb / ./rootvol / $ scone fspf encrypt fspf.pb > keytag We can now create a new container image with this file system protection file using this Dockerfile $ cat > Dockerfile << EOF FROM registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6 COPY fspf.pb / EOF $ docker build -t registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6-authenticated . We can run a container as follows: $ docker run -it registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh Let us activate the file shield: $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb Let's run python with authenticated file system: SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python","title":"Adding files from an existing container image"},{"location":"SCONE_Fileshield/#checking-the-file-system-shield","text":"Let's us check the file shield by creating a new python program ( helloworld-manual.py ) in side of a python container: > docker run -i registry.scontain.com:5050/sconecuratedimages/apps:python-2.7-alpine3.6-authenticated sh $ cat > helloworld-manual.py << EOF print \"Hello World\" EOF When we switch on the file shield, the execution of this program inside the enclave will fail: since this file was not part of the original file system, the file system shield will prevent accessing this file. $ export SCONE_FSPF_KEY = ... extract from data-original/keytag ... $ export SCONE_FSPF_TAG = ... extract from data-original/keytag ... $ export SCONE_FSPF = /fspf.pb $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld-manual.py ( fails ) We can, however, add a new file via programs that have access to the key of the fspf . We can, for example, write a python program to add a new python program to the file system. By default, we disable that the root fspf is updated. We can enable updates by setting environment variable SCONE_FSPF_MUTABLE=1 . We plan to permit updates of the root fspf by default in the near future (i.e., we will remove variable SCONE_FSPF_MUTABLE=1 ). $ SCONE_HEAP = 1000000000 SCONE_FSPF_MUTABLE = 1 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python << PYTHON f = open('helloworld.py', 'w') f.write('print \"Hello World\"\\n') f.close() PYTHON ``` The tag of the file system protection file is now changed. We can determine the new TAG with the help of command scone fspf show : $ export SCONE_FSPF_TAG = $( scone fspf show --tag /fspf.pb ) Now, we can run the new helloworld.py : $ SCONE_HEAP = 1000000000 SCONE_ALLOW_DLOPEN = 2 SCONE_ALPINE = 1 SCONE_VERSION = 1 /usr/local/bin/python helloworld.py ... Hello World","title":"Checking the File System Shield"},{"location":"SCONE_Fileshield/#extended-example","text":"To learn how to use multiple file system protection files, please have a look at the following screencast. Below is the script that is executed in the screencast: docker run -it -v $PWD :/mnt registry.scontain.com:5050/sconecuratedimages/crosscompilers mkdir -p /example mkdir -p /mnt/authenticated/ mkdir -p /mnt/encrypted/ cd /example mkdir -p .original scone fspf create fspf.pb # add protection regions scone fspf addr fspf.pb / -e --ephemeral scone fspf addr fspf.pb /mnt/authenticated -a --kernel /mnt/authenticated scone fspf addr fspf.pb /mnt/encrypted -e --kernel /mnt/encrypted # add files # enclave program should expect the files (directories) found by the client in ./original in /mnt/authenticated scone fspf addf fspf.pb /mnt/authenticated ./original # enclave program should expect the files (directories) found by the client in ./original in encrypted form in /mnt/encrypted # the client will write the encrypted files to ./mnt/encrypted scone fspf addf fspf.pb /mnt/encrypted ./original ./mnt/encrypted KEYTAG = ` scone fspf encrypt fspf.pb ` export SCONE_FSPF_KEY = ` echo $KEYTAG | awk '{print $11}' ` export SCONE_FSPF_TAG = ` echo $KEYTAG | awk '{print $9}' ` echo \"SCONE_FSPF_KEY= ${ SCONE_FSPF_KEY } ; SCONE_FSPF_TAG= ${ SCONE_FSPF_TAG } \" cat > example.c << EOF #include <stdio.h> int main() { FILE *fp = fopen(\"/mnt/authenticated/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); fp = fopen(\"/mnt/encrypted/hello\", \"w\"); fprintf(fp, \"hello world\\n\"); fclose(fp); } EOF scone gcc example.c -o sgxex cat > /etc/sgx-musl.conf << EOF Q 4 e -1 0 0 s -1 0 0 e -1 1 0 s -1 1 0 e -1 2 0 s -1 2 0 e -1 3 0 s -1 3 0 EOF SCONE_FSPF = fspf.pb ./sgxex cat /mnt/authenticated/hello cat /mnt/encrypted/hello cat > cat.c << EOF #include <stdio.h> int main() { char buf[80]; FILE *fp = fopen(\"/mnt/authenticated/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); fp = fopen(\"/mnt/encrypted/hello\", \"r\"); fgets(buf, sizeof(buf), fp); fclose(fp); printf(\"read: '%s'\\n\", buf); } EOF scone gcc cat.c -o native_cat ./native_cat scone gcc cat.c -o sgxcat SCONE_FSPF = fspf.pb ./sgxcat","title":"Extended Example"},{"location":"SCONE_GENERATE_IMAGE/","text":"Generating Container Image with SCONE We show how to generate a Docker image that contains our hello world running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section. Prerequisites Check that all prerequisites from SCONE Tutorial are satisfied. Clone the SCONE_TUTORIAL before you start creating a hello world image. Generate HelloWorld image We generate a hello world container image. > cd SCONE_TUTORIAL/CreateImage You can either execute all step manually by copy&pasting all instructions or you can just execute > docker login > sudo ./Dockerfile.sh and watch the outputs. Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"registry.scontain.com:5050/sconecuratedimages/helloworld\" We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device CONTAINER_ID = ` docker run -d -it $MOUNT_SGXDEVICE -v $( pwd ) :/mnt registry.scontain.com:5050/sconecuratedimages/crosscompilers bash -c \" set -e printf 'Q 1\\ne 0 0 0\\ns 1 0 0\\n' > /etc/sgx-musl.conf sgxmusl-hw-async-gcc /mnt/hello_world.c -o /usr/local/bin/sgx_hello_world \" ` Note that above will fail if you do not have access to the SGX device /dev/isgx . Turn the container into an image: IMAGE_ID = $( docker commit -p -c 'CMD sgx_hello_world' $CONTAINER_ID $IMAGE_NAME : $TAG ) You can run this image by executing: sudo docker run $MOUNT_SGXDEVICE $IMAGE_NAME : $TAG You can push this image to Docker. However, ensure that you first login to docker: sudo docker login before you push the image to docker hub: sudo docker push $IMAGE_NAME : $TAG Note: this will fail in case you do not have the permission to push to this repository. Screencast","title":"SCONE Create Image"},{"location":"SCONE_GENERATE_IMAGE/#generating-container-image-with-scone","text":"We show how to generate a Docker image that contains our hello world running inside of an enclave and pushing this to docker hub. We only show this for the statically-linked binary. You can see that this code is quite awkward. It is much easier to generate images with a Dockerfile - which we show in the next section.","title":"Generating Container Image with SCONE"},{"location":"SCONE_GENERATE_IMAGE/#prerequisites","text":"Check that all prerequisites from SCONE Tutorial are satisfied. Clone the SCONE_TUTORIAL before you start creating a hello world image.","title":"Prerequisites"},{"location":"SCONE_GENERATE_IMAGE/#generate-helloworld-image","text":"We generate a hello world container image. > cd SCONE_TUTORIAL/CreateImage You can either execute all step manually by copy&pasting all instructions or you can just execute > docker login > sudo ./Dockerfile.sh and watch the outputs. Please change the image name to a repository on docker hub to which you can write: > export TAG = \"latest\" > export IMAGE_NAME = \"registry.scontain.com:5050/sconecuratedimages/helloworld\" We generate container and compile hello world inside of this container with the help of our standard SCONE cross compiler. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device CONTAINER_ID = ` docker run -d -it $MOUNT_SGXDEVICE -v $( pwd ) :/mnt registry.scontain.com:5050/sconecuratedimages/crosscompilers bash -c \" set -e printf 'Q 1\\ne 0 0 0\\ns 1 0 0\\n' > /etc/sgx-musl.conf sgxmusl-hw-async-gcc /mnt/hello_world.c -o /usr/local/bin/sgx_hello_world \" ` Note that above will fail if you do not have access to the SGX device /dev/isgx . Turn the container into an image: IMAGE_ID = $( docker commit -p -c 'CMD sgx_hello_world' $CONTAINER_ID $IMAGE_NAME : $TAG ) You can run this image by executing: sudo docker run $MOUNT_SGXDEVICE $IMAGE_NAME : $TAG You can push this image to Docker. However, ensure that you first login to docker: sudo docker login before you push the image to docker hub: sudo docker push $IMAGE_NAME : $TAG Note: this will fail in case you do not have the permission to push to this repository.","title":"Generate HelloWorld image"},{"location":"SCONE_GENERATE_IMAGE/#screencast","text":"","title":"Screencast"},{"location":"SCONE_Publications/","text":"SCONE Related Publications Enclosed is a list of publications related to SCONE. Note that the original paper about SCONE is our OSDI 2016 paper. During the last years, SCONE has evolved quite a bit and continues to evolve. In particular, some of the limitations of the original implementation has long been addressed. Since one can find some wrong or at least out-of-date info about SCONE, here is a short summary of features: SCONE supports security policies and transparent attestation . The security policy permit to generate secrets and certificates. The policy part was published in DSN2020. We will continue to extend the policy to support more application domains. native application support : while we still recommend the recompilation of existing applications, SCONE supports to sconify native applications, i.e., we can extend existing binaries to automatically run inside of enclaves. libc : we support binaries that use musl libc (Alpine Linux) and glibc (most other Linux distributions). We support Alpine Linux, Ubuntu, and RHEL/Centos. We will add other Linux distributions in future. Just drop us a note if you need a different distribution. SCONE supports containers and in particular, Kubernetes (which we prefer for deployment). However, sconified applications and service also run inside of VMs as well as on baremetal . SCONE supports static linking as well as dynamic linking . Shared libraries can also be loaded after the application has started. The shared libraries have to be protected with the SCONE filesystem shields. SCONE supports fork . However, fork is inherently slow in enclaves (since Intel SGX does not support efficient copy-on-write memory) and hence, we typically try to avoid using fork as much as possible by 1) running applications in the foreground, 2) using threads instead of processes, or 3) using the distributed version of an application instead of multiprocessing version. We have been working on side-channel protection. Our Usenix ATC2018 and Usenix Security 2020 papers describe parts of our effort. We continue to work on extending SCONE to have a practical way to address sidechannels of very sensitive application domains like eHealth. SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016 This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system call interface. Authors : Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer Media : pdf , slides , audio , url Abstract : In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers. We describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput. Bibtex @inproceedings { 199364 , author = {Sergei Arnautov and Bohdan Trach and Franz Gregor and Thomas Knauth and Andre Martin and Christian Priebe and Joshua Lind and Divya Muthukumaran and Dan O{\\textquoteright}Keeffe and Mark L. Stillwell and David Goltzsche and Dave Eyers and R{\\\"u}diger Kapitza and Peter Pietzuch and Christof Fetzer} , title = {{SCONE}: Secure Linux Containers with Intel {SGX}} , booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)} , year = {2016} , isbn = {978-1-931971-33-1} , address = {Savannah, GA} , pages = {689--703} , url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/arnautov} , publisher = {{USENIX} Association} , } Building Critical Applications Using Microservices, IEEE Security & Privacy, Volume: 14 Issue: 6, December 2016 Author : Christof Fetzer Media : pdf , html Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves. Bibtex @ARTICLE { 7782696 , author = {C. Fetzer} , journal = {IEEE Security Privacy} , title = {Building Critical Applications Using Microservices} , year = {2016} , volume = {14} , number = {6} , pages = {86-89} , keywords = {trusted computing;critical applications;critical software correctness;legacy hardware;microservice confidentiality;microservice integrity;microservice-based system;secure enclaves;software components;trustworthy systems;Buildings;Containers;Kernel;Linux;Security;Linux;hardware;microservices;secure enclaves;security;software} , doi = {10.1109/MSP.2016.129} , ISSN = {1540-7993} , month = {Nov} , } SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017 To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding the overheads, please see this paper. This won the best paper award of EuroSys 2017. Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer Media : pdf , html Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx. Bibtex @inproceedings { Kuvaiskii:2017:SMS:3064176.3064192 , author = {Kuvaiskii, Dmitrii and Oleksenko, Oleksii and Arnautov, Sergei and Trach, Bohdan and Bhatotia, Pramod and Felber, Pascal and Fetzer, Christof} , title = {SGXBOUNDS: Memory Safety for Shielded Execution} , booktitle = {Proceedings of the Twelfth European Conference on Computer Systems} , series = {EuroSys '17} , year = {2017} , isbn = {978-1-4503-4938-3} , location = {Belgrade, Serbia} , pages = {205--221} , numpages = {17} , url = {http://doi.acm.org/10.1145/3064176.3064192} , doi = {10.1145/3064176.3064192} , acmid = {3064192} , publisher = {ACM} , address = {New York, NY, USA} , } FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017 This paper describes our new lock-free queue for our asynchronous system calls. Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach Media : pdf , html Abstract : With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability. Bibtex @INPROCEEDINGS { 7967181 , author = {S. Arnautov and P. Felber and C. Fetzer and B. Trach} , booktitle = {2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)} , title = {FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue} , year = {2017} , volume = {} , number = {} , pages = {907-916} , keywords = {microprocessor chips;multi-threading;multiprocessing systems;operating systems (computers);queueing theory;FFQ;FIFO queue;consumer threads;fast single producer-multiple consumer concurrent FIFO queue;hardware threads;lock-free consumers;multicore architectures;multiple producer;operating systems;secure application;single producer variant;synchronization operations;Algorithm design and analysis;Context;Instruction sets;Message systems;Scalability;Synchronization;Throughput;FIFO queue;SPMC queue;concurrent FIFO queue;concurrent queue;lock-free algorithm;wait-free algorithm} , doi = {10.1109/IPDPS.2017.41} , ISSN = {} , month = {May} , } PESOS: Policy Enhanced Secure Object Store, EuroSys 2018 Authors : Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden) Media : pdf , html Abstract : Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language. Pesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small. Bibtex @inproceedings { Krahn:2018:PEP:3190508.3190518 , author = {Krahn, Robert and Trach, Bohdan and Vahldiek-Oberwagner, Anjo and Knauth, Thomas and Bhatotia, Pramod and Fetzer, Christof} , title = {Pesos: Policy Enhanced Secure Object Store} , booktitle = {Proceedings of the Thirteenth EuroSys Conference} , series = {EuroSys '18} , year = {2018} , isbn = {978-1-4503-5584-1} , location = {Porto, Portugal} , pages = {25:1--25:17} , articleno = {25} , numpages = {17} , url = {http://doi.acm.org/10.1145/3190508.3190518} , doi = {10.1145/3190508.3190518} , acmid = {3190518} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {intel SGX, kinetic disks, policy language, storage security} , } ShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018 Authors : Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer Media : pdf , html , video Abstract : Middleboxes that process confidential data cannot be securely deployed in untrusted cloud environments. To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic. Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic. This motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers. ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate. Bibtex @inproceedings { Trach:2018:SSM:3185467.3185469 , author = {Trach, Bohdan and Krohmer, Alfred and Gregor, Franz and Arnautov, Sergei and Bhatotia, Pramod and Fetzer, Christof} , title = {ShieldBox: Secure Middleboxes Using Shielded Execution} , booktitle = {Proceedings of the Symposium on SDN Research} , series = {SOSR '18} , year = {2018} , isbn = {978-1-4503-5664-0} , location = {Los Angeles, CA, USA} , pages = {2:1--2:14} , articleno = {2} , numpages = {14} , url = {http://doi.acm.org/10.1145/3185467.3185469} , doi = {10.1145/3185467.3185469} , acmid = {3185469} , publisher = {ACM} , address = {New York, NY, USA} , } Varys: Protecting SGX enclaves from practical side-channel attacks, USENIX ATC 2018 Authors : Oleksii Oleksenko, Bohdan Trach, Robert Krahn, and Andr\u00e9 Martin, TU Dresden; Mark Silberstein, Technion; Christof Fetzer, TU Dresden Media : pdf , slides , audio , url Abstract : Numerous recent works have experimentally shown that Intel Software Guard Extensions (SGX) are vulnerable to cache timing and page table side-channel attacks which could be used to circumvent the data confidentiality guarantees provided by SGX. Existing mechanisms that protect against these attacks either incur high execution costs, are ineffective against certain attack variants, or require significant code modifications. We present Varys, a system that protects unmodified programs running in SGX enclaves from cache timing and page table side-channel attacks. Varys takes a pragmatic approach of strict reservation of physical cores to security-sensitive threads, thereby preventing the attacker from accessing shared CPU resources during enclave execution. The key challenge that we are addressing is that of maintaining the core reservation in the presence of an untrusted OS. Varys fully protects against all L1/L2 cache timing attacks and significantly raises the bar for page table side-channel attacks - all with only 15% overhead on average for Phoenix and PARSEC benchmarks. Additionally, we propose a set of minor hardware extensions that hold the potential to extend Varys' security guarantees to L3 cache and further improve its performance. Bibtex @inproceedings { 216033 , author = {Oleksii Oleksenko and Bohdan Trach and Robert Krahn and Mark Silberstein and Christof Fetzer} , title = {Varys: Protecting {SGX} Enclaves from Practical Side-Channel Attacks} , booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)} , year = {2018} , isbn = {978-1-931971-44-7} , address = {Boston, MA} , pages = {227--240} , url = {https://www.usenix.org/conference/atc18/presentation/oleksenko} , publisher = {{USENIX} Association} , } SGX-PySpark: Secure Distributed Data Analytics, WWW2019 Authors : Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. Media : https://doi.org/10.1145/3308558.3314129 Abstract : Data analytics is central to modern online services, particularly those data-driven. Often this entails the processing of large-scale datasets which may contain private, personal and sensitive information relating to individuals and organisations. Particular challenges arise where cloud is used to store and process the sensitive data. In such settings, security and privacy concerns become paramount, as the cloud provider is trusted to guarantee the security of the services they offer, including data confidentiality. Therefore, the issue this work tackles is \u201cHow to securely perform data analytics in a public cloud?\u201d To assist this question, we design and implement SGX-PySpark- a secure distributed data analytics system which relies on a trusted execution environment (TEE) such as Intel SGX to provide strong security guarantees. To build SGX-PySpark, we integrate PySpark - a widely used framework for data analytics in industry to support a wide range of queries, with SCONE - a shielded execution framework using Intel SGX. Bibtex @inproceedings { LeQuoc:2019:SSD:3308558.3314129 , author = {Le Quoc, Do and Gregor, Franz and Singh, Jatinder and Fetzer, Christof} , title = {SGX-PySpark: Secure Distributed Data Analytics} , booktitle = {The World Wide Web Conference} , series = {WWW '19} , year = {2019} , isbn = {978-1-4503-6674-8} , location = {San Francisco, CA, USA} , pages = {3564--3563} , numpages = {0} , url = {http://doi.acm.org/10.1145/3308558.3314129} , doi = {10.1145/3308558.3314129} , acmid = {3314129} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {Confidential computing, data analytics, distributed system, security} , } Clemmys: towards secure remote execution in FaaS, SYSTOR '19 Authors : Bohdan Trach, Oleksii Oleksenko, Franz Gregor, Pramod Bhatotia, Christof Fetzer Abstract : We introduce Clemmys, a security-first serverless platform that ensures confidentiality and integrity of users' functions and data as they are processed on untrusted cloud premises, while keeping the cost of protection low. We provide a design for hardening FaaS platforms with Intel SGX---a hardware-based shielded execution technology. We explain the protocol that our system uses to ensure confidentiality and integrity of data, and integrity of function chains. To overcome performance and latency issues that are inherent in SGX applications, we apply several SGX-specific optimizations to the runtime system: we use SGXv2 to speed up the enclave startup and perform batch EPC augmentation. To evaluate our approach, we implement our design over Apache Open-Whisk, a popular serverless platform. Lastly, we show that Clemmys achieved same throughput and similar latency as native Apache OpenWhisk, while allowing it to withstand several new attack vectors. Conference : SYSTOR '19: Proceedings of the 12 th ACM International Conference on Systems and StorageMay 2019 Pages 44\u201354, https://doi.org/10.1145/3319647.3325835 Bibtext @inproceedings { 10.1145/3319647.3325835 , author = {Trach, Bohdan and Oleksenko, Oleksii and Gregor, Franz and Bhatotia, Pramod and Fetzer, Christof} , title = {Clemmys: Towards Secure Remote Execution in FaaS} , year = {2019} , isbn = {9781450367493} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3319647.3325835} , doi = {10.1145/3319647.3325835} , abstract = {We introduce Clemmys, a security-first serverless platform that ensures confidentiality and integrity of users' functions and data as they are processed on untrusted cloud premises, while keeping the cost of protection low. We provide a design for hardening FaaS platforms with Intel SGX---a hardware-based shielded execution technology. We explain the protocol that our system uses to ensure confidentiality and integrity of data, and integrity of function chains. To overcome performance and latency issues that are inherent in SGX applications, we apply several SGX-specific optimizations to the runtime system: we use SGXv2 to speed up the enclave startup and perform batch EPC augmentation. To evaluate our approach, we implement our design over Apache Open-Whisk, a popular serverless platform. Lastly, we show that Clemmys achieved same throughput and similar latency as native Apache OpenWhisk, while allowing it to withstand several new attack vectors.} , booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage} , pages = {44\u201354} , numpages = {11} , location = {Haifa, Israel} , series = {SYSTOR '19} } Trust Management as a Service: Enabling Trusted Execution in the Face of Byzantine Stakeholders, DSN2020 Authors : Gregor, F., W. Ozga, S. Vaucher, R. Pires, D. Le Quoc, S. Arnautov, A. Martin, V. Schiavoni, P. Felber, and C. Fetzer Abstract : Trust is arguably the most important challenge for critical services both deployed as well as accessed remotely over the network. These systems are exposed to a wide diversity of threats, ranging from bugs to exploits, active attacks, rogue operators, or simply careless administrators. To protect such applications, one needs to guarantee that they are properly configured and securely provisioned with the \"secrets\" (e.g., encryption keys) necessary to preserve not only the confidentiality, integrity and freshness of their data but also their code. Furthermore, these secrets should not be kept under the control of a single stakeholder\u2014which might be compromised and would represent a single point of failure\u2014and they must be protected across software versions in the sense that attackers cannot get access to them via malicious updates. Traditional approaches for solving these challenges often use ad hoc techniques and ultimately rely on a hardware security module (HSM) as root of trust. We propose a more powerful and generic approach to trust management that instead relies on trusted execution environments (TEEs) and a set of stakeholders as root of trust. Our system, PAL\u00c6MON, can operate as a managed service deployed in an untrusted environment, i.e., one can delegate its operations to an untrusted cloud provider with the guarantee that data will remain confidential despite not trusting any individual human (even with root access) nor system software. PAL\u00c6MON addresses in a secure, efficient and cost-effective way five main challenges faced when developing trusted networked applications and services. Our evaluation on a range of benchmarks and real applications shows that PAL\u00c6MON performs efficiently and can protect secrets of services without any change to their source code. Media : https://ieeexplore.ieee.org/document/9153433 Conference : 50 th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2020) Bibtex @INPROCEEDINGS { 9153433 , author = {F. {Gregor} and W. {Ozga} and S. {Vaucher} and R. {Pires} and D. {Le Quoc} and S. {Arnautov} and A. {Martin} and V. {Schiavoni} and P. {Felber} and C. {Fetzer}} , booktitle = {2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)} , title = {Trust Management as a Service: Enabling Trusted Execution in the Face of Byzantine Stakeholders} , year = {2020} , volume = {} , number = {} , pages = {502-514} ,} T-Lease: A Trusted Lease Primitive for Distributed Systems, SOCC2020 Authors : B. Trach, R. Faqeh, O. Oleksenko, W. Ozga, P. Bhatotia, C. Fetzer Abstract : A lease is an important primitive for building distributed protocols, and it is ubiquitously employed in distributed systems. However, the scope of the classic lease abstraction is restricted to the trusted computing infrastructure. Unfortunately, this important primitive cannot be employed in the untrusted computing infrastructure because the trusted execution environments (TEEs) do not provide a trusted time source. In the untrusted environment, an adversary can easily manipulate the system clock to violate the correctness properties of lease-based systems. We tackle this problem by introducing em trusted lease---a lease that maintains its correctness properties even in the presence of a clock-manipulating attacker. To achieve these properties, we follow a \"trust but verify\" approach for an untrusted timer, and transform it into a trusted timing primitive by leveraging two hardware-assisted ISA extensions (Intel TSX and SGX) available in commodity CPUs. We provide a design and implementation of trusted lease in a system called T-Lease---the first trusted lease system that achieves high security, performance, and precision. For the application developers, T-Lease exposes an easy-to-use generic APIs that facilitate its usage to build a wide range of distributed protocols. Conference : 2020 ACM Symposium on Cloud Computing Media : doi: 10.1145/3419111.3421273 Bibtext @inproceedings { 10.1145/3419111.3421273 , author = {Trach, Bohdan and Faqeh, Rasha and Oleksenko, Oleksii and Ozga, Wojciech and Bhatotia, Pramod and Fetzer, Christof} , title = {T-Lease: A Trusted Lease Primitive for Distributed Systems} , year = {2020} , isbn = {9781450381376} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3419111.3421273} , doi = {10.1145/3419111.3421273} , abstract = {A lease is an important primitive for building distributed protocols, and it is ubiquitously employed in distributed systems. However, the scope of the classic lease abstraction is restricted to the trusted computing infrastructure. Unfortunately, this important primitive cannot be employed in the untrusted computing infrastructure because the trusted execution environments (TEEs) do not provide a trusted time source. In the untrusted environment, an adversary can easily manipulate the system clock to violate the correctness properties of lease-based systems.We tackle this problem by introducing trusted lease---a lease that maintains its correctness properties even in the presence of a clock-manipulating attacker. To achieve these properties, we follow a \"trust but verify\" approach for an untrusted timer, and transform it into a trusted timing primitive by leveraging two hardware-assisted ISA extensions (Intel TSX and SGX) available in commodity CPUs. We provide a design and implementation of trusted lease in a system called T-Lease---the first trusted lease system that achieves high security, performance, and precision. For the application developers, T-Lease exposes an easy-to-use generic APIs that facilitate its usage to build a wide range of distributed protocols.} , booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing} , pages = {387\u2013400} , numpages = {14} , location = {Virtual Event, USA} , series = {SoCC '20} } A practical approach for updating an integrity-enforced OS, Middleware 2020 Authors : W. Ozga, D. Le Quoc, C. Fetzer Abstract : Trusted computing defines how to securely measure, store, and verify the integrity of software controlling a computer. One of the major challenge that makes them hard to be applied in practice is the issue of software updates. Specifically, an operating system update causes the integrity violation because it changes the well-known initial state trusted by remote verifiers, such as integrity monitoring systems. Consequently, the integrity monitoring of remote computers becomes unreliable due to the high amount of false positives. We address this problem by adding an extra level of indirection between the operating system and software repositories. We propose a trusted software repository (TSR), a secure proxy framework that overcomes the shortcomings of previous approaches by sanitizing software packages. Sanitization consists of modifying unsafe installation scripts and adding digital signatures in a way the software packages can be installed in the operating system without violating its integrity. TSR leverages shielded execution, i.e., Intel SGX, to achieve strong confidentiality and integrity guarantees of the sanitization process. Moreover, TSR is transparent to package managers, and requires no changes in the software packages building and distributing processes. Our evaluation shows that running TSR inside Intel SGX is practical; since it induces only \u223c 1.18\u00d7 performance overhead during package sanitization compared to the native execution without using Intel SGX. Also, TSR supports 99.76% of packages available in the main and community repositories of Alpine Linux while increasing the total repository size by 3.6%. Conference : ACM/IFIP Middleware 2020 Media : doi: 10.1145/3423211.3425674 Bibtext @inproceedings { 10.1145/3423211.3425674 , author = {Ozga, Wojciech and Quoc, Do Le and Fetzer, Christof} , title = {A Practical Approach for Updating an Integrity-Enforced Operating System} , year = {2020} , isbn = {9781450381536} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3423211.3425674} , doi = {10.1145/3423211.3425674} , abstract = {Trusted computing defines how to securely measure, store, and verify the integrity of software controlling a computer. One of the major challenge that make them hard to be applied in practice is the issue with software updates. Specifically, an operating system update causes the integrity violation because it changes the well-known initial state trusted by remote verifiers, such as integrity monitoring systems. Consequently, the integrity monitoring of remote computers becomes unreliable due to the high amount of false positives.We address this problem by adding an extra level of indirection between the operating system and software repositories. We propose a trusted software repository (TSR), a secure proxy that overcomes the shortcomings of previous approaches by sanitizing software packages. Sanitization consists of modifying unsafe installation scripts and adding digital signatures in a way software packages can be installed in the operating system without violating its integrity. TSR leverages shielded execution, i.e., Intel SGX, to achieve confidentiality and integrity guarantees of the sanitization process.TSR is transparent to package managers, and requires no changes in the software packages building and distributing processes. Our evaluation shows that running TSR inside SGX is practical; since it induces only ~ 1.18\\texttimes{} performance overhead during package sanitization compared to the native execution without SGX. TSR supports 99.76% of packages available in the main and community repositories of Alpine Linux while increasing the total repository size by 3.6%.} , booktitle = {Proceedings of the 21st International Middleware Conference} , pages = {311\u2013325} , numpages = {15} , keywords = {intel software guard extensions, integrity measurement architecture (IMA), trusted computing, software updates} , location = {Delft, Netherlands} , series = {Middleware '20} } secureTF: A Secure TensorFlow Framework, Middleware 2020 Authors : D. Le Quoc, F. Gregor, R. Kunkel, S. Arnautov, P. Bhatotia, C. Fetzer Abstract : Data-driven intelligent applications in modern online services have become ubiquitous. These applications are usually hosted in the untrusted cloud computing infrastructure. This poses significant security risks since these applications rely on applying machine learning algorithms on large datasets which may contain private and sensitive information. To tackle this challenge, we designed and implemented secureTF, a generic and secure machine learning framework based on Tensorflow. secureTF enables secure execution of unmodified TensorFlow applications on the untrusted cloud computing infrastructure. secureTF is built from ground-up based on the security properties provided by Trusted Execution Environments (TEEs). However, it extends the trust of a volatile memory region (or secure enclave) provided by the single node TEE to secure a distributed computing infrastructure required for supporting unmodified stateful machine learning applications running in the cloud. The paper reports on our experiences about the system design choices and the system deployment in production use-cases. We conclude with the lessons learned based on the limitations of our commercially available platform and discuss open research problems for the future work. Conference : ACM/IFIP Middleware 2020 Media : doi: 10.1145/3423211.3425687 Bibtext @inproceedings { 10.1145/3423211.3425687 , author = {Quoc, Do Le and Gregor, Franz and Arnautov, Sergei and Kunkel, Roland and Bhatotia, Pramod and Fetzer, Christof} , title = {SecureTF: A Secure TensorFlow Framework} , year = {2020} , isbn = {9781450381536} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3423211.3425687} , doi = {10.1145/3423211.3425687} , abstract = {Data-driven intelligent applications in modern online services have become ubiquitous. These applications are usually hosted in the untrusted cloud computing infrastructure. This poses significant security risks since these applications rely on applying machine learning algorithms on large datasets which may contain private and sensitive information.To tackle this challenge, we designed secureTF, a distributed secure machine learning framework based on Tensorflow for the untrusted cloud infrastructure. secureTF is a generic platform to support unmodified TensorFlow applications, while providing end-to-end security for the input data, ML model, and application code. secureTF is built from ground-up based on the security properties provided by Trusted Execution Environments (TEEs). However, it extends the trust of a volatile memory region (or secure enclave) provided by the single node TEE to secure a distributed infrastructure required for supporting unmodified stateful machine learning applications running in the cloud.The paper reports on our experiences about the system design choices and the system deployment in production use-cases. We conclude with the lessons learned based on the limitations of our commercially available platform, and discuss open research problems for the future work.} , booktitle = {Proceedings of the 21st International Middleware Conference} , pages = {44\u201359} , numpages = {16} , keywords = {tensorflow, intel software guard extensions (Intel SGX), confidential computing, secure machine learning} , location = {Delft, Netherlands} , series = {Middleware '20} } TEEMon: A continuous performance monitoring framework for TEEs, Middleware 2020 Authors : R. Krahn, D. Le Quoc, D. Dragoti, F. Gregor, V. Schiavoni, C. Souza, P. Felber, A. Brito, C. Fetzer Abstract : Trusted Execution Environments (TEEs), such as Intel Software Guard eXtensions (SGX), are considered as a promising approach to resolve security challenges in clouds. TEEs protect the confidentiality and integrity of application code and data even against privileged attackers with root and physical access by providing an isolated secure memory area, i.e., enclaves. The security guarantees are provided by the CPU, thus even if system software is compromised, the attacker can never access the enclave\u2019s content. While this approach ensures strong security guarantees for applications, it also introduces a considerable runtime overhead in part by the limited availability of protected memory (enclave page cache). Currently, only a limited number of performance measurement tools for TEE-based applications exist and none offer performance monitoring and analysis during runtime. This paper presents TEEMon, the first continuous performance monitoring and analysis tool for TEE-based applications. TEEMon provides not only fine-grained performance metrics during runtime, but also assists the analysis of identifying causes of performance bottlenecks, e.g., excessive system calls. Our approach smoothly integrates with existing open-source tools (e.g., Prometheus or Grafana) towards a holistic monitoring solution, particularly optimized for systems deployed through Docker containers or Kubernetes and offers several dedicated metrics and visualizations. Our evaluation shows that TEEMon\u2019s overhead ranges from 5% to 17%. Conference : ACM/IFIP Middleware 2020 Media : doi: 10.1145/3423211.3425677 Bibtext @inproceedings { 10.1145/3423211.3425677 , author = {Krahn, Robert and Dragoti, Donald and Gregor, Franz and Quoc, Do Le and Schiavoni, Valerio and Felber, Pascal and Souza, Clenimar and Brito, Andrey and Fetzer, Christof} , title = {TEEMon: A Continuous Performance Monitoring Framework for TEEs} , year = {2020} , isbn = {9781450381536} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3423211.3425677} , doi = {10.1145/3423211.3425677} , abstract = {Trusted Execution Environments (TEEs), such as Intel Software Guard eXtensions (SGX), are considered as a promising approach to resolve security challenges in clouds. TEEs protect the confidentiality and integrity of application code and data even against privileged attackers with root and physical access by providing an isolated secure memory area, i.e., enclaves. The security guarantees are provided by the CPU, thus even if system software is compromised, the attacker can never access the enclave's content. While this approach ensures strong security guarantees for applications, it also introduces a considerable runtime overhead in part by the limited availability of protected memory (enclave page cache). Currently, only a limited number of performance measurement tools for TEE-based applications exist and none offer performance monitoring and analysis during runtime.This paper presents TEEMon, the first continuous performance monitoring and analysis tool for TEE-based applications. TEEMon provides not only fine-grained performance metrics during runtime, but also assists the analysis of identifying causes of performance bottlenecks, e.g., excessive system calls. Our approach smoothly integrates with existing open-source tools (e.g., Prometheus or Grafana) towards a holistic monitoring solution, particularly optimized for systems deployed through Docker containers or Kubernetes and offers several dedicated metrics and visualizations. Our evaluation shows that TEEMon's overhead ranges from 5% to 17%.} , booktitle = {Proceedings of the 21st International Middleware Conference} , pages = {178\u2013192} , numpages = {15} , keywords = {Computerized monitoring, Performance monitoring, Trusted Execution Environments} , location = {Delft, Netherlands} , series = {Middleware '20} } Towards Formalization of Enhanced Privacy ID (EPID)-based Remote Attestation in Intel SGX, DSD 2020 Authors : Sardar, Muhammad Usama and Quoc, Do Le and Fetzer, Christof Conference : 2020 23 rd Euromicro Conference on Digital System Design (DSD) Bibtext @inproceedings { Sardar2020EPID , author = {Sardar, Muhammad Usama and Quoc, Do Le and Fetzer, Christof} , booktitle = {2020 23rd Euromicro Conference on Digital System Design (DSD)} , doi = {10.1109/DSD51259.2020.00099} , isbn = {978-1-7281-9535-3} , month = {aug} , pages = {604--607} , publisher = {IEEE} , title = {{Towards Formalization of Enhanced Privacy ID (EPID)-based Remote Attestation in Intel SGX}} , url = {https://ieeexplore.ieee.org/document/9217791/} , year = {2020} } Formal Foundations for Intel SGX Data Center Attestation Primitives, Formal Methods and Software Engineering, 2020 Authors : Sardar, Muhammad Usama and Faqeh, Rasha and Fetzer, Christof Conference : Formal Methods and Software Engineering, 2020 Bibtext @incollection { Sardar2020DCAP , address = {Cham} , author = {Sardar, Muhammad Usama and Faqeh, Rasha and Fetzer, Christof} , booktitle = {Formal Methods and Software Engineering} , doi = {10.1007/978-3-030-63406-3_16} , editor = {Lin, Shang-Wei and Hou, Zhe and Mahoney, Brendan} , isbn = {978-3-030-63405-6} , keywords = {Data center attestation primitives,Data centers,Formal verification,Intel SGX,Remote attestation,Trusted execution environment,data center attestation,data centers,formal verification,intel sgx,remote attestation,trusted execution environment} , pages = {268--283} , publisher = {Springer International Publishing} , series = {Lecture Notes in Computer Science} , title = {{Formal Foundations for Intel SGX Data Center Attestation Primitives}} , url = {http://link.springer.com/10.1007/978-3-030-63406-3{\\_}16} , volume = {12531} , year = {2020} } SpecFuzz: Bringing Spectre-type vulnerabilities to the surface, Usenix Security 2020 Authors : Oleksii Oleksenko, Bohdan Trach, Mark Silberstein, Christof Fetzer Abstract : SpecFuzz is the first tool that enables dynamic testing for speculative execution vulnerabilities (e.g., Spectre). The key is a novel concept of speculation exposure: The program is instrumented to simulate speculative execution in software by forcefully executing the code paths that could be triggered due to mispredictions, thereby making the speculative memory accesses visible to integrity checkers (e.g., AddressSanitizer). Combined with the conventional fuzzing techniques, speculation exposure enables more precise identification of potential vulnerabilities compared to state-of-the-art static analyzers. Our prototype for detecting Spectre V1 vulnerabilities successfully identifies all known variations of Spectre V1 and decreases the mitigation overheads across the evaluated applications, reducing the amount of instrumented branches by up to 77% given a sufficient test coverage. Media : html , paper , slides , video Conference : Usenix Security 2020 Bibtext @inproceedings { 251530 , author = {Oleksii Oleksenko and Bohdan Trach and Mark Silberstein and Christof Fetzer} , title = {SpecFuzz: Bringing Spectre-type vulnerabilities to the surface} , booktitle = {29th {USENIX} Security Symposium ({USENIX} Security 20)} , year = {2020} , isbn = {978-1-939133-17-5} , pages = {1481--1498} , url = {https://www.usenix.org/conference/usenixsecurity20/presentation/oleksenko} , publisher = {{USENIX} Association} , month = aug , } SGXTuner: Performance Enhancement of Intel SGX Applications via Stochastic Optimization, IEEE Transactions on Dependable and Secure Computing, 2021 Authors : G. Mazzeo, S. Arnautov, C. Fetzer, and L. Romano Abstract : Intel SGX has started to be widely adopted. Cloud providers (Microsoft Azure, IBM Cloud, Alibaba Cloud) are offering new solutions, implementing data-in-use protection via SGX. A major challenge faced by both academia and industry is providing transparent SGX support to legacy applications. The approach with the highest consensus is linking the target software with SGX-extended libc libraries. Unfortunately, the increased security entails a dramatic performance penalty, which is mainly due to the intrinsic overhead of context switches, and the limited size of protected memory. Performance optimization is non-trivial since it depends on key parameters whose manual tuning is a very long process. We present the architecture of an automated tool, called SGXTuner, which is able to find the best setting of SGX-extended libc library parameters, by iteratively adjusting such parameters based on continuous monitoring of performance data. The tool is to a large extent algorithm agnostic. We decided to base the current implementation on a particular type of stochastic optimization algorithm, specifically Simulated Annealing. A massive experimental campaign was conducted on a relevant case study. Three client-server applications Memcached, Redis, and Apache were compiled with SCONE's sgx-musl and tuned for best performance. Results demonstrate the effectiveness of SGXTuner. Media : doi: 10.1109/TDSC.2021.3064391 Bibtext @ARTICLE { 9372883 , author = {G. {Mazzeo} and S. {Arnautov} and C. {Fetzer} and L. {Romano}} , journal = {IEEE Transactions on Dependable and Secure Computing} , title = {SGXTuner: Performance Enhancement of Intel SGX Applications via Stochastic Optimization} , year = {2021} , volume = {} , number = {} , pages = {1-1} , doi = {10.1109/TDSC.2021.3064391} }","title":"Publications"},{"location":"SCONE_Publications/#scone-related-publications","text":"Enclosed is a list of publications related to SCONE. Note that the original paper about SCONE is our OSDI 2016 paper. During the last years, SCONE has evolved quite a bit and continues to evolve. In particular, some of the limitations of the original implementation has long been addressed. Since one can find some wrong or at least out-of-date info about SCONE, here is a short summary of features: SCONE supports security policies and transparent attestation . The security policy permit to generate secrets and certificates. The policy part was published in DSN2020. We will continue to extend the policy to support more application domains. native application support : while we still recommend the recompilation of existing applications, SCONE supports to sconify native applications, i.e., we can extend existing binaries to automatically run inside of enclaves. libc : we support binaries that use musl libc (Alpine Linux) and glibc (most other Linux distributions). We support Alpine Linux, Ubuntu, and RHEL/Centos. We will add other Linux distributions in future. Just drop us a note if you need a different distribution. SCONE supports containers and in particular, Kubernetes (which we prefer for deployment). However, sconified applications and service also run inside of VMs as well as on baremetal . SCONE supports static linking as well as dynamic linking . Shared libraries can also be loaded after the application has started. The shared libraries have to be protected with the SCONE filesystem shields. SCONE supports fork . However, fork is inherently slow in enclaves (since Intel SGX does not support efficient copy-on-write memory) and hence, we typically try to avoid using fork as much as possible by 1) running applications in the foreground, 2) using threads instead of processes, or 3) using the distributed version of an application instead of multiprocessing version. We have been working on side-channel protection. Our Usenix ATC2018 and Usenix Security 2020 papers describe parts of our effort. We continue to work on extending SCONE to have a practical way to address sidechannels of very sensitive application domains like eHealth.","title":"SCONE Related Publications"},{"location":"SCONE_Publications/#scone-secure-linux-containers-with-intel-sgx-usenix-osdi-2016","text":"This paper describes how we support unmodified applications inside of enclaves. The focus is on our asynchronous system call interface. Authors : Sergei Arnautov, Bohdan Trach, Franz Gregor, Thomas Knauth, Andr\u00e9 Martin, Christian Priebe, Joshua Lind, Divya Muthukumaran, Daniel O'Keeffe, Mark L Stillwell, David Goltzsche, Dave Eyers, R\u00fcdiger Kapitza, Peter Pietzuch, Christof Fetzer Media : pdf , slides , audio , url Abstract : In multi-tenant environments, Linux containers managed by Docker or Kubernetes have a lower resource footprint, faster startup times, and higher I/O performance compared to virtual machines (VMs) on hypervisors. Yet their weaker isolation guarantees, enforced through software kernel mechanisms, make it easier for attackers to compromise the confidentiality and integrity of application data within containers. We describe SCONE, a secure container mechanism for Docker that uses the SGX trusted execution support of Intel CPUs to protect container processes from outside attacks. The design of SCONE leads to (i) a small trusted computing base (TCB) and (ii) a low performance overhead: SCONE offers a secure C standard library interface that transparently encrypts/decrypts I/O data; to reduce the performance impact of thread synchronization and system calls within SGX enclaves, SCONE supports user-level threading and asynchronous system calls. Our evaluation shows that it protects unmodified applications with SGX, achieving 0.6x\u20131.2x of native throughput. Bibtex @inproceedings { 199364 , author = {Sergei Arnautov and Bohdan Trach and Franz Gregor and Thomas Knauth and Andre Martin and Christian Priebe and Joshua Lind and Divya Muthukumaran and Dan O{\\textquoteright}Keeffe and Mark L. Stillwell and David Goltzsche and Dave Eyers and R{\\\"u}diger Kapitza and Peter Pietzuch and Christof Fetzer} , title = {{SCONE}: Secure Linux Containers with Intel {SGX}} , booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)} , year = {2016} , isbn = {978-1-931971-33-1} , address = {Savannah, GA} , pages = {689--703} , url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/arnautov} , publisher = {{USENIX} Association} , }","title":"SCONE: Secure Linux Containers with Intel SGX, USENIX, OSDI 2016"},{"location":"SCONE_Publications/#building-critical-applications-using-microservices-ieee-security-privacy-volume-14-issue-6-december-2016","text":"Author : Christof Fetzer Media : pdf , html Abstract : Safeguarding the correctness of critical software is a grand challenge. A microservice-based system is described that builds trustworthy systems on top of legacy hardware and software components, ensuring microservices' integrity, confidentiality, and correct execution with the help of secure enclaves. Bibtex @ARTICLE { 7782696 , author = {C. Fetzer} , journal = {IEEE Security Privacy} , title = {Building Critical Applications Using Microservices} , year = {2016} , volume = {14} , number = {6} , pages = {86-89} , keywords = {trusted computing;critical applications;critical software correctness;legacy hardware;microservice confidentiality;microservice integrity;microservice-based system;secure enclaves;software components;trustworthy systems;Buildings;Containers;Kernel;Linux;Security;Linux;hardware;microservices;secure enclaves;security;software} , doi = {10.1109/MSP.2016.129} , ISSN = {1540-7993} , month = {Nov} , }","title":"Building Critical Applications Using Microservices, IEEE Security &amp; Privacy, Volume: 14 Issue: 6, December 2016"},{"location":"SCONE_Publications/#sgxbounds-memory-safety-for-shielded-execution-eurosys-2017","text":"To protect the code running inside of an enclave, we implemented a novel bounds checker for enclaves. While we had expected to just be able to use MPX, we had to realized that MPX does not perform that well inside of enclaves. For details regarding the overheads, please see this paper. This won the best paper award of EuroSys 2017. Authors : D. Kuvaiskii, O. Oleksenko, S. Arnautov, B. Trach, P. Bhatotia, P. Felber, C. Fetzer Media : pdf , html Abstract : Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution. In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds -- an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout. We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 18% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with four real-world case studies: SQLite, Memcached, Apache, and Nginx. Bibtex @inproceedings { Kuvaiskii:2017:SMS:3064176.3064192 , author = {Kuvaiskii, Dmitrii and Oleksenko, Oleksii and Arnautov, Sergei and Trach, Bohdan and Bhatotia, Pramod and Felber, Pascal and Fetzer, Christof} , title = {SGXBOUNDS: Memory Safety for Shielded Execution} , booktitle = {Proceedings of the Twelfth European Conference on Computer Systems} , series = {EuroSys '17} , year = {2017} , isbn = {978-1-4503-4938-3} , location = {Belgrade, Serbia} , pages = {205--221} , numpages = {17} , url = {http://doi.acm.org/10.1145/3064176.3064192} , doi = {10.1145/3064176.3064192} , acmid = {3064192} , publisher = {ACM} , address = {New York, NY, USA} , }","title":"SGXBounds: Memory Safety for Shielded Execution, EuroSys 2017"},{"location":"SCONE_Publications/#ffq-a-fast-single-producermultiple-consumer-concurrent-fifo-queue-ipdps-2017","text":"This paper describes our new lock-free queue for our asynchronous system calls. Authors : Sergei Arnautov, Pascal Felber, Christof Fetzer and Bohdan Trach Media : pdf , html Abstract : With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi- producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we pro- vide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro- benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability. Bibtex @INPROCEEDINGS { 7967181 , author = {S. Arnautov and P. Felber and C. Fetzer and B. Trach} , booktitle = {2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)} , title = {FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue} , year = {2017} , volume = {} , number = {} , pages = {907-916} , keywords = {microprocessor chips;multi-threading;multiprocessing systems;operating systems (computers);queueing theory;FFQ;FIFO queue;consumer threads;fast single producer-multiple consumer concurrent FIFO queue;hardware threads;lock-free consumers;multicore architectures;multiple producer;operating systems;secure application;single producer variant;synchronization operations;Algorithm design and analysis;Context;Instruction sets;Message systems;Scalability;Synchronization;Throughput;FIFO queue;SPMC queue;concurrent FIFO queue;concurrent queue;lock-free algorithm;wait-free algorithm} , doi = {10.1109/IPDPS.2017.41} , ISSN = {} , month = {May} , }","title":"FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue, IPDPS 2017"},{"location":"SCONE_Publications/#pesos-policy-enhanced-secure-object-store-eurosys-2018","text":"Authors : Robert Krahn, Bohdan Trach (TU Dresden), Anjo Vahldiek-Oberwagner (MPI-SWS), Thomas Knauth (Intel/TU Dresden), Pramod Bhatotia (University of Edinburgh), and Christof Fetzer (TU Dresden) Media : pdf , html Abstract : Third-party storage services pose the risk of integrity and confidentiality violations as the current storage policy enforcement mechanisms are spread across many layers in the system stack. To mitigate these security vulnerabilities, we present the design and implementation of Pesos, a Policy Enhanced Secure Object Store (Pesos) for untrusted third-party storage providers. Pesos allows clients to specify per-object security policies, concisely and separately from the storage stack, and enforces these policies by securely mediating the I/O in the persistence layer through a single uni ed enforcement layer. More broadly, Pesos exposes a rich set of storage policies ensuring the integrity, confidentiality, and access accounting for data storage through a declarative policy language. Pesos enforces these policies on untrusted commodity plat- forms by leveraging a combination of two trusted computing technologies: Intel SGX for trusted execution environment (TEE) and Kinetic Open Storage for trusted storage. We have implemented Pesos as a fully-functional storage system supporting many useful end-to-end storage features, and a range of effective performance optimizations. We evaluated Pesos using a range of micro-benchmarks, and real-world use cases. Our evaluation shows that Pesos incurs reasonable performance overheads for the enforcement of policies while keeping the trusted computing base (TCB) small. Bibtex @inproceedings { Krahn:2018:PEP:3190508.3190518 , author = {Krahn, Robert and Trach, Bohdan and Vahldiek-Oberwagner, Anjo and Knauth, Thomas and Bhatotia, Pramod and Fetzer, Christof} , title = {Pesos: Policy Enhanced Secure Object Store} , booktitle = {Proceedings of the Thirteenth EuroSys Conference} , series = {EuroSys '18} , year = {2018} , isbn = {978-1-4503-5584-1} , location = {Porto, Portugal} , pages = {25:1--25:17} , articleno = {25} , numpages = {17} , url = {http://doi.acm.org/10.1145/3190508.3190518} , doi = {10.1145/3190508.3190518} , acmid = {3190518} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {intel SGX, kinetic disks, policy language, storage security} , }","title":"PESOS: Policy Enhanced Secure Object Store, EuroSys 2018"},{"location":"SCONE_Publications/#shieldbox-secure-middleboxes-using-shielded-execution-sigcomm-sosr-2018","text":"Authors : Bohdan Trach, Alfred Krohmer, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, Christof Fetzer Media : pdf , html , video Abstract : Middleboxes that process confidential data cannot be securely deployed in untrusted cloud environments. To securely outsource middleboxes to the cloud, state-of-the-art systems advocate network processing over the encrypted traffic. Unfortunately, these systems support only restrictive functionalities, and incur prohibitively high overheads.% due to the complex computations involved over the encrypted traffic. This motivated the design of ShieldBox---a secure middlebox framework for deploying high-performance network functions (NFs) over untrusted commodity servers. ShieldBox securely processes encrypted traffic inside a secure container by leveraging shielded execution. More specifically, ShieldBox builds on hardware-assisted memory protection based on Intel SGX to provide strong confidentiality and integrity guarantees. For middlebox developers, ShieldBox exposes a generic interface based on Click to design and implement a wide-range of NFs using its out-of-the-box elements and C++ extensions. For network operators, ShieldBox provides configuration and attestation service for seamless and verifiable deployment of middleboxes. We have implemented ShieldBox supporting important end-to-end features required for secure network processing, and performance optimizations. Our extensive evaluation shows that ShieldBox achieves a near-native throughput and latency to securely process confidential data at line rate. Bibtex @inproceedings { Trach:2018:SSM:3185467.3185469 , author = {Trach, Bohdan and Krohmer, Alfred and Gregor, Franz and Arnautov, Sergei and Bhatotia, Pramod and Fetzer, Christof} , title = {ShieldBox: Secure Middleboxes Using Shielded Execution} , booktitle = {Proceedings of the Symposium on SDN Research} , series = {SOSR '18} , year = {2018} , isbn = {978-1-4503-5664-0} , location = {Los Angeles, CA, USA} , pages = {2:1--2:14} , articleno = {2} , numpages = {14} , url = {http://doi.acm.org/10.1145/3185467.3185469} , doi = {10.1145/3185467.3185469} , acmid = {3185469} , publisher = {ACM} , address = {New York, NY, USA} , }","title":"ShieldBox: Secure Middleboxes using Shielded Execution, SIGCOMM SOSR 2018"},{"location":"SCONE_Publications/#varys-protecting-sgx-enclaves-from-practical-side-channel-attacks-usenix-atc-2018","text":"Authors : Oleksii Oleksenko, Bohdan Trach, Robert Krahn, and Andr\u00e9 Martin, TU Dresden; Mark Silberstein, Technion; Christof Fetzer, TU Dresden Media : pdf , slides , audio , url Abstract : Numerous recent works have experimentally shown that Intel Software Guard Extensions (SGX) are vulnerable to cache timing and page table side-channel attacks which could be used to circumvent the data confidentiality guarantees provided by SGX. Existing mechanisms that protect against these attacks either incur high execution costs, are ineffective against certain attack variants, or require significant code modifications. We present Varys, a system that protects unmodified programs running in SGX enclaves from cache timing and page table side-channel attacks. Varys takes a pragmatic approach of strict reservation of physical cores to security-sensitive threads, thereby preventing the attacker from accessing shared CPU resources during enclave execution. The key challenge that we are addressing is that of maintaining the core reservation in the presence of an untrusted OS. Varys fully protects against all L1/L2 cache timing attacks and significantly raises the bar for page table side-channel attacks - all with only 15% overhead on average for Phoenix and PARSEC benchmarks. Additionally, we propose a set of minor hardware extensions that hold the potential to extend Varys' security guarantees to L3 cache and further improve its performance. Bibtex @inproceedings { 216033 , author = {Oleksii Oleksenko and Bohdan Trach and Robert Krahn and Mark Silberstein and Christof Fetzer} , title = {Varys: Protecting {SGX} Enclaves from Practical Side-Channel Attacks} , booktitle = {2018 {USENIX} Annual Technical Conference ({USENIX} {ATC} 18)} , year = {2018} , isbn = {978-1-931971-44-7} , address = {Boston, MA} , pages = {227--240} , url = {https://www.usenix.org/conference/atc18/presentation/oleksenko} , publisher = {{USENIX} Association} , }","title":"Varys: Protecting SGX enclaves from practical side-channel attacks, USENIX ATC 2018"},{"location":"SCONE_Publications/#sgx-pyspark-secure-distributed-data-analytics-www2019","text":"Authors : Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. Media : https://doi.org/10.1145/3308558.3314129 Abstract : Data analytics is central to modern online services, particularly those data-driven. Often this entails the processing of large-scale datasets which may contain private, personal and sensitive information relating to individuals and organisations. Particular challenges arise where cloud is used to store and process the sensitive data. In such settings, security and privacy concerns become paramount, as the cloud provider is trusted to guarantee the security of the services they offer, including data confidentiality. Therefore, the issue this work tackles is \u201cHow to securely perform data analytics in a public cloud?\u201d To assist this question, we design and implement SGX-PySpark- a secure distributed data analytics system which relies on a trusted execution environment (TEE) such as Intel SGX to provide strong security guarantees. To build SGX-PySpark, we integrate PySpark - a widely used framework for data analytics in industry to support a wide range of queries, with SCONE - a shielded execution framework using Intel SGX. Bibtex @inproceedings { LeQuoc:2019:SSD:3308558.3314129 , author = {Le Quoc, Do and Gregor, Franz and Singh, Jatinder and Fetzer, Christof} , title = {SGX-PySpark: Secure Distributed Data Analytics} , booktitle = {The World Wide Web Conference} , series = {WWW '19} , year = {2019} , isbn = {978-1-4503-6674-8} , location = {San Francisco, CA, USA} , pages = {3564--3563} , numpages = {0} , url = {http://doi.acm.org/10.1145/3308558.3314129} , doi = {10.1145/3308558.3314129} , acmid = {3314129} , publisher = {ACM} , address = {New York, NY, USA} , keywords = {Confidential computing, data analytics, distributed system, security} , }","title":"SGX-PySpark: Secure Distributed Data Analytics, WWW2019"},{"location":"SCONE_Publications/#clemmys-towards-secure-remote-execution-in-faas-systor-19","text":"Authors : Bohdan Trach, Oleksii Oleksenko, Franz Gregor, Pramod Bhatotia, Christof Fetzer Abstract : We introduce Clemmys, a security-first serverless platform that ensures confidentiality and integrity of users' functions and data as they are processed on untrusted cloud premises, while keeping the cost of protection low. We provide a design for hardening FaaS platforms with Intel SGX---a hardware-based shielded execution technology. We explain the protocol that our system uses to ensure confidentiality and integrity of data, and integrity of function chains. To overcome performance and latency issues that are inherent in SGX applications, we apply several SGX-specific optimizations to the runtime system: we use SGXv2 to speed up the enclave startup and perform batch EPC augmentation. To evaluate our approach, we implement our design over Apache Open-Whisk, a popular serverless platform. Lastly, we show that Clemmys achieved same throughput and similar latency as native Apache OpenWhisk, while allowing it to withstand several new attack vectors. Conference : SYSTOR '19: Proceedings of the 12 th ACM International Conference on Systems and StorageMay 2019 Pages 44\u201354, https://doi.org/10.1145/3319647.3325835 Bibtext @inproceedings { 10.1145/3319647.3325835 , author = {Trach, Bohdan and Oleksenko, Oleksii and Gregor, Franz and Bhatotia, Pramod and Fetzer, Christof} , title = {Clemmys: Towards Secure Remote Execution in FaaS} , year = {2019} , isbn = {9781450367493} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3319647.3325835} , doi = {10.1145/3319647.3325835} , abstract = {We introduce Clemmys, a security-first serverless platform that ensures confidentiality and integrity of users' functions and data as they are processed on untrusted cloud premises, while keeping the cost of protection low. We provide a design for hardening FaaS platforms with Intel SGX---a hardware-based shielded execution technology. We explain the protocol that our system uses to ensure confidentiality and integrity of data, and integrity of function chains. To overcome performance and latency issues that are inherent in SGX applications, we apply several SGX-specific optimizations to the runtime system: we use SGXv2 to speed up the enclave startup and perform batch EPC augmentation. To evaluate our approach, we implement our design over Apache Open-Whisk, a popular serverless platform. Lastly, we show that Clemmys achieved same throughput and similar latency as native Apache OpenWhisk, while allowing it to withstand several new attack vectors.} , booktitle = {Proceedings of the 12th ACM International Conference on Systems and Storage} , pages = {44\u201354} , numpages = {11} , location = {Haifa, Israel} , series = {SYSTOR '19} }","title":"Clemmys: towards secure remote execution in FaaS, SYSTOR '19"},{"location":"SCONE_Publications/#trust-management-as-a-service-enabling-trusted-execution-in-the-face-of-byzantine-stakeholders-dsn2020","text":"Authors : Gregor, F., W. Ozga, S. Vaucher, R. Pires, D. Le Quoc, S. Arnautov, A. Martin, V. Schiavoni, P. Felber, and C. Fetzer Abstract : Trust is arguably the most important challenge for critical services both deployed as well as accessed remotely over the network. These systems are exposed to a wide diversity of threats, ranging from bugs to exploits, active attacks, rogue operators, or simply careless administrators. To protect such applications, one needs to guarantee that they are properly configured and securely provisioned with the \"secrets\" (e.g., encryption keys) necessary to preserve not only the confidentiality, integrity and freshness of their data but also their code. Furthermore, these secrets should not be kept under the control of a single stakeholder\u2014which might be compromised and would represent a single point of failure\u2014and they must be protected across software versions in the sense that attackers cannot get access to them via malicious updates. Traditional approaches for solving these challenges often use ad hoc techniques and ultimately rely on a hardware security module (HSM) as root of trust. We propose a more powerful and generic approach to trust management that instead relies on trusted execution environments (TEEs) and a set of stakeholders as root of trust. Our system, PAL\u00c6MON, can operate as a managed service deployed in an untrusted environment, i.e., one can delegate its operations to an untrusted cloud provider with the guarantee that data will remain confidential despite not trusting any individual human (even with root access) nor system software. PAL\u00c6MON addresses in a secure, efficient and cost-effective way five main challenges faced when developing trusted networked applications and services. Our evaluation on a range of benchmarks and real applications shows that PAL\u00c6MON performs efficiently and can protect secrets of services without any change to their source code. Media : https://ieeexplore.ieee.org/document/9153433 Conference : 50 th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2020) Bibtex @INPROCEEDINGS { 9153433 , author = {F. {Gregor} and W. {Ozga} and S. {Vaucher} and R. {Pires} and D. {Le Quoc} and S. {Arnautov} and A. {Martin} and V. {Schiavoni} and P. {Felber} and C. {Fetzer}} , booktitle = {2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)} , title = {Trust Management as a Service: Enabling Trusted Execution in the Face of Byzantine Stakeholders} , year = {2020} , volume = {} , number = {} , pages = {502-514} ,}","title":"Trust Management as a Service: Enabling Trusted Execution in the Face of Byzantine Stakeholders, DSN2020"},{"location":"SCONE_Publications/#t-lease-a-trusted-lease-primitive-for-distributed-systems-socc2020","text":"Authors : B. Trach, R. Faqeh, O. Oleksenko, W. Ozga, P. Bhatotia, C. Fetzer Abstract : A lease is an important primitive for building distributed protocols, and it is ubiquitously employed in distributed systems. However, the scope of the classic lease abstraction is restricted to the trusted computing infrastructure. Unfortunately, this important primitive cannot be employed in the untrusted computing infrastructure because the trusted execution environments (TEEs) do not provide a trusted time source. In the untrusted environment, an adversary can easily manipulate the system clock to violate the correctness properties of lease-based systems. We tackle this problem by introducing em trusted lease---a lease that maintains its correctness properties even in the presence of a clock-manipulating attacker. To achieve these properties, we follow a \"trust but verify\" approach for an untrusted timer, and transform it into a trusted timing primitive by leveraging two hardware-assisted ISA extensions (Intel TSX and SGX) available in commodity CPUs. We provide a design and implementation of trusted lease in a system called T-Lease---the first trusted lease system that achieves high security, performance, and precision. For the application developers, T-Lease exposes an easy-to-use generic APIs that facilitate its usage to build a wide range of distributed protocols. Conference : 2020 ACM Symposium on Cloud Computing Media : doi: 10.1145/3419111.3421273 Bibtext @inproceedings { 10.1145/3419111.3421273 , author = {Trach, Bohdan and Faqeh, Rasha and Oleksenko, Oleksii and Ozga, Wojciech and Bhatotia, Pramod and Fetzer, Christof} , title = {T-Lease: A Trusted Lease Primitive for Distributed Systems} , year = {2020} , isbn = {9781450381376} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3419111.3421273} , doi = {10.1145/3419111.3421273} , abstract = {A lease is an important primitive for building distributed protocols, and it is ubiquitously employed in distributed systems. However, the scope of the classic lease abstraction is restricted to the trusted computing infrastructure. Unfortunately, this important primitive cannot be employed in the untrusted computing infrastructure because the trusted execution environments (TEEs) do not provide a trusted time source. In the untrusted environment, an adversary can easily manipulate the system clock to violate the correctness properties of lease-based systems.We tackle this problem by introducing trusted lease---a lease that maintains its correctness properties even in the presence of a clock-manipulating attacker. To achieve these properties, we follow a \"trust but verify\" approach for an untrusted timer, and transform it into a trusted timing primitive by leveraging two hardware-assisted ISA extensions (Intel TSX and SGX) available in commodity CPUs. We provide a design and implementation of trusted lease in a system called T-Lease---the first trusted lease system that achieves high security, performance, and precision. For the application developers, T-Lease exposes an easy-to-use generic APIs that facilitate its usage to build a wide range of distributed protocols.} , booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing} , pages = {387\u2013400} , numpages = {14} , location = {Virtual Event, USA} , series = {SoCC '20} }","title":"T-Lease: A Trusted Lease Primitive for Distributed Systems, SOCC2020"},{"location":"SCONE_Publications/#a-practical-approach-for-updating-an-integrity-enforced-os-middleware-2020","text":"Authors : W. Ozga, D. Le Quoc, C. Fetzer Abstract : Trusted computing defines how to securely measure, store, and verify the integrity of software controlling a computer. One of the major challenge that makes them hard to be applied in practice is the issue of software updates. Specifically, an operating system update causes the integrity violation because it changes the well-known initial state trusted by remote verifiers, such as integrity monitoring systems. Consequently, the integrity monitoring of remote computers becomes unreliable due to the high amount of false positives. We address this problem by adding an extra level of indirection between the operating system and software repositories. We propose a trusted software repository (TSR), a secure proxy framework that overcomes the shortcomings of previous approaches by sanitizing software packages. Sanitization consists of modifying unsafe installation scripts and adding digital signatures in a way the software packages can be installed in the operating system without violating its integrity. TSR leverages shielded execution, i.e., Intel SGX, to achieve strong confidentiality and integrity guarantees of the sanitization process. Moreover, TSR is transparent to package managers, and requires no changes in the software packages building and distributing processes. Our evaluation shows that running TSR inside Intel SGX is practical; since it induces only \u223c 1.18\u00d7 performance overhead during package sanitization compared to the native execution without using Intel SGX. Also, TSR supports 99.76% of packages available in the main and community repositories of Alpine Linux while increasing the total repository size by 3.6%. Conference : ACM/IFIP Middleware 2020 Media : doi: 10.1145/3423211.3425674 Bibtext @inproceedings { 10.1145/3423211.3425674 , author = {Ozga, Wojciech and Quoc, Do Le and Fetzer, Christof} , title = {A Practical Approach for Updating an Integrity-Enforced Operating System} , year = {2020} , isbn = {9781450381536} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3423211.3425674} , doi = {10.1145/3423211.3425674} , abstract = {Trusted computing defines how to securely measure, store, and verify the integrity of software controlling a computer. One of the major challenge that make them hard to be applied in practice is the issue with software updates. Specifically, an operating system update causes the integrity violation because it changes the well-known initial state trusted by remote verifiers, such as integrity monitoring systems. Consequently, the integrity monitoring of remote computers becomes unreliable due to the high amount of false positives.We address this problem by adding an extra level of indirection between the operating system and software repositories. We propose a trusted software repository (TSR), a secure proxy that overcomes the shortcomings of previous approaches by sanitizing software packages. Sanitization consists of modifying unsafe installation scripts and adding digital signatures in a way software packages can be installed in the operating system without violating its integrity. TSR leverages shielded execution, i.e., Intel SGX, to achieve confidentiality and integrity guarantees of the sanitization process.TSR is transparent to package managers, and requires no changes in the software packages building and distributing processes. Our evaluation shows that running TSR inside SGX is practical; since it induces only ~ 1.18\\texttimes{} performance overhead during package sanitization compared to the native execution without SGX. TSR supports 99.76% of packages available in the main and community repositories of Alpine Linux while increasing the total repository size by 3.6%.} , booktitle = {Proceedings of the 21st International Middleware Conference} , pages = {311\u2013325} , numpages = {15} , keywords = {intel software guard extensions, integrity measurement architecture (IMA), trusted computing, software updates} , location = {Delft, Netherlands} , series = {Middleware '20} }","title":"A practical approach for updating an integrity-enforced OS, Middleware 2020"},{"location":"SCONE_Publications/#securetf-a-secure-tensorflow-framework-middleware-2020","text":"Authors : D. Le Quoc, F. Gregor, R. Kunkel, S. Arnautov, P. Bhatotia, C. Fetzer Abstract : Data-driven intelligent applications in modern online services have become ubiquitous. These applications are usually hosted in the untrusted cloud computing infrastructure. This poses significant security risks since these applications rely on applying machine learning algorithms on large datasets which may contain private and sensitive information. To tackle this challenge, we designed and implemented secureTF, a generic and secure machine learning framework based on Tensorflow. secureTF enables secure execution of unmodified TensorFlow applications on the untrusted cloud computing infrastructure. secureTF is built from ground-up based on the security properties provided by Trusted Execution Environments (TEEs). However, it extends the trust of a volatile memory region (or secure enclave) provided by the single node TEE to secure a distributed computing infrastructure required for supporting unmodified stateful machine learning applications running in the cloud. The paper reports on our experiences about the system design choices and the system deployment in production use-cases. We conclude with the lessons learned based on the limitations of our commercially available platform and discuss open research problems for the future work. Conference : ACM/IFIP Middleware 2020 Media : doi: 10.1145/3423211.3425687 Bibtext @inproceedings { 10.1145/3423211.3425687 , author = {Quoc, Do Le and Gregor, Franz and Arnautov, Sergei and Kunkel, Roland and Bhatotia, Pramod and Fetzer, Christof} , title = {SecureTF: A Secure TensorFlow Framework} , year = {2020} , isbn = {9781450381536} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3423211.3425687} , doi = {10.1145/3423211.3425687} , abstract = {Data-driven intelligent applications in modern online services have become ubiquitous. These applications are usually hosted in the untrusted cloud computing infrastructure. This poses significant security risks since these applications rely on applying machine learning algorithms on large datasets which may contain private and sensitive information.To tackle this challenge, we designed secureTF, a distributed secure machine learning framework based on Tensorflow for the untrusted cloud infrastructure. secureTF is a generic platform to support unmodified TensorFlow applications, while providing end-to-end security for the input data, ML model, and application code. secureTF is built from ground-up based on the security properties provided by Trusted Execution Environments (TEEs). However, it extends the trust of a volatile memory region (or secure enclave) provided by the single node TEE to secure a distributed infrastructure required for supporting unmodified stateful machine learning applications running in the cloud.The paper reports on our experiences about the system design choices and the system deployment in production use-cases. We conclude with the lessons learned based on the limitations of our commercially available platform, and discuss open research problems for the future work.} , booktitle = {Proceedings of the 21st International Middleware Conference} , pages = {44\u201359} , numpages = {16} , keywords = {tensorflow, intel software guard extensions (Intel SGX), confidential computing, secure machine learning} , location = {Delft, Netherlands} , series = {Middleware '20} }","title":"secureTF: A Secure TensorFlow Framework, Middleware 2020"},{"location":"SCONE_Publications/#teemon-a-continuous-performance-monitoring-framework-for-tees-middleware-2020","text":"Authors : R. Krahn, D. Le Quoc, D. Dragoti, F. Gregor, V. Schiavoni, C. Souza, P. Felber, A. Brito, C. Fetzer Abstract : Trusted Execution Environments (TEEs), such as Intel Software Guard eXtensions (SGX), are considered as a promising approach to resolve security challenges in clouds. TEEs protect the confidentiality and integrity of application code and data even against privileged attackers with root and physical access by providing an isolated secure memory area, i.e., enclaves. The security guarantees are provided by the CPU, thus even if system software is compromised, the attacker can never access the enclave\u2019s content. While this approach ensures strong security guarantees for applications, it also introduces a considerable runtime overhead in part by the limited availability of protected memory (enclave page cache). Currently, only a limited number of performance measurement tools for TEE-based applications exist and none offer performance monitoring and analysis during runtime. This paper presents TEEMon, the first continuous performance monitoring and analysis tool for TEE-based applications. TEEMon provides not only fine-grained performance metrics during runtime, but also assists the analysis of identifying causes of performance bottlenecks, e.g., excessive system calls. Our approach smoothly integrates with existing open-source tools (e.g., Prometheus or Grafana) towards a holistic monitoring solution, particularly optimized for systems deployed through Docker containers or Kubernetes and offers several dedicated metrics and visualizations. Our evaluation shows that TEEMon\u2019s overhead ranges from 5% to 17%. Conference : ACM/IFIP Middleware 2020 Media : doi: 10.1145/3423211.3425677 Bibtext @inproceedings { 10.1145/3423211.3425677 , author = {Krahn, Robert and Dragoti, Donald and Gregor, Franz and Quoc, Do Le and Schiavoni, Valerio and Felber, Pascal and Souza, Clenimar and Brito, Andrey and Fetzer, Christof} , title = {TEEMon: A Continuous Performance Monitoring Framework for TEEs} , year = {2020} , isbn = {9781450381536} , publisher = {Association for Computing Machinery} , address = {New York, NY, USA} , url = {https://doi.org/10.1145/3423211.3425677} , doi = {10.1145/3423211.3425677} , abstract = {Trusted Execution Environments (TEEs), such as Intel Software Guard eXtensions (SGX), are considered as a promising approach to resolve security challenges in clouds. TEEs protect the confidentiality and integrity of application code and data even against privileged attackers with root and physical access by providing an isolated secure memory area, i.e., enclaves. The security guarantees are provided by the CPU, thus even if system software is compromised, the attacker can never access the enclave's content. While this approach ensures strong security guarantees for applications, it also introduces a considerable runtime overhead in part by the limited availability of protected memory (enclave page cache). Currently, only a limited number of performance measurement tools for TEE-based applications exist and none offer performance monitoring and analysis during runtime.This paper presents TEEMon, the first continuous performance monitoring and analysis tool for TEE-based applications. TEEMon provides not only fine-grained performance metrics during runtime, but also assists the analysis of identifying causes of performance bottlenecks, e.g., excessive system calls. Our approach smoothly integrates with existing open-source tools (e.g., Prometheus or Grafana) towards a holistic monitoring solution, particularly optimized for systems deployed through Docker containers or Kubernetes and offers several dedicated metrics and visualizations. Our evaluation shows that TEEMon's overhead ranges from 5% to 17%.} , booktitle = {Proceedings of the 21st International Middleware Conference} , pages = {178\u2013192} , numpages = {15} , keywords = {Computerized monitoring, Performance monitoring, Trusted Execution Environments} , location = {Delft, Netherlands} , series = {Middleware '20} }","title":"TEEMon: A continuous performance monitoring framework for TEEs, Middleware 2020"},{"location":"SCONE_Publications/#towards-formalization-of-enhanced-privacy-id-epid-based-remote-attestation-in-intel-sgx-dsd-2020","text":"Authors : Sardar, Muhammad Usama and Quoc, Do Le and Fetzer, Christof Conference : 2020 23 rd Euromicro Conference on Digital System Design (DSD) Bibtext @inproceedings { Sardar2020EPID , author = {Sardar, Muhammad Usama and Quoc, Do Le and Fetzer, Christof} , booktitle = {2020 23rd Euromicro Conference on Digital System Design (DSD)} , doi = {10.1109/DSD51259.2020.00099} , isbn = {978-1-7281-9535-3} , month = {aug} , pages = {604--607} , publisher = {IEEE} , title = {{Towards Formalization of Enhanced Privacy ID (EPID)-based Remote Attestation in Intel SGX}} , url = {https://ieeexplore.ieee.org/document/9217791/} , year = {2020} }","title":"Towards Formalization of Enhanced Privacy ID (EPID)-based Remote Attestation in Intel SGX, DSD 2020"},{"location":"SCONE_Publications/#formal-foundations-for-intel-sgx-data-center-attestation-primitives-formal-methods-and-software-engineering-2020","text":"Authors : Sardar, Muhammad Usama and Faqeh, Rasha and Fetzer, Christof Conference : Formal Methods and Software Engineering, 2020 Bibtext @incollection { Sardar2020DCAP , address = {Cham} , author = {Sardar, Muhammad Usama and Faqeh, Rasha and Fetzer, Christof} , booktitle = {Formal Methods and Software Engineering} , doi = {10.1007/978-3-030-63406-3_16} , editor = {Lin, Shang-Wei and Hou, Zhe and Mahoney, Brendan} , isbn = {978-3-030-63405-6} , keywords = {Data center attestation primitives,Data centers,Formal verification,Intel SGX,Remote attestation,Trusted execution environment,data center attestation,data centers,formal verification,intel sgx,remote attestation,trusted execution environment} , pages = {268--283} , publisher = {Springer International Publishing} , series = {Lecture Notes in Computer Science} , title = {{Formal Foundations for Intel SGX Data Center Attestation Primitives}} , url = {http://link.springer.com/10.1007/978-3-030-63406-3{\\_}16} , volume = {12531} , year = {2020} }","title":"Formal Foundations for Intel SGX Data Center Attestation Primitives, Formal Methods and Software Engineering, 2020"},{"location":"SCONE_Publications/#specfuzz-bringing-spectre-type-vulnerabilities-to-the-surface-usenix-security-2020","text":"Authors : Oleksii Oleksenko, Bohdan Trach, Mark Silberstein, Christof Fetzer Abstract : SpecFuzz is the first tool that enables dynamic testing for speculative execution vulnerabilities (e.g., Spectre). The key is a novel concept of speculation exposure: The program is instrumented to simulate speculative execution in software by forcefully executing the code paths that could be triggered due to mispredictions, thereby making the speculative memory accesses visible to integrity checkers (e.g., AddressSanitizer). Combined with the conventional fuzzing techniques, speculation exposure enables more precise identification of potential vulnerabilities compared to state-of-the-art static analyzers. Our prototype for detecting Spectre V1 vulnerabilities successfully identifies all known variations of Spectre V1 and decreases the mitigation overheads across the evaluated applications, reducing the amount of instrumented branches by up to 77% given a sufficient test coverage. Media : html , paper , slides , video Conference : Usenix Security 2020 Bibtext @inproceedings { 251530 , author = {Oleksii Oleksenko and Bohdan Trach and Mark Silberstein and Christof Fetzer} , title = {SpecFuzz: Bringing Spectre-type vulnerabilities to the surface} , booktitle = {29th {USENIX} Security Symposium ({USENIX} Security 20)} , year = {2020} , isbn = {978-1-939133-17-5} , pages = {1481--1498} , url = {https://www.usenix.org/conference/usenixsecurity20/presentation/oleksenko} , publisher = {{USENIX} Association} , month = aug , }","title":"SpecFuzz: Bringing Spectre-type vulnerabilities to the surface, Usenix Security 2020"},{"location":"SCONE_Publications/#sgxtuner-performance-enhancement-of-intel-sgx-applications-via-stochastic-optimization-ieee-transactions-on-dependable-and-secure-computing-2021","text":"Authors : G. Mazzeo, S. Arnautov, C. Fetzer, and L. Romano Abstract : Intel SGX has started to be widely adopted. Cloud providers (Microsoft Azure, IBM Cloud, Alibaba Cloud) are offering new solutions, implementing data-in-use protection via SGX. A major challenge faced by both academia and industry is providing transparent SGX support to legacy applications. The approach with the highest consensus is linking the target software with SGX-extended libc libraries. Unfortunately, the increased security entails a dramatic performance penalty, which is mainly due to the intrinsic overhead of context switches, and the limited size of protected memory. Performance optimization is non-trivial since it depends on key parameters whose manual tuning is a very long process. We present the architecture of an automated tool, called SGXTuner, which is able to find the best setting of SGX-extended libc library parameters, by iteratively adjusting such parameters based on continuous monitoring of performance data. The tool is to a large extent algorithm agnostic. We decided to base the current implementation on a particular type of stochastic optimization algorithm, specifically Simulated Annealing. A massive experimental campaign was conducted on a relevant case study. Three client-server applications Memcached, Redis, and Apache were compiled with SCONE's sgx-musl and tuned for best performance. Results demonstrate the effectiveness of SGXTuner. Media : doi: 10.1109/TDSC.2021.3064391 Bibtext @ARTICLE { 9372883 , author = {G. {Mazzeo} and S. {Arnautov} and C. {Fetzer} and L. {Romano}} , journal = {IEEE Transactions on Dependable and Secure Computing} , title = {SGXTuner: Performance Enhancement of Intel SGX Applications via Stochastic Optimization} , year = {2021} , volume = {} , number = {} , pages = {1-1} , doi = {10.1109/TDSC.2021.3064391} }","title":"SGXTuner: Performance Enhancement of Intel SGX Applications via Stochastic Optimization, IEEE Transactions on Dependable and Secure Computing, 2021"},{"location":"SCONE_TUTORIAL/","text":"SCONE Hello World Install sgxmusl cross compiler image Ensure that you installed the various CAS_session_lang_0_3/crosscompilers container image: > docker image ls CAS_session_lang_0_3/* REPOSITORY TAG IMAGE ID CREATED SIZE CAS_session_lang_0_3/crosscompilers latest dff7975b7f32 7 hours ago 1 .57GB CAS_session_lang_0_3/crosscompilers scone dff7975b7f32 7 hours ago 1 .57GB If the cross compiler image is not yet installed, read Section SCONE Curated Container Images to learn how to install the SCONE cross compiler image. If the docker command fails, please ensure that docker is indeed installed ( docker installation ). Also, on some systems you might need to use sudo to run docker commands 1 . Install the tutorial Clone the tutorial: > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git Native Hello World Ensure that hello world runs natively on your machine: > cd SCONE_TUTORIAL/HelloWorld/ > gcc hello_world.c -o native_hello_world > ./native_hello_world Hello World Note that the generated executable, i.e., sim_hello_world , will only run on Linux. Statically-Linked Hello World The default cross compiler variant that runs hello world inside of an enclave is scone gcc and you can find this in container CAS_session_lang_0_3/crosscompilers . This variant requires access to the SGX device. We determine which SGX device to mount with function determine_sgx_device . > docker run --rm $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/crosscompilers scone-gcc hello_world.c -o sgx_hello_world This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically linked binary since it is wrapped in a dynamically linked loader program: > ldd ./sgx_hello_world linux-vdso.so.1 = > ( 0x00007ffcf73ad000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007f7c2a0e9000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007f7c29d1f000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007f7c2a306000 ) Ensure that file /etc/sgx-musl.conf exists. If not, store some default file like: > printf \"Q 1\\ne 0 0 0\\ns 1 0 0\\n\" | sudo tee /etc/sgx-musl.conf To run sgx_hello_world , in an enclave, just execute: > ./sgx_hello_world Hello World To see some more debug messages, set environment variable SCONE_VERSION=1 : > SCONE_VERSION = 1 ./sgx_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_ALLOW_DLOPEN = no Revision: 9b355b99170ad434010353bb9f4dca24e532b1b7 Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Hello World The debug outputs SCONE_MODE=hw shows that sgx_hello_world runs in hardware mode, i.e., inside an SGX enclave. Note. The compilation as well as the hello world program will fail in case you do not have an SGX driver installed. Dynamically-Linked Hello World > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/muslgcc gcc hello_world.c -o dyn_hello_world To run this natively, just execute the following: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/muslgcc ./dyn_hello_world To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable SCONE_VERSION=1 : Hardware Mode vs Simulation Mode For debugging, we support three different modes for execution: hardware, simulation, and automatic : hardware : by setting environment variable to SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave. simulation : by setting environment variable to SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable. automatic : by setting environment variable to SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode) NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave. Execution on a SGX-capable machine > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello World Execution on a non-SGX machine If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime /usr/src/myapp/dyn_hello_world [ Error ] Could not create enclave: Error opening SGX device You could run this in simulation mode as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = SIM -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 Hello World Alternatively, you could run this program in automatic mode: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = AUTO -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 HelloWorld Run STRACE Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows: > docker run --cap-add SYS_PTRACE -it --rm $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/crosscompilers strace -f /usr/src/myapp/sgx_hello_world > strace.log Hello World head strace.log execve ( \"/usr/src/myapp/sgx_hello_world\" , [ \"/usr/src/myapp/sgx_hello_world\" ] , [ /* 10 vars */ ]) = 0 brk ( NULL ) = 0x10e8000 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory ) mmap ( NULL, 8192 , PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 ) = 0x7f17f07f1000 access ( \"/etc/ld.so.preload\" , R_OK ) = -1 ENOENT ( No such file or directory ) open ( \"/etc/ld.so.cache\" , O_RDONLY | O_CLOEXEC ) = 3 fstat ( 3 , { st_mode = S_IFREG | 0644 , st_size = 18506 , ... }) = 0 mmap ( NULL, 18506 , PROT_READ, MAP_PRIVATE, 3 , 0 ) = 0x7f17f07ec000 close ( 3 ) = 0 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory ) Screencast Follow the steps described in https://docs.docker.com/install/linux/linux-postinstall/ on how to avoid using sudo to run docker . \u21a9","title":"SCONE Static vs Dynamic"},{"location":"SCONE_TUTORIAL/#scone-hello-world","text":"","title":"SCONE Hello World"},{"location":"SCONE_TUTORIAL/#install-sgxmusl-cross-compiler-image","text":"Ensure that you installed the various CAS_session_lang_0_3/crosscompilers container image: > docker image ls CAS_session_lang_0_3/* REPOSITORY TAG IMAGE ID CREATED SIZE CAS_session_lang_0_3/crosscompilers latest dff7975b7f32 7 hours ago 1 .57GB CAS_session_lang_0_3/crosscompilers scone dff7975b7f32 7 hours ago 1 .57GB If the cross compiler image is not yet installed, read Section SCONE Curated Container Images to learn how to install the SCONE cross compiler image. If the docker command fails, please ensure that docker is indeed installed ( docker installation ). Also, on some systems you might need to use sudo to run docker commands 1 .","title":"Install sgxmusl cross compiler image"},{"location":"SCONE_TUTORIAL/#install-the-tutorial","text":"Clone the tutorial: > git clone https://github.com/christoffetzer/SCONE_TUTORIAL.git","title":"Install the tutorial"},{"location":"SCONE_TUTORIAL/#native-hello-world","text":"Ensure that hello world runs natively on your machine: > cd SCONE_TUTORIAL/HelloWorld/ > gcc hello_world.c -o native_hello_world > ./native_hello_world Hello World Note that the generated executable, i.e., sim_hello_world , will only run on Linux.","title":"Native Hello World"},{"location":"SCONE_TUTORIAL/#statically-linked-hello-world","text":"The default cross compiler variant that runs hello world inside of an enclave is scone gcc and you can find this in container CAS_session_lang_0_3/crosscompilers . This variant requires access to the SGX device. We determine which SGX device to mount with function determine_sgx_device . > docker run --rm $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/crosscompilers scone-gcc hello_world.c -o sgx_hello_world This generates a statically linked binary. However, as we mentioned above, the binary looks like a dynamically linked binary since it is wrapped in a dynamically linked loader program: > ldd ./sgx_hello_world linux-vdso.so.1 = > ( 0x00007ffcf73ad000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007f7c2a0e9000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007f7c29d1f000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007f7c2a306000 ) Ensure that file /etc/sgx-musl.conf exists. If not, store some default file like: > printf \"Q 1\\ne 0 0 0\\ns 1 0 0\\n\" | sudo tee /etc/sgx-musl.conf To run sgx_hello_world , in an enclave, just execute: > ./sgx_hello_world Hello World To see some more debug messages, set environment variable SCONE_VERSION=1 : > SCONE_VERSION = 1 ./sgx_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw export SCONE_SGXBOUNDS = no export SCONE_ALLOW_DLOPEN = no Revision: 9b355b99170ad434010353bb9f4dca24e532b1b7 Branch: master Configure options: --enable-file-prot --enable-shared --enable-debug --prefix = /scone/src/built/cross-compiler/x86_64-linux-musl Hello World The debug outputs SCONE_MODE=hw shows that sgx_hello_world runs in hardware mode, i.e., inside an SGX enclave. Note. The compilation as well as the hello world program will fail in case you do not have an SGX driver installed.","title":"Statically-Linked Hello World"},{"location":"SCONE_TUTORIAL/#dynamically-linked-hello-world","text":"> docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/muslgcc gcc hello_world.c -o dyn_hello_world To run this natively, just execute the following: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/muslgcc ./dyn_hello_world To run a dynamically-linked binary in an enclave, you need to run this in a special runtime environment. In this environment you can ask binaries to run inside of enclaves by setting environment SCONE_ALPINE=1 . To indicate that we are indeed running inside an enclave, we ask to issue some debug messages from inside the enclave by setting environment variable SCONE_VERSION=1 :","title":"Dynamically-Linked Hello World"},{"location":"SCONE_TUTORIAL/#hardware-mode-vs-simulation-mode","text":"For debugging, we support three different modes for execution: hardware, simulation, and automatic : hardware : by setting environment variable to SCONE_MODE=HW , SCONE will enforce running this application inside an SGX enclave. simulation : by setting environment variable to SCONE_MODE=SIM , SCONE will enforce running this application in native mode (i.e., outside of an enclave). This will run all SCONE functionality but outside enclaves. This is intended for development and debugging on machines that are not SGX-capable. automatic : by setting environment variable to SCONE_MODE=AUTO , SCONE will run the application inside of an SGX enclave if available and otherwise in simulation mode. (This is the default mode) NOTE : In production mode, you must only permit running in hardware mode. Scone ensures this with the help of remote attestation : the SCONE configuration and attestation service (CAS) will only provide configuration information and secrets to an application only after it has proven (with the help of SGX CPU extensions) that it is indeed running inside an SGX enclave.","title":"Hardware Mode vs Simulation Mode"},{"location":"SCONE_TUTORIAL/#execution-on-a-sgx-capable-machine","text":"> docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = hw Configure parameters: 1 .1.15 Hello World","title":"Execution on a SGX-capable machine"},{"location":"SCONE_TUTORIAL/#execution-on-a-non-sgx-machine","text":"If you run this inside a container without access to SGX (/dev/isgx), for example, when running on a Mac, you will see the following error message: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = HW -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime /usr/src/myapp/dyn_hello_world [ Error ] Could not create enclave: Error opening SGX device You could run this in simulation mode as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = SIM -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime /usr/src/myapp/dyn_hello_world export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 Hello World Alternatively, you could run this program in automatic mode: > docker run --rm -v \" $PWD \" :/usr/src/myapp -e SCONE_MODE = AUTO -e SCONE_ALPINE = 1 -e SCONE_VERSION = 1 CAS_session_lang_0_3/crosscompilers:runtime export SCONE_QUEUES = 4 export SCONE_SLOTS = 256 export SCONE_SIGPIPE = 0 export SCONE_MMAP32BIT = 0 export SCONE_SSPINS = 100 export SCONE_SSLEEP = 4000 export SCONE_KERNEL = 0 export SCONE_HEAP = 67108864 export SCONE_CONFIG = /etc/sgx-musl.conf export SCONE_MODE = sim Configure parameters: 1 .1.15 HelloWorld","title":"Execution on a non-SGX machine"},{"location":"SCONE_TUTORIAL/#run-strace","text":"Lets see how we can trace the program. Say, you have compile the program as shown above. After that you enter a cross compiler container and strace hello world as follows: > docker run --cap-add SYS_PTRACE -it --rm $MOUNT_SGXDEVICE -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp CAS_session_lang_0_3/crosscompilers strace -f /usr/src/myapp/sgx_hello_world > strace.log Hello World head strace.log execve ( \"/usr/src/myapp/sgx_hello_world\" , [ \"/usr/src/myapp/sgx_hello_world\" ] , [ /* 10 vars */ ]) = 0 brk ( NULL ) = 0x10e8000 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory ) mmap ( NULL, 8192 , PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 ) = 0x7f17f07f1000 access ( \"/etc/ld.so.preload\" , R_OK ) = -1 ENOENT ( No such file or directory ) open ( \"/etc/ld.so.cache\" , O_RDONLY | O_CLOEXEC ) = 3 fstat ( 3 , { st_mode = S_IFREG | 0644 , st_size = 18506 , ... }) = 0 mmap ( NULL, 18506 , PROT_READ, MAP_PRIVATE, 3 , 0 ) = 0x7f17f07ec000 close ( 3 ) = 0 access ( \"/etc/ld.so.nohwcap\" , F_OK ) = -1 ENOENT ( No such file or directory )","title":"Run STRACE"},{"location":"SCONE_TUTORIAL/#screencast","text":"Follow the steps described in https://docs.docker.com/install/linux/linux-postinstall/ on how to avoid using sudo to run docker . \u21a9","title":"Screencast"},{"location":"SCONE_toolchain/","text":"SCONE SGX Toolchain SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran. The objective of these (cross-)compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves. To simplify the use of these (cross-)compilers, SCONE maintains curated container image that includes these cross-compilers. Compiler variants Depending on if you want to generate a dynamically-linked or a statically-linked binary, you can use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. Portability of SCONE programs Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code base, i.e., it includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we recommend both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs one does not change the build process when moving to SCONE. Loading Shared Libraries after startup SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java and Python. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2). For production enclaves, you will need to protect the integrity of the shared libraries with the help of the SCONE file shield . Dynamically-Linked Binaries The easiest to get started is to compile your programs such that the generated code is position independent ( -fPIC ), the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ), your binary is dynamically linked (i.e., do not use -static ), and link against musl as your libc (i.e., not glibc or any other libc). When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the file system shield . To simplify the compiling of your programs for scone, we make available a docker image registry.scontain.com:5050/sconecuratedimages/muslgcc which includes gcc and g++ support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options. Statically-Linked Binaries For statically linked binaries, we make available a (private) docker image registry.scontain.com:5050/sconecuratedimages/crosscompilers which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves. Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program web-srv-go , you will still see dynamic dependencies: $ ldd web-srv-go linux-vdso.so.1 = > ( 0x00007ffe423fd000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007effa344f000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007effa3085000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007effa366c000 ) The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked loader program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked. Using the cross compiler container How to use the compiler: use this as a base image and build your programs inside of a container we a Dockerfile ), or map volumes such that the compiler can compile files living outside the container (see SCONE Tutorial ). For an example how to use the crosscompilers, see how to compile a programs written in GO . Example Note on some systems you will need to run docker with root permissions, i.e., in this case you should prefix a > docker ... command with sudo , i.e., you execute > sudo docker ... One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is in file myapp.c in your current directory ( $PWD ). You can compile this code as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/muslgcc gcc myapp.c This call will generate a binary a.out in your working directory. This binary is dynamically linked against musl: > ldd a.out /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) libc.musl-x86_64.so.1 = > /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/muslgcc ./a.out To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our hello world in Section SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of Dockerfiles . In most cases, you might just set to use one of our crosscompilers in your configure script or Makefile . A simple way is to use the Docker image registry.scontain.com:5050/sconecuratedimages/crosscompilers as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of cargo which is scone-cargo . Debugger support We also support gdb to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image registry.scontain.com:5050/sconecuratedimages/crosscompilers as scone-gdb . For example on how to use the debugger, see how to debug a program written in GO . Also, we support an IDE to debug a program running in an enclave in debug mode. We describe how to integrate with Eclipse .","title":"SCONE SGX toolchain"},{"location":"SCONE_toolchain/#scone-sgx-toolchain","text":"SCONE comes with compiler support for popular languages: C, C++, GO, Rust as well as Fortran. The objective of these (cross-)compilers are to compile applications - generally without source code changes - such that they can run inside of SGX enclaves. To simplify the use of these (cross-)compilers, SCONE maintains curated container image that includes these cross-compilers.","title":"SCONE SGX Toolchain"},{"location":"SCONE_toolchain/#compiler-variants","text":"Depending on if you want to generate a dynamically-linked or a statically-linked binary, you can use a standard compiler (dynamic) or you need to use a cross compiler (static). The compiler can run on any system, i.e., does not require SGX to run. Portability of SCONE programs Independently, if you use a dynamic or static linking, the hash of an enclave (MRENCLAVE) will encompass the whole code base, i.e., it includes all libraries. Any updates of a library on your host might prevent the execution of a SCONE binary because of a wrong MRENCLAVE. Hence, we recommend to use only statically-linked programs on the host. In containers, which have a more controlled environment, we recommend both statically as well as dynamically linked binaries. The main advantage of dynamic linking is that for many programs one does not change the build process when moving to SCONE. Loading Shared Libraries after startup SCONE supports the loading of dynamic libraries after a program has already started inside of an enclave. This feature is required by modern languages like Java and Python. Enabling general loading of dynamic library introduces the risk that one could load malicious code inside of an enclave. Hence, we switch this feature off by default. For debugging programs, you can enable this feature via an environment variable ( export SCONE_ALLOW_DLOPEN=2). For production enclaves, you will need to protect the integrity of the shared libraries with the help of the SCONE file shield .","title":"Compiler variants"},{"location":"SCONE_toolchain/#dynamically-linked-binaries","text":"The easiest to get started is to compile your programs such that the generated code is position independent ( -fPIC ), the thread local storage model is global-dynamic ( -ftls-model=global-dynamic ), your binary is dynamically linked (i.e., do not use -static ), and link against musl as your libc (i.e., not glibc or any other libc). When a program is started, SCONE uses its own dynamic link loader to replace libc by a SCONE libc. The SCONE dynamic linker will load the program inside a new SGX enclave and SCONE libc will enable programs to run inside the SGX enclaves, e.g., execute system calls and protect them from attacks via shields like the file system shield . To simplify the compiling of your programs for scone, we make available a docker image registry.scontain.com:5050/sconecuratedimages/muslgcc which includes gcc and g++ support. The options will by default be set as shown above. You need, however, to make sure that your Makefiles will not overwrite these options.","title":"Dynamically-Linked Binaries"},{"location":"SCONE_toolchain/#statically-linked-binaries","text":"For statically linked binaries, we make available a (private) docker image registry.scontain.com:5050/sconecuratedimages/crosscompilers which can produce statically linked binaries. In the statically linked binaries, we replace the interface to the operating system (i.e., libc) by a variant that enables programs to run inside Intel SGX enclaves. Note that a statically linked binaries might look like a dynamically-linked binary. For example, if you look at a statically-linked program web-srv-go , you will still see dynamic dependencies: $ ldd web-srv-go linux-vdso.so.1 = > ( 0x00007ffe423fd000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007effa344f000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007effa3085000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007effa366c000 ) The reason for that is that the statically linked binary that runs inside of an enclave is wrapped in a dynamically linked loader program. The loader program creates the enclave, moves the program code inside the enclave and starts threads that will enter the enclave. The code that is moved inside the enclave is, however, statically linked.","title":"Statically-Linked Binaries"},{"location":"SCONE_toolchain/#using-the-cross-compiler-container","text":"How to use the compiler: use this as a base image and build your programs inside of a container we a Dockerfile ), or map volumes such that the compiler can compile files living outside the container (see SCONE Tutorial ). For an example how to use the crosscompilers, see how to compile a programs written in GO .","title":"Using the cross compiler container"},{"location":"SCONE_toolchain/#example","text":"Note on some systems you will need to run docker with root permissions, i.e., in this case you should prefix a > docker ... command with sudo , i.e., you execute > sudo docker ... One can run the above compiler inside of a container while the compiled files reside outside the container. Say, your code is in file myapp.c in your current directory ( $PWD ). You can compile this code as follows: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/muslgcc gcc myapp.c This call will generate a binary a.out in your working directory. This binary is dynamically linked against musl: > ldd a.out /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) libc.musl-x86_64.so.1 = > /lib/ld-musl-x86_64.so.1 ( 0x7fb0379f9000 ) This binary can run natively only if you have musl installed at the correct position in your development machine (and your development machine runs Linux). Alternatively, you can run the binary in a container: > docker run --rm -v \" $PWD \" :/usr/src/myapp -w /usr/src/myapp registry.scontain.com:5050/sconecuratedimages/muslgcc ./a.out To run this inside of SGX enclaves with SCONE, you need access to the SCONE runtime systems. For more details, see our hello world in Section SCONE Tutorial . This is not very convenient and hence, we provide a) a simpler version with the help of Dockerfiles . In most cases, you might just set to use one of our crosscompilers in your configure script or Makefile . A simple way is to use the Docker image registry.scontain.com:5050/sconecuratedimages/crosscompilers as a base image and then clone your code inside the container and set one or more of our compilers ( scone-gcc, scone-g++, scone-gccgo, scone-gfortran, and scone-rustc ) to be used in your build. For Rust, we support also our variant of cargo which is scone-cargo .","title":"Example"},{"location":"SCONE_toolchain/#debugger-support","text":"We also support gdb to debug applications running inside of enclaves. To get started, we recommend that you first ensure that your program runs natively linked against musl. Most programs will do - after all, the Alpine Linux distribution is completely based on musl. The debugger is available in image registry.scontain.com:5050/sconecuratedimages/crosscompilers as scone-gdb . For example on how to use the debugger, see how to debug a program written in GO . Also, we support an IDE to debug a program running in an enclave in debug mode. We describe how to integrate with Eclipse .","title":"Debugger support"},{"location":"aboutScone/","text":"About SCONE The SCONE platform is commercially supported by scontain.com . The SCONE platform has been developed at the Systems Engineering group at TU Dresden in the context of the following EU H2020 projects : Sereca which investigates how to use Intel SGX enclave in the context of reactive programs written in Vert.x . Secure Cloud which focuses on the processing of big data in untrusted clouds. The first paper about SCONE has been published in OSDI 2016 with our colleagues from Imperial College London, Technische Universit\u00e4t Braunschweig and University of Otago. More papers about SCONE can be found here . We investigate use cases and extensions of SCONE in the context of the following EU H2020 projects : SELIS : we investigate how to secure data processing within a Shared European Logistics Intelligent Information Space with the help of SCONE. ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform. LEGATO : a new EU project in which we address high integrity computations inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves. Computing Resources scontain.com can provide access to SGX-capable machines. Consulting Services scontain.com provides consulting services as well as helping you to port your applications to SGX. Contact If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at info@scontain.com .","title":"About"},{"location":"aboutScone/#about-scone","text":"The SCONE platform is commercially supported by scontain.com . The SCONE platform has been developed at the Systems Engineering group at TU Dresden in the context of the following EU H2020 projects : Sereca which investigates how to use Intel SGX enclave in the context of reactive programs written in Vert.x . Secure Cloud which focuses on the processing of big data in untrusted clouds. The first paper about SCONE has been published in OSDI 2016 with our colleagues from Imperial College London, Technische Universit\u00e4t Braunschweig and University of Otago. More papers about SCONE can be found here . We investigate use cases and extensions of SCONE in the context of the following EU H2020 projects : SELIS : we investigate how to secure data processing within a Shared European Logistics Intelligent Information Space with the help of SCONE. ATMOSPHERE : a new EU project in which we address secure data management services. This will help to extend the SCONE platform. LEGATO : a new EU project in which we address high integrity computations inside of enclaves to be able to detect and tolerate miscomputations inside of enclaves.","title":"About SCONE"},{"location":"aboutScone/#computing-resources","text":"scontain.com can provide access to SGX-capable machines.","title":"Computing Resources"},{"location":"aboutScone/#consulting-services","text":"scontain.com provides consulting services as well as helping you to port your applications to SGX.","title":"Consulting Services"},{"location":"aboutScone/#contact","text":"If you want to evaluate the SCONE platform, want to rent some SGX-capable computing resources, need SGX and SCONE-related consulting, or have some technical questions, please contact us at info@scontain.com .","title":"Contact"},{"location":"advantages/","text":"Advantages of SCONE SCONE simplifies building and operating confidential applications . You can rebuild existing applications to become confidential applications running on top of vanilla Kubernetes clusters (as long as the containers are permitted to use SGX). SCONE supports confidential multi-party computations . Modern software services are quite complex - involving multiple stakeholders to get services running. Not all of these stakeholder trust each other and we need to expect that some attackers have root access on the hosts on which the services execute. SCONE helps multiple stakeholders by supporting the composition of security policies of multiple stakeholders (see overview) . SCONE transparently attests applications . This ensures that an application indeed runs inside of an enclave. Only after a successful attestation, the application gets its keys to unlock the file system, its arguments and its environment variables - which all might contain secrets that need to be protected. SCONE has an integrated secrets and configuration management - simplifying the distribution of secrets without application changes by performing a transparent attestation of applications. The integrated key management allows clients to ensure that their data is protected from accesses by other clients and attackers ( see an example in the context of trusted DApps ). SCONE scales better than competing solutions since it uses an advanced thread management and a very efficient way to perform asynchronous system calls : when an enclave performs a system call, SCONE switches to another application thread while the system call is performed by threads running outside the enclave. This minimizes the need for the threads running inside the enclave to exit the enclave. Minimizing the enclave exits is particularly important looking at recent CPU microcode updates in the context of L1TF : The CPU needs to flush the L1 cache - which is an expensive operation. Single threaded applications can be tuned for low-latency system call processing. SCONE has smaller executables . SCONE is based on a modified C library instead of running a complete library OS inside of an enclave. This does not only reduces the size of the enclaves and hence, the number of software bugs inside the enclave. To see how large typical code sizes are and the defender's dilemma, have a look at our background section . Large code size does not only mean more bugs (expect about 0.61 bugs per 1000 lines) but also negatively impacts performance: SGX CPUs have limited EPC (enclave page cache) and larger memory footprints result in general in worse performance. SCONE comes with a toolchain . While SCONE supports binaries compiled for Alpine Linux, we recommend to recompile binaries to minimize code size and to ensure better performance and security of the applications. Also, the crosscompiler ensures that the correct model for thread local variables is used. SCONE comes with curated images . Since compiling and configuring applications is an effort, we provide common applications like Vault , nginx , MariaDB , Apache , etc. SCONE supports binary compatibility . We support binary compatibility for Alpine Linux , i.e., we can run native Alpine applications without modifications / recompilation inside of SGX enclaves. SCONE protects the OS interface . SCONE provides shields to protect the interaction with the operating system interface. For example, it provides the transparent encryption of files ( example ). While the OS interface has more calls than the VMM interface used by a library OS (like Haven), we decided in SCONE to protect the OS interface instead since it provides us with more specific semantics which in turn simplifies the shielding. SCONE ensures better Linux compatibility . By providing a native OS interface, SCONE reduces compatibility issues of the application. A library OS will most likely not be 100% compatible with the latest Linux kernel. SCONE is hardware independent . The design of SCONE is such that we can support other TEEs (trusted execution environments) when they become available. In this way, one does not have to port applications to different TEEs. SCONE supports various package managers . While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux. In particular, you can deploy SCONE-based application with your favorite package manager.","title":"Advantages of SCONE"},{"location":"advantages/#advantages-of-scone","text":"SCONE simplifies building and operating confidential applications . You can rebuild existing applications to become confidential applications running on top of vanilla Kubernetes clusters (as long as the containers are permitted to use SGX). SCONE supports confidential multi-party computations . Modern software services are quite complex - involving multiple stakeholders to get services running. Not all of these stakeholder trust each other and we need to expect that some attackers have root access on the hosts on which the services execute. SCONE helps multiple stakeholders by supporting the composition of security policies of multiple stakeholders (see overview) . SCONE transparently attests applications . This ensures that an application indeed runs inside of an enclave. Only after a successful attestation, the application gets its keys to unlock the file system, its arguments and its environment variables - which all might contain secrets that need to be protected. SCONE has an integrated secrets and configuration management - simplifying the distribution of secrets without application changes by performing a transparent attestation of applications. The integrated key management allows clients to ensure that their data is protected from accesses by other clients and attackers ( see an example in the context of trusted DApps ). SCONE scales better than competing solutions since it uses an advanced thread management and a very efficient way to perform asynchronous system calls : when an enclave performs a system call, SCONE switches to another application thread while the system call is performed by threads running outside the enclave. This minimizes the need for the threads running inside the enclave to exit the enclave. Minimizing the enclave exits is particularly important looking at recent CPU microcode updates in the context of L1TF : The CPU needs to flush the L1 cache - which is an expensive operation. Single threaded applications can be tuned for low-latency system call processing. SCONE has smaller executables . SCONE is based on a modified C library instead of running a complete library OS inside of an enclave. This does not only reduces the size of the enclaves and hence, the number of software bugs inside the enclave. To see how large typical code sizes are and the defender's dilemma, have a look at our background section . Large code size does not only mean more bugs (expect about 0.61 bugs per 1000 lines) but also negatively impacts performance: SGX CPUs have limited EPC (enclave page cache) and larger memory footprints result in general in worse performance. SCONE comes with a toolchain . While SCONE supports binaries compiled for Alpine Linux, we recommend to recompile binaries to minimize code size and to ensure better performance and security of the applications. Also, the crosscompiler ensures that the correct model for thread local variables is used. SCONE comes with curated images . Since compiling and configuring applications is an effort, we provide common applications like Vault , nginx , MariaDB , Apache , etc. SCONE supports binary compatibility . We support binary compatibility for Alpine Linux , i.e., we can run native Alpine applications without modifications / recompilation inside of SGX enclaves. SCONE protects the OS interface . SCONE provides shields to protect the interaction with the operating system interface. For example, it provides the transparent encryption of files ( example ). While the OS interface has more calls than the VMM interface used by a library OS (like Haven), we decided in SCONE to protect the OS interface instead since it provides us with more specific semantics which in turn simplifies the shielding. SCONE ensures better Linux compatibility . By providing a native OS interface, SCONE reduces compatibility issues of the application. A library OS will most likely not be 100% compatible with the latest Linux kernel. SCONE is hardware independent . The design of SCONE is such that we can support other TEEs (trusted execution environments) when they become available. In this way, one does not have to port applications to different TEEs. SCONE supports various package managers . While SCONE focuses on securing containers and cloud-native applications, SCONE can help you to secure almost any program running on top of Linux. In particular, you can deploy SCONE-based application with your favorite package manager.","title":"Advantages of SCONE"},{"location":"aks/","text":"Confidential Azure Kubernetes Service (AKS) A convenient way to operate SCONE-based applications is to use Azure Kubernetes Services (AKS). SCONE is fully compatible with AKS. Very soon, we will release new features to more easily use some of the advanced AKS features: enable Microsoft Azure Attestation (MAA) - as an alternative to using native Intel DCAP - via the SCONE CAS policy, and enable retrieval of secrets from Azure Key Vault (AKV) via SCONE CAS policies. This will simplify the development and operation of confidential applications. For example, one can develop an application in a local Kubernetes cluster and operate the same application in AKS. In the local cluster, one uses Intel DCAP-based attestation - which ships with the SCONE platform - and secrets generated by SCONE CAS. On AKV, one can use MAA (or, native Intel DCAP) for attestation and retrieve secrets from AKV - via a simple SCONE CAS policy change. This means that there is no need to modify the application. For example, one can inject the secrets retrieved from AKV into configuration files as usually. In the next sections, we introduce examples on how to execute SCONE-based applications on AKS. This includes a simple Confidential Flask Demo , and an end-to-end, always-encrypted and attested Confidential Document Management service. Standard Languages SCONE supports the following Programming languages: C C# C++ Fortran GO Java Python PyPy Mono Node R Rust Standard Applications (SconeApps) We support a variety of applications on AKS that can be deployed with helm and will add more over time: Application Description database Umbrella chart to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy mariadb Deploy MariaDB SCONE , i.e., MariaDB running inside of SGX enclaves, to Kubernetes maxscale Deploy MaxScale SCONE , i.e., Maxscale running inside of SGX enclaves and optionally, an HAProxy as Ingress memcached Deploy memcached inside of SGX enclaves, generate and inject TLS certificates to secure communication. openvino OpenVINO (Open Visual Inference and Neural network Optimization) is a toolkit facilitating the optimization and deployment of Deep Learning models pytorch Deploy pytorch inside of SGX enclaves. spark Apache Spark is an open-source distributed general-purpose cluster-computing framework. tensorflow Machine Learning framework by Google tensorflowlite Deploy machine learning models Visual Studio Code Deploy VisualStudio Code and the SCONE CrossCompiler to be able to edit and run your confidential applications inside of a Kubernetes cluster. Zookeeper Deploy Zookeeper cluster inside of SGX enclaves. Individual Applications We show next how to sconify an existing container image such that the application can be executed securely inside of an SGX enclave, and all files are encrypted by SCONE. The community edition requires the existence of an already sconified binary of another image while the standard edition can covert an existing binary to run inside of SGX enclaves. Setup Steps First, you need to get access to confidential AKS . Second, you need to set up helm, SGX Plugin, and LAS . Note that you can use the AKS SGX Plugin instead of the SCONE SGX Plugin. Third, you deploy your application or a SconeApps like MariaDB .","title":"Confidential AKS"},{"location":"aks/#confidential-azure-kubernetes-service-aks","text":"A convenient way to operate SCONE-based applications is to use Azure Kubernetes Services (AKS). SCONE is fully compatible with AKS. Very soon, we will release new features to more easily use some of the advanced AKS features: enable Microsoft Azure Attestation (MAA) - as an alternative to using native Intel DCAP - via the SCONE CAS policy, and enable retrieval of secrets from Azure Key Vault (AKV) via SCONE CAS policies. This will simplify the development and operation of confidential applications. For example, one can develop an application in a local Kubernetes cluster and operate the same application in AKS. In the local cluster, one uses Intel DCAP-based attestation - which ships with the SCONE platform - and secrets generated by SCONE CAS. On AKV, one can use MAA (or, native Intel DCAP) for attestation and retrieve secrets from AKV - via a simple SCONE CAS policy change. This means that there is no need to modify the application. For example, one can inject the secrets retrieved from AKV into configuration files as usually. In the next sections, we introduce examples on how to execute SCONE-based applications on AKS. This includes a simple Confidential Flask Demo , and an end-to-end, always-encrypted and attested Confidential Document Management service.","title":"Confidential Azure Kubernetes Service (AKS)"},{"location":"aks/#standard-languages","text":"SCONE supports the following Programming languages: C C# C++ Fortran GO Java Python PyPy Mono Node R Rust","title":"Standard Languages"},{"location":"aks/#standard-applications-sconeapps","text":"We support a variety of applications on AKS that can be deployed with helm and will add more over time: Application Description database Umbrella chart to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy mariadb Deploy MariaDB SCONE , i.e., MariaDB running inside of SGX enclaves, to Kubernetes maxscale Deploy MaxScale SCONE , i.e., Maxscale running inside of SGX enclaves and optionally, an HAProxy as Ingress memcached Deploy memcached inside of SGX enclaves, generate and inject TLS certificates to secure communication. openvino OpenVINO (Open Visual Inference and Neural network Optimization) is a toolkit facilitating the optimization and deployment of Deep Learning models pytorch Deploy pytorch inside of SGX enclaves. spark Apache Spark is an open-source distributed general-purpose cluster-computing framework. tensorflow Machine Learning framework by Google tensorflowlite Deploy machine learning models Visual Studio Code Deploy VisualStudio Code and the SCONE CrossCompiler to be able to edit and run your confidential applications inside of a Kubernetes cluster. Zookeeper Deploy Zookeeper cluster inside of SGX enclaves.","title":"Standard Applications (SconeApps)"},{"location":"aks/#individual-applications","text":"We show next how to sconify an existing container image such that the application can be executed securely inside of an SGX enclave, and all files are encrypted by SCONE. The community edition requires the existence of an already sconified binary of another image while the standard edition can covert an existing binary to run inside of SGX enclaves.","title":"Individual Applications"},{"location":"aks/#setup-steps","text":"First, you need to get access to confidential AKS . Second, you need to set up helm, SGX Plugin, and LAS . Note that you can use the AKS SGX Plugin instead of the SCONE SGX Plugin. Third, you deploy your application or a SconeApps like MariaDB .","title":"Setup Steps"},{"location":"aks_charts/","text":"AKS Applications We support the following confidential applications on AKS: Chart Description cas Deploy the SCONE Configuration and Attestation Service (CAS) to Kubernetes . database Umbrella chart to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy mariadb Deploy MariaDB SCONE , i.e., MariaDB running inside of SGX enclaves, to Kubernetes memcached Deploy memcached inside of SGX enclaves, generate and inject TLS certificates to secure communication. maxscale Deploy MaxScale SCONE , i.e., Maxscale running inside of SGX enclaves and optionally, an HAProxy as Ingress nginx Deploy nginx inside of SGX enclaves, generate and inject TLS certificates to secure communication. openvino OpenVINO (Open Visual Inference and Neural network Optimization) is a toolkit facilitating the optimization and deployment of Deep Learning models pytorch Deploy pytorch inside of SGX enclaves. spark Apache Spark is an open-source distributed general-purpose cluster-computing framework. tensorflow Machine Learning framework by Google tensorflowlite Deploy machine learning models Visual Studio Code Deploy VisualStudio Code and the SCONE CrossCompiler to be able to edit and run your confidential applications inside of a Kubernetes cluster. Zookeeper Deploy Zookeeper cluster inside of SGX enclaves.","title":"AKS Applications"},{"location":"aks_charts/#aks-applications","text":"We support the following confidential applications on AKS: Chart Description cas Deploy the SCONE Configuration and Attestation Service (CAS) to Kubernetes . database Umbrella chart to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy mariadb Deploy MariaDB SCONE , i.e., MariaDB running inside of SGX enclaves, to Kubernetes memcached Deploy memcached inside of SGX enclaves, generate and inject TLS certificates to secure communication. maxscale Deploy MaxScale SCONE , i.e., Maxscale running inside of SGX enclaves and optionally, an HAProxy as Ingress nginx Deploy nginx inside of SGX enclaves, generate and inject TLS certificates to secure communication. openvino OpenVINO (Open Visual Inference and Neural network Optimization) is a toolkit facilitating the optimization and deployment of Deep Learning models pytorch Deploy pytorch inside of SGX enclaves. spark Apache Spark is an open-source distributed general-purpose cluster-computing framework. tensorflow Machine Learning framework by Google tensorflowlite Deploy machine learning models Visual Studio Code Deploy VisualStudio Code and the SCONE CrossCompiler to be able to edit and run your confidential applications inside of a Kubernetes cluster. Zookeeper Deploy Zookeeper cluster inside of SGX enclaves.","title":"AKS Applications"},{"location":"aks_integration/","text":"AKS Integration Azure Kubernetes Service (AKS) provides a variety of new features for confidential services like: Azure Key Vault (AKV) to manage secrets, aad-token to retrieve secrets from Azure Key Vault (AKV), and MAA (Microsoft Azure Attestation) to attest services We will support these features starting with SCONE release 5.3.0. You can read and watch at our tutorial to learn how to use these new features. aad-token CAS can retrieve access tokens (aad-token) for Microsoft Azure Active Directory (AAD) . While they are primarily used to import secrets from Azure Key Vault (see below), they can also be consumed by application services. Parameters To retrieve an aad-token, one has to specify secrets: tenant_id : The ID of an AAD tenant (required) client_id : The ID of an AAD application of the referenced AAD tenant (required) Either of the following credentials are required: application_secret : A shared secret that was registered with AAD private_key and certificate_thumbprint : A PEM-encoded PKCS#8 RSA private key belonging to a certificate that was registered with AAD, and a thumbprint of this certificate that was returned during registration. Note The application must have application API permissions to access secrets of an Azure Key Vault. For information on how to set up an Azure application for API access, see https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-app . The Key Vault permissions for the application must then be configured using Azure access policies or Azure role-based access control . Example In a CAS policy, one can define aad-tokens as follows: secrets : - name : ad_token_1 kind : aad-token tenant_id : e7cac514-f1aa-4e0d-b207-50b1d9a89d21 client_id : 3f2c210e-83b4-4217-9eee-4f747d8aeeb3 application_secret : \"dHmbyTBF4JNAN3JJobeD\" - name : ad_token_2 kind : aad-token tenant_id : e7cac514-f1aa-4e0d-b207-50b1d9a89d21 client_id : 3f2c210e-83b4-4217-9eee-4f747d8aeeb3 private_key : | -----BEGIN PRIVATE KEY----- MIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCbkIjwCh2zXbUs ... s5is0+EmTLXoWCqftUG5RQ== -----END PRIVATE KEY----- certificate_thumbprint : B033DB596639F3CA02D6537055E85B8EFE060756 Protecting the credentials to retrieve the AAD Token To protect the credentials define within a policy P, policy P defines who can read P. In quite a few cases, one wants to permit that anybody can read policy P . However, one does not want to expose the credentials to retrieve, say, the aad-token ad_token_1 . Our recommendation is to define the credentials to retrieve ad_token_1 in a second CAS policy, say, P2 . Policy P2 specifies the credentials for ad_token_1 and exports ad_token_1 to policy P . One can define P2 such that nobody can read P2 . Policy P imports ad_token_1 from P2 . Policy P2 could state that anybody can read P2 : a reader would only learn the name ad_token_1 , but it would not be able to read token ad_token_1 nor the credentials used to retrieve ad_token_1 . Granting Access As we already mentioned above, one can limit who can retrieve a policy. Secret Placeholders Assuming a secret named ad_token_1 , use $$SCONE::ad_token_1$$ in program arguments, environment variables or secret injection files. AAD token secrets do not support any format suffix. The token can be used to authenticate directly for Microsoft Azure services. Token Lifetime Tokens are only valid for a limited amount of time (a couple of minutes), which means they can only be used successfully immediately after program start. Azure Key Vault Integration Existing secret values can be imported from a Microsoft Azure Key Vault (AKV) . This requires: a new import_akv mapping which specifies the vault to import from an aad-token secret authorized to access this vault Example secrets : - name : db_encryption_key kind : binary import_akv : vault : myvaultname.vault.azure.net secret_name : abc token : $$SCONE::db-aad-token$$ - name : db-aad-token kind : aad-token ... import_akv Parameters: vault : The address of the key vault to use (required) secret_name : The name of the secret that should be fetched from the vault (optional). If omitted, the name of the secret as defined in the session will be used token : A reference to an aad-token secret of the form $$SCONE::<secret-name>$$ . This token will be used to authenticate requests against the vault. If the session contains exactly one aad-token secret, the parameter is optional, and this secret will be used by default. If the session contains multiple aad-token secrets, the parameter must be specified. import vs import_akv import and import_akv are mutually exclusive. The secret's kind is optional and will be inferred if omitted. Secret Placeholders Assuming a secret named db_encryption_key , use $$SCONE::db_encryption_key$$ in program arguments, environment variables or secret injection files. These secrets support the specified CAS format suffixes. However, PKCS#12-encoded certificates are represented as strings. Certificate Representation PEM-encoded certificates imported from a key vault will be represented as X.509 secrets. These support format suffix. PKCS#12-encoded certificates, on the other hand, will be represented as plain text secrets, i.e. suffixes such as :privatekey cannot be used in secret placeholders.","title":"AKS Integration"},{"location":"aks_integration/#aks-integration","text":"Azure Kubernetes Service (AKS) provides a variety of new features for confidential services like: Azure Key Vault (AKV) to manage secrets, aad-token to retrieve secrets from Azure Key Vault (AKV), and MAA (Microsoft Azure Attestation) to attest services We will support these features starting with SCONE release 5.3.0. You can read and watch at our tutorial to learn how to use these new features.","title":"AKS Integration"},{"location":"aks_integration/#aad-token","text":"CAS can retrieve access tokens (aad-token) for Microsoft Azure Active Directory (AAD) . While they are primarily used to import secrets from Azure Key Vault (see below), they can also be consumed by application services.","title":"aad-token"},{"location":"aks_integration/#parameters","text":"To retrieve an aad-token, one has to specify secrets: tenant_id : The ID of an AAD tenant (required) client_id : The ID of an AAD application of the referenced AAD tenant (required) Either of the following credentials are required: application_secret : A shared secret that was registered with AAD private_key and certificate_thumbprint : A PEM-encoded PKCS#8 RSA private key belonging to a certificate that was registered with AAD, and a thumbprint of this certificate that was returned during registration. Note The application must have application API permissions to access secrets of an Azure Key Vault. For information on how to set up an Azure application for API access, see https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-app . The Key Vault permissions for the application must then be configured using Azure access policies or Azure role-based access control .","title":"Parameters"},{"location":"aks_integration/#example","text":"In a CAS policy, one can define aad-tokens as follows: secrets : - name : ad_token_1 kind : aad-token tenant_id : e7cac514-f1aa-4e0d-b207-50b1d9a89d21 client_id : 3f2c210e-83b4-4217-9eee-4f747d8aeeb3 application_secret : \"dHmbyTBF4JNAN3JJobeD\" - name : ad_token_2 kind : aad-token tenant_id : e7cac514-f1aa-4e0d-b207-50b1d9a89d21 client_id : 3f2c210e-83b4-4217-9eee-4f747d8aeeb3 private_key : | -----BEGIN PRIVATE KEY----- MIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCbkIjwCh2zXbUs ... s5is0+EmTLXoWCqftUG5RQ== -----END PRIVATE KEY----- certificate_thumbprint : B033DB596639F3CA02D6537055E85B8EFE060756 Protecting the credentials to retrieve the AAD Token To protect the credentials define within a policy P, policy P defines who can read P. In quite a few cases, one wants to permit that anybody can read policy P . However, one does not want to expose the credentials to retrieve, say, the aad-token ad_token_1 . Our recommendation is to define the credentials to retrieve ad_token_1 in a second CAS policy, say, P2 . Policy P2 specifies the credentials for ad_token_1 and exports ad_token_1 to policy P . One can define P2 such that nobody can read P2 . Policy P imports ad_token_1 from P2 . Policy P2 could state that anybody can read P2 : a reader would only learn the name ad_token_1 , but it would not be able to read token ad_token_1 nor the credentials used to retrieve ad_token_1 .","title":"Example"},{"location":"aks_integration/#granting-access","text":"As we already mentioned above, one can limit who can retrieve a policy.","title":"Granting Access"},{"location":"aks_integration/#secret-placeholders","text":"Assuming a secret named ad_token_1 , use $$SCONE::ad_token_1$$ in program arguments, environment variables or secret injection files. AAD token secrets do not support any format suffix. The token can be used to authenticate directly for Microsoft Azure services. Token Lifetime Tokens are only valid for a limited amount of time (a couple of minutes), which means they can only be used successfully immediately after program start.","title":"Secret Placeholders"},{"location":"aks_integration/#azure-key-vault-integration","text":"Existing secret values can be imported from a Microsoft Azure Key Vault (AKV) . This requires: a new import_akv mapping which specifies the vault to import from an aad-token secret authorized to access this vault","title":"Azure Key Vault Integration"},{"location":"aks_integration/#example_1","text":"secrets : - name : db_encryption_key kind : binary import_akv : vault : myvaultname.vault.azure.net secret_name : abc token : $$SCONE::db-aad-token$$ - name : db-aad-token kind : aad-token ... import_akv Parameters: vault : The address of the key vault to use (required) secret_name : The name of the secret that should be fetched from the vault (optional). If omitted, the name of the secret as defined in the session will be used token : A reference to an aad-token secret of the form $$SCONE::<secret-name>$$ . This token will be used to authenticate requests against the vault. If the session contains exactly one aad-token secret, the parameter is optional, and this secret will be used by default. If the session contains multiple aad-token secrets, the parameter must be specified. import vs import_akv import and import_akv are mutually exclusive. The secret's kind is optional and will be inferred if omitted.","title":"Example"},{"location":"aks_integration/#secret-placeholders_1","text":"Assuming a secret named db_encryption_key , use $$SCONE::db_encryption_key$$ in program arguments, environment variables or secret injection files. These secrets support the specified CAS format suffixes. However, PKCS#12-encoded certificates are represented as strings. Certificate Representation PEM-encoded certificates imported from a key vault will be represented as X.509 secrets. These support format suffix. PKCS#12-encoded certificates, on the other hand, will be represented as plain text secrets, i.e. suffixes such as :privatekey cannot be used in secret placeholders.","title":"Secret Placeholders"},{"location":"aks_setup/","text":"AKS Setup Simplified Deployment In our SconeApps , we also support the native AKS SGX Plugin, i.e., the is no need to install the SCONE SGX Plugin on AKS. MAA-based attestation Right now, SCONE-based confidential services are attested using DCAP or EPID-based attestation. Starting with SCONE 5.3.0, one can enable Microsoft Azure Attestation (MAA) and import secrets from Azure Key Vault (AKV) via SCONE CAS policies. SCONE-based confidential applications can be deploy with helm , i.e., the Kubernetes Package Manager on AKS. To do so, you need to ensure helm has access to our SconeApps helm charts : SconeApps ensure that our SCONE LAS (local attestation service) is installed with helm: LAS installation AKS is compatible with helm , i.e., one can deploy applications with helm as soon as your confidential AKS cluster is running. You need to specify --useSGXDevPlugin=azure to use the Azure SGX Plugin and --set sgxEpcMem=16 (in MiB) to specify the required EPC size. We support a variety of helm charts : we provide confidential variants of, for example, mariadb, maxscale, memcached, nginx, openvino, pytorch, spark, teemon, tensorflow, TensorFlowLite , and Zookeeper . To set up an AKS cluster, use the AKS command line interface. A quick walk-through on how to deploy a Kubernetes cluster and how to deploy applications on AKS, please follow this description . Compatibility Note that SCONE confidential services will stay compatible with any Kubernetes cluster, i.e., you can deploy some confidential services on your own Kubernetes cluster while others run on AKS. On AKS, you can enable additional Azure services in the policy, like, using MAA for attestation. Outside of AKS, you can enable DCAP-based attestation and use SCONE CAS for secret management. Deploying with helm makes it very easy to redeploy workloads on different Kubernetes clusters, like, moving services from your development Kubernetes cluster to AKS.","title":"AKS Setup"},{"location":"aks_setup/#aks-setup","text":"Simplified Deployment In our SconeApps , we also support the native AKS SGX Plugin, i.e., the is no need to install the SCONE SGX Plugin on AKS. MAA-based attestation Right now, SCONE-based confidential services are attested using DCAP or EPID-based attestation. Starting with SCONE 5.3.0, one can enable Microsoft Azure Attestation (MAA) and import secrets from Azure Key Vault (AKV) via SCONE CAS policies. SCONE-based confidential applications can be deploy with helm , i.e., the Kubernetes Package Manager on AKS. To do so, you need to ensure helm has access to our SconeApps helm charts : SconeApps ensure that our SCONE LAS (local attestation service) is installed with helm: LAS installation AKS is compatible with helm , i.e., one can deploy applications with helm as soon as your confidential AKS cluster is running. You need to specify --useSGXDevPlugin=azure to use the Azure SGX Plugin and --set sgxEpcMem=16 (in MiB) to specify the required EPC size. We support a variety of helm charts : we provide confidential variants of, for example, mariadb, maxscale, memcached, nginx, openvino, pytorch, spark, teemon, tensorflow, TensorFlowLite , and Zookeeper . To set up an AKS cluster, use the AKS command line interface. A quick walk-through on how to deploy a Kubernetes cluster and how to deploy applications on AKS, please follow this description . Compatibility Note that SCONE confidential services will stay compatible with any Kubernetes cluster, i.e., you can deploy some confidential services on your own Kubernetes cluster while others run on AKS. On AKS, you can enable additional Azure services in the policy, like, using MAA for attestation. Outside of AKS, you can enable DCAP-based attestation and use SCONE CAS for secret management. Deploying with helm makes it very easy to redeploy workloads on different Kubernetes clusters, like, moving services from your development Kubernetes cluster to AKS.","title":"AKS Setup"},{"location":"appsecurity/","text":"Application-Oriented Security SCONE supports developers and service providers , to protect the confidentiality and integrity of their applications - even when running in environments that cannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like microservice-based applications as well as cloud-native applications . However, SCONE can protect most programs running on top of Linux. SCONE supports developers and service providers to ensure end-to-end encryption in the sense that data is always encrypted , i.e., while being transmitted, while being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect. For example, a service that operates only on encrypted data might not need to be protected with SGX. Keep it simple Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.* Ease of Use SCONE supports strong application-oriented security with a workflow like Docker, in particular, SCONE supports Dockerfiles . This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container. The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Kubernetes can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Kubernetes, failed containers will automatically be restarted on an appropriate host.","title":"Application-Oriented Security"},{"location":"appsecurity/#application-oriented-security","text":"SCONE supports developers and service providers , to protect the confidentiality and integrity of their applications - even when running in environments that cannot be completely trusted. SCONE's focus is on supporting the development of programs running inside of containers like microservice-based applications as well as cloud-native applications . However, SCONE can protect most programs running on top of Linux. SCONE supports developers and service providers to ensure end-to-end encryption in the sense that data is always encrypted , i.e., while being transmitted, while being at rest and even while being processed. The latter has only recently become possible with the help of a novel CPU extension by Intel (SGX). To reduce the required computing resources, a service provider can decide what to protect and what not to protect. For example, a service that operates only on encrypted data might not need to be protected with SGX. Keep it simple Our general recommendation is, however, that developers should protect all parts of an application. The cost of computing resources have been dropping dramatically and hence, the reduction in cost might not be justified when compared with the potential costs - and also loss of reputation - by data breaches. SCONE supports horizontal scalability, i.e., throughput and latency can typically be controlled via the number of instances of a service.*","title":"Application-Oriented Security"},{"location":"appsecurity/#ease-of-use","text":"SCONE supports strong application-oriented security with a workflow like Docker, in particular, SCONE supports Dockerfiles . This simplifies the construction and operation of applications consisting of a set of containers. This fits, in particular, modern cloud-native applications consisting of microservices and each microservice runs either in a standard or a secure container. The Docker Engine itself is not protected. The Docker Engine, like the operating system, never sees any plain text data. This facilitates that the Docker Engine or the Kubernetes can be managed by a cloud provider. SCONE helps a service providers to ensure the confidentiality and integrity of the application data while the cloud provider will ensure the availability of the service. For example, with the help of Kubernetes, failed containers will automatically be restarted on an appropriate host.","title":"Ease of Use"},{"location":"attestation/","text":"Attestation Remote attestation allows SCONE to trust software that is executed on remote hosts that are potentially under control of an adversary. As such it is of the greatest importance to fulfill SCONE's security guarantees. SCONE CAS, our Configuration and Attestation Service, will only provision a program's configuration (secrets) once it ensured that the program was not manipulated. Policy View Attestation Protocol In SCONE, the CAS is the remote challenger that wants to establish trust into the application enclave. In general the attestation flow is as follows: The enclave is started and obtains CAS' address and its supposed service to execute via the environment. It connects to CAS and requests its service's configuration. As part of this request the enclave offers attestation collaterals that can be used by CAS to attest the enclave. CAS decides based on the requested service's security configuration which collaterals to employ, optionally requesting more data - typically quotes - from the enclave. Once CAS is convinced the enclave is trustworthy the requested configuration is sent to the enclave, and the enclave application is started. sequenceDiagram participant CAS participant Enclave Enclave <---> CAS: TLS protected connection Enclave -> CAS: EnclaveHello{kind=ConfigurationRequest{\\n service_id=\"session/service\",\\n attestation_collateral=[scone_quote, epid, dcap_quote]\\n}} note left: `service_id` is provided by the user\\nvia the `SCONE_CONFIG_ID` environ var. note right: Security properties of the service are queried and\\nattestation mechanism is chosen accordingly. CAS -> Enclave: AttestationRequest Enclave -> CAS: Quote note right: Quote is verified against expectations CAS -> Enclave: Configuration The identity of the enclave and quote are bound through the enclave's cryptographic identity. When the enclave starts, a random key pair is generated which is used (as part of a certificate) to establish a protected communication channel to CAS. This enclave cryptographic identity is hashed using SHA-512 and cryptographically embedded into all quotes of the enclave. Therefore, CAS can verify that the TLS connection is terminated in the same enclave from which it receives the quotes. This also serves as a freshness indicator since the enclave's identity is always randomly generated. The attestation collaterals offered by the enclave depend on the enclave's and its platform's remote attestation capabilities. For example, right now most platforms do not support DCAP attestation. This mechanism is extendable with additional attestation protocols in the future. At the moment three attestation mechanism are supported: SCONE Attestation Intel EPID Attestation Intel DCAP Attestation Attestation Negotiation With attestation negotiation, the challenger (cas) and attestee (enclave) agree on an attestation mechanism (SCONE, EPID, DCAP, TPM, ...) based on their capabilities and requirements. From a privacy perspective, the attestee exposes all details to the challenger while the challenger could choose to hide their preferences, i.e. which attestation mechanism they trust. Along those lines, the challenger shall drive the negotiation. The attestee only reports their capabilities and obeys to the challenger's requests. ** TODO: Verify of challenger by attestee. Attestation Library We generalize SCONE Attestation to be able to use SCONE's attestation with other frameworks (Intel's SGX SDK, Graphene, SGX-LKL, and OpenEnclave etc). To achieve this, we basically have to wrap the cas client code of the runtime into a library libscone-enclave-client that can be linked into the other enclave frameworks. This is relatively straight forward. Issues arise at the interfaces where libscone-enclave-client wants to interact with outside services, such as CAS and LAS, and where it wants to interact with the SGX hardware, e.g., to obtain the SGX report. Definition of API ## Interface Draft // NOTE: No error handling yet /// SCONE Enclave Client Framework Abstraction Layer struct SCONE_EC_FAL { int64_t ( * create_tcp_socket ( const char * address , const uint16_t port )); int64_t ( * read_from_socket ( int64_t socket_id , char * buffer , uint64_t len )); int64_t ( * write_to_socket ( int64_t socket_id , const char * buffer , const uint64_t len )); int64_t ( * close_socket ( int64_t socket_id )); enum SCONE_EC_report_getter_result ( * sgx_report_get ()); } enum SCONE_EC_report_getter_result { OK , NO_HARDWARE_SUPPORT , } struct SCONE_EC_builder * SCONE_EC_create_builder ( session_name , service_name , cas_address , fal ); void SCONE_EC_builder_set_scone_las_address ( las_address ); // optional: only attest with cas that have the specified cas key hash -> \"easy\" in if library is done in Rust void SCONE_EC_builder_set_cas_key ( hash ); // optional: only attest with cas that have the specified cas software key hash void SCONE_EC_builder_set_cas_software_key ( hash ); struct SCONE_EC_client * SCONE_EC_builder_construct ( struct SCONE_EC_builder ** , struct SCONE_EC_client ** ); struct SCONE_EC_attested_client * SCONE_EC_client_attest ( struct SCONE_EC_client ** , struct SCONE_EC_attested_client ** ); struct SCONE_EC_secure_config * SCONE_EC_attested_client_get_SCONE_config (); SCONE_EC_attested_client_get_fspf_key ( path , tag ); SCONE_EC_attested_client_update_fspf_tag ( path , tag ); SCONE_EC_attested_client_free (); // // optionally later to use it without las // struct SCONE_EC_quote_getter { // u64 quote_buffer_size(), // get_quote(report, buffer, buffer length), // } // void SCONE_EC_builder_set_epid_quote_getter(quote_getter, epid_group); // void SCONE_EC_builder_set_dcap_quote_getter(quote_getter);// // optionally later // void SCONE_EC_builder_set_identity(cert, key); Data Center Attestation Primitives (DCAP) Data Center Attestation Primitives (DCAP) is an attestation scheme in which Intel removed themselves from the critical path reducing their possibility of mounting denial-of-service attacks on SGX users. DCAP Quotes DCAP quotes contain TCB version information, like CPU SVN, at various locations: QE report, ISV enclave report, QE certification data The Enclave-CAS protocol It needs to support DCAP and while we're on it we should also add \"attestation negotiation\" where CAS and enclave negotiate which attestation scheme to use. This should also support the \"no-attestation\" attestation scheme allowing us to use CAS with enclaves in SIM mode finally. -> #618, #619 Enclave-LAS communication has to be adapted as well, but here differences are not that severe. Obviously, the logic in the enclave and CAS has to be adapted to handle the changed protocol. Quote Retrieval Quote Verification","title":"Attestation"},{"location":"attestation/#attestation","text":"Remote attestation allows SCONE to trust software that is executed on remote hosts that are potentially under control of an adversary. As such it is of the greatest importance to fulfill SCONE's security guarantees. SCONE CAS, our Configuration and Attestation Service, will only provision a program's configuration (secrets) once it ensured that the program was not manipulated.","title":"Attestation"},{"location":"attestation/#policy-view","text":"","title":"Policy View"},{"location":"attestation/#attestation-protocol","text":"In SCONE, the CAS is the remote challenger that wants to establish trust into the application enclave. In general the attestation flow is as follows: The enclave is started and obtains CAS' address and its supposed service to execute via the environment. It connects to CAS and requests its service's configuration. As part of this request the enclave offers attestation collaterals that can be used by CAS to attest the enclave. CAS decides based on the requested service's security configuration which collaterals to employ, optionally requesting more data - typically quotes - from the enclave. Once CAS is convinced the enclave is trustworthy the requested configuration is sent to the enclave, and the enclave application is started. sequenceDiagram participant CAS participant Enclave Enclave <---> CAS: TLS protected connection Enclave -> CAS: EnclaveHello{kind=ConfigurationRequest{\\n service_id=\"session/service\",\\n attestation_collateral=[scone_quote, epid, dcap_quote]\\n}} note left: `service_id` is provided by the user\\nvia the `SCONE_CONFIG_ID` environ var. note right: Security properties of the service are queried and\\nattestation mechanism is chosen accordingly. CAS -> Enclave: AttestationRequest Enclave -> CAS: Quote note right: Quote is verified against expectations CAS -> Enclave: Configuration The identity of the enclave and quote are bound through the enclave's cryptographic identity. When the enclave starts, a random key pair is generated which is used (as part of a certificate) to establish a protected communication channel to CAS. This enclave cryptographic identity is hashed using SHA-512 and cryptographically embedded into all quotes of the enclave. Therefore, CAS can verify that the TLS connection is terminated in the same enclave from which it receives the quotes. This also serves as a freshness indicator since the enclave's identity is always randomly generated. The attestation collaterals offered by the enclave depend on the enclave's and its platform's remote attestation capabilities. For example, right now most platforms do not support DCAP attestation. This mechanism is extendable with additional attestation protocols in the future. At the moment three attestation mechanism are supported: SCONE Attestation Intel EPID Attestation Intel DCAP Attestation","title":"Attestation Protocol"},{"location":"attestation/#attestation-negotiation","text":"With attestation negotiation, the challenger (cas) and attestee (enclave) agree on an attestation mechanism (SCONE, EPID, DCAP, TPM, ...) based on their capabilities and requirements. From a privacy perspective, the attestee exposes all details to the challenger while the challenger could choose to hide their preferences, i.e. which attestation mechanism they trust. Along those lines, the challenger shall drive the negotiation. The attestee only reports their capabilities and obeys to the challenger's requests. ** TODO: Verify of challenger by attestee.","title":"Attestation Negotiation"},{"location":"attestation/#attestation-library","text":"We generalize SCONE Attestation to be able to use SCONE's attestation with other frameworks (Intel's SGX SDK, Graphene, SGX-LKL, and OpenEnclave etc). To achieve this, we basically have to wrap the cas client code of the runtime into a library libscone-enclave-client that can be linked into the other enclave frameworks. This is relatively straight forward. Issues arise at the interfaces where libscone-enclave-client wants to interact with outside services, such as CAS and LAS, and where it wants to interact with the SGX hardware, e.g., to obtain the SGX report.","title":"Attestation Library"},{"location":"attestation/#definition-of-api","text":"## Interface Draft // NOTE: No error handling yet /// SCONE Enclave Client Framework Abstraction Layer struct SCONE_EC_FAL { int64_t ( * create_tcp_socket ( const char * address , const uint16_t port )); int64_t ( * read_from_socket ( int64_t socket_id , char * buffer , uint64_t len )); int64_t ( * write_to_socket ( int64_t socket_id , const char * buffer , const uint64_t len )); int64_t ( * close_socket ( int64_t socket_id )); enum SCONE_EC_report_getter_result ( * sgx_report_get ()); } enum SCONE_EC_report_getter_result { OK , NO_HARDWARE_SUPPORT , } struct SCONE_EC_builder * SCONE_EC_create_builder ( session_name , service_name , cas_address , fal ); void SCONE_EC_builder_set_scone_las_address ( las_address ); // optional: only attest with cas that have the specified cas key hash -> \"easy\" in if library is done in Rust void SCONE_EC_builder_set_cas_key ( hash ); // optional: only attest with cas that have the specified cas software key hash void SCONE_EC_builder_set_cas_software_key ( hash ); struct SCONE_EC_client * SCONE_EC_builder_construct ( struct SCONE_EC_builder ** , struct SCONE_EC_client ** ); struct SCONE_EC_attested_client * SCONE_EC_client_attest ( struct SCONE_EC_client ** , struct SCONE_EC_attested_client ** ); struct SCONE_EC_secure_config * SCONE_EC_attested_client_get_SCONE_config (); SCONE_EC_attested_client_get_fspf_key ( path , tag ); SCONE_EC_attested_client_update_fspf_tag ( path , tag ); SCONE_EC_attested_client_free (); // // optionally later to use it without las // struct SCONE_EC_quote_getter { // u64 quote_buffer_size(), // get_quote(report, buffer, buffer length), // } // void SCONE_EC_builder_set_epid_quote_getter(quote_getter, epid_group); // void SCONE_EC_builder_set_dcap_quote_getter(quote_getter);// // optionally later // void SCONE_EC_builder_set_identity(cert, key);","title":"Definition of API"},{"location":"attestation/#data-center-attestation-primitives-dcap","text":"Data Center Attestation Primitives (DCAP) is an attestation scheme in which Intel removed themselves from the critical path reducing their possibility of mounting denial-of-service attacks on SGX users.","title":"Data Center Attestation Primitives (DCAP)"},{"location":"attestation/#dcap-quotes","text":"DCAP quotes contain TCB version information, like CPU SVN, at various locations: QE report, ISV enclave report, QE certification data","title":"DCAP Quotes"},{"location":"attestation/#the-enclave-cas-protocol","text":"It needs to support DCAP and while we're on it we should also add \"attestation negotiation\" where CAS and enclave negotiate which attestation scheme to use. This should also support the \"no-attestation\" attestation scheme allowing us to use CAS with enclaves in SIM mode finally. -> #618, #619 Enclave-LAS communication has to be adapted as well, but here differences are not that severe. Obviously, the logic in the enclave and CAS has to be adapted to handle the changed protocol.","title":"The Enclave-CAS protocol"},{"location":"attestation/#quote-retrieval","text":"","title":"Quote Retrieval"},{"location":"attestation/#quote-verification","text":"","title":"Quote Verification"},{"location":"attestation_by_client/","text":"Attesting Remote Services and Applications Problem Description Consider that you operate a confidential service S or an application A consisting of a set of multiple services. In both cases, clients connect to a service S via TLS. Each client wants to ensure that S (and all services used by S ) runs inside of an enclave, runs the expected code, and it was properly configured. A client C is running at a remote site, i.e., not in the same cluster as S . How can C attest that S is properly set up? Approach Service Deployment The provide of S / A performs the following steps: Create a policy P_S for service S and all services of application A stores this in a CAS CS via scone session create . ensure that these policies do not define any explicit secrets values if you need explicit secret values, import them from another policy with very limited read access (see secret sharing ) permit clients to read the service policy P_S (see access control ) P_S generates or imports a CA certificate and generates with this a certificate Ce for the service S , P_S exports CA certificate that generated C ( see export_public ). service S will use Ce and its private key for TLS (see secret injection files ) Client (Runtime) Each client C when connecting to S via TLS, C only accepts CA as valid certificate authority for the TLS certificate provided by S (by limiting the root certificates to CA or by defining name constraints for the root certificates) since the policy of S ensures that only a correctly configured service, executing inside of an enclave, can get access to a certificate Ce issued by CA , being able to establish a TLS connection to S means, that S satisfies all constraints of its security policy P_S . Client (Development/Deployment) When client C is built or, alternatively, during deployment time, one: attests CAS CS via CLI scone cas attest , one verifies the session of each services of application A using scone session verify , and retrieve the CA certificate from CAS via its REST API (see flask demo ).","title":"Attestation by client"},{"location":"attestation_by_client/#attesting-remote-services-and-applications","text":"","title":"Attesting Remote Services and Applications"},{"location":"attestation_by_client/#problem-description","text":"Consider that you operate a confidential service S or an application A consisting of a set of multiple services. In both cases, clients connect to a service S via TLS. Each client wants to ensure that S (and all services used by S ) runs inside of an enclave, runs the expected code, and it was properly configured. A client C is running at a remote site, i.e., not in the same cluster as S . How can C attest that S is properly set up?","title":"Problem Description"},{"location":"attestation_by_client/#approach","text":"","title":"Approach"},{"location":"attestation_by_client/#service-deployment","text":"The provide of S / A performs the following steps: Create a policy P_S for service S and all services of application A stores this in a CAS CS via scone session create . ensure that these policies do not define any explicit secrets values if you need explicit secret values, import them from another policy with very limited read access (see secret sharing ) permit clients to read the service policy P_S (see access control ) P_S generates or imports a CA certificate and generates with this a certificate Ce for the service S , P_S exports CA certificate that generated C ( see export_public ). service S will use Ce and its private key for TLS (see secret injection files )","title":"Service Deployment"},{"location":"attestation_by_client/#client-runtime","text":"Each client C when connecting to S via TLS, C only accepts CA as valid certificate authority for the TLS certificate provided by S (by limiting the root certificates to CA or by defining name constraints for the root certificates) since the policy of S ensures that only a correctly configured service, executing inside of an enclave, can get access to a certificate Ce issued by CA , being able to establish a TLS connection to S means, that S satisfies all constraints of its security policy P_S .","title":"Client (Runtime)"},{"location":"attestation_by_client/#client-developmentdeployment","text":"When client C is built or, alternatively, during deployment time, one: attests CAS CS via CLI scone cas attest , one verifies the session of each services of application A using scone session verify , and retrieve the CA certificate from CAS via its REST API (see flask demo ).","title":"Client (Development/Deployment)"},{"location":"background/","text":"SCONE Background Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data. Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack isolated hosts or on OpenStack baremetal clouds . Problem: Defender's Dilemma Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems. A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities. For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected. To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect. SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release: Linux Lines of Code (StefanPohl, CC0, original } OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines): OpenStack Lines of Code (OpenHub original ) To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code: Docker Lines of Code (OpenHub original ) Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too. SCONE Approach The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team. A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves. Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to confidential microservices. Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.","title":"SCONE Background"},{"location":"background/#scone-background","text":"Cloud Security . The objective of SCONE is to help service providers to build secure applications for public, private or hybrid clouds. This means that the focus of SCONE is on application-oriented security and not on the security of the underlying cloud system. Of course, SCONE-based applications benefit from strong security properties of the underlying cloud because this minimizes, for example, the attack surface of SCONE-based applications and by providing higher availability. SCONE helps to ensure the security of an application, i.e., the application's integrity and confidentiality, even if the security of the underlying cloud or system software would be compromised. The security of applications is ensured with the help of Intel SGX enclaves. Workflow . SCONE combines strong security with the ease of use of Docker. SCONE supports a workflow very similar to that of Docker. It supports the construction of applications consisting of multiple containers while ensuring end-to-end encryption between all application components in the sense that all network traffic, all files and even all computation is encrypted. A service provider can ensure the confidentiality and integrity of all application data. In particular, SCONE supports the construction of applications such that no higher privileged software like the operating system or the hypervisor, nor any system administrator with root access nor cold boot attacks can gain access to application data. Side Channel Attacks . Side Channel attacks on Intel SGX are the focus of a several recent research papers. First, mounting a successful side-channels is much more difficult than just dumping the memory of an existing application. In SCONE, we provide scheduling within enclaves which makes it more difficult for an attacker to determine which core is executing what function. Moreover, we are working on a compiler extension that will harden applications against side channel attacks. Until will release this extension, a pragmatic solution would be to run applications that might be susceptible to side channel attacks either on OpenStack isolated hosts or on OpenStack baremetal clouds .","title":"SCONE Background"},{"location":"background/#problem-defenders-dilemma","text":"Traditionally, one ensures the security of an application by ensuring that the system software, i.e., the hypervisor, operating system and cloud software is trustworthy. This not only protects the integrity and confidentiality of the system data but also protects the security of the applications. A service provider running applications in the cloud must trust all system software and also all administrators who have root or physical access to these systems. A popular way to intrude into a system is to steal the credentials of a system administrator. With these root credentials, one gains access to all data being processed in this system as well as all keys that are kept in main memory or in some plain text files. If stealing credentials would be too difficult, an attacker will look for other ways to attack a system, like, exploiting known code vulnerabilities. For an attacker, it might be sufficient to exploit a single vulnerability in either the application or the system software to violate the application security. The problem is that the defenders must protect against the exploitation of all code vulnerabilities that might exist in the source code. A service provider might not have access to all source code of the system software that the cloud provider uses to operate the cloud. Even if the source code were available, this will typically be too large to be inspected. To show that this is a difficult problem, let's look at the number of lines of source code of common system software components. While lines of source code is not an ideal indicator for the number of vulnerabilities, it gives some indication of the problem we are facing. Some security researchers state that given the current state of the art, only code with up to 10,000s of lines of code can be reasonably inspected. Just the system software itself contains millions of lines of code. This is orders of magnitudes more than we can reasonably expect to be able to inspect. SCONE runs on top of Linux - which contains millions of lines of code and is still growing in size with each release: Linux Lines of Code (StefanPohl, CC0, original } OpenStack is a popular open source software to manage clouds. OpenStack - despite being relatively young - has been growing dramatically over the years that it has already reached 5 million lines of code (including comments and blank lines): OpenStack Lines of Code (OpenHub original ) To manage containers, we need an engine like Docker. Docker is younger than OpenStack but has nevertheless reached already more than 180,000 lines of code: Docker Lines of Code (OpenHub original ) Code complexity .There is no one-to-one correlation between lines of codes and bugs. Static analysis of open source code repositories indicates approximately 0.61 defects per 1,000 LOC. A recent analysis of Linux shows that, despite an increasing number of defects being fixed, there are always approximately 5,000 defects waiting to be fixed. Not all of these defects can, however, be exploited for security attacks. Another analysis found that approximately 500 security-relevant bugs were fixed in Linux over the past five years - bugs that had been in the kernel for five years before being discovered and fixed. Commercial code had a slightly higher defect density than open source projects. Hence, we need to expect vulnerabilities in commercial software too.","title":"Problem: Defender's Dilemma"},{"location":"background/#scone-approach","text":"The approach of SCONE is to partition the code and to place essential components of an application into separate enclaves. Practically, it is quite difficult to split an existing code base of a single process into one component that runs inside an enclave and a component that runs outside of an enclave. However, many modern applications - like cloud-native applications - are already partitioned in several components running in separate address spaces. These components are typically called microservices. This partitioning facilitates a more intelligent scaling of services as well as a scaling of the development team. A large application might consist of a variety of microservices. Not all microservices of an application need to run inside enclaves to protect the application\u2019s integrity and confidentiality. For example, some services might only process encrypted data, like encrypted log data, and do not need to run inside enclaves. Also, the resource manager does not need to run in an enclave either. However, we recommend that each microservice that has the credential to send requests to at least one microservice running inside an enclave, should itself also run inside of an enclave to restrict the access to confidential microservices. Current SGX-capable CPUs have a limited EPC (Extended Page Cache) size. If the working set of a microservice does not fit inside the EPC, overheads can become high. The usage of microservices supports horizontal scalability. This helps to cope with limited EPC (extended page cache) by spreading secure microservices across different hosts.","title":"SCONE Approach"},{"location":"binary_fs/","text":"SCONE Binary File System The SCONE Binary File System (Binary FS) is a mechanism with which the user can embed a file system image into an enclave binary's executable. Thereby, SGX' enclave protection mechanism is extended to the enclave's file system since the enclave's measurement (MRENCLAVE) will reflect the file system state. This is a particularly appealing solution for interpreted languages such as Python or Java where the actual program code (e.g. PyTorch or Kafka) is not part of the interpreter binary but read from byte code files in the file system. Embedding a file system into a binary means that the binary file has a dependency for a dynamic library that contains the necessary files. We create this dependency with the help of patchelf --add-needed . Such an approach provides flexibility of patching an existing binary (e.g. Python or Java) rather than a long recompilation of this binary with an object file. Note that, 1) modifications the program does to the binary fs are not persistent, after each start the binary fs is in its initial state, and 2) the binary fs replaces the entire file system, i.e. an enclave with binary fs won't be able to read or write from the host's file system. You can utilize SCONE volumes to persistently store data on the host's file system. TL;DR A fully functioning single Dockerfile which runs a Python hello_world.py program using the binary-fs could look like this: # First stage: apply the binary-fs FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 AS binary-fs RUN echo \"print('Hello World!')\" > /hello-world.py # remove object archive as very large and not necessary for executing the python program # & apply scone binaryfs with SCONE_MODE=auto, as build will not have access to /dev/isgx RUN rm /usr/lib/python3.7/config-3.7m-x86_64-linux-gnu/libpython3.7m.a && \\ SCONE_MODE = auto scone binaryfs / /binary-fs.c -v \\ --include '/usr/lib/python3.7/*' \\ --include /hello-world.py # Second stage: compile the binary fs FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine AS crosscompiler COPY --from = binary-fs /binary-fs.c /. RUN scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so # Third stage: patch the binary-fs into the enclave executable FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 COPY --from = crosscompiler /libbinary-fs.so /. RUN apk add --no-cache patchelf && \\ patchelf --add-needed libbinary-fs.so ` which python3 ` && \\ apk del patchelf ENV SCONE_HEAP = 512M ENV SCONE_LOG = debug ENV LD_LIBRARY_PATH = \"/\" CMD sh -c \"python3 /hello-world.py\" Basic Usage SCONE Binary FS is implemented as a SCONE runtime extension with which users can modify certain behavior of the SCONE runtime. The particular interface that has to be implemented for the Binary FS (as all other SCONE runtime extension interfaces) is described in the scone_rt_ext.h header file of the SCONE cross compiler installation that can be accessed in the cross-compiler container with cat /opt/scone/cross-compiler/x86_64-linux-musl/include/scone_rt_ext.h . This is the source code of a \"hello world\" example of a binary fs SCONE runtime extension: #include <stdlib.h> #include <scone_rt_ext.h> binary_fs_config_t fs_config = { . binary_fs_file_t_size = sizeof ( binary_fs_file_t ), }; binary_fs_file_t * files [] = { & ( binary_fs_file_t ){ . path = \"/farewell/english\" , . content = \"Goodbye\" , . size = 8 , }, NULL }; scone_rt_hook_status_t scone_rt_hook_binary_fs ( binary_fs_interface_version_t * version , binary_fs_config_t * config , binary_fs_file_t ** file []) { * version = BINARY_FS_VERSION_V1 ; * config = fs_config ; * file = files ; return SCONE_HOOK_STATUS_SUCCESS ; } It has to be compiled and linked into the enclave binary to make use of it: # Compilation of binary fs $ scone gcc ./binary-fs.c -shared -o ./binary-fs.so # Linking into enclave binary $ scone gcc ./print_file.c ./binary-fs.so -o print_file # Printing file that is part of the binary fs in the host file system $ cat /farewell/english cat: /farewell/english: No such file or directory # Printing the file from the enclave $ ./print_file /farewell/english Goodbye # Printing file that exists in the host file system $ cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 20 .04 DISTRIB_CODENAME = focal DISTRIB_DESCRIPTION = \"Ubuntu 20.04.1 LTS\" # But not in the binary fs $ ./print_file /etc/lsb-release No such file or directory Generating Binary FS Images from existing File Systems The SCONE CLI provides the binaryfs command with which an existing directory tree can be turned into a binary fs source code file. We illustrate the usage of this command by creating a binary fs source code image for Barbican (a python application). Within the SCONE Python image registry.scontain.com:5050/sconecuratedimages/apps:barbican-11-alpine we execute: scone binaryfs / /binary-fs.c \\ --include '/etc/barbican/*' \\ --include '/usr/lib/python3.7/*' \\ --include /lib/libssl.so.1.1 \\ --include /lib/libcrypto.so.1.1 \\ --include '/lib/libz.so.1*' \\ --include '/usr/lib/libbz2.so.1*' \\ --include '/usr/lib/libsqlite3.so.0*' \\ --include '/usr/lib/libev.so.4*' \\ --include '/usr/lib/libffi.so.6*' \\ --include '/usr/lib/libexpat.so.1*' \\ --include /start-barbican.py We must then compile the binary-fs.c file using the registry.scontain.com:5050/sconecuratedimages/crosscompilers image, as it contains the scone gcc : scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so Moving on, we move the generated libbinary-fs.so back to our original Python image. In this image, we link the .so file to the enclave binary using patchelf : apk add patchelf patchelf --add-needed libbinary-fs.so ` which python3 ` Currently, patchelf does not update the signature of the binary, although it changes the binary's contents. Thus, running the binary without further environment modification will produce an error. To ensure that we update the corresponding signature, we must set the SCONE_HEAP environment variable to a different value than it was before. The setting of this variable will prompt SCONE to recompute the signature of the executable. The enclave executable will then function properly: SCONE_HEAP = 512M python3 /start-barbican.py What Files to include in the File System To find out what files to include in the binary-fs , simply run your image with strace and identify which files the binary opens: docker run -it --rm --device /dev/isgx -e SCONE_LOG = trace --cap-add SYS_PTRACE $IMAGE sh apk add strace strace -o strace.log -f $EXECUTABLE grep \"open\" strace.log The output will then show which files the binary has accessed. Be sure to include these files in the binary fs to make your application function properly. Note that you do not have to include files that are already dependencies of your binary; i.e., do not include files that appear in ldd $(which your_binary) . Possible Networking Problems Note that the binary-fs will not have access to /etc/ files which are only available at deployment on e.g. a cluster. These files may include /etc/resolv.conf , which your service needs to access the DNS. If you are deploying your service with binary-fs to a Kubernetes cluster, include the following injection file in your policy for resolv.conf : images : - name : your_image injection_files : ... # Network files - path : /etc/resolv.conf content : | nameserver 10.96.0.10 search sgx-scone.svc.cluster.local svc.cluster.local cluster.local options ndots:5 This addresses the Cluster's CoreDNS service, with which your service will be able to address other services within the Cluster. What executables are supported Currently, only binaries compiled as position-independent code (pie) can use the binary fs. For all other binaries, applying the binary fs may result in the binary having segmentation fault errors or the binary ignoring the binary fs. To ensure your binary is compatible, execute file $(which your_binary) and check the output if it is a pie executable. To illustrate: $ file $( which python3.7 ) /usr/bin/python3.7: ELF 64 -bit LSB pie executable, x86-64, version 1 ( SYSV ) , dynamically linked, interpreter /opt/scone/lib/ld-scone-x86_64.so.1, bad note name size 0xb57f3448, bad note name size 0xb57f3448, stripped # pie executable -> binary fs compatible","title":"scone binary fs"},{"location":"binary_fs/#scone-binary-file-system","text":"The SCONE Binary File System (Binary FS) is a mechanism with which the user can embed a file system image into an enclave binary's executable. Thereby, SGX' enclave protection mechanism is extended to the enclave's file system since the enclave's measurement (MRENCLAVE) will reflect the file system state. This is a particularly appealing solution for interpreted languages such as Python or Java where the actual program code (e.g. PyTorch or Kafka) is not part of the interpreter binary but read from byte code files in the file system. Embedding a file system into a binary means that the binary file has a dependency for a dynamic library that contains the necessary files. We create this dependency with the help of patchelf --add-needed . Such an approach provides flexibility of patching an existing binary (e.g. Python or Java) rather than a long recompilation of this binary with an object file. Note that, 1) modifications the program does to the binary fs are not persistent, after each start the binary fs is in its initial state, and 2) the binary fs replaces the entire file system, i.e. an enclave with binary fs won't be able to read or write from the host's file system. You can utilize SCONE volumes to persistently store data on the host's file system.","title":"SCONE Binary File System"},{"location":"binary_fs/#tldr","text":"A fully functioning single Dockerfile which runs a Python hello_world.py program using the binary-fs could look like this: # First stage: apply the binary-fs FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 AS binary-fs RUN echo \"print('Hello World!')\" > /hello-world.py # remove object archive as very large and not necessary for executing the python program # & apply scone binaryfs with SCONE_MODE=auto, as build will not have access to /dev/isgx RUN rm /usr/lib/python3.7/config-3.7m-x86_64-linux-gnu/libpython3.7m.a && \\ SCONE_MODE = auto scone binaryfs / /binary-fs.c -v \\ --include '/usr/lib/python3.7/*' \\ --include /hello-world.py # Second stage: compile the binary fs FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine AS crosscompiler COPY --from = binary-fs /binary-fs.c /. RUN scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so # Third stage: patch the binary-fs into the enclave executable FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 COPY --from = crosscompiler /libbinary-fs.so /. RUN apk add --no-cache patchelf && \\ patchelf --add-needed libbinary-fs.so ` which python3 ` && \\ apk del patchelf ENV SCONE_HEAP = 512M ENV SCONE_LOG = debug ENV LD_LIBRARY_PATH = \"/\" CMD sh -c \"python3 /hello-world.py\"","title":"TL;DR"},{"location":"binary_fs/#basic-usage","text":"SCONE Binary FS is implemented as a SCONE runtime extension with which users can modify certain behavior of the SCONE runtime. The particular interface that has to be implemented for the Binary FS (as all other SCONE runtime extension interfaces) is described in the scone_rt_ext.h header file of the SCONE cross compiler installation that can be accessed in the cross-compiler container with cat /opt/scone/cross-compiler/x86_64-linux-musl/include/scone_rt_ext.h . This is the source code of a \"hello world\" example of a binary fs SCONE runtime extension: #include <stdlib.h> #include <scone_rt_ext.h> binary_fs_config_t fs_config = { . binary_fs_file_t_size = sizeof ( binary_fs_file_t ), }; binary_fs_file_t * files [] = { & ( binary_fs_file_t ){ . path = \"/farewell/english\" , . content = \"Goodbye\" , . size = 8 , }, NULL }; scone_rt_hook_status_t scone_rt_hook_binary_fs ( binary_fs_interface_version_t * version , binary_fs_config_t * config , binary_fs_file_t ** file []) { * version = BINARY_FS_VERSION_V1 ; * config = fs_config ; * file = files ; return SCONE_HOOK_STATUS_SUCCESS ; } It has to be compiled and linked into the enclave binary to make use of it: # Compilation of binary fs $ scone gcc ./binary-fs.c -shared -o ./binary-fs.so # Linking into enclave binary $ scone gcc ./print_file.c ./binary-fs.so -o print_file # Printing file that is part of the binary fs in the host file system $ cat /farewell/english cat: /farewell/english: No such file or directory # Printing the file from the enclave $ ./print_file /farewell/english Goodbye # Printing file that exists in the host file system $ cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 20 .04 DISTRIB_CODENAME = focal DISTRIB_DESCRIPTION = \"Ubuntu 20.04.1 LTS\" # But not in the binary fs $ ./print_file /etc/lsb-release No such file or directory","title":"Basic Usage"},{"location":"binary_fs/#generating-binary-fs-images-from-existing-file-systems","text":"The SCONE CLI provides the binaryfs command with which an existing directory tree can be turned into a binary fs source code file. We illustrate the usage of this command by creating a binary fs source code image for Barbican (a python application). Within the SCONE Python image registry.scontain.com:5050/sconecuratedimages/apps:barbican-11-alpine we execute: scone binaryfs / /binary-fs.c \\ --include '/etc/barbican/*' \\ --include '/usr/lib/python3.7/*' \\ --include /lib/libssl.so.1.1 \\ --include /lib/libcrypto.so.1.1 \\ --include '/lib/libz.so.1*' \\ --include '/usr/lib/libbz2.so.1*' \\ --include '/usr/lib/libsqlite3.so.0*' \\ --include '/usr/lib/libev.so.4*' \\ --include '/usr/lib/libffi.so.6*' \\ --include '/usr/lib/libexpat.so.1*' \\ --include /start-barbican.py We must then compile the binary-fs.c file using the registry.scontain.com:5050/sconecuratedimages/crosscompilers image, as it contains the scone gcc : scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so Moving on, we move the generated libbinary-fs.so back to our original Python image. In this image, we link the .so file to the enclave binary using patchelf : apk add patchelf patchelf --add-needed libbinary-fs.so ` which python3 ` Currently, patchelf does not update the signature of the binary, although it changes the binary's contents. Thus, running the binary without further environment modification will produce an error. To ensure that we update the corresponding signature, we must set the SCONE_HEAP environment variable to a different value than it was before. The setting of this variable will prompt SCONE to recompute the signature of the executable. The enclave executable will then function properly: SCONE_HEAP = 512M python3 /start-barbican.py","title":"Generating Binary FS Images from existing File Systems"},{"location":"binary_fs/#what-files-to-include-in-the-file-system","text":"To find out what files to include in the binary-fs , simply run your image with strace and identify which files the binary opens: docker run -it --rm --device /dev/isgx -e SCONE_LOG = trace --cap-add SYS_PTRACE $IMAGE sh apk add strace strace -o strace.log -f $EXECUTABLE grep \"open\" strace.log The output will then show which files the binary has accessed. Be sure to include these files in the binary fs to make your application function properly. Note that you do not have to include files that are already dependencies of your binary; i.e., do not include files that appear in ldd $(which your_binary) .","title":"What Files to include in the File System"},{"location":"binary_fs/#possible-networking-problems","text":"Note that the binary-fs will not have access to /etc/ files which are only available at deployment on e.g. a cluster. These files may include /etc/resolv.conf , which your service needs to access the DNS. If you are deploying your service with binary-fs to a Kubernetes cluster, include the following injection file in your policy for resolv.conf : images : - name : your_image injection_files : ... # Network files - path : /etc/resolv.conf content : | nameserver 10.96.0.10 search sgx-scone.svc.cluster.local svc.cluster.local cluster.local options ndots:5 This addresses the Cluster's CoreDNS service, with which your service will be able to address other services within the Cluster.","title":"Possible Networking Problems"},{"location":"binary_fs/#what-executables-are-supported","text":"Currently, only binaries compiled as position-independent code (pie) can use the binary fs. For all other binaries, applying the binary fs may result in the binary having segmentation fault errors or the binary ignoring the binary fs. To ensure your binary is compatible, execute file $(which your_binary) and check the output if it is a pie executable. To illustrate: $ file $( which python3.7 ) /usr/bin/python3.7: ELF 64 -bit LSB pie executable, x86-64, version 1 ( SYSV ) , dynamically linked, interpreter /opt/scone/lib/ld-scone-x86_64.so.1, bad note name size 0xb57f3448, bad note name size 0xb57f3448, stripped # pie executable -> binary fs compatible","title":"What executables are supported"},{"location":"blender/","text":"Blender Use Case Blender is an open-source software 3D creation suite that in particular, supports rendering. We show how to render images with the help of blender on a remote, untrusted host. In this use case, we show how to execute blender using simple ssh to run blender remotely: we encrypt the input file(s), ship the input files to the remote host, execute blender inside an enclave, and ship the results back to the local host. This Blender image also runs as a trusted DApp for the iExec platform . Assuming the Python scripts are located in the INPUTS directory, we can compute the output image on host faye as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python --host faye The output of this script is stored in file simple_sphere.png directory OUTPUTS . Opening this file - which was computed inside an enclave with all inputs, outputs, the scripts, and the processing being always encrypted , results in the following output. For some more examples, scroll down. Technical Background Blender is a relative large binary that is almost 60MB large. It has multiple extensions / addons implemented in Python. These extensions are dynamically loaded when they are needed. The source files of these extensions are located at different places in the filesystem: As we show below, a user can add more Python extensions that help to render images and animations. These python extensions might contain intellectual properties of the user. The confidentiality as well as the integrity of these extensions mus be protected. The blender image contains all the above files but not the user extensions. We need to ensure the integrity of the blender application as well as the extensions , i.e., an attacker cannot modify the blender image without being detected. We split the file system of the blender image in the following file regions: the root file system itself is not protected. In this unprotected part is the blender binary as well as a script to start the blender image. the /usr file region is authenticated , i.e., the integrity of all files in this region is verified by the SCONE runtime before being passed to blender . This file region is generated by the image creator - in this case, scontain.com. the /encryptedInputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. The files in this region are provided by the user: with the help of a CLI (see below), the user encrypts the files and pushes these to the host on which blender is executed. the /encryptedOutputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. This region is initially empty but is still generated by the user with the help of a CLI (see below), the client encrypts the files and pushes these to the host on which blender is executed. Secrets Management We can summarize the responsibilities of the image creator and the user as follows. The image creator ensures the integrity of blender and all extensions. The user ensures that both the confidentiality as well as the integrity of the user extensions: We should be explicit about the different roles in the context of the blender use case: the image creator encrypts the file system, i.e., defines the /usr authenticated file region and permits the user to map protected regions /encryptedInputs and /encryptedOutputs into the blender image. The image creator controls the key and tag of the root file system but must not be able to access the user-controlled protected regions. The image creator defines the expected tag of the file system as well as the expected MrEnclave . the user controls the protected regions /encryptedInputs and /encryptedOutputs . However, the user must not be able to modify the authenticated files in the root file systems. Otherwise, an attacker might be able to use this right to modify the images of other users. Moreover, the user must not be able to access the protected file regions of other users. an attacker must not be able to read the protected regions of any user nor must an attacker be able to modify the authenticated files in the root filesystem. Based on these different roles, we need to manage the keys when executing the blender image. The key and tag of the blender image must be controlled by the image creator. In particular, neither a user nor an attacker must be able to get access to this key. Otherwise, this entity might be able to modify the image - which we would actually detect with the help of the tag. However, in case of protected images to protect the intellectual property of the image creator, this entity could read the content of the filesystem. The key and tag of the protected volumes of a user must only be visible by the user itself. Note, however, that all keys and tags must be accessible by the SCONE runtime of the blender image itself. In other words, the blender image must be able to get acceas to the keys while we need to prevent the user as well as the image creator to see all keys. The SCONE CAS (Configuration and Attestation System) can manage the keys in such a way that an image can gain access to the keys of multiple entities. The main mechanism to support this is the opaque export and import of keys. The creator of the blender image can export the key of root filesystem but can restrict the export to blender that is identified by a MrEnclave as well as tag of the file system. Sessions A session is a security policy that defines secrets and who can access these secrets. The session of the image will define the key and tag and also defines if and what other sessions can access this information. In this case, we export to blender services. CAS ensures that only blender images that have the expected image tag (i.e., fspf_tag ) and enclaves with the expected MrEnclave (i.e., mrenclaves ) can access the key (i.e., fspf_key ). The session will also determine the arguments of blender. In this case, the base image defines that an python program /encryptedInputs/application.py is executed. The python program itself is defined in a volume that is provided by the user, i.e., it is not under the control of the image creator. name: scone:blender digest: create services: - name: blender image_name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] pwd: / command: blender -b -P /encryptedInputs/application.py fspf_path: /usr/fspf.pb fspf_key: $FSPF_KEY fspf_tag: $FSPF_TAG environment: SCONE_NO_MMAP_ACCESS: 1 SCONE_ALLOW_DLOPEN: 1 images: - name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs exports: - namespace: service name: blender export: \"*\" - namespace: image name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python export: \"*\" In this case, the volumes themselves are defined by the user. To simplify this process of defining the session, creating encrypted volumes and pushing these to the site at which the image is executed, we provide a simple CLI (Command Line Interface). When a user executes blender, this blender container will be started in the context of a CAS session. The session defines the actually keys and tags of the volumes that are mapped into the blender container. The actually session is derived by the CLI from a session template that looks like this: name: $SESSION digest: create services: - name: blender import: scone:blender image_name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python command: blender -b -P /encryptedInputs/application.py pwd: / volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python import: scone:blender CLI Most users, they probably do not want to learn about sessions and the details how to protect their intellectual property. They expect instead that their IP is properly protected, and it is easy to use. We provide a simple CLI to execute applications remotely. For now, the CLI expects that the application images have a certain input directory (i.e., /encryptedInputs ) and a certain output directory (i.e., /encryptedOutputs ). The CLI expects that the unencrypted input directory is mapped into the CLI container at /inputs , the unencrypted output directory is mapped into the CLI container at /decryptedOutputs 1 : , and a directory that contains some configuration data (which are automatically generated) at /conf . To remotely start a container with the help of ssh , we need to give the CLI container access via command add-host . After giving ssh access, we can start the blend applications. To do so, we can give it docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python --host faye Blender Scripting Let us show an example with an encrypted Python program that is executed on the remote site along with blender to render an image. We take an example from github by Nikolai Janakiev : This Python program is unmodified an will be shipped and run in an encrypted fashion on the remote host that executes the blender image. While this is probably not necessary for open source code to protect its confidentiality , it is important to protect its integrity : if an attacker could change the code, it could get access to all data of the image and could modify the results. In general, we want to protect both the integrity as well as the confidentiality of the Python code. import bpy from math import pi from mathutils import Euler tau = 2 * pi # Check if script is opened in Blender program import os , sys if ( bpy . context . space_data == None ): cwd = os . path . dirname ( os . path . abspath ( __file__ )) else : cwd = os . path . dirname ( bpy . context . space_data . text . filepath ) # Get folder of script and add current working directory to path sys . path . append ( cwd ) import utils def createSphere ( origin = ( 0 , 0 , 0 )): # Create icosphere bpy . ops . mesh . primitive_ico_sphere_add ( location = origin ) obj = bpy . context . object return obj if __name__ == '__main__' : # Remove all elements utils . removeAll () # Create camera bpy . ops . object . add ( type = 'CAMERA' , location = ( 0 , - 3.5 , 0 )) cam = bpy . context . object cam . rotation_euler = Euler (( pi / 2 , 0 , 0 ), 'XYZ' ) # Make this the current camera bpy . context . scene . camera = cam # Create lamps utils . rainbowLights () # Create object and its material sphere = createSphere () utils . setSmooth ( sphere , 3 ) # Specify folder to save rendering render_folder = os . path . join ( cwd , '../encryptedOutputs' ) if ( not os . path . exists ( render_folder )): os . mkdir ( render_folder ) # Render image rnd = bpy . data . scenes [ 'Scene' ] . render rnd . resolution_x = 500 rnd . resolution_y = 500 rnd . resolution_percentage = 100 rnd . filepath = os . path . join ( render_folder , 'simple_sphere.png' ) bpy . ops . render . render ( write_still = True ) We have to copy this scripts - together with some the utils module - in folder INPUTS . This folder will be encrypted and pushed to remote host which is given by argument --host ALIAS . We can execute this Python script on host faye as shown above. More Examples We rendered a few more examples by Nikolai Janakiev in an always encrypted fashion: This will eventually be changed to /outputs . \u21a9","title":"Blender"},{"location":"blender/#blender-use-case","text":"Blender is an open-source software 3D creation suite that in particular, supports rendering. We show how to render images with the help of blender on a remote, untrusted host. In this use case, we show how to execute blender using simple ssh to run blender remotely: we encrypt the input file(s), ship the input files to the remote host, execute blender inside an enclave, and ship the results back to the local host. This Blender image also runs as a trusted DApp for the iExec platform . Assuming the Python scripts are located in the INPUTS directory, we can compute the output image on host faye as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python --host faye The output of this script is stored in file simple_sphere.png directory OUTPUTS . Opening this file - which was computed inside an enclave with all inputs, outputs, the scripts, and the processing being always encrypted , results in the following output. For some more examples, scroll down.","title":"Blender Use Case"},{"location":"blender/#technical-background","text":"Blender is a relative large binary that is almost 60MB large. It has multiple extensions / addons implemented in Python. These extensions are dynamically loaded when they are needed. The source files of these extensions are located at different places in the filesystem: As we show below, a user can add more Python extensions that help to render images and animations. These python extensions might contain intellectual properties of the user. The confidentiality as well as the integrity of these extensions mus be protected. The blender image contains all the above files but not the user extensions. We need to ensure the integrity of the blender application as well as the extensions , i.e., an attacker cannot modify the blender image without being detected. We split the file system of the blender image in the following file regions: the root file system itself is not protected. In this unprotected part is the blender binary as well as a script to start the blender image. the /usr file region is authenticated , i.e., the integrity of all files in this region is verified by the SCONE runtime before being passed to blender . This file region is generated by the image creator - in this case, scontain.com. the /encryptedInputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. The files in this region are provided by the user: with the help of a CLI (see below), the user encrypts the files and pushes these to the host on which blender is executed. the /encryptedOutputs file region is protected , i.e., both the integrity as well as the confidentiality of files in this region are ensured. This region is initially empty but is still generated by the user with the help of a CLI (see below), the client encrypts the files and pushes these to the host on which blender is executed.","title":"Technical Background"},{"location":"blender/#secrets-management","text":"We can summarize the responsibilities of the image creator and the user as follows. The image creator ensures the integrity of blender and all extensions. The user ensures that both the confidentiality as well as the integrity of the user extensions: We should be explicit about the different roles in the context of the blender use case: the image creator encrypts the file system, i.e., defines the /usr authenticated file region and permits the user to map protected regions /encryptedInputs and /encryptedOutputs into the blender image. The image creator controls the key and tag of the root file system but must not be able to access the user-controlled protected regions. The image creator defines the expected tag of the file system as well as the expected MrEnclave . the user controls the protected regions /encryptedInputs and /encryptedOutputs . However, the user must not be able to modify the authenticated files in the root file systems. Otherwise, an attacker might be able to use this right to modify the images of other users. Moreover, the user must not be able to access the protected file regions of other users. an attacker must not be able to read the protected regions of any user nor must an attacker be able to modify the authenticated files in the root filesystem. Based on these different roles, we need to manage the keys when executing the blender image. The key and tag of the blender image must be controlled by the image creator. In particular, neither a user nor an attacker must be able to get access to this key. Otherwise, this entity might be able to modify the image - which we would actually detect with the help of the tag. However, in case of protected images to protect the intellectual property of the image creator, this entity could read the content of the filesystem. The key and tag of the protected volumes of a user must only be visible by the user itself. Note, however, that all keys and tags must be accessible by the SCONE runtime of the blender image itself. In other words, the blender image must be able to get acceas to the keys while we need to prevent the user as well as the image creator to see all keys. The SCONE CAS (Configuration and Attestation System) can manage the keys in such a way that an image can gain access to the keys of multiple entities. The main mechanism to support this is the opaque export and import of keys. The creator of the blender image can export the key of root filesystem but can restrict the export to blender that is identified by a MrEnclave as well as tag of the file system.","title":"Secrets Management"},{"location":"blender/#sessions","text":"A session is a security policy that defines secrets and who can access these secrets. The session of the image will define the key and tag and also defines if and what other sessions can access this information. In this case, we export to blender services. CAS ensures that only blender images that have the expected image tag (i.e., fspf_tag ) and enclaves with the expected MrEnclave (i.e., mrenclaves ) can access the key (i.e., fspf_key ). The session will also determine the arguments of blender. In this case, the base image defines that an python program /encryptedInputs/application.py is executed. The python program itself is defined in a volume that is provided by the user, i.e., it is not under the control of the image creator. name: scone:blender digest: create services: - name: blender image_name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] pwd: / command: blender -b -P /encryptedInputs/application.py fspf_path: /usr/fspf.pb fspf_key: $FSPF_KEY fspf_tag: $FSPF_TAG environment: SCONE_NO_MMAP_ACCESS: 1 SCONE_ALLOW_DLOPEN: 1 images: - name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python mrenclaves: [$MRENCLAVE] tags: [demo] volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs exports: - namespace: service name: blender export: \"*\" - namespace: image name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python export: \"*\" In this case, the volumes themselves are defined by the user. To simplify this process of defining the session, creating encrypted volumes and pushing these to the site at which the image is executed, we provide a simple CLI (Command Line Interface). When a user executes blender, this blender container will be started in the context of a CAS session. The session defines the actually keys and tags of the volumes that are mapped into the blender container. The actually session is derived by the CLI from a session template that looks like this: name: $SESSION digest: create services: - name: blender import: scone:blender image_name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python command: blender -b -P /encryptedInputs/application.py pwd: / volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python import: scone:blender","title":"Sessions"},{"location":"blender/#cli","text":"Most users, they probably do not want to learn about sessions and the details how to protect their intellectual property. They expect instead that their IP is properly protected, and it is easy to use. We provide a simple CLI to execute applications remotely. For now, the CLI expects that the application images have a certain input directory (i.e., /encryptedInputs ) and a certain output directory (i.e., /encryptedOutputs ). The CLI expects that the unencrypted input directory is mapped into the CLI container at /inputs , the unencrypted output directory is mapped into the CLI container at /decryptedOutputs 1 : , and a directory that contains some configuration data (which are automatically generated) at /conf . To remotely start a container with the help of ssh , we need to give the CLI container access via command add-host . After giving ssh access, we can start the blend applications. To do so, we can give it docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application registry.scontain.com:5050/sconecuratedimages/iexecsgx:blender_python --host faye","title":"CLI"},{"location":"blender/#blender-scripting","text":"Let us show an example with an encrypted Python program that is executed on the remote site along with blender to render an image. We take an example from github by Nikolai Janakiev : This Python program is unmodified an will be shipped and run in an encrypted fashion on the remote host that executes the blender image. While this is probably not necessary for open source code to protect its confidentiality , it is important to protect its integrity : if an attacker could change the code, it could get access to all data of the image and could modify the results. In general, we want to protect both the integrity as well as the confidentiality of the Python code. import bpy from math import pi from mathutils import Euler tau = 2 * pi # Check if script is opened in Blender program import os , sys if ( bpy . context . space_data == None ): cwd = os . path . dirname ( os . path . abspath ( __file__ )) else : cwd = os . path . dirname ( bpy . context . space_data . text . filepath ) # Get folder of script and add current working directory to path sys . path . append ( cwd ) import utils def createSphere ( origin = ( 0 , 0 , 0 )): # Create icosphere bpy . ops . mesh . primitive_ico_sphere_add ( location = origin ) obj = bpy . context . object return obj if __name__ == '__main__' : # Remove all elements utils . removeAll () # Create camera bpy . ops . object . add ( type = 'CAMERA' , location = ( 0 , - 3.5 , 0 )) cam = bpy . context . object cam . rotation_euler = Euler (( pi / 2 , 0 , 0 ), 'XYZ' ) # Make this the current camera bpy . context . scene . camera = cam # Create lamps utils . rainbowLights () # Create object and its material sphere = createSphere () utils . setSmooth ( sphere , 3 ) # Specify folder to save rendering render_folder = os . path . join ( cwd , '../encryptedOutputs' ) if ( not os . path . exists ( render_folder )): os . mkdir ( render_folder ) # Render image rnd = bpy . data . scenes [ 'Scene' ] . render rnd . resolution_x = 500 rnd . resolution_y = 500 rnd . resolution_percentage = 100 rnd . filepath = os . path . join ( render_folder , 'simple_sphere.png' ) bpy . ops . render . render ( write_still = True ) We have to copy this scripts - together with some the utils module - in folder INPUTS . This folder will be encrypted and pushed to remote host which is given by argument --host ALIAS . We can execute this Python script on host faye as shown above.","title":"Blender Scripting"},{"location":"blender/#more-examples","text":"We rendered a few more examples by Nikolai Janakiev in an always encrypted fashion: This will eventually be changed to /outputs . \u21a9","title":"More Examples"},{"location":"buildingApps/","text":"Building SCONE-based applications SCONE supports running applications written in common programming languages inside of Intel SGX enclaves without source code changes . These languages include compiled languages like C, Rust, C++, GO, and Fortran and interpreted / just-in-time languages like Python and Java . For compiled languages, our recommend approach to run an application with SCONE is as follows: Use of precompiled binary: For many common applications like nginx and memcached , we already support a curated image image on registry.scontain.com . Ask us for help : if a standard application is not yet a curated image on registry.scontain.com , send us an email to see if we can help you with this. Cross-compile : you can cross-compile your application with the help of the SCONE cross-compilers, for example, have a look how to compile C programs , and No Cross-Compilation: , you can run native Alpine-Linux applications inside of enclaves without recompilation . Use Cross-Compilation instead of native compilation While SCONE supports executing programs without recompilations for Alpine Linux, we recommend to always cross-compile : The interface to the operating system needs to be replaced, i.e., libc . Hence, one needs not only to provide the same version of libc but one must ensure that all bits are represented in the same way as in the native libc. This is difficult to achieve and better left to the compiler. For stability , we therefore recommend cross-compilation since the compiler checks that all the dependencies have the matching versions, all data types are bit compatible and includes the correct libraries statically in the binary. In this way, an application will have a unique and known MrEnclave .","title":"(No) Cross-Compilation"},{"location":"buildingApps/#building-scone-based-applications","text":"SCONE supports running applications written in common programming languages inside of Intel SGX enclaves without source code changes . These languages include compiled languages like C, Rust, C++, GO, and Fortran and interpreted / just-in-time languages like Python and Java . For compiled languages, our recommend approach to run an application with SCONE is as follows: Use of precompiled binary: For many common applications like nginx and memcached , we already support a curated image image on registry.scontain.com . Ask us for help : if a standard application is not yet a curated image on registry.scontain.com , send us an email to see if we can help you with this. Cross-compile : you can cross-compile your application with the help of the SCONE cross-compilers, for example, have a look how to compile C programs , and No Cross-Compilation: , you can run native Alpine-Linux applications inside of enclaves without recompilation . Use Cross-Compilation instead of native compilation While SCONE supports executing programs without recompilations for Alpine Linux, we recommend to always cross-compile : The interface to the operating system needs to be replaced, i.e., libc . Hence, one needs not only to provide the same version of libc but one must ensure that all bits are represented in the same way as in the native libc. This is difficult to achieve and better left to the compiler. For stability , we therefore recommend cross-compilation since the compiler checks that all the dependencies have the matching versions, all data types are bit compatible and includes the correct libraries statically in the binary. In this way, an application will have a unique and known MrEnclave .","title":"Building SCONE-based applications"},{"location":"cas-api/","text":"NOTE Sorry, this is not yet part of the online documentation.","title":"NOTE"},{"location":"cas-api/#note","text":"Sorry, this is not yet part of the online documentation.","title":"NOTE"},{"location":"cas_blender_example/","text":"Posting a Session We show how to interact with CAS s with the help of curl - this might be helpful during development since it simplifies quick tests. We provide a scone command line interface that can be executed inside of an enclave itself. It can perform an attestation of CAS as well as creating and verifying policies. Hence, we recommend to use the scone CLI . We assume that you already started a CAS instance and a LAS instance on your local host. Alternatively, you can use our public CAS instance at domain scone-cas.cf . Hence, we set the address of CAS as follows: export SCONE_CAS_ADDR = 127 .0.0.1 If you use one of our public CAS instance , set it as follows: export SCONE_CAS_ADDR = scone-cas.cf Client Certificate To interact with CAS, we need to create a client certificate. When we create a session, it is associated with the client certificate of the creator. Any access to this session requires that the client knows the private key of the client certificate. Let's create a client certificate without a password. Note that you would typically add a password! mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Hello World Session Let's create a minimal session: cat > session.yml <<EOF name: blender digest: create services: - name: application image_name: registry.scontain.com:5050/sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: / environment: SCONE_MODE: hw images: - name: registry.scontain.com:5050/sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] tags: [demo] EOF We can now upload the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar like this: Created Session[id=00ed7ade-bba6-4d43-9135-51d0ca2da9ba, name=blender, status=Pending] Session already exists If the session with name \"blender\" already exists - which will be the case when you use scone-cas.cf - the following error message is issued: Could not create successor session. Invalid previous session digest: ... In case the session with name blender already exists, you must chose a different session name. We can read the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key https:// $SCONE_CAS_ADDR :8081/session/blender This will result in an output like this: --- name: blender digest: 313c6c3b824f0a560c445c8ef0cf69781345aae753bdbeaedbfff15c5a348099 board_members: [] board_policy: minimum: 0 timeout: 30 images: - name: \"registry.scontain.com:5050/sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 tags: - demo services: - name: application image_name: \"registry.scontain.com:5050/sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 environment: SCONE_MODE: hw command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: /","title":"Posting Sessions"},{"location":"cas_blender_example/#posting-a-session","text":"We show how to interact with CAS s with the help of curl - this might be helpful during development since it simplifies quick tests. We provide a scone command line interface that can be executed inside of an enclave itself. It can perform an attestation of CAS as well as creating and verifying policies. Hence, we recommend to use the scone CLI . We assume that you already started a CAS instance and a LAS instance on your local host. Alternatively, you can use our public CAS instance at domain scone-cas.cf . Hence, we set the address of CAS as follows: export SCONE_CAS_ADDR = 127 .0.0.1 If you use one of our public CAS instance , set it as follows: export SCONE_CAS_ADDR = scone-cas.cf","title":"Posting a Session"},{"location":"cas_blender_example/#client-certificate","text":"To interact with CAS, we need to create a client certificate. When we create a session, it is associated with the client certificate of the creator. Any access to this session requires that the client knows the private key of the client certificate. Let's create a client certificate without a password. Note that you would typically add a password! mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi","title":"Client Certificate"},{"location":"cas_blender_example/#hello-world-session","text":"Let's create a minimal session: cat > session.yml <<EOF name: blender digest: create services: - name: application image_name: registry.scontain.com:5050/sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: / environment: SCONE_MODE: hw images: - name: registry.scontain.com:5050/sconecuratedimages/iexec:blender mrenclaves: [96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2] tags: [demo] EOF We can now upload the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar like this: Created Session[id=00ed7ade-bba6-4d43-9135-51d0ca2da9ba, name=blender, status=Pending] Session already exists If the session with name \"blender\" already exists - which will be the case when you use scone-cas.cf - the following error message is issued: Could not create successor session. Invalid previous session digest: ... In case the session with name blender already exists, you must chose a different session name. We can read the session as follows: curl -k -s --cert conf/client.crt --key conf/client-key.key https:// $SCONE_CAS_ADDR :8081/session/blender This will result in an output like this: --- name: blender digest: 313c6c3b824f0a560c445c8ef0cf69781345aae753bdbeaedbfff15c5a348099 board_members: [] board_policy: minimum: 0 timeout: 30 images: - name: \"registry.scontain.com:5050/sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 tags: - demo services: - name: application image_name: \"registry.scontain.com:5050/sconecuratedimages/iexec:blender\" mrenclaves: - 96936b6760d1f59b18f2c1a3fa2be205a91d6667dfc6635e8d0bbc1687bc03f2 environment: SCONE_MODE: hw command: blender -b /encryptedInputs/iexec-rlc.blend -o /encryptedOutputs/ -f 1 pwd: /","title":"Hello World Session"},{"location":"cas_intro/","text":"SCONE CAS Concepts Objectives SCONE CAS (Configuration and Attestation Service) helps to address the following questions: How to provide services with secrets that do not yet have any secrets to authenticate themselves? (see how to provision secret GREETING in our Kubernetes Hello World! Tutorial ) How to inject secrets inside of files and services without any human ever being able to see these secrets? (see how to inject file /app/key.pem in our Kubernetes Hello World! Tutorial ) How to attest and executed encrypted code? (see how to run and attest encrypted Python code in our Kubernetes Hello World! Tutorial ) How to share secrets across several SCONE-based services? (see secret sharing ) How to manage secrets in case one cannot trust any individuals, i.e., how to protect against insider attacks? How to import external secrets into a system without any human ever being able to see these secrets? Secrets Secrets are managed by a security policy . A security policy can generate secrets and can define which programs can access a given secret. Since a security policy can change over time, we use the term session to denote such a sequence of instances of a security policies. CAS distinguishes between the following kinds of secrets: Internal: An internal secret is generated by CAS on behalf of a CAS security policy . These secrets are created by CAS and the access to these is controlled by the security policy. A security policy could, for example, enforce that a secret is never seen by a human. Imported: A client can import secrets into a session from other local sessions or from remote sessions, i.e., sessions belonging to a remote CAS service. Such secrets are called imported secrets . The security policy associated with an internal secret will still be enforced after the secret was imported. External: External secrets are secrets that have been created by external entities, and that CAS maintains. The external secrets can be securely imported into CAS without them ever being visible by humans. CAS Features Protect all secrets from being visible by humans: CAS can help to ensure that secrets are only visible inside trusted execution environments ( TEEs ), in particular Intel SGX. This is not only true for internal and imported secrets, but CAS also supports secure import of external secrets , such that the external secrets never are visible to humans, neither during nor after the import. CAS does not require application source code changes: CAS permits injecting secrets into an application, e.g., into its configuration files, into its source files of scripting languages like Python and JavaScript, or into its compiled binaries. Due to this injection of secrets, no changes need to be made to any of the source code of the application. To ensure that the secrets are not exposed during the injection, the secrets are only injected inside of the TEE where the application is executed. The injection is completely transparent to the application. CAS can be a managed service: One can delegate the operations of CAS to an untrusted cloud provider and still ensure the confidentiality , integrity and freshness of the secrets - despite having a comprehensive threat model . Comprehensive threat model: CAS does not need to trust root users, it actually does not trust any individual user. Hence, it also cannot trust the operating system or any input or output. However, a session can trust that there exists sufficiently many users with trusted devices. While we do not know in general who to trust, a session might define that it trusts that out of a group of F+1 users, at least one is honest and uses an uncompromised device. We refer to this group as the policy board. Each session can define its policy board and a value of F such that at least F+1 members of the policy board must approve any changes of the security policy of this session. Advanced Governance: Policy creations and changes must be approved by a policy board . The policy board can consist of humans as well as programs. We refer to these programs as automated policy checkers . Secure Policy Checking: Policy checkers can be used to, for example, approve simple policy changes automatically or to perform careful checking of a policy change and reject policy mistakes that a human board member (who might have a very limited time budget) might miss. The policy checkers - as well as all other components of CAS - run inside of individual trusted execution environments . If one assumes that the likelihood of wrong executions of such a policy checker is negligible, one can assign these policy checkers the power to approve or alternatively, to reject a policy change single-handedly. Alternatively, one can require that a minimum number of approvals are necessary. Secure External Extensions: Key store services (e.g., Barbican) provide plugins to support extensions. The issue with plugins is that a plugin might modify the behavior of a key store. Also, plugins would change the code signature of CAS (if they were loaded during startup). Instead of integrating extensions into CAS, we permit external additions, e.g., to import external secrets in a secure way into CAS.","title":"Concepts"},{"location":"cas_intro/#scone-cas-concepts","text":"","title":"SCONE CAS Concepts"},{"location":"cas_intro/#objectives","text":"SCONE CAS (Configuration and Attestation Service) helps to address the following questions: How to provide services with secrets that do not yet have any secrets to authenticate themselves? (see how to provision secret GREETING in our Kubernetes Hello World! Tutorial ) How to inject secrets inside of files and services without any human ever being able to see these secrets? (see how to inject file /app/key.pem in our Kubernetes Hello World! Tutorial ) How to attest and executed encrypted code? (see how to run and attest encrypted Python code in our Kubernetes Hello World! Tutorial ) How to share secrets across several SCONE-based services? (see secret sharing ) How to manage secrets in case one cannot trust any individuals, i.e., how to protect against insider attacks? How to import external secrets into a system without any human ever being able to see these secrets?","title":"Objectives"},{"location":"cas_intro/#secrets","text":"Secrets are managed by a security policy . A security policy can generate secrets and can define which programs can access a given secret. Since a security policy can change over time, we use the term session to denote such a sequence of instances of a security policies. CAS distinguishes between the following kinds of secrets: Internal: An internal secret is generated by CAS on behalf of a CAS security policy . These secrets are created by CAS and the access to these is controlled by the security policy. A security policy could, for example, enforce that a secret is never seen by a human. Imported: A client can import secrets into a session from other local sessions or from remote sessions, i.e., sessions belonging to a remote CAS service. Such secrets are called imported secrets . The security policy associated with an internal secret will still be enforced after the secret was imported. External: External secrets are secrets that have been created by external entities, and that CAS maintains. The external secrets can be securely imported into CAS without them ever being visible by humans.","title":"Secrets"},{"location":"cas_intro/#cas-features","text":"Protect all secrets from being visible by humans: CAS can help to ensure that secrets are only visible inside trusted execution environments ( TEEs ), in particular Intel SGX. This is not only true for internal and imported secrets, but CAS also supports secure import of external secrets , such that the external secrets never are visible to humans, neither during nor after the import. CAS does not require application source code changes: CAS permits injecting secrets into an application, e.g., into its configuration files, into its source files of scripting languages like Python and JavaScript, or into its compiled binaries. Due to this injection of secrets, no changes need to be made to any of the source code of the application. To ensure that the secrets are not exposed during the injection, the secrets are only injected inside of the TEE where the application is executed. The injection is completely transparent to the application. CAS can be a managed service: One can delegate the operations of CAS to an untrusted cloud provider and still ensure the confidentiality , integrity and freshness of the secrets - despite having a comprehensive threat model . Comprehensive threat model: CAS does not need to trust root users, it actually does not trust any individual user. Hence, it also cannot trust the operating system or any input or output. However, a session can trust that there exists sufficiently many users with trusted devices. While we do not know in general who to trust, a session might define that it trusts that out of a group of F+1 users, at least one is honest and uses an uncompromised device. We refer to this group as the policy board. Each session can define its policy board and a value of F such that at least F+1 members of the policy board must approve any changes of the security policy of this session. Advanced Governance: Policy creations and changes must be approved by a policy board . The policy board can consist of humans as well as programs. We refer to these programs as automated policy checkers . Secure Policy Checking: Policy checkers can be used to, for example, approve simple policy changes automatically or to perform careful checking of a policy change and reject policy mistakes that a human board member (who might have a very limited time budget) might miss. The policy checkers - as well as all other components of CAS - run inside of individual trusted execution environments . If one assumes that the likelihood of wrong executions of such a policy checker is negligible, one can assign these policy checkers the power to approve or alternatively, to reject a policy change single-handedly. Alternatively, one can require that a minimum number of approvals are necessary. Secure External Extensions: Key store services (e.g., Barbican) provide plugins to support extensions. The issue with plugins is that a plugin might modify the behavior of a key store. Also, plugins would change the code signature of CAS (if they were loaded during startup). Instead of integrating extensions into CAS, we permit external additions, e.g., to import external secrets in a secure way into CAS.","title":"CAS Features"},{"location":"community_version/","text":"SCONE Community Version The SCONE community version provides you access to most features of the SCONE platform. The community versions can help you to see if the SCONE Confidential Computing Platform is the appropriate choice for your application. You can also develop with the help of the SCONE community edition. The community is not intended for production and is provided as is . The commercial SCONE versions provide you not only support but also more features to improve the security of SCONE-based applications, and the ease of use to build and install applications. For example, tuning SCONE-based applications requires skills since there are multiple tuning parameters. We provide an application tuner as part of the SCONE commercial offering. intended for production and development . In the future, we plan to describe also the commercial features on this website. Getting access to the Community Version Just register a free account on gitlab.scontain.com .","title":"Community Version"},{"location":"community_version/#scone-community-version","text":"The SCONE community version provides you access to most features of the SCONE platform. The community versions can help you to see if the SCONE Confidential Computing Platform is the appropriate choice for your application. You can also develop with the help of the SCONE community edition. The community is not intended for production and is provided as is . The commercial SCONE versions provide you not only support but also more features to improve the security of SCONE-based applications, and the ease of use to build and install applications. For example, tuning SCONE-based applications requires skills since there are multiple tuning parameters. We provide an application tuner as part of the SCONE commercial offering. intended for production and development . In the future, we plan to describe also the commercial features on this website. Getting access to the Community Version Just register a free account on gitlab.scontain.com .","title":"SCONE Community Version"},{"location":"config/","text":"NOTE Sorry, this is not yet part of the online documentation.","title":"NOTE"},{"location":"config/#note","text":"Sorry, this is not yet part of the online documentation.","title":"NOTE"},{"location":"curated_images/","text":"SCONE Curated Images For simplicity, we maintain a set of curated images of popular applications: Application Name barbican registry.scontain.com:5050/sconecuratedimages/apps:barbican-alpine jdk registry.scontain.com:5050/sconecuratedimages/apps:openjdk-8-alpine jdk registry.scontain.com:5050/sconecuratedimages/apps:openjdk-11-alpine jdk registry.scontain.com:5050/sconecuratedimages/apps:openjdk-15-alpine lua registry.scontain.com:5050/sconecuratedimages/apps:lua-alpine mariadb registry.scontain.com:5050/sconecuratedimages/apps:mariadb-alpine maxscale registry.scontain.com:5050/sconecuratedimages/apps:maxscale-alpine memcached registry.scontain.com:5050/sconecuratedimages/apps:memcached-alpine mongodb registry.scontain.com:5050/sconecuratedimages/apps:mongodb-alpine nginx registry.scontain.com:5050/sconecuratedimages/apps:nginx-alpine node registry.scontain.com:5050/sconecuratedimages/apps:node-8-alpine node registry.scontain.com:5050/sconecuratedimages/apps:node-10-alpine pypy registry.scontain.com:5050/sconecuratedimages/apps:pypy3-6.0.0-alpine3.7 python registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 R registry.scontain.com:5050/sconecuratedimages/apps:R redis registry.scontain.com:5050/sconecuratedimages/apps:redis-6-alpine vault registry.scontain.com:5050/sconecuratedimages/apps:vault-alpine zookeeper registry.scontain.com:5050/sconecuratedimages/apps:zookeeper-alpine We support various versions of these images. Please read about Semantic Versioning to learn about the image names that you want to use. If any open-source application is not yet listed as a curated image, send us an email and we might be able to help you.","title":"Curated Images"},{"location":"curated_images/#scone-curated-images","text":"For simplicity, we maintain a set of curated images of popular applications: Application Name barbican registry.scontain.com:5050/sconecuratedimages/apps:barbican-alpine jdk registry.scontain.com:5050/sconecuratedimages/apps:openjdk-8-alpine jdk registry.scontain.com:5050/sconecuratedimages/apps:openjdk-11-alpine jdk registry.scontain.com:5050/sconecuratedimages/apps:openjdk-15-alpine lua registry.scontain.com:5050/sconecuratedimages/apps:lua-alpine mariadb registry.scontain.com:5050/sconecuratedimages/apps:mariadb-alpine maxscale registry.scontain.com:5050/sconecuratedimages/apps:maxscale-alpine memcached registry.scontain.com:5050/sconecuratedimages/apps:memcached-alpine mongodb registry.scontain.com:5050/sconecuratedimages/apps:mongodb-alpine nginx registry.scontain.com:5050/sconecuratedimages/apps:nginx-alpine node registry.scontain.com:5050/sconecuratedimages/apps:node-8-alpine node registry.scontain.com:5050/sconecuratedimages/apps:node-10-alpine pypy registry.scontain.com:5050/sconecuratedimages/apps:pypy3-6.0.0-alpine3.7 python registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 R registry.scontain.com:5050/sconecuratedimages/apps:R redis registry.scontain.com:5050/sconecuratedimages/apps:redis-6-alpine vault registry.scontain.com:5050/sconecuratedimages/apps:vault-alpine zookeeper registry.scontain.com:5050/sconecuratedimages/apps:zookeeper-alpine We support various versions of these images. Please read about Semantic Versioning to learn about the image names that you want to use. If any open-source application is not yet listed as a curated image, send us an email and we might be able to help you.","title":"SCONE Curated Images"},{"location":"dapps/","text":"Trusted DApps with SCONE and iExec This tutorial shows how to build a trusted DApp for the iExec platform . A DApp is a decentralized application that can be executed on hosts of a decentralized network. To learn about basic terminology and concepts of SCONE, we recommend that you read the preceding tutorial pages first. Note that SCONE supports most common programming languages: please have a look at our examples for C , C++ , Fortran , GO , Rust , Python , Java , and JavaScript/Node.js . In this tutorial, we show how to build a trusted DApp using C. The transfer service used by this use case is not operational anymore To try this example, you would need to replace the the \"transfer service\" by a more reliable way to transfer files. Eventually, we will rework this example to use a reliable service to transfer files. For now, we leave it online since it demonstrate some of the features of SCONE. Also, maybe somebody will send us a pull request for using github instead of tansfer.sh ? CLI Trusted DApp SCONE helps developers to protect the integrity as well as the confidentiality not only of data at rest (i.e., on disk) and data being transmitted but also all data stored in main memory . Note that any root user can easily dump the main memory of any application and in this way might gain secrets like the keys used to encrypt the data at rest. By trusted DApp , we refer to a DApp that executes inside a trusted execution environment (TEE) in such a way that not even an attacker with root rights can access the data of the application. SCONE simplifies to run applications inside of TEEs as well as to transparently encrypt and decrypt files. SCONE is designed to be independent of the actual TEE implementation. Right now -- since there is a lack of appropriate hardware alternatives -- SCONE supports only Intel SGX . SCONE Platform The SCONE platform helps to run DApps inside of TEEs without source code modifications. In this tutorial, we describe how to build a simple application to run with SCONE inside of an enclave , i.e., inside an Intel SGX TEE, show how to compile the simple trusted DApp with the help of an Alpine container, how to deploy and execute inside of an Alpine container, introduce the workflow of where and how data is encrypted, pushed to the application and then decrypted explain how keys are managed and passed to a trusted DApp, and show how to test this application on your own infrastructure. Trusted copy DApp We show how to implement a simple copy command as a trusted DApp. This is of course not a realistic DApp but it helps us to show all steps necessary to build and run trusted DApps. The copy DApp copies files located in an input directory /INPUTS to an output directory /OUTPUTS of the client the machine. With the help of SCONE, this copy DApp actually copies encrypted files such that the SCONE transparently decrypts and checks the integrity of files that are read by the application. This protects the confidentiality and integrity of the files without any need to change the application itself. Also, SCONE encrypts and integrity protects the files that are written by the application. Each file is encrypted with a random key generated and managed by SCONE inside the enclave. With the help of a CLI (command line interface deployed in a container, see below in CLIs , a client can encrypt and push files stored on its client machine to the copy DApp. The copy DApp copies the files. The CLI can decrypt the files after the execution of the DApp. We introduce the commands to perform this example below. Encrypted Volumes Containers support volumes as a way to persist data. Trusted DApps for the iExec platform have always two encrypted volumes mapped into their file system: an encrypted volume for input files mapped in the container in directory /encryptedInputs , and an encrypted volume for output files /encryptedOutputs . We show below how the input files are encrypted with the help of the CLI and pushed to the container. Moreover, we show how to decrypt the files with the help of the CLI. To access an encrypted volume, one needs a key and a tag . The key is used to encrypt the individual keys of the files of the volume, i.e., it is used to protect the confidentiality of the files. The tag describes the current state of the volume. Any change of any file in the volume will result in a new tag . The tag is used to protect the integrity and freshness of the files: any unauthorized modification of the volume will be detected since the correct tag can only be computed by parties knowing the key of the volume ( integrity protection ). any rollback to older version of the volume will be correctly encrypted but will have an old tag. Hence, rollbacks to older versions will be detected ( freshness protection ). Key and Tag Management The copy DApp requires access to keys and tags to read the encrypted input files (i.e., the input volume) and to write the encrypted output files (i.e., the output volume). Passing the keys and tags to the copy DApp is non-trivial since we need to ensure that no other DApp nor any attacker can read or modify the keys or the tags. To pass the keys and tags to a trusted DApp, SCONE provides a configuration and attestation service (CAS) . The CAS can ensure that only the copy DApp can access the keys and tags by verifying that it talks to an unmodified copy DApp . This process is called attestation and is performed with the help of the Intel SGX CPU extension. A trusted DApp is deployed with the help of a container image hosted on docker hub or some similar repository. We recommend that these container images are signed using Docker content trust . Note that content trust is not sufficient to ensure that we talk to an unmodified copy DApp since, for example, an attacker with root access could change this image on the host on which it is deployed. To establish trust between two communication partners, they typically authenticate each other with the help of TLS . An entity needs to know a private key to authenticate itself. When a trusted DApp like the copy DApp starts up it does not know such a private key yet. Note that any key that might be stored in plain text in the image, could be read by any entity that is permitted to access that image. One could encrypt the private key in the image but then the trust DApps would need to know the key to decrypt the private key. Note that the code that runs inside of an enclave, is in general not encrypted. Hence, one cannot store the private key inside of the code either. Attestation - unlike authentication - does not need a private key. It ensures that the correct code is executing. To distinguish between different instances of a trusted DApp, on startup, the SCONE runtime generates a random private/public key pair inside of the enclave. This means that the private key can be kept secret inside of the enclave. This public key is used to identify this instance and one can perform TLS authentication to ensure that one talks to a specific instance. To attest a trusted DApp, we need to describe the state of this DApp. In general, this consists of a hash value that describes the trusted DApp (called MrEnclave ) and a tag that describes the file systems state. In the case of the copy DApp, the file system (mounted at /) does not need to be attested - only the encrypted input volume (mounted at /encryptedInputs) and encrypted output volume (mounted at /encryptedOutputs) needs to be attested. When a trusted DApp starts up, it connects to the CAS to perform an attestation. This attestation ensures that MrEnclave is the expected value and that the file system and the mounted volumes are in the correct state, i.e., the tags have the expected values. Note that MrEnclave will depend on the heap and stack size, i.e., changing the values will require an update of the expected MrEnclave . A trusted DApp will typically write to the output volume. This will change the tag of the output volume. To ensure that a client can read the current state of the volumes written by a trusted DApp, the tags will be pushed to the CAS when a trusted DApp exits. Attesting CAS CAS is written in Rust , a safe and efficient programming language. The type-safety ensures that simple programming bugs can be exploited by attackers to hijack services. A client can attest a valid CAS in two ways: via the Intel attestations service, and/or via the certificates provided by CAS. Right now, the CLI verifies that it talks to a correct CAS with the help of TLS. The client only uses TLS and verifies that the CAS has the correct certificate. Only a CAS will be able to get this certificate. In a later version, we will additionally ensure that the CAS provides a correct report by the Intel attestations service. CAS is trusted, managed service : it can be operated by a provider but the provider is not able to access the sessions and in particular, the secrets of the clients. Right now, we run a public CAS service for debugging and testing. This CAS runs in debug mode , i.e., you should not use this CAS to pass secrets. For clients that need to run a private CAS services, we will enable clients to run their own CAS. Please contact us if you want to run your own CAS or if you need access to CAS running in production mode. To run a trusted DApp in production mode, you need either to get a MrSigner from Intel or you need access to Intel SGX CPU that supports flexible launch control. Contact us, if you want to run trusted DApps in production mode. Sessions We need to specify the value of MrEnclave and the state (i.e., the tags) of the of the volumes and in the general case, also the file system. To do so, CAS supports a security policies. We call such a security policy a session . A session specifies what volumes are mounted and where they are mounted, the keys and tags of these volumes and MrEnclave of the trusted DApp. One problem that we need to address is that each client of a trusted DApp cannot only protect its data from an attacker but also from other clients of the trusted DApp, the entity that generated the image and the entities that manage the CAS or the host on which the trusted DApp is executed. To address this issue, we provide a way to have individually encrypted volumes for each session. In other words, each client of a trusted DApp can customize the image by providing input and output volumes and only the instance of this client can access these volumes. No other client nor the image generator can access the keys and hence, these volumes . A client generates these keys (in the CLI). A session specifies these keys. A session is typically derived from a session template (more about session language here ). The session template of the copy DApp looks like this: name: $SESSION version: \"0.3\" services: - name: application image_name: registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo mrenclaves: [68112f06718fc390fb4a56e9d1f019b63285b6b6383ed73e1a5d2fa0b66e57c0] command: /application pwd: / environment: SCONE_MODE: hw volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs security: attestation: tolerate: [debug-mode, hyperthreading, outdated-tcb] ignore_advisories: \"*\" Note that in most cases you can start from a generic session template and you do not need to worry about the syntax and all the details of session descriptions. The session templates of trusted DApps look very similar to the session template above. The three differences are that the image name will be updated, the list of mrenclaves will be different, and one might want to specify arguments passed to the trusted DApp ( key command ). Building an DApp The source code of this application , you can retrieve from git clone https://github.com/scontain/copy_dapp.git cd copy_dapp The source code of the trusted copy DApp is file copy_files.c . We build this program with the help of the standard gcc that is part of Alpine Linux (see the Dockerfile below). For convenience, we provide an Alpine image (i.e., registry.scontain.com:5050/sconecuratedimages/muslgcc) that has gcc preinstalled. FROM registry.scontain.com:5050/sconecuratedimages/muslgcc COPY copy_files.c / # compile with vanilla gcc RUN gcc -Wall copy_files.c -o /copy_files FROM registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_runtime COPY --from = 0 /copy_files /application RUN apk add curl bash unzip zip ENTRYPOINT [ \"/application.sh\" ] To run the application inside of an enclave, we need the SCONE runtime environment. This runtime is part of image registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_runtime . Moreover, this image contains a script /application.sh that pulls the encrypted input and output volumes from some transfer service, executes the application inside of an enclave. For testing, the script can push the encrypted output volume to a specified transfer service (see CLI options in CLIs ). The image registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo is built by the script build-image.sh . This script also determines MrEnclave of the application and writes a session template copy_dapp.yml . Testing We now show how to test the trusted copy DApp. After building the image by executing ./build-image.sh We can execute this image on a SGX-capable host. This host has to have the SCONE local attestation service (LAS) and Docker installed. We describe in the installation section on how to install SGX driver and how to install LAS here . Compatibility of LAS and CAS If you are using one of our public CAS as a secretManagementService (it is current default behavior of our images), please insure that your local version of LAS has at least same major version as CAS that you are accessing. Current default CAS has version 4.2.1 , so your local LAS has to be 4.* . In what follows, we assume that the name of the host is stored in environment variable host . SETUP To execute the trusted copy DApp, we need to create some files to copy. We create an INPUTS directory and store files there. We also create OUTPUTS directory, KEYS directory to hold generated keys and ZIP to hold temporary files. We also copy the session template generated when building the container image. mkdir -p EXAMPLE cd EXAMPLE mkdir -p KEYS INPUTS OUTPUTS ZIP echo \"Hello world\" > INPUTS/f1.txt echo \"Hello together\" > INPUTS/f2.txt cp ../copy_dapp.yml KEYS/ Step 1 We encrypt these files and push the zipped up files for now to free.keep.sh . Let's execute the first step of encrypting the files and pushing these to the transfer service: CMD = $( docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /INPUTS:/inputs registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_cli encryptedpush --application registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo -t /conf/copy_dapp.yml ) Let's look at the output: echo $CMD should result in an output like: --sessionID 84424401249649941281869427 /application --secretManagementService 150 .165.85.86 --url https://free.keep.sh/NKHVjALFjubbHUNs/scone-upload.zip You can look in your browser at the URL to see the uploaded files! Step 2 Before running this application on the iExec platform, we might want to test it on some SGX-capable host. Ensure that the newest image is loaded on host . This step is only needed if it is expected that the image has changed since docker run does not pull/update the image. ssh $host docker pull registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo Ensure that you are executing a bash shell (execute bash ). We determine which SGX device to mount with function determine_sgx_device . Now, execute the command on host and we ask it to be pushed to free.keep.sh : determine_sgx_device URL = $( ssh $host docker run -t $MOUNT_SGXDEVICE --network host --rm registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo ${ CMD //[ $'\\t\\r\\n' ] } --push free.keep.sh ) Let's look at the output: echo $URL this results in an output like: https://free.keep.sh/0JitkN1PLHDHba1X/outputs.zip You can now download this (if running a bash shell) curl -L --output ZIP/encryptedOutputFiles.zip ${ URL //[ $'\\t\\r\\n' ] } Step 3 docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /ZIP:/encryptedOutputs -v $PWD /OUTPUTS:/decryptedOutputs registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_cli decrypt We can now look at the files in OUTPUTS directory: cat OUTPUTS/f1.txt results in Hello world and cat OUTPUTS/f2.txt results in Hello together . CLIs Now that you know how our example works, it is time to present you CLIs that we ship with our images. These CLIs have enough options that you can configure them in according to your needs. A CLI included into registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_cli scone.sh: a utility to either push encrypted files or decrypt result files ( C ) Scontain.com, 2021 . See https://sconedocs.github.io Usage: decrypt [ options ] --help This usage statement Encrypt mode: encryptedpush --inputDataFolder PATH --application APP -a, --application APP name of remote application that should be executed -s, --secretManagementService ADDRESS IP address or name of SCONE secret management service [ default 150 .165.85.86 ] -t, --sessiontemplate PATH read session template from local path like /conf/sessiontemplate -r, --remoteFileSystem TRANSFER hostname and port of service used to transfer files [ default: free.keep.sh ] -k, --insecure permits insecure SSL connections with remoteFileSystem -v, --verbose print progress messages --help encryptedpush usage statement Example: encryptedpush --application blender Decrypt mode: decrypt --help decrypt usage statement Example: decrypt And a CLI included into registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_runtime application.sh a utility to demonstrate how to push volumes to remote applications ( C ) Scontain.com, 2021 . See https://sconedocs.github.io -s, --secretManagementService ADDRESS IP address or name of SCONE secret management service [ default ] -u, --url URL read session template from local path like /conf/sessiontemplate -p, --push TRANSFER_SERVICE hostname and port of service used to push encrypted output files [ default: free.keep.sh ] -s, --sessionID SESSION ID of the session uploaded to secretManagementService -k, --insecure permits insecure SSL connections with TRANSFER_SERVICE -v, --verbose print progress messages --help application.sh usage statement Example: application.sh --sessionID 60534003595811684552131953 /application --secretManagementService 150 .165.85.86 --url https://free.keep.sh/KmdrBEvGKM66Jd04/scone-upload.zip --push free.keep.sh","title":"Trusted DApps"},{"location":"dapps/#trusted-dapps-with-scone-and-iexec","text":"This tutorial shows how to build a trusted DApp for the iExec platform . A DApp is a decentralized application that can be executed on hosts of a decentralized network. To learn about basic terminology and concepts of SCONE, we recommend that you read the preceding tutorial pages first. Note that SCONE supports most common programming languages: please have a look at our examples for C , C++ , Fortran , GO , Rust , Python , Java , and JavaScript/Node.js . In this tutorial, we show how to build a trusted DApp using C. The transfer service used by this use case is not operational anymore To try this example, you would need to replace the the \"transfer service\" by a more reliable way to transfer files. Eventually, we will rework this example to use a reliable service to transfer files. For now, we leave it online since it demonstrate some of the features of SCONE. Also, maybe somebody will send us a pull request for using github instead of tansfer.sh ?","title":"Trusted DApps with SCONE and iExec"},{"location":"dapps/#cli","text":"","title":"CLI"},{"location":"dapps/#trusted-dapp","text":"SCONE helps developers to protect the integrity as well as the confidentiality not only of data at rest (i.e., on disk) and data being transmitted but also all data stored in main memory . Note that any root user can easily dump the main memory of any application and in this way might gain secrets like the keys used to encrypt the data at rest. By trusted DApp , we refer to a DApp that executes inside a trusted execution environment (TEE) in such a way that not even an attacker with root rights can access the data of the application. SCONE simplifies to run applications inside of TEEs as well as to transparently encrypt and decrypt files. SCONE is designed to be independent of the actual TEE implementation. Right now -- since there is a lack of appropriate hardware alternatives -- SCONE supports only Intel SGX .","title":"Trusted DApp"},{"location":"dapps/#scone-platform","text":"The SCONE platform helps to run DApps inside of TEEs without source code modifications. In this tutorial, we describe how to build a simple application to run with SCONE inside of an enclave , i.e., inside an Intel SGX TEE, show how to compile the simple trusted DApp with the help of an Alpine container, how to deploy and execute inside of an Alpine container, introduce the workflow of where and how data is encrypted, pushed to the application and then decrypted explain how keys are managed and passed to a trusted DApp, and show how to test this application on your own infrastructure.","title":"SCONE Platform"},{"location":"dapps/#trusted-copy-dapp","text":"We show how to implement a simple copy command as a trusted DApp. This is of course not a realistic DApp but it helps us to show all steps necessary to build and run trusted DApps. The copy DApp copies files located in an input directory /INPUTS to an output directory /OUTPUTS of the client the machine. With the help of SCONE, this copy DApp actually copies encrypted files such that the SCONE transparently decrypts and checks the integrity of files that are read by the application. This protects the confidentiality and integrity of the files without any need to change the application itself. Also, SCONE encrypts and integrity protects the files that are written by the application. Each file is encrypted with a random key generated and managed by SCONE inside the enclave. With the help of a CLI (command line interface deployed in a container, see below in CLIs , a client can encrypt and push files stored on its client machine to the copy DApp. The copy DApp copies the files. The CLI can decrypt the files after the execution of the DApp. We introduce the commands to perform this example below.","title":"Trusted copy DApp"},{"location":"dapps/#encrypted-volumes","text":"Containers support volumes as a way to persist data. Trusted DApps for the iExec platform have always two encrypted volumes mapped into their file system: an encrypted volume for input files mapped in the container in directory /encryptedInputs , and an encrypted volume for output files /encryptedOutputs . We show below how the input files are encrypted with the help of the CLI and pushed to the container. Moreover, we show how to decrypt the files with the help of the CLI. To access an encrypted volume, one needs a key and a tag . The key is used to encrypt the individual keys of the files of the volume, i.e., it is used to protect the confidentiality of the files. The tag describes the current state of the volume. Any change of any file in the volume will result in a new tag . The tag is used to protect the integrity and freshness of the files: any unauthorized modification of the volume will be detected since the correct tag can only be computed by parties knowing the key of the volume ( integrity protection ). any rollback to older version of the volume will be correctly encrypted but will have an old tag. Hence, rollbacks to older versions will be detected ( freshness protection ).","title":"Encrypted Volumes"},{"location":"dapps/#key-and-tag-management","text":"The copy DApp requires access to keys and tags to read the encrypted input files (i.e., the input volume) and to write the encrypted output files (i.e., the output volume). Passing the keys and tags to the copy DApp is non-trivial since we need to ensure that no other DApp nor any attacker can read or modify the keys or the tags. To pass the keys and tags to a trusted DApp, SCONE provides a configuration and attestation service (CAS) . The CAS can ensure that only the copy DApp can access the keys and tags by verifying that it talks to an unmodified copy DApp . This process is called attestation and is performed with the help of the Intel SGX CPU extension. A trusted DApp is deployed with the help of a container image hosted on docker hub or some similar repository. We recommend that these container images are signed using Docker content trust . Note that content trust is not sufficient to ensure that we talk to an unmodified copy DApp since, for example, an attacker with root access could change this image on the host on which it is deployed. To establish trust between two communication partners, they typically authenticate each other with the help of TLS . An entity needs to know a private key to authenticate itself. When a trusted DApp like the copy DApp starts up it does not know such a private key yet. Note that any key that might be stored in plain text in the image, could be read by any entity that is permitted to access that image. One could encrypt the private key in the image but then the trust DApps would need to know the key to decrypt the private key. Note that the code that runs inside of an enclave, is in general not encrypted. Hence, one cannot store the private key inside of the code either. Attestation - unlike authentication - does not need a private key. It ensures that the correct code is executing. To distinguish between different instances of a trusted DApp, on startup, the SCONE runtime generates a random private/public key pair inside of the enclave. This means that the private key can be kept secret inside of the enclave. This public key is used to identify this instance and one can perform TLS authentication to ensure that one talks to a specific instance. To attest a trusted DApp, we need to describe the state of this DApp. In general, this consists of a hash value that describes the trusted DApp (called MrEnclave ) and a tag that describes the file systems state. In the case of the copy DApp, the file system (mounted at /) does not need to be attested - only the encrypted input volume (mounted at /encryptedInputs) and encrypted output volume (mounted at /encryptedOutputs) needs to be attested. When a trusted DApp starts up, it connects to the CAS to perform an attestation. This attestation ensures that MrEnclave is the expected value and that the file system and the mounted volumes are in the correct state, i.e., the tags have the expected values. Note that MrEnclave will depend on the heap and stack size, i.e., changing the values will require an update of the expected MrEnclave . A trusted DApp will typically write to the output volume. This will change the tag of the output volume. To ensure that a client can read the current state of the volumes written by a trusted DApp, the tags will be pushed to the CAS when a trusted DApp exits.","title":"Key and Tag Management"},{"location":"dapps/#attesting-cas","text":"CAS is written in Rust , a safe and efficient programming language. The type-safety ensures that simple programming bugs can be exploited by attackers to hijack services. A client can attest a valid CAS in two ways: via the Intel attestations service, and/or via the certificates provided by CAS. Right now, the CLI verifies that it talks to a correct CAS with the help of TLS. The client only uses TLS and verifies that the CAS has the correct certificate. Only a CAS will be able to get this certificate. In a later version, we will additionally ensure that the CAS provides a correct report by the Intel attestations service. CAS is trusted, managed service : it can be operated by a provider but the provider is not able to access the sessions and in particular, the secrets of the clients. Right now, we run a public CAS service for debugging and testing. This CAS runs in debug mode , i.e., you should not use this CAS to pass secrets. For clients that need to run a private CAS services, we will enable clients to run their own CAS. Please contact us if you want to run your own CAS or if you need access to CAS running in production mode. To run a trusted DApp in production mode, you need either to get a MrSigner from Intel or you need access to Intel SGX CPU that supports flexible launch control. Contact us, if you want to run trusted DApps in production mode.","title":"Attesting CAS"},{"location":"dapps/#sessions","text":"We need to specify the value of MrEnclave and the state (i.e., the tags) of the of the volumes and in the general case, also the file system. To do so, CAS supports a security policies. We call such a security policy a session . A session specifies what volumes are mounted and where they are mounted, the keys and tags of these volumes and MrEnclave of the trusted DApp. One problem that we need to address is that each client of a trusted DApp cannot only protect its data from an attacker but also from other clients of the trusted DApp, the entity that generated the image and the entities that manage the CAS or the host on which the trusted DApp is executed. To address this issue, we provide a way to have individually encrypted volumes for each session. In other words, each client of a trusted DApp can customize the image by providing input and output volumes and only the instance of this client can access these volumes. No other client nor the image generator can access the keys and hence, these volumes . A client generates these keys (in the CLI). A session specifies these keys. A session is typically derived from a session template (more about session language here ). The session template of the copy DApp looks like this: name: $SESSION version: \"0.3\" services: - name: application image_name: registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo mrenclaves: [68112f06718fc390fb4a56e9d1f019b63285b6b6383ed73e1a5d2fa0b66e57c0] command: /application pwd: / environment: SCONE_MODE: hw volumes: - name: encrypted-input-files fspf_tag: $INPUT_FSPF_TAG fspf_key: $INPUT_FSPF_KEY - name: encrypted-output-files fspf_tag: $OUTPUT_FSPF_TAG fspf_key: $OUTPUT_FSPF_KEY images: - name: registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo volumes: - name: encrypted-input-files path: /encryptedInputs - name: encrypted-output-files path: /encryptedOutputs security: attestation: tolerate: [debug-mode, hyperthreading, outdated-tcb] ignore_advisories: \"*\" Note that in most cases you can start from a generic session template and you do not need to worry about the syntax and all the details of session descriptions. The session templates of trusted DApps look very similar to the session template above. The three differences are that the image name will be updated, the list of mrenclaves will be different, and one might want to specify arguments passed to the trusted DApp ( key command ).","title":"Sessions"},{"location":"dapps/#building-an-dapp","text":"The source code of this application , you can retrieve from git clone https://github.com/scontain/copy_dapp.git cd copy_dapp The source code of the trusted copy DApp is file copy_files.c . We build this program with the help of the standard gcc that is part of Alpine Linux (see the Dockerfile below). For convenience, we provide an Alpine image (i.e., registry.scontain.com:5050/sconecuratedimages/muslgcc) that has gcc preinstalled. FROM registry.scontain.com:5050/sconecuratedimages/muslgcc COPY copy_files.c / # compile with vanilla gcc RUN gcc -Wall copy_files.c -o /copy_files FROM registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_runtime COPY --from = 0 /copy_files /application RUN apk add curl bash unzip zip ENTRYPOINT [ \"/application.sh\" ] To run the application inside of an enclave, we need the SCONE runtime environment. This runtime is part of image registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_runtime . Moreover, this image contains a script /application.sh that pulls the encrypted input and output volumes from some transfer service, executes the application inside of an enclave. For testing, the script can push the encrypted output volume to a specified transfer service (see CLI options in CLIs ). The image registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo is built by the script build-image.sh . This script also determines MrEnclave of the application and writes a session template copy_dapp.yml .","title":"Building an DApp"},{"location":"dapps/#testing","text":"We now show how to test the trusted copy DApp. After building the image by executing ./build-image.sh We can execute this image on a SGX-capable host. This host has to have the SCONE local attestation service (LAS) and Docker installed. We describe in the installation section on how to install SGX driver and how to install LAS here . Compatibility of LAS and CAS If you are using one of our public CAS as a secretManagementService (it is current default behavior of our images), please insure that your local version of LAS has at least same major version as CAS that you are accessing. Current default CAS has version 4.2.1 , so your local LAS has to be 4.* . In what follows, we assume that the name of the host is stored in environment variable host .","title":"Testing"},{"location":"dapps/#setup","text":"To execute the trusted copy DApp, we need to create some files to copy. We create an INPUTS directory and store files there. We also create OUTPUTS directory, KEYS directory to hold generated keys and ZIP to hold temporary files. We also copy the session template generated when building the container image. mkdir -p EXAMPLE cd EXAMPLE mkdir -p KEYS INPUTS OUTPUTS ZIP echo \"Hello world\" > INPUTS/f1.txt echo \"Hello together\" > INPUTS/f2.txt cp ../copy_dapp.yml KEYS/","title":"SETUP"},{"location":"dapps/#step-1","text":"We encrypt these files and push the zipped up files for now to free.keep.sh . Let's execute the first step of encrypting the files and pushing these to the transfer service: CMD = $( docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /INPUTS:/inputs registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_cli encryptedpush --application registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo -t /conf/copy_dapp.yml ) Let's look at the output: echo $CMD should result in an output like: --sessionID 84424401249649941281869427 /application --secretManagementService 150 .165.85.86 --url https://free.keep.sh/NKHVjALFjubbHUNs/scone-upload.zip You can look in your browser at the URL to see the uploaded files!","title":"Step 1"},{"location":"dapps/#step-2","text":"Before running this application on the iExec platform, we might want to test it on some SGX-capable host. Ensure that the newest image is loaded on host . This step is only needed if it is expected that the image has changed since docker run does not pull/update the image. ssh $host docker pull registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo Ensure that you are executing a bash shell (execute bash ). We determine which SGX device to mount with function determine_sgx_device . Now, execute the command on host and we ask it to be pushed to free.keep.sh : determine_sgx_device URL = $( ssh $host docker run -t $MOUNT_SGXDEVICE --network host --rm registry.scontain.com:5050/sconecuratedimages/dapps:copy_demo ${ CMD //[ $'\\t\\r\\n' ] } --push free.keep.sh ) Let's look at the output: echo $URL this results in an output like: https://free.keep.sh/0JitkN1PLHDHba1X/outputs.zip You can now download this (if running a bash shell) curl -L --output ZIP/encryptedOutputFiles.zip ${ URL //[ $'\\t\\r\\n' ] }","title":"Step 2"},{"location":"dapps/#step-3","text":"docker run -t --rm -v $PWD /KEYS:/conf -v $PWD /ZIP:/encryptedOutputs -v $PWD /OUTPUTS:/decryptedOutputs registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_cli decrypt We can now look at the files in OUTPUTS directory: cat OUTPUTS/f1.txt results in Hello world and cat OUTPUTS/f2.txt results in Hello together .","title":"Step 3"},{"location":"dapps/#clis","text":"Now that you know how our example works, it is time to present you CLIs that we ship with our images. These CLIs have enough options that you can configure them in according to your needs. A CLI included into registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_cli scone.sh: a utility to either push encrypted files or decrypt result files ( C ) Scontain.com, 2021 . See https://sconedocs.github.io Usage: decrypt [ options ] --help This usage statement Encrypt mode: encryptedpush --inputDataFolder PATH --application APP -a, --application APP name of remote application that should be executed -s, --secretManagementService ADDRESS IP address or name of SCONE secret management service [ default 150 .165.85.86 ] -t, --sessiontemplate PATH read session template from local path like /conf/sessiontemplate -r, --remoteFileSystem TRANSFER hostname and port of service used to transfer files [ default: free.keep.sh ] -k, --insecure permits insecure SSL connections with remoteFileSystem -v, --verbose print progress messages --help encryptedpush usage statement Example: encryptedpush --application blender Decrypt mode: decrypt --help decrypt usage statement Example: decrypt And a CLI included into registry.scontain.com:5050/sconecuratedimages/dapps:copy_dapp_runtime application.sh a utility to demonstrate how to push volumes to remote applications ( C ) Scontain.com, 2021 . See https://sconedocs.github.io -s, --secretManagementService ADDRESS IP address or name of SCONE secret management service [ default ] -u, --url URL read session template from local path like /conf/sessiontemplate -p, --push TRANSFER_SERVICE hostname and port of service used to push encrypted output files [ default: free.keep.sh ] -s, --sessionID SESSION ID of the session uploaded to secretManagementService -k, --insecure permits insecure SSL connections with TRANSFER_SERVICE -v, --verbose print progress messages --help application.sh usage statement Example: application.sh --sessionID 60534003595811684552131953 /application --secretManagementService 150 .165.85.86 --url https://free.keep.sh/KmdrBEvGKM66Jd04/scone-upload.zip --push free.keep.sh","title":"CLIs"},{"location":"dockerfileexample/","text":"Dockerfile Example We show now how to create a container image that contains a very simple hello world program running inside an enclave. The program is given in C but could be any other compiled language that we support like Rust , C++ and Fortran . Getting access Just register a free account on gitlab.scontain.com . Building images without the SCONE tool chain This example builds an image that contains the complete SCONE platform. Build your container images with a multi-stage build such that they only contain your binaries when you push your images to a public repository. Here is the dockerfile: cat > Dockerfile << EOF FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers RUN echo \"#include <stdio.h>\" > helloworld.c \\ && echo \"int main() {\" >> helloworld.c \\ && echo \"printf(\\\"Hello World!\\n\\\"); }\" >> helloworld.c RUN gcc -o helloworld helloworld.c CMD bash -c \"SCONE_VERSION=1 /helloworld\" EOF Let's generate an image ( helloworld ) with this Dockerfile: docker build --pull -t helloworld . Determine which SGX device to mount with function determine_sgx_device . Let's run the image as follows: determine_sgx_device docker run $MOUNT_SGXDEVICE --rm helloworld The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 597cdef086651d46652cab78a89386b790ed058427ce1a5feacc3da7bc731902 Hello World! Note In case you do not have a SGX driver installed, the run will fail. Run the program in simulation mode by executing docker run --rm helloworld Screencast","title":"Dockerfile Example"},{"location":"dockerfileexample/#dockerfile-example","text":"We show now how to create a container image that contains a very simple hello world program running inside an enclave. The program is given in C but could be any other compiled language that we support like Rust , C++ and Fortran . Getting access Just register a free account on gitlab.scontain.com . Building images without the SCONE tool chain This example builds an image that contains the complete SCONE platform. Build your container images with a multi-stage build such that they only contain your binaries when you push your images to a public repository. Here is the dockerfile: cat > Dockerfile << EOF FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers RUN echo \"#include <stdio.h>\" > helloworld.c \\ && echo \"int main() {\" >> helloworld.c \\ && echo \"printf(\\\"Hello World!\\n\\\"); }\" >> helloworld.c RUN gcc -o helloworld helloworld.c CMD bash -c \"SCONE_VERSION=1 /helloworld\" EOF Let's generate an image ( helloworld ) with this Dockerfile: docker build --pull -t helloworld . Determine which SGX device to mount with function determine_sgx_device . Let's run the image as follows: determine_sgx_device docker run $MOUNT_SGXDEVICE --rm helloworld The output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 597cdef086651d46652cab78a89386b790ed058427ce1a5feacc3da7bc731902 Hello World! Note In case you do not have a SGX driver installed, the run will fail. Run the program in simulation mode by executing docker run --rm helloworld","title":"Dockerfile Example"},{"location":"dockerfileexample/#screencast","text":"","title":"Screencast"},{"location":"dockerinstall/","text":"Installation of Docker Engine To install the docker engine, you can follow the official description . To simplify the installation of Docker, on Ubuntu you can install the docker engine by executing the following in a terminal. Note that this script will also add your user to the group docker in case you cannot execute docker without sudo . curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_docker.sh | bash","title":"Installing docker"},{"location":"dockerinstall/#installation-of-docker-engine","text":"To install the docker engine, you can follow the official description . To simplify the installation of Docker, on Ubuntu you can install the docker engine by executing the following in a terminal. Note that this script will also add your user to the group docker in case you cannot execute docker without sudo . curl -fssl https://raw.githubusercontent.com/SconeDocs/SH/master/install_docker.sh | bash","title":"Installation of Docker Engine"},{"location":"eclipse/","text":"Eclipse Integration In this section, we explain how to set up the Eclipse IDE such that you can develop and in particular, code that executed inside of an enclave. The instructions have been tested on Ubuntu 18.04. Installing Eclipse Download the Eclipse IDE for C/C++ Developers for Linux 64-bit from https://www.eclipse.org/downloads/packages/ Extract it to a suitable location, e.g. tar -xzf \"eclipse-cpp-2019-03-R-linux-gtk-x86_64.tar.gz\" -C ~/ (optional) If you want to have a desktop entry for eclipse, open or create ~/.local/share/applications/.desktop and add the following content: [Desktop Entry] Comment = Eclipse Terminal = false Name = Eclipse Exec = /home/myusername/eclipse/eclipse Type = Application Icon = /home/myusername/eclipse/icon.xpm (Make sure to adapt the paths accordingly.) Creating a workspace Create a new folder as your workspace, e.g.: mkdir ~/eclipse-workspace Clone or copy the code you want to work with into the workspace, e.g.: cd ~/eclipse-workspace && git clone ... Start Eclipse! (Use the desktop entry or run ~/eclipse/eclipse ) When prompted, select the created workspace folder and Launch Click on Workbench at the top right Configure Eclipse for developing applications using SCONE We show the steps to integrate SCONE with Eclipse. We assume that you have already installed scone-gcc and scone-gdb one the machine your are now installing Eclipse: In Eclipse, select File > Import... > C/C++ > Existing Code as Makefile Project > Next Existing Code Location : Select <your-path-to-workspace>/<path_to_project>/<your_project> . Select Linux GCC as Toolchain Click Finish Right-click the <your_project> project, select Properties > C/C++ Build > Environment Add the following entries: CC=scone-gcc , CFLAGS=-g -Og , PATH=<your-build-directory>/built/bin (Tick Add to all configurations for PATH . Your regular PATH will be added automatically.) Apply and Close Press Ctrl+B to build. Both SCONE (if configured) and the test applications will be built. While the <your_project> is selected, Select Run > Debug configurations... Double-click C/C++ Application Change Name to <your_app> , C/C++ Application to target/<your_app> Switch to tab Environment and add all environment variables used by SCONE, example: Switch to tab Debugger and change GDB debugger to scone-gdb If the build-oot.sh script is used to build outside the source directory, it is advised to re-map the sources back to their original location for Debugging purposes: Switch to tab Source and Add... a Path Mapping with Name SCONE mapping , Add Compilation path <your-scone-build-directory> and Local file system path <your-scone-source-directory> (Selecting the local path is tricky, as a button seems to not be rendered correctly. Click on the empty space highlighted in the screenshot below) OK > Apply > Debug When asked, Switch view The debugger is launched and the application is halted in main (can be disabled in the Debugger settings of the launch configuration). You can show local variables, Ctr+Click to follow references, etc. as usual Continue with F8","title":"Eclipse"},{"location":"eclipse/#eclipse-integration","text":"In this section, we explain how to set up the Eclipse IDE such that you can develop and in particular, code that executed inside of an enclave. The instructions have been tested on Ubuntu 18.04.","title":"Eclipse Integration"},{"location":"eclipse/#installing-eclipse","text":"Download the Eclipse IDE for C/C++ Developers for Linux 64-bit from https://www.eclipse.org/downloads/packages/ Extract it to a suitable location, e.g. tar -xzf \"eclipse-cpp-2019-03-R-linux-gtk-x86_64.tar.gz\" -C ~/ (optional) If you want to have a desktop entry for eclipse, open or create ~/.local/share/applications/.desktop and add the following content: [Desktop Entry] Comment = Eclipse Terminal = false Name = Eclipse Exec = /home/myusername/eclipse/eclipse Type = Application Icon = /home/myusername/eclipse/icon.xpm (Make sure to adapt the paths accordingly.)","title":"Installing Eclipse"},{"location":"eclipse/#creating-a-workspace","text":"Create a new folder as your workspace, e.g.: mkdir ~/eclipse-workspace Clone or copy the code you want to work with into the workspace, e.g.: cd ~/eclipse-workspace && git clone ... Start Eclipse! (Use the desktop entry or run ~/eclipse/eclipse ) When prompted, select the created workspace folder and Launch Click on Workbench at the top right","title":"Creating a workspace"},{"location":"eclipse/#configure-eclipse-for-developing-applications-using-scone","text":"We show the steps to integrate SCONE with Eclipse. We assume that you have already installed scone-gcc and scone-gdb one the machine your are now installing Eclipse: In Eclipse, select File > Import... > C/C++ > Existing Code as Makefile Project > Next Existing Code Location : Select <your-path-to-workspace>/<path_to_project>/<your_project> . Select Linux GCC as Toolchain Click Finish Right-click the <your_project> project, select Properties > C/C++ Build > Environment Add the following entries: CC=scone-gcc , CFLAGS=-g -Og , PATH=<your-build-directory>/built/bin (Tick Add to all configurations for PATH . Your regular PATH will be added automatically.) Apply and Close Press Ctrl+B to build. Both SCONE (if configured) and the test applications will be built. While the <your_project> is selected, Select Run > Debug configurations... Double-click C/C++ Application Change Name to <your_app> , C/C++ Application to target/<your_app> Switch to tab Environment and add all environment variables used by SCONE, example: Switch to tab Debugger and change GDB debugger to scone-gdb If the build-oot.sh script is used to build outside the source directory, it is advised to re-map the sources back to their original location for Debugging purposes: Switch to tab Source and Add... a Path Mapping with Name SCONE mapping , Add Compilation path <your-scone-build-directory> and Local file system path <your-scone-source-directory> (Selecting the local path is tricky, as a button seems to not be rendered correctly. Click on the empty space highlighted in the screenshot below) OK > Apply > Debug When asked, Switch view The debugger is launched and the application is halted in main (can be disabled in the Debugger settings of the launch configuration). You can show local variables, Ctr+Click to follow references, etc. as usual Continue with F8","title":"Configure Eclipse for developing applications using SCONE"},{"location":"ee_sconify_image/","text":"Sconify Container Image (Standard Edition) We show how to sconify an existing container image using a tool provided by the SCONE Standard Edition we sconify the binary of the application running inside the container, i.e., converting it such that it runs inside of an SGX enclave encrypt the files that the application accesses including libraries and code examples (like Python code) creating a SCONE policy (a session) to ensure that the encryption key of the filesystem and the secrets that we generate can only be accessed by this application sconify_image supports only Alpine Linux-based images This 1-step conversion supports native images based on Alpine Linux only. We plan to support Ubuntu-based images in the near future too. CI/CD Workflow In a typical workflow, one would perform the sconification of an image as part of the CI/CD pipeline: One would test the native image first, sconify the image and test the sconified image again. After a successful test, the binary would be pushed to a container image repository. Namespace To reduce the chances of a name conflict, we first generate a namespace. Let's create a random namespace for this example: export NS = \"NS- $RANDOM \" Policy Extension We sconify the same image as in the previous section , i.e., native_flask_restapi_image . This image requires the generation of a TLS certificate and a private key. To do so, we define a secret section for the generated policy that we store in a local file: mkdir -p policy cat > policy/secrets.yml <<EOF secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api EOF We also define the location in which the sconified application can read the certificate and the private key: cat > policy/injected_files.yml <<EOF injection_files: - path: \"/tls/flask.crt\" content: \\$\\$SCONE::flask.crt\\$\\$ - path: \"/tls/flask.key\" content: \"\\$\\$SCONE::flask.key\\$\\$\" EOF Creating Sconified Image To attest the CAS and uploaded a policy, we need access to the SGX device. Hence, we first determine the SGX device ( see ): determine_sgx_device mkdir -p cas We use a public CAS service ( --cas=4-2-1.scone-cas.cf ) that is running in debug mode and hence, we set --cas_debug to ensure that attestation of CAS passes despite the CAS being in debug mode. The clients key used in the attestation are stored in the local directory cas. SESSION = $( docker run -it --rm $MOUNT_SGXDEVICE -v \" $( pwd ) /policy:/policy\" -v \" $( pwd ) /cas:/root/.cas\" -v /var/run/docker.sock:/var/run/docker.sock registry.scontain.com:5050/sconecuratedimages/sconecli:sconify_image --name = flask --from = native_flask_restapi_image --to = new_image --cas = 4 -2-1.scone-cas.cf --cas_debug --binary = /usr/bin/python3 --dir = /home --dir = /usr/local/lib --dir = /app --dir = /usr/lib/python3.7 --dir = /lib --plain = /usr/lib/libpython3.7m.so.1.0 --secrets = /policy/secrets.yml --injectedfiles = /policy/injected_files.yml --create_namespace --namespace = $NS ) For production, you need to drop --cas_debug and to sign the binary You can now execute the image in the context of session $SESSION : please read the previous section to learn how to do so. Supported Options sconify_image supports the following options: --from=NATIVE_IMAGE : name of native image (mandatory) --to=TO_IMAGE : name of encrypted image (mandatory) --template=TEMPLATE_FILE : file containing policy template (default=session-template.yml) --session=SESSION_FILE : file that will contain the session (default=session.yml) --namespace=NAMESPACE : namespace of this session (default is a random namespace) --create_namespace : create namespace NAMESPACE first --dir=DIRECTORY : add directory to encrypt; add one option per directory --plain=DIRECTORIES : copy directories that are not encrypted --cas=CAS_ADDR : set the name of the CAS_ADDR --cas_debug : permit CAS to be in debug mode --base=BASE : set the base image used to generate the encrypted image --cli=CLI : set the SCONE CLI image --trace=TRACEFILE : use tracefile to create encrypted files --name=SESSION : name of CAS policy session --secrets=FILE : policy section that defines secrets --injectedfiles=FILE : policy section that defines the injected files --sign=KEYPATH : sign the binary (uses other arguments from environment) --heap=INT : heap size in [K,M,G] --stack=INT : stack size in [K,M,G] --dlopen=[0|1|2] : dlopen: 0 - disable, 1 - enable and require authentication (default), 2 - debug only [SCONE_ALLOW_DLOPEN] Next Step Using this approach, we 1) attest CAS and 2) ensure that the policy is only pushed to a trusted CAS. For production, you need to sign the service: for this, you need to provide a signing key that you need to pass via option --sign=KEYPATH .","title":"Sconify Image SE"},{"location":"ee_sconify_image/#sconify-container-image-standard-edition","text":"We show how to sconify an existing container image using a tool provided by the SCONE Standard Edition we sconify the binary of the application running inside the container, i.e., converting it such that it runs inside of an SGX enclave encrypt the files that the application accesses including libraries and code examples (like Python code) creating a SCONE policy (a session) to ensure that the encryption key of the filesystem and the secrets that we generate can only be accessed by this application sconify_image supports only Alpine Linux-based images This 1-step conversion supports native images based on Alpine Linux only. We plan to support Ubuntu-based images in the near future too.","title":"Sconify Container Image (Standard Edition)"},{"location":"ee_sconify_image/#cicd-workflow","text":"In a typical workflow, one would perform the sconification of an image as part of the CI/CD pipeline: One would test the native image first, sconify the image and test the sconified image again. After a successful test, the binary would be pushed to a container image repository.","title":"CI/CD Workflow"},{"location":"ee_sconify_image/#namespace","text":"To reduce the chances of a name conflict, we first generate a namespace. Let's create a random namespace for this example: export NS = \"NS- $RANDOM \"","title":"Namespace"},{"location":"ee_sconify_image/#policy-extension","text":"We sconify the same image as in the previous section , i.e., native_flask_restapi_image . This image requires the generation of a TLS certificate and a private key. To do so, we define a secret section for the generated policy that we store in a local file: mkdir -p policy cat > policy/secrets.yml <<EOF secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api EOF We also define the location in which the sconified application can read the certificate and the private key: cat > policy/injected_files.yml <<EOF injection_files: - path: \"/tls/flask.crt\" content: \\$\\$SCONE::flask.crt\\$\\$ - path: \"/tls/flask.key\" content: \"\\$\\$SCONE::flask.key\\$\\$\" EOF","title":"Policy Extension"},{"location":"ee_sconify_image/#creating-sconified-image","text":"To attest the CAS and uploaded a policy, we need access to the SGX device. Hence, we first determine the SGX device ( see ): determine_sgx_device mkdir -p cas We use a public CAS service ( --cas=4-2-1.scone-cas.cf ) that is running in debug mode and hence, we set --cas_debug to ensure that attestation of CAS passes despite the CAS being in debug mode. The clients key used in the attestation are stored in the local directory cas. SESSION = $( docker run -it --rm $MOUNT_SGXDEVICE -v \" $( pwd ) /policy:/policy\" -v \" $( pwd ) /cas:/root/.cas\" -v /var/run/docker.sock:/var/run/docker.sock registry.scontain.com:5050/sconecuratedimages/sconecli:sconify_image --name = flask --from = native_flask_restapi_image --to = new_image --cas = 4 -2-1.scone-cas.cf --cas_debug --binary = /usr/bin/python3 --dir = /home --dir = /usr/local/lib --dir = /app --dir = /usr/lib/python3.7 --dir = /lib --plain = /usr/lib/libpython3.7m.so.1.0 --secrets = /policy/secrets.yml --injectedfiles = /policy/injected_files.yml --create_namespace --namespace = $NS ) For production, you need to drop --cas_debug and to sign the binary You can now execute the image in the context of session $SESSION : please read the previous section to learn how to do so.","title":"Creating Sconified Image"},{"location":"ee_sconify_image/#supported-options","text":"sconify_image supports the following options: --from=NATIVE_IMAGE : name of native image (mandatory) --to=TO_IMAGE : name of encrypted image (mandatory) --template=TEMPLATE_FILE : file containing policy template (default=session-template.yml) --session=SESSION_FILE : file that will contain the session (default=session.yml) --namespace=NAMESPACE : namespace of this session (default is a random namespace) --create_namespace : create namespace NAMESPACE first --dir=DIRECTORY : add directory to encrypt; add one option per directory --plain=DIRECTORIES : copy directories that are not encrypted --cas=CAS_ADDR : set the name of the CAS_ADDR --cas_debug : permit CAS to be in debug mode --base=BASE : set the base image used to generate the encrypted image --cli=CLI : set the SCONE CLI image --trace=TRACEFILE : use tracefile to create encrypted files --name=SESSION : name of CAS policy session --secrets=FILE : policy section that defines secrets --injectedfiles=FILE : policy section that defines the injected files --sign=KEYPATH : sign the binary (uses other arguments from environment) --heap=INT : heap size in [K,M,G] --stack=INT : stack size in [K,M,G] --dlopen=[0|1|2] : dlopen: 0 - disable, 1 - enable and require authentication (default), 2 - debug only [SCONE_ALLOW_DLOPEN]","title":"Supported Options"},{"location":"ee_sconify_image/#next-step","text":"Using this approach, we 1) attest CAS and 2) ensure that the policy is only pushed to a trusted CAS. For production, you need to sign the service: for this, you need to provide a signing key that you need to pass via option --sign=KEYPATH .","title":"Next Step"},{"location":"faq/","text":"Frequently Asked Questions Attestation How can I verify the authenticity and integrity of my running running in enclave, if I access it remotely One approach is to store the encrypted certificate in an encrypted file region and SCONE CAS gives the service access to the file key only after a successful attestation. If the service can authenticate itself with the correct certificate, this means that it passed the attestation. How do I bind an interpreter (Python, JavaScript, Java) with its scripts/programs in a secure container? The problem is that the scripts/programs are not measured during the attestation of the enclave, i.e., it is not included in MrEnclave . This code must be protected with the SCONE file shield, i.e., this must be encrypted and/or authenticated. The current state of the file system is checked during attestation and this ensures the integrity of the code (i.e., no modification), the freshness (i.e., no old version is used), and the confidentiality (i.e., an adversary cannot see the code - in case the code is stored an encrypted file region). In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. How can one restrict the initial script that the protected interpreter executes? The arguments of the code executed inside of an enclave must be passed via SCONE CAS. In other words, the initial script is protected by CAS, i.e., by passing the arguments to Python only after attestation via a secure channel directly into the enclave. In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. This means that only code that is already include in an encrypted / authenticated file region can be executed. How can I pass secrets to my enclave? You could pass these secrets as environment variables via CAS or you can store these in encrypted files. SCONE will pass the environment variables and the file encryption key only after a successful attestation to the application via TLS. Certificate Verification fails SCONE CAS provides certification authorities (CAs), i.e, it can generate a CA and this CA can generate certificates on behalf of security policies (aka sessions). Sometimes the verification of these generated certificate fails. Often this indicates that the common_name or the dns names (Subject Alternative Name) are not properly set in the policy. Simulation Mode Program crashes/stops in simulation mode Simulation mode assumes a modern CPU with instructions like AESNI and SSE and AVX extensions. If they are not available, your program will exit with an error message. Attestation of programs in simulation mode While we permit to run applications in simulation mode, i.e., to run on machines without an SGX-capable CPU, we do not support the attestation of programs in simulation mode. In case you want to use attestation, you need to run on an SGX-capable machine. Memory related issues My process/enclave is getting killed without any error message Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. This means that the enclave might request so much memory that the Linux Out-Of-Memory-Killer might kill the process in which this enclave runs. Ensure that you have enough free main memory such that your enclave can start. My machine has lots of main memory but still processes are getting killed Try to figure out - using tools like top - which processes are using up your memory. Sometimes, you might have too many active docker containers running: check this with docker ps -a . Try to run docker containers with docker run --rm to enforce automatic cleanup after a container exits. My program does not start up Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. We cannot estimate how much memory your application needs. Hence, for SGX version 1, we provide environment variables SCONE_HEAP and SCONE_STACK . For Java and GO programs, set SCONE_HEAP to at least 1G . The default heap size is 64MB. The default stack size is 64KBytes. This is too little for some applications like Python and MariaDB. A good value for applications that use lots of stack seem to be 4MBytes, i.e., set export SCONE_STACK=4M . My program has a very large VIRT memory footprint SCONE reserves 64GB of the virtual address space for each enclave using the SGX driver. Hence, when monitoring a process, e.g., with top , VIRT is reported as at least 64g . Note that the important measure is the physical memory used: top reports this in column RES (given in KiB)). Crash Failures My program crashes / gets killed SGX version 1: check that your program has a sufficiently large heap by setting environment variable SCONE_HEAP ( see above ) SGX version 1: check that your program has sufficiently large stacks by setting environment variable SCONE_STACK ( see above ). SGX version 1: check that you machine has sufficient main memory see above ) Run your program with scone-gdb to determine where your program crashes Running inside of enclaves How do I know that I run inside of an enclave? Set environment variable SCONE_VERSION=1 : you will see in what mode your program is running. How can I experimentally show that I run inside of an enclave? Please check out our memory dump tutorial on this. How can I enforce/verify that a service/program runs inside of an enclave? You need to attest that your program runs inside of an enclave. SCONE supports transparent attestation with the help of CAS . On the application level, one often does not want to perform an explicit attestation but an implicit attestation with the help of TLS to reduce/eliminate the amount of reengineering that is needed. The idea is that a service can only provide the correct TLS certificate if it runs inside an enclave. To do so, one would give the enclave an encrypted TLS private key in the file system (can be generate by Scone CAS if this is requested) and the enclave gets only access to the encryption key after a successful attestation. The decryption of the TLS private key is done transparently by SCONE. Does SCONE support enclaves in production mode? Note that by default SCONE runs in debug or pre-release mode. For production enclaves, you still need to get an enclave signer key from Intel. This will change when Intel makes flexible launch control available. Shielding Does SCONE ensure the security of incoming and outgoing TCP connections to/from a service running in an enclave? Please use TLS in your service. If your service does not support TLS out of the box, use our TCP shield. How do I encrypt stdin/stdout/stderr? Send us an email on how to use the terminal shield How do I encrypt pipes? Send us an email regarding the encryption of pipes. How do I encrypt TCP connections? Context: Services like memcached do not support TLS out of the box. Send us an email on how to use the TCP shield. How do I transparently encrypt/decrypt files? Enable the file shield. Are all files in a container encrypted? No, only those in an encrypted file region. What do I need to do to protect the files my service needs, e.g. HTTPS encryption key and certificate You need to encrypt them when you build you container image. Dynamic Libraries What are protected dynamic libraries? The dynamic libraries loaded during start up of program are included in the hash of the enclave, i.e., any modification of any of these libraries will change MrEnclave . In that sense, the dynamic libraries are integrity protected since any modifications will result in a failed attestation. One can determine the dynamic libraries which are loaded during startup with command ldd for native binaries. Note that depending on how the binary is build (static linking, dynamic linking, for Alpine Linux or for Ubuntu), ldd might only print the dynamic libraries used to start the enclave. To enable the loading of protected dynamic libraries , i.e., encrypted or authenticated shared libraries, set environment variable SCONE_ALLOW_DLOPEN=1 in your session policy. If you want to disallow this, do not define SCONE_ALLOW_DLOPEN . What are unprotected dynamic libraries? An unprotected shared library , i.e., a shared library that is neither encrypted nor authenticated by the filesystem shield). It is loaded after the program has started by the application with a call to function dlopen (or similar). These libraries are not integrity protected by MrEnclave since they are loaded after MrEnclave was computed. To ensure the integrity of these shared library they have to be located in an authenticated or an encrypted file region. To enable the loading of unprotected dynamic libraries after startup, set environment variable SCONE_ALLOW_DLOPEN=2 . This will also enable loading libraries during startup. Never use SCONE_ALLOW_DLOPEN=2 in production mode . Encrypted Code and Libraries One can encrypt code by pushing your main code in a shared library that is stored in an encrypted file region. In this way, you can protect the integrity as well as the confidentiality of your code. Library Path If you experience undefined symbols like __cxa_init_primary_exception , ensure that your library path ( LD_LIBRARY_PATH ) is set such that if finds the SCONE libraries. For example, inside our SCONE crosscompiler container, you might set export LD_LIBRARY_PATH = /opt/scone/cross-compiler/x86_64-linux-musl/lib/ Resource Usage CPU utilization / number of threads is higher than in native mode By default, we use multiple threads to serve inside the enclave and to serve system calls outside the enclave. If theses threads have no work to do, they go to sleep. You can reduce the number of threads / CPU utilization by specifying a SCONE configuration file which uses less threads. For example, you could use one thread inside the enclave and one outside the enclave with this configuration file: cat > /etc/sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF The CPU utilization is still higher than in native mode In our experience, then newest version of SCONE needs less than 1-2% CPU utilization when a service is idling. If the CPU utilization of a service is higher than the native version during idle periods, you could try to tune the the backoff behavior of the queues by setting parameters L and P appropriately. Note that the standard values should in most cases do not need any tuning. The memory usage is higher than in native mode The issue is that SGX v1 enclaves must allocate all memory at startup since enclaves are fixed (- this will change with SGXv2). You can reduce memory consumption by setting SCONE_HEAP and SCONE_STACK . In SGX v2 we will allocate memory on demand, i.e., we will be more memory efficient. Note that in SGX v2, we will be able to dynamically adjust the size of the heap and the stack sizes automatically, i.e., one does not need to allocate all memory in the beginning. This will also reduce the startup times. High startup times Since in SGX v1 one needs to allocate all memory at the start of an enclave, startup times can be very large. In SGX v2, we will be able to dynamically scale the size of an enclave during runtime. Since we will allocate less memory during startup time, this will reduce the startup times. Side-Channel Protection The newest microcode of new Intel CPUs protects against L1TF side channels when hyperthreading is disabled. Please ensure that your CPU microcode is up-to-date: You can follow the following instruction to update the microcode of your CPU . Unimplemented Functions Some function returns ENOSYS Sometimes it is difficult to diagnose why a function fails. In most cases, the issue is that we do not yet support fork . You can check which functions might not be supported by running your application with environment variable SCONE_LOG=7 . Driver Issues No Access to SGX device Ensure that you installed the Intel SGX driver on your machine and that the SGX driver is accessible in your container .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#attestation","text":"","title":"Attestation"},{"location":"faq/#how-can-i-verify-the-authenticity-and-integrity-of-my-running-running-in-enclave-if-i-access-it-remotely","text":"One approach is to store the encrypted certificate in an encrypted file region and SCONE CAS gives the service access to the file key only after a successful attestation. If the service can authenticate itself with the correct certificate, this means that it passed the attestation.","title":"How can I verify the authenticity and integrity of my running running in enclave, if I access it remotely"},{"location":"faq/#how-do-i-bind-an-interpreter-python-javascript-java-with-its-scriptsprograms-in-a-secure-container","text":"The problem is that the scripts/programs are not measured during the attestation of the enclave, i.e., it is not included in MrEnclave . This code must be protected with the SCONE file shield, i.e., this must be encrypted and/or authenticated. The current state of the file system is checked during attestation and this ensures the integrity of the code (i.e., no modification), the freshness (i.e., no old version is used), and the confidentiality (i.e., an adversary cannot see the code - in case the code is stored an encrypted file region). In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail.","title":"How do I bind an interpreter (Python, JavaScript, Java) with its scripts/programs in a secure container?"},{"location":"faq/#how-can-one-restrict-the-initial-script-that-the-protected-interpreter-executes","text":"The arguments of the code executed inside of an enclave must be passed via SCONE CAS. In other words, the initial script is protected by CAS, i.e., by passing the arguments to Python only after attestation via a secure channel directly into the enclave. In a future version of SCONE, we will by default enable an option in which all files must be encrypted/authenticated, i.e., any access to an unprotected file will fail. This means that only code that is already include in an encrypted / authenticated file region can be executed.","title":"How can one restrict the initial script that the protected interpreter executes?"},{"location":"faq/#how-can-i-pass-secrets-to-my-enclave","text":"You could pass these secrets as environment variables via CAS or you can store these in encrypted files. SCONE will pass the environment variables and the file encryption key only after a successful attestation to the application via TLS.","title":"How can I pass secrets to my enclave?"},{"location":"faq/#certificate-verification-fails","text":"SCONE CAS provides certification authorities (CAs), i.e, it can generate a CA and this CA can generate certificates on behalf of security policies (aka sessions). Sometimes the verification of these generated certificate fails. Often this indicates that the common_name or the dns names (Subject Alternative Name) are not properly set in the policy.","title":"Certificate Verification fails"},{"location":"faq/#simulation-mode","text":"","title":"Simulation Mode"},{"location":"faq/#program-crashesstops-in-simulation-mode","text":"Simulation mode assumes a modern CPU with instructions like AESNI and SSE and AVX extensions. If they are not available, your program will exit with an error message.","title":"Program crashes/stops in simulation mode"},{"location":"faq/#attestation-of-programs-in-simulation-mode","text":"While we permit to run applications in simulation mode, i.e., to run on machines without an SGX-capable CPU, we do not support the attestation of programs in simulation mode. In case you want to use attestation, you need to run on an SGX-capable machine.","title":"Attestation of programs in simulation mode"},{"location":"faq/#memory-related-issues","text":"","title":"Memory related issues"},{"location":"faq/#my-processenclave-is-getting-killed-without-any-error-message","text":"Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. This means that the enclave might request so much memory that the Linux Out-Of-Memory-Killer might kill the process in which this enclave runs. Ensure that you have enough free main memory such that your enclave can start.","title":"My process/enclave is getting killed without any error message"},{"location":"faq/#my-machine-has-lots-of-main-memory-but-still-processes-are-getting-killed","text":"Try to figure out - using tools like top - which processes are using up your memory. Sometimes, you might have too many active docker containers running: check this with docker ps -a . Try to run docker containers with docker run --rm to enforce automatic cleanup after a container exits.","title":"My machine has lots of main memory but still processes are getting killed"},{"location":"faq/#my-program-does-not-start-up","text":"Context : For SGX version 1, we have to preallocate all memory an enclave can use during its startup. We cannot estimate how much memory your application needs. Hence, for SGX version 1, we provide environment variables SCONE_HEAP and SCONE_STACK . For Java and GO programs, set SCONE_HEAP to at least 1G . The default heap size is 64MB. The default stack size is 64KBytes. This is too little for some applications like Python and MariaDB. A good value for applications that use lots of stack seem to be 4MBytes, i.e., set export SCONE_STACK=4M .","title":"My program does not start up"},{"location":"faq/#my-program-has-a-very-large-virt-memory-footprint","text":"SCONE reserves 64GB of the virtual address space for each enclave using the SGX driver. Hence, when monitoring a process, e.g., with top , VIRT is reported as at least 64g . Note that the important measure is the physical memory used: top reports this in column RES (given in KiB)).","title":"My program has a very large VIRT memory footprint"},{"location":"faq/#crash-failures","text":"","title":"Crash Failures"},{"location":"faq/#my-program-crashes-gets-killed","text":"SGX version 1: check that your program has a sufficiently large heap by setting environment variable SCONE_HEAP ( see above ) SGX version 1: check that your program has sufficiently large stacks by setting environment variable SCONE_STACK ( see above ). SGX version 1: check that you machine has sufficient main memory see above ) Run your program with scone-gdb to determine where your program crashes","title":"My program crashes / gets killed"},{"location":"faq/#running-inside-of-enclaves","text":"","title":"Running inside of enclaves"},{"location":"faq/#how-do-i-know-that-i-run-inside-of-an-enclave","text":"Set environment variable SCONE_VERSION=1 : you will see in what mode your program is running.","title":"How do I know that I run inside of an enclave?"},{"location":"faq/#how-can-i-experimentally-show-that-i-run-inside-of-an-enclave","text":"Please check out our memory dump tutorial on this.","title":"How can I experimentally show that I run inside of an enclave?"},{"location":"faq/#how-can-i-enforceverify-that-a-serviceprogram-runs-inside-of-an-enclave","text":"You need to attest that your program runs inside of an enclave. SCONE supports transparent attestation with the help of CAS . On the application level, one often does not want to perform an explicit attestation but an implicit attestation with the help of TLS to reduce/eliminate the amount of reengineering that is needed. The idea is that a service can only provide the correct TLS certificate if it runs inside an enclave. To do so, one would give the enclave an encrypted TLS private key in the file system (can be generate by Scone CAS if this is requested) and the enclave gets only access to the encryption key after a successful attestation. The decryption of the TLS private key is done transparently by SCONE.","title":"How can I enforce/verify that a service/program runs inside of an enclave?"},{"location":"faq/#does-scone-support-enclaves-in-production-mode","text":"Note that by default SCONE runs in debug or pre-release mode. For production enclaves, you still need to get an enclave signer key from Intel. This will change when Intel makes flexible launch control available.","title":"Does SCONE support enclaves in production mode?"},{"location":"faq/#shielding","text":"","title":"Shielding"},{"location":"faq/#does-scone-ensure-the-security-of-incoming-and-outgoing-tcp-connections-tofrom-a-service-running-in-an-enclave","text":"Please use TLS in your service. If your service does not support TLS out of the box, use our TCP shield.","title":"Does SCONE ensure the security of incoming and outgoing TCP connections to/from a service running in an enclave?"},{"location":"faq/#how-do-i-encrypt-stdinstdoutstderr","text":"Send us an email on how to use the terminal shield","title":"How do I encrypt stdin/stdout/stderr?"},{"location":"faq/#how-do-i-encrypt-pipes","text":"Send us an email regarding the encryption of pipes.","title":"How do I encrypt pipes?"},{"location":"faq/#how-do-i-encrypt-tcp-connections","text":"Context: Services like memcached do not support TLS out of the box. Send us an email on how to use the TCP shield.","title":"How do I encrypt TCP connections?"},{"location":"faq/#how-do-i-transparently-encryptdecrypt-files","text":"Enable the file shield.","title":"How do I transparently encrypt/decrypt files?"},{"location":"faq/#are-all-files-in-a-container-encrypted","text":"No, only those in an encrypted file region.","title":"Are all files in a container encrypted?"},{"location":"faq/#what-do-i-need-to-do-to-protect-the-files-my-service-needs-eg-https-encryption-key-and-certificate","text":"You need to encrypt them when you build you container image.","title":"What do I need to do to protect the files my service needs, e.g. HTTPS encryption key and certificate"},{"location":"faq/#dynamic-libraries","text":"","title":"Dynamic Libraries"},{"location":"faq/#what-are-protected-dynamic-libraries","text":"The dynamic libraries loaded during start up of program are included in the hash of the enclave, i.e., any modification of any of these libraries will change MrEnclave . In that sense, the dynamic libraries are integrity protected since any modifications will result in a failed attestation. One can determine the dynamic libraries which are loaded during startup with command ldd for native binaries. Note that depending on how the binary is build (static linking, dynamic linking, for Alpine Linux or for Ubuntu), ldd might only print the dynamic libraries used to start the enclave. To enable the loading of protected dynamic libraries , i.e., encrypted or authenticated shared libraries, set environment variable SCONE_ALLOW_DLOPEN=1 in your session policy. If you want to disallow this, do not define SCONE_ALLOW_DLOPEN .","title":"What are protected dynamic libraries?"},{"location":"faq/#what-are-unprotected-dynamic-libraries","text":"An unprotected shared library , i.e., a shared library that is neither encrypted nor authenticated by the filesystem shield). It is loaded after the program has started by the application with a call to function dlopen (or similar). These libraries are not integrity protected by MrEnclave since they are loaded after MrEnclave was computed. To ensure the integrity of these shared library they have to be located in an authenticated or an encrypted file region. To enable the loading of unprotected dynamic libraries after startup, set environment variable SCONE_ALLOW_DLOPEN=2 . This will also enable loading libraries during startup. Never use SCONE_ALLOW_DLOPEN=2 in production mode .","title":"What are unprotected dynamic libraries?"},{"location":"faq/#encrypted-code-and-libraries","text":"One can encrypt code by pushing your main code in a shared library that is stored in an encrypted file region. In this way, you can protect the integrity as well as the confidentiality of your code.","title":"Encrypted Code and Libraries"},{"location":"faq/#library-path","text":"If you experience undefined symbols like __cxa_init_primary_exception , ensure that your library path ( LD_LIBRARY_PATH ) is set such that if finds the SCONE libraries. For example, inside our SCONE crosscompiler container, you might set export LD_LIBRARY_PATH = /opt/scone/cross-compiler/x86_64-linux-musl/lib/","title":"Library Path"},{"location":"faq/#resource-usage","text":"","title":"Resource Usage"},{"location":"faq/#cpu-utilization-number-of-threads-is-higher-than-in-native-mode","text":"By default, we use multiple threads to serve inside the enclave and to serve system calls outside the enclave. If theses threads have no work to do, they go to sleep. You can reduce the number of threads / CPU utilization by specifying a SCONE configuration file which uses less threads. For example, you could use one thread inside the enclave and one outside the enclave with this configuration file: cat > /etc/sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF","title":"CPU utilization / number of threads is higher than in native mode"},{"location":"faq/#the-cpu-utilization-is-still-higher-than-in-native-mode","text":"In our experience, then newest version of SCONE needs less than 1-2% CPU utilization when a service is idling. If the CPU utilization of a service is higher than the native version during idle periods, you could try to tune the the backoff behavior of the queues by setting parameters L and P appropriately. Note that the standard values should in most cases do not need any tuning.","title":"The CPU utilization is still higher than in native mode"},{"location":"faq/#the-memory-usage-is-higher-than-in-native-mode","text":"The issue is that SGX v1 enclaves must allocate all memory at startup since enclaves are fixed (- this will change with SGXv2). You can reduce memory consumption by setting SCONE_HEAP and SCONE_STACK . In SGX v2 we will allocate memory on demand, i.e., we will be more memory efficient. Note that in SGX v2, we will be able to dynamically adjust the size of the heap and the stack sizes automatically, i.e., one does not need to allocate all memory in the beginning. This will also reduce the startup times.","title":"The memory usage is higher than in native mode"},{"location":"faq/#high-startup-times","text":"Since in SGX v1 one needs to allocate all memory at the start of an enclave, startup times can be very large. In SGX v2, we will be able to dynamically scale the size of an enclave during runtime. Since we will allocate less memory during startup time, this will reduce the startup times.","title":"High startup times"},{"location":"faq/#side-channel-protection","text":"The newest microcode of new Intel CPUs protects against L1TF side channels when hyperthreading is disabled. Please ensure that your CPU microcode is up-to-date: You can follow the following instruction to update the microcode of your CPU .","title":"Side-Channel Protection"},{"location":"faq/#unimplemented-functions","text":"","title":"Unimplemented Functions"},{"location":"faq/#some-function-returns-enosys","text":"Sometimes it is difficult to diagnose why a function fails. In most cases, the issue is that we do not yet support fork . You can check which functions might not be supported by running your application with environment variable SCONE_LOG=7 .","title":"Some function returns ENOSYS"},{"location":"faq/#driver-issues","text":"","title":"Driver Issues"},{"location":"faq/#no-access-to-sgx-device","text":"Ensure that you installed the Intel SGX driver on your machine and that the SGX driver is accessible in your container .","title":"No Access to SGX device"},{"location":"federated_machine_learning/","text":"Confidential Federated Machine Learning In this use case, we demonstrate how SCONE supports confidential federated machine learning. SCONE can protect the confidentiality and integrity of the training data, the generated model, and the interference In addition, not only the integrity but also the confidentiality of the code can be protected with the help of SGX and SCONE. For more details, please have a look at this video:","title":"Federated Machine Learning"},{"location":"federated_machine_learning/#confidential-federated-machine-learning","text":"In this use case, we demonstrate how SCONE supports confidential federated machine learning. SCONE can protect the confidentiality and integrity of the training data, the generated model, and the interference In addition, not only the integrity but also the confidentiality of the code can be protected with the help of SGX and SCONE. For more details, please have a look at this video:","title":"Confidential Federated Machine Learning"},{"location":"firstcontainer/","text":"Creating Your First SCONE program Hello World in Simulation Mode Let's start with a simple hello world program that we run inside a container on top of SCONE 2 . We first need to start the SCONE crosscompiler. The crosscompiler container image is hosted in a private repository on Docker hub and can be started with the help of docker: docker run -it registry.scontain.com:5050/sconecuratedimages/crosscompilers A docker engine must be installed and you need access to registry.scontain.com:5050/sconecuratedimages/crosscompilers You need to install a docker engine . In some docker installations, you might have to replace \"docker\" by \"sudo docker\". Just register a free account on gitlab.scontain.com . Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with the SCONE crosscompiler (i.e., gcc): gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we did not give the container access to SGX, the program runs in simulation mode , i.e., the SCONE software runs but we do not use Intel SGX enclaves. Use simulation mode only for development and debugging This mode must not be used for production since programs do not run inside of enclaves . Simulation mode will run on modern Intel CPUs - even those without Intel SGX. It might, however, fail on old CPUs without AES hardware support. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 501194b1da9d4e86828353349cc7f9ef310b0dd1 Enclave hash: a01127f2190ed5ecd21f9fd432e4d07f7f250ad1e1808d9c0305e75505383c44 Hello World The output shows that SCONE is running in simulation mode : export SCONE_MODE=sim Background Info The most convenient way to use SCONE for development is to enable automatic (a.k.a. AUTO ) mode 1 . In AUTO mode, you neither need access to SGX-capable CPUs nor do you need to install any new software on your host: you only need to have access to a Docker engine. If you have access to an SGX-capable CPU and you give the container access to the sgx device, SCONE will run applications inside of SGX enclaves. Otherwise, the applications will run in simulation mode. Let's see in the next chapter how to run the hello world program inside an Intel SGX enclave. This is the default mode: see description of environment variable SCONE_MODE . \u21a9 Just register a free account on gitlab.scontain.com . \u21a9","title":"Simulation Mode"},{"location":"firstcontainer/#creating-your-first-scone-program","text":"","title":"Creating Your First SCONE program"},{"location":"firstcontainer/#hello-world-in-simulation-mode","text":"Let's start with a simple hello world program that we run inside a container on top of SCONE 2 . We first need to start the SCONE crosscompiler. The crosscompiler container image is hosted in a private repository on Docker hub and can be started with the help of docker: docker run -it registry.scontain.com:5050/sconecuratedimages/crosscompilers A docker engine must be installed and you need access to registry.scontain.com:5050/sconecuratedimages/crosscompilers You need to install a docker engine . In some docker installations, you might have to replace \"docker\" by \"sudo docker\". Just register a free account on gitlab.scontain.com . Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with the SCONE crosscompiler (i.e., gcc): gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we did not give the container access to SGX, the program runs in simulation mode , i.e., the SCONE software runs but we do not use Intel SGX enclaves. Use simulation mode only for development and debugging This mode must not be used for production since programs do not run inside of enclaves . Simulation mode will run on modern Intel CPUs - even those without Intel SGX. It might, however, fail on old CPUs without AES hardware support. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 501194b1da9d4e86828353349cc7f9ef310b0dd1 Enclave hash: a01127f2190ed5ecd21f9fd432e4d07f7f250ad1e1808d9c0305e75505383c44 Hello World The output shows that SCONE is running in simulation mode : export SCONE_MODE=sim","title":"Hello World in Simulation Mode"},{"location":"firstcontainer/#background-info","text":"The most convenient way to use SCONE for development is to enable automatic (a.k.a. AUTO ) mode 1 . In AUTO mode, you neither need access to SGX-capable CPUs nor do you need to install any new software on your host: you only need to have access to a Docker engine. If you have access to an SGX-capable CPU and you give the container access to the sgx device, SCONE will run applications inside of SGX enclaves. Otherwise, the applications will run in simulation mode. Let's see in the next chapter how to run the hello world program inside an Intel SGX enclave. This is the default mode: see description of environment variable SCONE_MODE . \u21a9 Just register a free account on gitlab.scontain.com . \u21a9","title":"Background Info"},{"location":"flask_demo/","text":"A Confidential Flask-Based Application We demonstrate with the help of a simple Flask-based Service multiple features of the SCONE platform: we show that we can execute unmodified Python programs inside of SGX enclaves we show how to encrypt the Python program to protect the confidentiality and integrity of the Python code how to implicitly attest other services with the help of TLS, i.e., to ensure that one communicates with a service that satisfy its security policy. we demonstrate how Redis, an in-memory data structure store, and the Python flask attest each other via TLS without needing to change the code of neither Redis nor the Flask-based service. we show how to generate TLS certificates with the help of a policy: a SCONE security policy describes how to attest applications and services (i.e., describe the code, the filesystem state, the environment, the node on which to execute, and secrets). a SCONE policy can generate secrets and in particular, key-pairs and TLS certificates. we show how to execute this example on a local computer with the help of docker-compose on a generic Kubernetes cluster, and on Azure Kubernetes Service (AKS). Next Step In the second version of this example, we simplify the workflow in the sense that we use a generic script to transform an existing native container image into an encrypted, confidential container image. Flask-Based Confidential Service We implement a simple Flask-based service. The Python code implements a REST API: to store patient records (i.e., POST to resource /patient/<string:patient_id> ) to retrieve patient records (i.e., GET of resource /patient/<string:patient_id> ) to retrieve some score for a patient (i.e., GET of ressource '/score/<string:patient_id> ) The Python code is executed inside of an enclave to ensure that even users with root access cannot read the patient data. TLS Certificates The service uses a Redis instance to store the resources. The communication between 1) the Flask-based service and its clients and 2) Redis and the application is encrypted with the help of TLS. To do so, we need to provision the application and Redis with multiple keys and certificates: Redis client certificate Redis server certificate Flask server certificate Redis and the Flask-based service, require that the private keys and certificates are stored in the filesystem. We generate and provision these TLS-related files with the help of a SCONE policy . To do so, we generate secrets related to the Flask-based service. We specify in the flask policy that a private key ( api_ca_key ) for a new certificate authority (CA) is generated a certificate ( api_ca_cert ) for a certification authority is generated using the private key (i.e., api_ca_key ), and making this certificate available to everybody (see export_public: true ) we generate a private key for the certificate used by the REST API (i.e., flask_key ) we generate a certificate ( flask ) with the help of CA api_ca_cert and assign it a dns name api . The SCONE policy is based on Yaml and the flask policy contains the following section to define these secrets: secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api The private keys and certificates are expected at certain locations in the file system. SCONE permits to map these secrets into the filesystem of the Flask-based service: these files are only visible to the service inside of an SGX enclave after a successful attestation (see below) and in particular, not visible on the outside i.e., in the filesystem of the container. To map the private keys and certificates into the filesystem of a service, we specify in the policy which secrets are visible to a service at which path. In the flask policy this is done as follows: images: - name: flask_restapi_image injection_files: - path: /tls/flask.crt content: $$SCONE::flask.crt$$ - path: /tls/flask.key content: $$SCONE::flask.key$$ And in the Python program, one can just access these files as normal files. One can create a SSL context (see code ): app . run ( host = '0.0.0.0' , port = 4996 , threaded = True , ssl_context = (( \"/tls/flask.crt\" , \"/tls/flask.key\" ))) While we do not show how to enforce client authentication of the REST API, we show how to do this for Redis in the next section. TLS-based Mutual Attestation The communication between the Flask-based service, say, S S and Redis instance, say, R R is encrypted via TLS. Actually, we make sure that the service S S and instance R R attest each other. Attestation means that S S ensures that R R satisfies all requirements specified in R R 's security policy and R R ensures that S S satisfies all the requirements of S S 's policy. Of course, this should be done without changing the code of neither S S nor R R . In case that S S and R R are using TLS with client authentication, this is straightforward to enforce. (If this is not the case, please contact us for an alternative.) To ensure mutual attestation, the operator of Redis defines a policy in which it defines a certification authority ( redis_ca_cert ) and defines both a Redis certificate ( redis_ca_cert ) as well as a Redis client certificate ( redis_client_cert ). The client certificate and the private key ( redis_client_key ) are exported to the policy of the Flask service S S . The policy for this looks like this: secrets: - name: redis_key kind: private-key - name: redis # automatically generate Redis server certificate kind: x509 private_key: redis_key issuer: redis_ca_cert dns: - redis - name: redis_client_key kind: private-key export: - session: $FLASK_SESSION - name: redis_client_cert # automatically generate client certificate kind: x509 issuer: redis_ca_cert private_key: redis_client_key export: - session: $FLASK_SESSION # export client cert/key to client session - name: redis_ca_key kind: private-key - name: redis_ca_cert # export session CA certificate as Redis CA certificate kind: x509-ca private_key: redis_ca_key export: - session: $FLASK_SESSION # export the session CA certificate to client session Note that $FLASK_SESSION is replaced by the unique name of the policy of S S . The security policies are in this example on the same SCONE CAS (Configuration and Attestation Service) . In more complex scenarios, the policies could also be stored on separate SCONE CAS instances operated by different entities. The flask service can import the Redis CA certificate, client certificate and private key as follows: secrets: - name: redis_client_key import: session: $REDIS_SESSION secret: redis_client_key - name: redis_client_cert import: session: $REDIS_SESSION secret: redis_client_cert - name: redis_ca_cert import: session: $REDIS_SESSION secret: redis_ca_cert These secrets are made available to the Flask-based service in the filesystem (i.e., files /tls/redis-ca.crt , /tls/client.crt and /tls/client.key ) via the following entries in its security policy: images: - name: flask_restapi_image injection_files: - path: /tls/redis-ca.crt content: $$SCONE::redis_ca_cert.chain$$ - path: /tls/client.crt content: $$SCONE::redis_client_cert.crt$$ - path: /tls/client.key content: $$SCONE::redis_client_cert.key$$ Note that before uploading a policy to SCONE CAS, one first attests that one indeed communicates with a genuine SCONE CAS running inside of a production enclave. This is done with the help of a SCONE CAS CLI . Code The source code of this example is open source and available on github: git clone https://github.com/scontain/flask_example.git cd flask_example Run Service On Local Computer You can use docker-compose to run this example on your local SGX-enabled computer as follows. You first generate an encrypted image using script create_image.sh . This generates some environment variables that stored in file myenv and are loaded via source myenv . The service and Redis are started with docker-compose up . ./create_image.sh source myenv docker-compose up We use a public instance of SCONE CAS in this example. Testing the service Retrieve the API certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api:4996 curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /patient/patient_3 --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /score/patient_3 --resolve api:4996:127.0.0.1 The output might look as follows: $ curl --cacert cacert.pem -X POST https://localhost:4996/patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/patient/patient_3 --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/score/patient_3 --resolve api:4996:127.0.0.1 {\"id\":\"patient_3\",\"score\":0.2781606437899131} Execution on a Kubernetes Cluster and AKS You can run this example on a Kubernetes cluster or Azure Kubernetes Service (AKS). Install SCONE services Get access to SconeApps (see https://sconedocs.github.io/helm/ ): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update Give SconeApps access to the private docker images by generating an access token on https://gitlab.scontain.com/-/profile/personal_access_tokens to read_registry and set: export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Start LAS: helm install las sconeapps/las --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone4.2.1 --set service.hostPort = true If you use a local cas, you can start this cas service as follows: helm install cas sconeapps/cas --set image = registry.scontain.com:5050/sconecuratedimages/services:cas-scone4.2.1 Install the SGX device plugin for Kubernetes: helm install sgxdevplugin sconeapps/sgxdevplugin Run the application Start by creating a Docker image and setting its name. Remember to specify a repository to which you are allowed to push: export IMAGE = registry.scontain.com:5050/sconecuratedimages/application:v0.4 # please change to an image that you can push ./create_image.sh source myenv docker push $IMAGE Use the Helm chart in deploy/helm to deploy the application to a Kubernetes cluster. helm install api-v1 deploy/helm \\ --set image = $IMAGE \\ --set scone.cas = $SCONE_CAS_ADDR \\ --set scone.flask_session = $FLASK_SESSION /flask_restapi \\ --set scone.redis_session = $REDIS_SESSION /redis \\ --set service.type = NodePort \\ --set redis.image = registry.scontain.com:5050/sconecuratedimages/apps:redis-6-alpine-scone4.2.1 NOTE : Setting service.type=LoadBalancer will allow the application to get traffic from the internet (through a managed LoadBalancer). Test the application After all resources are Running , you can test the API via Helm: helm test api-v1 Helm will run a pod with a couple of pre-set queries to check if the API is working properly. Access the application If the application is exposed to the world through a service of type LoadBalancer, you can retrieve its CA certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Retrieve the service public IP address: export SERVICE_IP = $( kubectl get svc --namespace default api-v1-example --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api Now you can perform queries such as: curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:443: ${ SERVICE_IP } Clean up helm delete cas helm delete las helm delete sgxdevplugin helm delete api-v1 kubectl delete pod api-v1-example-test-api Next, we introduce the different sconified Python versions that we support.","title":"Flask Demo"},{"location":"flask_demo/#a-confidential-flask-based-application","text":"We demonstrate with the help of a simple Flask-based Service multiple features of the SCONE platform: we show that we can execute unmodified Python programs inside of SGX enclaves we show how to encrypt the Python program to protect the confidentiality and integrity of the Python code how to implicitly attest other services with the help of TLS, i.e., to ensure that one communicates with a service that satisfy its security policy. we demonstrate how Redis, an in-memory data structure store, and the Python flask attest each other via TLS without needing to change the code of neither Redis nor the Flask-based service. we show how to generate TLS certificates with the help of a policy: a SCONE security policy describes how to attest applications and services (i.e., describe the code, the filesystem state, the environment, the node on which to execute, and secrets). a SCONE policy can generate secrets and in particular, key-pairs and TLS certificates. we show how to execute this example on a local computer with the help of docker-compose on a generic Kubernetes cluster, and on Azure Kubernetes Service (AKS). Next Step In the second version of this example, we simplify the workflow in the sense that we use a generic script to transform an existing native container image into an encrypted, confidential container image.","title":"A Confidential Flask-Based Application"},{"location":"flask_demo/#flask-based-confidential-service","text":"We implement a simple Flask-based service. The Python code implements a REST API: to store patient records (i.e., POST to resource /patient/<string:patient_id> ) to retrieve patient records (i.e., GET of resource /patient/<string:patient_id> ) to retrieve some score for a patient (i.e., GET of ressource '/score/<string:patient_id> ) The Python code is executed inside of an enclave to ensure that even users with root access cannot read the patient data.","title":"Flask-Based Confidential Service"},{"location":"flask_demo/#tls-certificates","text":"The service uses a Redis instance to store the resources. The communication between 1) the Flask-based service and its clients and 2) Redis and the application is encrypted with the help of TLS. To do so, we need to provision the application and Redis with multiple keys and certificates: Redis client certificate Redis server certificate Flask server certificate Redis and the Flask-based service, require that the private keys and certificates are stored in the filesystem. We generate and provision these TLS-related files with the help of a SCONE policy . To do so, we generate secrets related to the Flask-based service. We specify in the flask policy that a private key ( api_ca_key ) for a new certificate authority (CA) is generated a certificate ( api_ca_cert ) for a certification authority is generated using the private key (i.e., api_ca_key ), and making this certificate available to everybody (see export_public: true ) we generate a private key for the certificate used by the REST API (i.e., flask_key ) we generate a certificate ( flask ) with the help of CA api_ca_cert and assign it a dns name api . The SCONE policy is based on Yaml and the flask policy contains the following section to define these secrets: secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api The private keys and certificates are expected at certain locations in the file system. SCONE permits to map these secrets into the filesystem of the Flask-based service: these files are only visible to the service inside of an SGX enclave after a successful attestation (see below) and in particular, not visible on the outside i.e., in the filesystem of the container. To map the private keys and certificates into the filesystem of a service, we specify in the policy which secrets are visible to a service at which path. In the flask policy this is done as follows: images: - name: flask_restapi_image injection_files: - path: /tls/flask.crt content: $$SCONE::flask.crt$$ - path: /tls/flask.key content: $$SCONE::flask.key$$ And in the Python program, one can just access these files as normal files. One can create a SSL context (see code ): app . run ( host = '0.0.0.0' , port = 4996 , threaded = True , ssl_context = (( \"/tls/flask.crt\" , \"/tls/flask.key\" ))) While we do not show how to enforce client authentication of the REST API, we show how to do this for Redis in the next section.","title":"TLS Certificates"},{"location":"flask_demo/#tls-based-mutual-attestation","text":"The communication between the Flask-based service, say, S S and Redis instance, say, R R is encrypted via TLS. Actually, we make sure that the service S S and instance R R attest each other. Attestation means that S S ensures that R R satisfies all requirements specified in R R 's security policy and R R ensures that S S satisfies all the requirements of S S 's policy. Of course, this should be done without changing the code of neither S S nor R R . In case that S S and R R are using TLS with client authentication, this is straightforward to enforce. (If this is not the case, please contact us for an alternative.) To ensure mutual attestation, the operator of Redis defines a policy in which it defines a certification authority ( redis_ca_cert ) and defines both a Redis certificate ( redis_ca_cert ) as well as a Redis client certificate ( redis_client_cert ). The client certificate and the private key ( redis_client_key ) are exported to the policy of the Flask service S S . The policy for this looks like this: secrets: - name: redis_key kind: private-key - name: redis # automatically generate Redis server certificate kind: x509 private_key: redis_key issuer: redis_ca_cert dns: - redis - name: redis_client_key kind: private-key export: - session: $FLASK_SESSION - name: redis_client_cert # automatically generate client certificate kind: x509 issuer: redis_ca_cert private_key: redis_client_key export: - session: $FLASK_SESSION # export client cert/key to client session - name: redis_ca_key kind: private-key - name: redis_ca_cert # export session CA certificate as Redis CA certificate kind: x509-ca private_key: redis_ca_key export: - session: $FLASK_SESSION # export the session CA certificate to client session Note that $FLASK_SESSION is replaced by the unique name of the policy of S S . The security policies are in this example on the same SCONE CAS (Configuration and Attestation Service) . In more complex scenarios, the policies could also be stored on separate SCONE CAS instances operated by different entities. The flask service can import the Redis CA certificate, client certificate and private key as follows: secrets: - name: redis_client_key import: session: $REDIS_SESSION secret: redis_client_key - name: redis_client_cert import: session: $REDIS_SESSION secret: redis_client_cert - name: redis_ca_cert import: session: $REDIS_SESSION secret: redis_ca_cert These secrets are made available to the Flask-based service in the filesystem (i.e., files /tls/redis-ca.crt , /tls/client.crt and /tls/client.key ) via the following entries in its security policy: images: - name: flask_restapi_image injection_files: - path: /tls/redis-ca.crt content: $$SCONE::redis_ca_cert.chain$$ - path: /tls/client.crt content: $$SCONE::redis_client_cert.crt$$ - path: /tls/client.key content: $$SCONE::redis_client_cert.key$$ Note that before uploading a policy to SCONE CAS, one first attests that one indeed communicates with a genuine SCONE CAS running inside of a production enclave. This is done with the help of a SCONE CAS CLI .","title":"TLS-based Mutual Attestation"},{"location":"flask_demo/#code","text":"The source code of this example is open source and available on github: git clone https://github.com/scontain/flask_example.git cd flask_example","title":"Code"},{"location":"flask_demo/#run-service-on-local-computer","text":"You can use docker-compose to run this example on your local SGX-enabled computer as follows. You first generate an encrypted image using script create_image.sh . This generates some environment variables that stored in file myenv and are loaded via source myenv . The service and Redis are started with docker-compose up . ./create_image.sh source myenv docker-compose up We use a public instance of SCONE CAS in this example.","title":"Run Service On Local Computer"},{"location":"flask_demo/#testing-the-service","text":"Retrieve the API certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api:4996 curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /patient/patient_3 --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /score/patient_3 --resolve api:4996:127.0.0.1 The output might look as follows: $ curl --cacert cacert.pem -X POST https://localhost:4996/patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/patient/patient_3 --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/score/patient_3 --resolve api:4996:127.0.0.1 {\"id\":\"patient_3\",\"score\":0.2781606437899131}","title":"Testing the service"},{"location":"flask_demo/#execution-on-a-kubernetes-cluster-and-aks","text":"You can run this example on a Kubernetes cluster or Azure Kubernetes Service (AKS).","title":"Execution on a Kubernetes Cluster and AKS"},{"location":"flask_demo/#install-scone-services","text":"Get access to SconeApps (see https://sconedocs.github.io/helm/ ): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update Give SconeApps access to the private docker images by generating an access token on https://gitlab.scontain.com/-/profile/personal_access_tokens to read_registry and set: export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Start LAS: helm install las sconeapps/las --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone4.2.1 --set service.hostPort = true If you use a local cas, you can start this cas service as follows: helm install cas sconeapps/cas --set image = registry.scontain.com:5050/sconecuratedimages/services:cas-scone4.2.1 Install the SGX device plugin for Kubernetes: helm install sgxdevplugin sconeapps/sgxdevplugin","title":"Install SCONE services"},{"location":"flask_demo/#run-the-application","text":"Start by creating a Docker image and setting its name. Remember to specify a repository to which you are allowed to push: export IMAGE = registry.scontain.com:5050/sconecuratedimages/application:v0.4 # please change to an image that you can push ./create_image.sh source myenv docker push $IMAGE Use the Helm chart in deploy/helm to deploy the application to a Kubernetes cluster. helm install api-v1 deploy/helm \\ --set image = $IMAGE \\ --set scone.cas = $SCONE_CAS_ADDR \\ --set scone.flask_session = $FLASK_SESSION /flask_restapi \\ --set scone.redis_session = $REDIS_SESSION /redis \\ --set service.type = NodePort \\ --set redis.image = registry.scontain.com:5050/sconecuratedimages/apps:redis-6-alpine-scone4.2.1 NOTE : Setting service.type=LoadBalancer will allow the application to get traffic from the internet (through a managed LoadBalancer).","title":"Run the application"},{"location":"flask_demo/#test-the-application","text":"After all resources are Running , you can test the API via Helm: helm test api-v1 Helm will run a pod with a couple of pre-set queries to check if the API is working properly.","title":"Test the application"},{"location":"flask_demo/#access-the-application","text":"If the application is exposed to the world through a service of type LoadBalancer, you can retrieve its CA certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Retrieve the service public IP address: export SERVICE_IP = $( kubectl get svc --namespace default api-v1-example --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api Now you can perform queries such as: curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:443: ${ SERVICE_IP }","title":"Access the application"},{"location":"flask_demo/#clean-up","text":"helm delete cas helm delete las helm delete sgxdevplugin helm delete api-v1 kubectl delete pod api-v1-example-test-api Next, we introduce the different sconified Python versions that we support.","title":"Clean up"},{"location":"glossary/","text":"Glossary attestation Process of proving the integrity and authenticity of the attestee's software and/or hardware component to a verifier. This includes validating if a given software is executed on a given hardware. local attestation Attestation executed locally, e.g., one software component validates the integrity and authenticity of another software component which is executed on the same hardware. In Intel SGX, the CPU creates a report containing integrity information of the attested enclave whose keyed-MAC can only be verified, and changed for that matter, by the verifier enclave running on the same platform. remote attestation Attestation executed remotely, i.e., the component which does the validation and the component which is validated are executed on different machines. In Intel SGX, the report received during local attestation is signed with the quoting enclave's private key making the integrity of the quote - the signed report - remotely verifiable. mutual attestation Process of two components proving to each other the integrity and authenticity of their software and/or hardware components. This can include multiple aspects like that neither their code nor their filesystem was modified. Configuration and Attestation Service (CAS) The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client. SCONE CAS The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client. Local Attestation Service (LAS) A per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave. SCONE LAS The SCONE Local Attestation Service (LAS) is a per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave. secure boot A boot procedure which allows only the execution of firmware, bootloaders and operating systems which are digitally signed by a (well) defined set of acceptable signers. measured boot A boot procedure which measures the state of the system at each boot step. This measurement can be accessed to verify the current state of a given system. Compared to secure boot, measured boot will not prevent an \"insecure\" state of the system. cloud-native application An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers. cloud provider An entity providing cloud services (PaaS, IaaS, MaaS etc.) to its customers. It is assumed that a cloud provider is in physical or logical control of the hardware and system software used to provide the cloud services. container An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions. Kubernetes An open source container orchestration platform. More information at https://kubernetes.io/ Kubeapps A dashboard to deploy and manage Kubernetes based applications. It can be used to deploy and manage SCONE-based confidential applications. curated image A container image of a popular service maintained by scontain.com. enclave This is an alias for SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC. Docker image A snapshot of a container's state that can be used to initialize new containers with the same state. Docker registry A Docker registry stores Docker images for the purpose of easy distribution comparable to the app stores of Android or iOS. EPC A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads. microservice A rather small component which offers a single service. service provider A company operating an application - typically, making these available via the Internet. We use this a general term that includes different models like Software as a Service (SaaS) providers as well as Hosted service providers etc. SCONE SCONE (Secure CONtainer Environment) is a software platform for confidential computing allowing the trustworthy execution of unmodified x86 source code within Intel SGX enclaves. It consists of components enabling the execution inside enclaves such as the SCONE runtime and the C, C++, C#, Fortran, Go, and Rust SCONE cross-compilers, components ensuring the trustworthiness of this execution and deployment in clouds such as the CAS and SCONE client. SCONE Docker image A SCONE Docker image is a Docker image that contains an SCONE-enabled executable and is additionally annotated via image labels with metadata allowing the attestation of the started SCONE-enabled executable and the image's file system content. SCONE container A SCONE container is a running instance of a SCONE Docker image. SCONE microservice Microservice which is a SCONE-enabled executable. SCONE-enabled executable An executable created by a SCONE cross-compiler. The actual program will be executed within an enclave and utilises the SCONE runtime. SCONE runtime The runtime environment necessary to execute a SCONE-enabled executable. At the moment this consists of a modified C-library based on the musl library. SCONE cross-compiler Compilers for various programming languages such as C, C++, Rust, Go, and Fortran which compile source code into a SCONE-enabled executable. SCONE client A program that is used to configure SCONE-enabled executables. It allows the user to push confidential configurations to the CAS and encrypt files to ensure their content is only accessible by specific SCONE-enabled executables executed inside enclaves. SCONE infrastructure The SCONE infrastructure summarises all components necessary to deploy and run a SCONE-enabled executable. This includes Docker components like the Docker daemon and the Docker registry as well as the SCONE client and additional services like CAS and LAS. Secure container A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE. Secure program A program that executes inside an enclave. SGX (Software Guard eXtension) A CPU extension by Intel that permits to create SGX enclaves. SGX Software Guard eXtension (SGX) is a CPU extension by Intel that permits to create enclaves, i.e., a protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. SGX enclave A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC. threading SCONE uses different kind of threads: ethread : a thread that executes application threads inside of an enclave lthread : an application thread. Typically, created by the application directly or indirectly via a pthread_create call. In SCONE, this pthread_create call will create a lthread . The lthread is executed by some ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly. sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave trusted computing base (TCB) The set of hardware and software components which can break the security policy. Therefore one has to trust that these components are not malicious or faulty. trusted execution environment A trusted execution environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity. TEE A Trusted Execution Environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity. release mode In SGX we distinguish between debug, pre-release and release mode. In release mode, the enclave will be launched in production mode, i.e., any access - including debug access - will be prevented. Moreover, the code is compiled with optimizations switched on and without debug symbols. production mode Any access - including debug access - to an enclave in production mode will be prevented. SIM mode The application does not run inside of an enclave. Instead it is executed in native mode. Still all SCONE software executes. We do, however, not support attestation in SIM mode. CLI A Command Line Interface (CLI) is a textual interface to a computer. It is useful for scripting (i.e., automation) and typically, used for system and service administration. CC Confidential computing is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest. Confidential Computing Confidential computing (CC) is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest. vanilla In computer science, a system and or a software is called vanilla when there is no need to customize it for a certain use case. confidentiality In the context of information security the term confidentiality means that information is only made available to authorized entities. In the context of SCONE this means that information (data, code, secrets, and policies) are encrypted and the encryption key is only accessible to authorized entities. Authorization is defined in a SCONE CAS policy and access to this policy is also defined in the policy itself. integrity In the context of information security the term integrity means that one ensures the accuracy and completeness of information. Integrity implies freshness, i.e., that one reads the last data that was stored. While the freshness of SGX enclaves are ensured by hardware and in transit it is ensured by TLS, the freshness of data at rest is explicitly enforced by SCONE. MRSIGNER A public key that identifies the signer of an enclave. This is also called the Signing Identity. MRSIGNER is also used to verify the signature of the enclave. MRENCLAVE An enclave is identified by a hash value which is called MrEnclave. This hash value is determined by the initial content of the pages of an enclave and their access rights. This means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave. MariaDB MariaDB is an open source, relational database. MariaDB is a fork of MySQL. This community effort is led by some of the original developers of MySQL. sconeapps sconeapps is a repo with helm charts to install SCONE-based confidential applications like LAS, CAS and MariaDB. tokenizer A tokenizer replaces a sensitive data item with a non-sensitive token, that has no exploitable meaning. This token can be translated to the original data item by authorized clients. environment variable An environment variable is a variable defined in the context of a shell. Environment variables can change the behavior of processes. In the context of SCONE, one can define environment variables that change the behavior of the SCONE runtime - which is linked with processes running inside of enclaves. TLS Transport Layer Security (TLS) is a security protocol to provide private, authenticated and reliable communication between two entities. Helm Is a very popular package manager for Kubernetes.","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#attestation","text":"Process of proving the integrity and authenticity of the attestee's software and/or hardware component to a verifier. This includes validating if a given software is executed on a given hardware.","title":"attestation"},{"location":"glossary/#local-attestation","text":"Attestation executed locally, e.g., one software component validates the integrity and authenticity of another software component which is executed on the same hardware. In Intel SGX, the CPU creates a report containing integrity information of the attested enclave whose keyed-MAC can only be verified, and changed for that matter, by the verifier enclave running on the same platform.","title":"local attestation"},{"location":"glossary/#remote-attestation","text":"Attestation executed remotely, i.e., the component which does the validation and the component which is validated are executed on different machines. In Intel SGX, the report received during local attestation is signed with the quoting enclave's private key making the integrity of the quote - the signed report - remotely verifiable.","title":"remote attestation"},{"location":"glossary/#mutual-attestation","text":"Process of two components proving to each other the integrity and authenticity of their software and/or hardware components. This can include multiple aspects like that neither their code nor their filesystem was modified.","title":"mutual attestation"},{"location":"glossary/#configuration-and-attestation-service-cas","text":"The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client.","title":"Configuration and Attestation Service (CAS)"},{"location":"glossary/#scone-cas","text":"The SCONE Configuration and Attestation Service (CAS) is a component of the SCONE infrastructure. Programs executed in enclaves, in particular, an SCONE-enabled executable, connect to CAS to obtain their confidential configuration. CAS provisions this configuration only after it has verified the integrity and authenticity of the requesting enclave. Additionally CAS checks that the requesting enclave is allowed to obtain the confidential configuration. Initially, configurations are pushed to the CAS with the SCONE client.","title":"SCONE CAS"},{"location":"glossary/#local-attestation-service-las","text":"A per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave.","title":"Local Attestation Service (LAS)"},{"location":"glossary/#scone-las","text":"The SCONE Local Attestation Service (LAS) is a per-platform-service enabling remote attestation of SGX enclaves independently of the framework (i.e., SCONE or Intel SDK) used to create the enclave. It separates the development of SCONE-enabled applications from the Intel SDK by providing a stable interface to the attestation facilities of Intel's SDK and decouples the availability of applications deployed on the SCONE platform from Intel's Attestation Service, in conjunction with the CAS, through the introduction of an independent quoting enclave.","title":"SCONE LAS"},{"location":"glossary/#secure-boot","text":"A boot procedure which allows only the execution of firmware, bootloaders and operating systems which are digitally signed by a (well) defined set of acceptable signers.","title":"secure boot"},{"location":"glossary/#measured-boot","text":"A boot procedure which measures the state of the system at each boot step. This measurement can be accessed to verify the current state of a given system. Compared to secure boot, measured boot will not prevent an \"insecure\" state of the system.","title":"measured boot"},{"location":"glossary/#cloud-native-application","text":"An application designed to run inside of a cloud. One requirement is that the application is deployed with the help of containers.","title":"cloud-native application"},{"location":"glossary/#cloud-provider","text":"An entity providing cloud services (PaaS, IaaS, MaaS etc.) to its customers. It is assumed that a cloud provider is in physical or logical control of the hardware and system software used to provide the cloud services.","title":"cloud provider"},{"location":"glossary/#container","text":"An light-weight alternative to a virtual machine (VM). The isolation of containers is implemented by the operating system. Docker and Kubernetes use Linux for isolation. In the case of VMs, the isolation is implemented with the help of CPU extensions.","title":"container"},{"location":"glossary/#kubernetes","text":"An open source container orchestration platform. More information at https://kubernetes.io/","title":"Kubernetes"},{"location":"glossary/#kubeapps","text":"A dashboard to deploy and manage Kubernetes based applications. It can be used to deploy and manage SCONE-based confidential applications.","title":"Kubeapps"},{"location":"glossary/#curated-image","text":"A container image of a popular service maintained by scontain.com.","title":"curated image"},{"location":"glossary/#enclave","text":"This is an alias for SGX enclave . A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.","title":"enclave"},{"location":"glossary/#docker-image","text":"A snapshot of a container's state that can be used to initialize new containers with the same state.","title":"Docker image"},{"location":"glossary/#docker-registry","text":"A Docker registry stores Docker images for the purpose of easy distribution comparable to the app stores of Android or iOS.","title":"Docker registry"},{"location":"glossary/#epc","text":"A cache of memory pages belonging to enclaves. This cache resides in a reserved part of the main memory that is directly managed by the CPU (and not by the operating system or the hypervisor). The data in this cache is encrypted. Unlike enclave pages residing in the main memory, the CPU can encrypt and decrypt individual cache lines residing inside the EPC. This results in low overheads.","title":"EPC"},{"location":"glossary/#microservice","text":"A rather small component which offers a single service.","title":"microservice"},{"location":"glossary/#service-provider","text":"A company operating an application - typically, making these available via the Internet. We use this a general term that includes different models like Software as a Service (SaaS) providers as well as Hosted service providers etc.","title":"service provider"},{"location":"glossary/#scone","text":"SCONE (Secure CONtainer Environment) is a software platform for confidential computing allowing the trustworthy execution of unmodified x86 source code within Intel SGX enclaves. It consists of components enabling the execution inside enclaves such as the SCONE runtime and the C, C++, C#, Fortran, Go, and Rust SCONE cross-compilers, components ensuring the trustworthiness of this execution and deployment in clouds such as the CAS and SCONE client.","title":"SCONE"},{"location":"glossary/#scone-docker-image","text":"A SCONE Docker image is a Docker image that contains an SCONE-enabled executable and is additionally annotated via image labels with metadata allowing the attestation of the started SCONE-enabled executable and the image's file system content.","title":"SCONE Docker image"},{"location":"glossary/#scone-container","text":"A SCONE container is a running instance of a SCONE Docker image.","title":"SCONE container"},{"location":"glossary/#scone-microservice","text":"Microservice which is a SCONE-enabled executable.","title":"SCONE microservice"},{"location":"glossary/#scone-enabled-executable","text":"An executable created by a SCONE cross-compiler. The actual program will be executed within an enclave and utilises the SCONE runtime.","title":"SCONE-enabled executable"},{"location":"glossary/#scone-runtime","text":"The runtime environment necessary to execute a SCONE-enabled executable. At the moment this consists of a modified C-library based on the musl library.","title":"SCONE runtime"},{"location":"glossary/#scone-cross-compiler","text":"Compilers for various programming languages such as C, C++, Rust, Go, and Fortran which compile source code into a SCONE-enabled executable.","title":"SCONE cross-compiler"},{"location":"glossary/#scone-client","text":"A program that is used to configure SCONE-enabled executables. It allows the user to push confidential configurations to the CAS and encrypt files to ensure their content is only accessible by specific SCONE-enabled executables executed inside enclaves.","title":"SCONE client"},{"location":"glossary/#scone-infrastructure","text":"The SCONE infrastructure summarises all components necessary to deploy and run a SCONE-enabled executable. This includes Docker components like the Docker daemon and the Docker registry as well as the SCONE client and additional services like CAS and LAS.","title":"SCONE infrastructure"},{"location":"glossary/#secure-container","text":"A container which uses additional hardware isolation mechanisms, i.e., SGX to provide better application security. In particular, a secure container runs one or more secure programs . Additionally, the integrity and confidentiality of files inside a secure container are protected by SCONE.","title":"Secure container"},{"location":"glossary/#secure-program","text":"A program that executes inside an enclave.","title":"Secure program"},{"location":"glossary/#sgx-software-guard-extension","text":"A CPU extension by Intel that permits to create SGX enclaves.","title":"SGX (Software Guard eXtension)"},{"location":"glossary/#sgx","text":"Software Guard eXtension (SGX) is a CPU extension by Intel that permits to create enclaves, i.e., a protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range.","title":"SGX"},{"location":"glossary/#sgx-enclave","text":"A protected area inside the address space of a program such that only the code inside this enclave can access the data and code stored in this address range. All pages belonging to an enclave are encrypted by the CPU and only the CPU knows the encryption key. These pages can reside in the main memory or the EPC.","title":"SGX enclave"},{"location":"glossary/#threading","text":"SCONE uses different kind of threads: ethread : a thread that executes application threads inside of an enclave lthread : an application thread. Typically, created by the application directly or indirectly via a pthread_create call. In SCONE, this pthread_create call will create a lthread . The lthread is executed by some ethread . In this way, we can quickly switch to another application thread whenever an application thread would get block. In this way, we reduce the number of enclave entries and exits - which are costly. sthread : a thread that runs outside of the enclave and that executes system calls on behalf of the threads running inside the enclave","title":"threading"},{"location":"glossary/#trusted-computing-base-tcb","text":"The set of hardware and software components which can break the security policy. Therefore one has to trust that these components are not malicious or faulty.","title":"trusted computing base (TCB)"},{"location":"glossary/#trusted-execution-environment","text":"A trusted execution environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity.","title":"trusted execution environment"},{"location":"glossary/#tee","text":"A Trusted Execution Environment (TEE) is piece of hardware which allows secure processing. Usually the achieved protection goals are confidentiality and integrity.","title":"TEE"},{"location":"glossary/#release-mode","text":"In SGX we distinguish between debug, pre-release and release mode. In release mode, the enclave will be launched in production mode, i.e., any access - including debug access - will be prevented. Moreover, the code is compiled with optimizations switched on and without debug symbols.","title":"release mode"},{"location":"glossary/#production-mode","text":"Any access - including debug access - to an enclave in production mode will be prevented.","title":"production mode"},{"location":"glossary/#sim-mode","text":"The application does not run inside of an enclave. Instead it is executed in native mode. Still all SCONE software executes. We do, however, not support attestation in SIM mode.","title":"SIM mode"},{"location":"glossary/#cli","text":"A Command Line Interface (CLI) is a textual interface to a computer. It is useful for scripting (i.e., automation) and typically, used for system and service administration.","title":"CLI"},{"location":"glossary/#cc","text":"Confidential computing is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest.","title":"CC"},{"location":"glossary/#confidential-computing","text":"Confidential computing (CC) is an approach to secure data in use. With SCONE CC, one can protect data, code, and secrets in use as well as in transit and at rest.","title":"Confidential Computing"},{"location":"glossary/#vanilla","text":"In computer science, a system and or a software is called vanilla when there is no need to customize it for a certain use case.","title":"vanilla"},{"location":"glossary/#confidentiality","text":"In the context of information security the term confidentiality means that information is only made available to authorized entities. In the context of SCONE this means that information (data, code, secrets, and policies) are encrypted and the encryption key is only accessible to authorized entities. Authorization is defined in a SCONE CAS policy and access to this policy is also defined in the policy itself.","title":"confidentiality"},{"location":"glossary/#integrity","text":"In the context of information security the term integrity means that one ensures the accuracy and completeness of information. Integrity implies freshness, i.e., that one reads the last data that was stored. While the freshness of SGX enclaves are ensured by hardware and in transit it is ensured by TLS, the freshness of data at rest is explicitly enforced by SCONE.","title":"integrity"},{"location":"glossary/#mrsigner","text":"A public key that identifies the signer of an enclave. This is also called the Signing Identity. MRSIGNER is also used to verify the signature of the enclave.","title":"MRSIGNER"},{"location":"glossary/#mrenclave","text":"An enclave is identified by a hash value which is called MrEnclave. This hash value is determined by the initial content of the pages of an enclave and their access rights. This means that some of the SCONE environment variables like SCONE_HEAP and SCONE_ALLOW_DLOPEN will affect MrEnclave.","title":"MRENCLAVE"},{"location":"glossary/#mariadb","text":"MariaDB is an open source, relational database. MariaDB is a fork of MySQL. This community effort is led by some of the original developers of MySQL.","title":"MariaDB"},{"location":"glossary/#sconeapps","text":"sconeapps is a repo with helm charts to install SCONE-based confidential applications like LAS, CAS and MariaDB.","title":"sconeapps"},{"location":"glossary/#tokenizer","text":"A tokenizer replaces a sensitive data item with a non-sensitive token, that has no exploitable meaning. This token can be translated to the original data item by authorized clients.","title":"tokenizer"},{"location":"glossary/#environment-variable","text":"An environment variable is a variable defined in the context of a shell. Environment variables can change the behavior of processes. In the context of SCONE, one can define environment variables that change the behavior of the SCONE runtime - which is linked with processes running inside of enclaves.","title":"environment variable"},{"location":"glossary/#tls","text":"Transport Layer Security (TLS) is a security protocol to provide private, authenticated and reliable communication between two entities.","title":"TLS"},{"location":"glossary/#helm","text":"Is a very popular package manager for Kubernetes.","title":"Helm"},{"location":"groupcacheUseCase/","text":"groupcache Example groupcache is a memcached-like library written in GO: it implements a peer-to-peer caching service. We use groupcache to show how to build a little more complex GO program with SCONE. Typically, one would build a container image for groupcache with a Dockerfile . Since our emphasis is to explain how to build such programs, we first show the individual steps to compile and execute groupcache and second, please read how to build container images with a multi-stage build . Note You might want to read how to compile GO programs with SCONE first. Building groupcache - without shielding First, start a crosscompiler container. Determine which SGX device to mount with function determine_sgx_device : determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers Second, install the go command to simplify the building of groupcache : apk update apk add go git curl Now you can build the groupcache library as follows: go get -compiler gccgo -u github.com/golang/groupcache Note that flag -compiler gccgo is required to ensure that the scone-gccgo is used to compile groupcache . That's it! Example OK, we should show how to use group cache. We show this for a simple application from fiorix : cat > groupcache.go << EOF // Simple groupcache example: https://github.com/golang/groupcache // Running 3 instances: // go run groupcache.go -addr=:8080 -pool=http://127.0.0.1:8080,http://127.0.0.1:8081,http://127.0.0.1:8082 // go run groupcache.go -addr=:8081 -pool=http://127.0.0.1:8081,http://127.0.0.1:8080,http://127.0.0.1:8082 // go run groupcache.go -addr=:8082 -pool=http://127.0.0.1:8082,http://127.0.0.1:8080,http://127.0.0.1:8081 // Testing: // curl localhost:8080/color?name=red package main import ( \"errors\" \"flag\" \"log\" \"net/http\" \"strings\" \"context\" \"github.com/golang/groupcache\" ) var Store = map[string][]byte{ \"red\": []byte(\"#FF0000\"), \"green\": []byte(\"#00FF00\"), \"blue\": []byte(\"#0000FF\"), } var Group = groupcache.NewGroup(\"foobar\", 64<<20, groupcache.GetterFunc( func(ctx context.Context, key string, dest groupcache.Sink) error { log.Println(\"looking up\", key) v, ok := Store[key] if !ok { return errors.New(\"color not found\") } dest.SetBytes(v) return nil }, )) func main() { addr := flag.String(\"addr\", \":8080\", \"server address\") peers := flag.String(\"pool\", \"http://localhost:8080\", \"server pool list\") flag.Parse() http.HandleFunc(\"/color\", func(w http.ResponseWriter, r *http.Request) { color := r.FormValue(\"name\") var b []byte err := Group.Get(nil, color, groupcache.AllocatingByteSliceSink(&b)) if err != nil { http.Error(w, err.Error(), http.StatusNotFound) return } w.Write(b) w.Write([]byte{'\\n'}) }) p := strings.Split(*peers, \",\") pool := groupcache.NewHTTPPool(p[0]) pool.Set(p...) http.ListenAndServe(*addr, nil) } EOF Let's compile this with scone-gccgo : export SCONE_HEAP = 1G go build -compiler gccgo -buildmode = exe -gccgoflags -g groupcache.go Run groupcache in the background: ./groupcache -addr = :8080 -pool = http://127.0.0.1:8080 & And let's query groupcache: curl localhost:8080/color?name = green #00FF00 curl localhost:8080/color?name = red #FF0000 Shielding While the above code runs inside of an enclave, there are multiple security issues if that code would run in an untrusted environment: The peers communicate via http instead of https. This means that executing in an enclave does not improve the security since an attacker can just look into the network traffic of groupcache. The use of https instead of http would require a certificate and a private key. The arguments (i.e., -addr and -pool) are passed via command line, i.e., we can neither trust the integrity nor the confidentiality of these arguments. This groupcache service logs error messages on stderr. How can we be sure if the code runs indeed in an enclave? After all, SCONE supports simulation mode. Alternative: Manual Modifications One could manually modify the program to use https instead of http , one could encrypt the output with AES. However, this would require that we have to change groupcache not only to encrypt all output but also to manage the key for encrypting the output. The private key of the certificate is typically stored in an unencrypted file and protected via the access control of the file system. Since we do not trust the operating system, we would need to encrypt the private key in the file system. We would need to attest the groupcache and after successful attestation pass the encryption keys and the arguments to groupcache via a secure channel. Alternative: SCONE shielding Many programs would require such or similar changes as groupcache . Hence, SCONE provides a way to shield programs without the need to modify these programs. This main advantages of that approach is that one does not have to put in the engineering to modify the code - which is difficult and bug-prone one can easily keep up with upstream code changes without the need to continuously patch the upstream code one does not risk a lock-in by having SGX-specific or SCONE-specific code modifications We will show in later sections how we can shield this application with the help of SCONE such that no source code changes are necessary, and we only need to define a description what shields should be activated In this way, we can address all of the above issues that we described.","title":"GO example"},{"location":"groupcacheUseCase/#groupcache-example","text":"groupcache is a memcached-like library written in GO: it implements a peer-to-peer caching service. We use groupcache to show how to build a little more complex GO program with SCONE. Typically, one would build a container image for groupcache with a Dockerfile . Since our emphasis is to explain how to build such programs, we first show the individual steps to compile and execute groupcache and second, please read how to build container images with a multi-stage build . Note You might want to read how to compile GO programs with SCONE first.","title":"groupcache Example"},{"location":"groupcacheUseCase/#building-groupcache-without-shielding","text":"First, start a crosscompiler container. Determine which SGX device to mount with function determine_sgx_device : determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers Second, install the go command to simplify the building of groupcache : apk update apk add go git curl Now you can build the groupcache library as follows: go get -compiler gccgo -u github.com/golang/groupcache Note that flag -compiler gccgo is required to ensure that the scone-gccgo is used to compile groupcache . That's it!","title":"Building groupcache - without shielding"},{"location":"groupcacheUseCase/#example","text":"OK, we should show how to use group cache. We show this for a simple application from fiorix : cat > groupcache.go << EOF // Simple groupcache example: https://github.com/golang/groupcache // Running 3 instances: // go run groupcache.go -addr=:8080 -pool=http://127.0.0.1:8080,http://127.0.0.1:8081,http://127.0.0.1:8082 // go run groupcache.go -addr=:8081 -pool=http://127.0.0.1:8081,http://127.0.0.1:8080,http://127.0.0.1:8082 // go run groupcache.go -addr=:8082 -pool=http://127.0.0.1:8082,http://127.0.0.1:8080,http://127.0.0.1:8081 // Testing: // curl localhost:8080/color?name=red package main import ( \"errors\" \"flag\" \"log\" \"net/http\" \"strings\" \"context\" \"github.com/golang/groupcache\" ) var Store = map[string][]byte{ \"red\": []byte(\"#FF0000\"), \"green\": []byte(\"#00FF00\"), \"blue\": []byte(\"#0000FF\"), } var Group = groupcache.NewGroup(\"foobar\", 64<<20, groupcache.GetterFunc( func(ctx context.Context, key string, dest groupcache.Sink) error { log.Println(\"looking up\", key) v, ok := Store[key] if !ok { return errors.New(\"color not found\") } dest.SetBytes(v) return nil }, )) func main() { addr := flag.String(\"addr\", \":8080\", \"server address\") peers := flag.String(\"pool\", \"http://localhost:8080\", \"server pool list\") flag.Parse() http.HandleFunc(\"/color\", func(w http.ResponseWriter, r *http.Request) { color := r.FormValue(\"name\") var b []byte err := Group.Get(nil, color, groupcache.AllocatingByteSliceSink(&b)) if err != nil { http.Error(w, err.Error(), http.StatusNotFound) return } w.Write(b) w.Write([]byte{'\\n'}) }) p := strings.Split(*peers, \",\") pool := groupcache.NewHTTPPool(p[0]) pool.Set(p...) http.ListenAndServe(*addr, nil) } EOF Let's compile this with scone-gccgo : export SCONE_HEAP = 1G go build -compiler gccgo -buildmode = exe -gccgoflags -g groupcache.go Run groupcache in the background: ./groupcache -addr = :8080 -pool = http://127.0.0.1:8080 & And let's query groupcache: curl localhost:8080/color?name = green #00FF00 curl localhost:8080/color?name = red #FF0000","title":"Example"},{"location":"groupcacheUseCase/#shielding","text":"While the above code runs inside of an enclave, there are multiple security issues if that code would run in an untrusted environment: The peers communicate via http instead of https. This means that executing in an enclave does not improve the security since an attacker can just look into the network traffic of groupcache. The use of https instead of http would require a certificate and a private key. The arguments (i.e., -addr and -pool) are passed via command line, i.e., we can neither trust the integrity nor the confidentiality of these arguments. This groupcache service logs error messages on stderr. How can we be sure if the code runs indeed in an enclave? After all, SCONE supports simulation mode.","title":"Shielding"},{"location":"groupcacheUseCase/#alternative-manual-modifications","text":"One could manually modify the program to use https instead of http , one could encrypt the output with AES. However, this would require that we have to change groupcache not only to encrypt all output but also to manage the key for encrypting the output. The private key of the certificate is typically stored in an unencrypted file and protected via the access control of the file system. Since we do not trust the operating system, we would need to encrypt the private key in the file system. We would need to attest the groupcache and after successful attestation pass the encryption keys and the arguments to groupcache via a secure channel.","title":"Alternative: Manual Modifications"},{"location":"groupcacheUseCase/#alternative-scone-shielding","text":"Many programs would require such or similar changes as groupcache . Hence, SCONE provides a way to shield programs without the need to modify these programs. This main advantages of that approach is that one does not have to put in the engineering to modify the code - which is difficult and bug-prone one can easily keep up with upstream code changes without the need to continuously patch the upstream code one does not risk a lock-in by having SGX-specific or SCONE-specific code modifications We will show in later sections how we can shield this application with the help of SCONE such that no source code changes are necessary, and we only need to define a description what shields should be activated In this way, we can address all of the above issues that we described.","title":"Alternative: SCONE shielding"},{"location":"hardwaremode/","text":"Running \"Hello World\" inside of an enclave So far , we showed how to run a hello world program using simulation mode . Let's show how to run this program in hardware mode , i.e., the hello world program runs inside an Intel SGX enclave. Actually, the only change is to give the container access to the SGX device by adding $MOUNT_SGXDEVICE . Detailed Description We first need to start a container which includes the SCONE crosscompiler and give the container access to the Intel SGX driver. We determine which SGX device to mount with function determine_sgx_device : determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers The docker engine and the Intel SGX driver must be installed. Read about how to install a docker engine and to install the Intel SGX driver . In some installations, you might have to replace \"docker\" by \"sudo docker\". To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Hardware mode is only supported in Linux, since the Intel SGX driver is only available on Linux. Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we have given the container access to the SGX driver, this runs in hardware mode . Use this mode only for development and debugging The program runs inside of a hardware enclave . However, the enclave is in debug mode, i.e., one can actually introspect the content of the enclave. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 9d601c360ce9b6100e35dc42ec2800c1c20478328a0d4450d8d5163c00289dea Hello World The output shows that SCONE is running in hardware mode : export SCONE_MODE=hw Background Info For ease of use, we create all Docker images such that applications run inside of enclaves if enclaves are available ( AUTO mode ). If SGX is not available, they run in simulation mode, i.e., outside of an enclave but all SCONE software is running. To disable simulation mode, you can set environment variable SCONE_MODE=HW : docker run $MOUNT_SGXDEVICE -e \"SCONE_MODE=HW\" -it registry.scontain.com:5050/sconecuratedimages/crosscompilers If you would start your container in hardware mode but forget to give it access to the sgx device, i.e., docker run -e \"SCONE_MODE=HW\" -it registry.scontain.com:5050/sconecuratedimages/crosscompilers compilation of the helloworld will succeed but running the helloworld program will fail: bash-4.4# echo $SCONE_MODE HW bash-4.4# ls -l /dev/isgx ls: /dev/isgx: No such file or directory bash-4.4# ./helloworld [SCONE|ERROR] ./tools/starter-exec.c:993:_dl_main(): Could not create enclave: Error opening SGX device When running your software in operations, you would force the programs to run inside of enclaves: this can be enforced with the help of SCONE configuration and attestation service . Environment Variables To simplify the development with SCONE, you can control the behavior of SCONE with a set of environment variables, i.e., variables defined by your shell. Section SCONE Environment Variables describes these in details.","title":"Hardware Mode"},{"location":"hardwaremode/#running-hello-world-inside-of-an-enclave","text":"So far , we showed how to run a hello world program using simulation mode . Let's show how to run this program in hardware mode , i.e., the hello world program runs inside an Intel SGX enclave. Actually, the only change is to give the container access to the SGX device by adding $MOUNT_SGXDEVICE .","title":"Running \"Hello World\" inside of  an enclave"},{"location":"hardwaremode/#detailed-description","text":"We first need to start a container which includes the SCONE crosscompiler and give the container access to the Intel SGX driver. We determine which SGX device to mount with function determine_sgx_device : determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers The docker engine and the Intel SGX driver must be installed. Read about how to install a docker engine and to install the Intel SGX driver . In some installations, you might have to replace \"docker\" by \"sudo docker\". To be able to use hardware mode, programs need access to the SGX device. If your hosts have already a Intel SGX driver installed, you are all set. Hardware mode is only supported in Linux, since the Intel SGX driver is only available on Linux. Now execute the following command inside the container to create the hello world program: cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program: ./helloworld This will print Hello World . Since we have given the container access to the SGX driver, this runs in hardware mode . Use this mode only for development and debugging The program runs inside of a hardware enclave . However, the enclave is in debug mode, i.e., one can actually introspect the content of the enclave. SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=hw export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: b1e014e64b4d332a51802580ec3252370ffe44bb Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 9d601c360ce9b6100e35dc42ec2800c1c20478328a0d4450d8d5163c00289dea Hello World The output shows that SCONE is running in hardware mode : export SCONE_MODE=hw","title":"Detailed Description"},{"location":"hardwaremode/#background-info","text":"For ease of use, we create all Docker images such that applications run inside of enclaves if enclaves are available ( AUTO mode ). If SGX is not available, they run in simulation mode, i.e., outside of an enclave but all SCONE software is running. To disable simulation mode, you can set environment variable SCONE_MODE=HW : docker run $MOUNT_SGXDEVICE -e \"SCONE_MODE=HW\" -it registry.scontain.com:5050/sconecuratedimages/crosscompilers If you would start your container in hardware mode but forget to give it access to the sgx device, i.e., docker run -e \"SCONE_MODE=HW\" -it registry.scontain.com:5050/sconecuratedimages/crosscompilers compilation of the helloworld will succeed but running the helloworld program will fail: bash-4.4# echo $SCONE_MODE HW bash-4.4# ls -l /dev/isgx ls: /dev/isgx: No such file or directory bash-4.4# ./helloworld [SCONE|ERROR] ./tools/starter-exec.c:993:_dl_main(): Could not create enclave: Error opening SGX device When running your software in operations, you would force the programs to run inside of enclaves: this can be enforced with the help of SCONE configuration and attestation service . Environment Variables To simplify the development with SCONE, you can control the behavior of SCONE with a set of environment variables, i.e., variables defined by your shell. Section SCONE Environment Variables describes these in details.","title":"Background Info"},{"location":"hello_world_kubernetes/","text":"SCONE on Kubernetes This tutorial shows how to deploy confidential applications with the help of SCONE in a Kubernetes cluster. The tutorial is organized as follows: Before you begin Hello World! Hello World! with remote attestation Hello World! with TLS certificates auto-generated by CAS Encrypt your source code An executable version of this code can be cloned as follows: git clone https://github.com/scontain/hello-world-scone-on-kubernetes.git Note that you need to get access to the images Send us an email to get access. You can run this demo by executing: cd hello-world-scone-on-kubernetes ./run_hello_world.sh Before you begin A Kubernetes cluster and a separate namespace This tutorial assumes that you have access to a Kubernetes cluster (through kubectl ), and that you are deploying everything to a separate namespace, called hello-scone . Namespaces are a good way to separate resources, not to mention how it makes the clean-up process a lot easier at the end: kubectl create namespace hello-scone Don't forget to specify it by adding -n hello-scone to your kubectl commands. Install SGX device plugin SGX applications need access to the Intel SGX driver, a char device located at the host machine. The device name might differ depending on the driver implementation, but the most common ones are /dev/sgx (DCAP), /dev/sgx/enclave (in-tree) and /dev/isgx (IAS/out-of-tree). If you are in a cluster context, you need the right device mounted into the container. In Kubernetes, mounting host devices requires your applications to run in privileged mode. We provide an SGX device plugin for Kubernetes, which lets you request the SGX driver through the resource.limits field (just like CPU or RAM). The device plugin handles different driver names transparently, and also allows your applications to run unprivileged (which is always good for security). The SGX device plugin can be easily installed with the help of Helm: helm install sgxdevplugin sconeapps/sgxdevplugin Please refer to Kubernetes SGX plugin documentation for more details and Helm setup instructions. This tutorial assumes that you have the SGX device plugin running on your cluster. NOTE : If you can't install the SGX device plugin for some reason, you have to mount the Intel SGX driver device manually into your container. To do so, first figure out which driver is installed in your cluster . Then update all the manifests below by removing the resource.limits field, and adding a hostPath and the respective volumeMount definitions. You also have to run the container in privileged mode by adding privileged: true to securityContext . Check a sample manifest in file example-sgx-manual-mount.yamlr . NOTE : If you don't have Intel SGX in your cluster, you can run SCONE in simulated mode by setting the environment variable SCONE_MODE=sim . The application will run encrypted in memory, but Intel SGX security guarantees do not apply. Set yourself a workspace A lot of files are created through this tutorial, and they have to be somewhere. Choose whatever directory you want to run the commands below, but it might be a good idea to create a temporary one: cd $( mktemp -d ) We use the following base image: export BASE_IMAGE = sconecuratedimages/kubernetes:python-3.7.3-alpine3.10-scone4.2 Have fun! Set a Configuration and Attestation Service (CAS) When remote attestation enters the scene, you will need a CAS to provide attestation, configuration and secrets. Export now the address of your CAS. If you don't have your own CAS, use our public one at scone.ml: export SCONE_CAS_ADDR = 4 -2-1.scone-cas.cf Now, generate client certificates (needed to submit new sessions to CAS): mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Create a CAS namespace for your sessions. export NS = HelloWorld- $RANDOM - $RANDOM cat > namespace.yaml <<EOF name: $NS access_policy: read: - CREATOR update: - CREATOR EOF curl -v -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @namespace.yaml -X POST https:// $SCONE_CAS_ADDR :8081/v1/sessions Hello World! Let's start by writing a simple Python webserver: it only returns the message Hello World! and the value of the environment variable GREETING . mkdir -p app cat > app/server.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 8080), HTTPHelloWorldHandler) httpd.serve_forever() EOF To build this application with SCONE, simply use our Python 3.7 curated image as base. The Python interpreter will run inside of an enclave! cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 8080 COPY app /app CMD [ \"python3\", \"/app/server.py\" ] EOF Set the name of the image. If you already have an image, skip the building process. export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.1 docker build --pull . -t $IMAGE && docker push $IMAGE Deploying the application with Kubernetes is also simple: we just need a deployment and a service exposing it. Write the Kubernetes manifests: cat > app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: hello-world replicas: 1 template: metadata: labels: run: hello-world spec: containers: - name: hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: GREETING value: howdy! resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: hello-world labels: run: hello-world spec: ports: - port: 8080 protocol: TCP selector: run: hello-world EOF Now, submit the manifests to the Kubernetes API, using kubectl command: kubectl create -f app.yaml -n hello-scone Now that everything is deployed, you can access your Python app running on your cluster. Forward your local 8080 port to the service port: kubectl port-forward svc/hello-world 8080 :8080 -n hello-scone The application will be available at your http://localhost:8080 : $ curl localhost:8080 Hello World! $GREETING is: howdy! Run with remote attestation SCONE provides a remote attestation feature, so you make sure your application is running unmodified. It's also possible to have secrets and configuration delivered directly to the attested enclave! The remote attestation is provided by two components: LAS (local attestation service, runs on the cluster) and CAS (a trusted service that runs elsewhere. We provide a public one in 4-2-1.scone-cas.cf). You can deploy LAS to your cluster with the help of a DaemonSet, deploying one LAS instance per cluster node. As your application has to contact the LAS container running in the same host, we use the default Docker interface (172.17.0.1) as our LAS address. cat > las.yaml << EOF apiVersion: apps/v1 kind: DaemonSet metadata: name: local-attestation labels: k8s-app: local-attestation spec: selector: matchLabels: k8s-app: local-attestation template: metadata: labels: k8s-app: local-attestation spec: hostNetwork: true containers: - name: local-attestation image: sconecuratedimages/kubernetes:las-scone4.2 resources: limits: sgx.k8s.io/sgx: 1 ports: - containerPort: 18766 hostPort: 18766 EOF kubectl create -f las.yaml -n hello-scone To setup remote attestation, you will need a session file and the MRENCLAVE , which is a unique signature of your application. Extract MRENCLAVE of your application by running its container with the environment variable SCONE_HASH set to 1 : MRENCLAVE = ` docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ` Then create a session file. Please note that we are also defining a secret GREETING . Only the MRENCLAVES registered in this session file will be allowed to see such secret. cat > session.yaml << EOF name: $NS~hello-k8s-scone version: \"0.2\" services: - name: application mrenclaves: [$MRENCLAVE] command: python3 /app/server.py pwd: / environment: GREETING: hello from SCONE!!! EOF Now, post the session file to your CAS of choice: curl -v -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yaml -X POST https:// $SCONE_CAS_ADDR :8081/v1/sessions Once you submit the session file, you just need to inject the CAS address into your Deployment manifest. To showcase that, we're creating a new Deployment: cat > attested-app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world spec: selector: matchLabels: run: attested-hello-world replicas: 1 template: metadata: labels: run: attested-hello-world spec: containers: - name: attested-hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: $NS~hello-k8s-scone/application - name: SCONE_LAS_ADDR value: 172.17.0.1:18766 resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: attested-hello-world labels: run: attested-hello-world spec: ports: - port: 8080 protocol: TCP selector: run: attested-hello-world EOF NOTE : The environment defined in the Kubernetes manifest won't be considered once your app is attested. The environment will be retrieved from your session file. Deploy your attested app: kubectl create -f attested-app.yaml -n hello-scone Once again, forward your local port 8082 to the service port: kubectl port-forward svc/attested-hello-world 8082 :8080 -n hello-scone The attested application will be available at your http://localhost:8082 : $ curl localhost:8082 Hello World! $GREETING is: hello from SCONE!!! TLS with certificates auto-generated by CAS The CAS service can also generate secrets and certificates automatically. Combined with access policies, it means that such auto-generated secrets and certificates will be seen only by certain applications. No human (e.g. a system operator) will ever see them: they only exist inside of SGX enclaves. To showcase such feature, let's use the same application as last example. But now, we serve the traffic over TLS, and we let CAS generate the server certificates. Start by rewriting our application to serve with TLS: cat > app/server-tls.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os import socket import ssl class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 4443), HTTPHelloWorldHandler) httpd.socket = ssl.wrap_socket(httpd.socket, keyfile=\"/app/key.pem\", certfile=\"/app/cert.pem\", server_side=True) httpd.serve_forever() EOF Our updated Dockerfile looks like this: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app /app CMD [ \"python3\" ] EOF Build the updated image: export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.2 docker build --pull . -t $IMAGE && docker push $IMAGE The magic is done in the session file. First, extract the MRENCLAVE again: MRENCLAVE = $( docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ) Now, create a Session file to be submitted to CAS. The certificates are defined in the secrets field, and are injected into the filesystem through images.injection_files : cat > session-tls-certs.yaml << EOF name: $NS~hello-k8s-scone-tls-certs version: \"0.2\" services: - name: application image_name: application_image mrenclaves: [$MRENCLAVE] command: python3 /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with TLS and auto-generated certs!!! images: - name: application_image injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls-certs.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session The steps to run your app are similar to before: let's create a Kubernetes manifest and submit it. cat > attested-app-tls-certs.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls-certs spec: selector: matchLabels: run: attested-hello-world-tls-certs replicas: 1 template: metadata: labels: run: attested-hello-world-tls-certs spec: containers: - name: attested-hello-world-tls-certs image: $IMAGE imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: $NS~hello-k8s-scone-tls-certs/application - name: SCONE_LAS_ADDR value: \"172.17.0.1\" resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls-certs labels: run: attested-hello-world-tls-certs spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls-certs EOF kubectl create -f attested-app-tls-certs.yaml -n hello-scone Now it's time to access your app. Again, forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls-certs 8083 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8083 Hello World! $GREETING is: hello from SCONE with TLS and auto-generated certs!!! Encrypt your source code Moving further, you can run the exact same application, but now the server source code will be encrypted using SCONE's file protection feature, and only the Python interpreter that you register will be able to read them (after being attested by CAS). Encrypt the source code and filesystem Let's encrypt the source code using SCONE's Fileshield. Run a SCONE CLI container with access to the files: docker run -it --rm -e SCONE_MODE = sim -v $PWD :/tutorial $BASE_IMAGE sh Inside the container, create an encrypted region /app and add all the files in /tutorial/app (the plain files) to it. Lastly, encrypt the key itself. cd /tutorial rm -rf app_image && mkdir -p app_image/app cd app_image scone fspf create fspf.pb scone fspf addr fspf.pb / --not-protected --kernel / scone fspf addr fspf.pb /app --encrypted --kernel /app scone fspf addf fspf.pb /app /tutorial/app /tutorial/app_image/app scone fspf encrypt fspf.pb > /tutorial/app/keytag The contents of /tutorial/app_image/app are now encrypted. Try to cat them from inside the container: $ cd /tutorial/app_image/app && ls server-tls.py $ cat server-tls.py $\ufffd\u0287 ( E@\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffdId\ufffdZ\ufffd\ufffd\ufffdg3uc\ufffd\ufffdm\u046a\ufffdZ.$\ufffd__\ufffd! ) \u04b9S\ufffd\ufffd\u0660\ufffd\ufffd\ufffd ( \ufffd\ufffd \ufffdX\ufffdW\u03d0\ufffd { $\ufffd\ufffd\ufffd | \ufffd\u05d6H\ufffd\ufffd\u01d3\ufffd\ufffd! ; 2 \ufffd\ufffd\ufffd\ufffd>@z4-\ufffdh\ufffd\ufffd3i\u077es\ufffdt\ufffd7\ufffd<>H4:\ufffd\ufffd\ufffd\ufffd ( 9 = \ufffda ) \ufffdj?\ufffd\ufffdp\ufffd\ufffd\ufffd\ufffdq\ufffd\u07e73\ufffd } \ufffd\ufffdH\u0638a\ufffd | \ufffd\ufffd\ufffdw-\ufffdq\ufffd\ufffd96\ufffd\ufffd\ufffdo\ufffd9\u0303\ufffdEv\ufffdv\ufffd\ufffd*$\ufffd\ufffd\ufffd\ufffdTU/\ufffd\ufffd\ufffd\u04d4\ufffd\ufffdv\ufffdG\ufffd\ufffd\ufffdT\ufffd1<\u06b9\ufffd\ufffdC#p | i\ufffd\ufffdA\u01a2\u02c5_!6\ufffd\ufffd\ufffdF\ufffd\ufffd\ufffdw\ufffd@ \ufffdC\ufffd\ufffdJ\ufffd\ufffd\ufffd+81\ufffd8\ufffd Build the new image You can now exit the container. Let's build the server image with our files, now encrypted: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app_image / CMD [ \"python3\" ] EOF Choose the new image name and build it: export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone-0.3 docker build --pull . -t $IMAGE && docker push $IMAGE Run in Kubernetes Time to deploy our server to Kubernetes. Let's setup the attestation first, so we make sure that only our application (identified by its MRENCLAVE ) will have access to the secrets to read the encrypted filesystem, as well as our secret greeting. Extract the MRENCLAVE : MRENCLAVE = $( docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ) Extract SCONE_FSPF_KEY and SCONE_FSPF_TAG : export SCONE_FSPF_KEY = $( cat app/keytag | awk '{print $11}' ) export SCONE_FSPF_TAG = $( cat app/keytag | awk '{print $9}' ) export SCONE_FSPF = /fspf.pb Create a session file: cat > session-tls.yaml << EOF name: $NS~hello-k8s-scone-tls version: \"0.2\" services: - name: application image_name: application_image mrenclaves: [$MRENCLAVE] command: python3 /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with encrypted source code and auto-generated certs!!! fspf_path: $SCONE_FSPF fspf_key: $SCONE_FSPF_KEY fspf_tag: $SCONE_FSPF_TAG images: - name: application_image injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session Create the manifest and submit it to your Kubernetes cluster: cat > attested-app-tls.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls spec: selector: matchLabels: run: attested-hello-world-tls replicas: 1 template: metadata: labels: run: attested-hello-world-tls spec: containers: - name: attested-hello-world-tls image: $IMAGE imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: $NS~hello-k8s-scone-tls/application - name: SCONE_LAS_ADDR value: 172.17.0.1 resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls labels: run: attested-hello-world-tls spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls EOF kubectl create -f attested-app-tls.yaml -n hello-scone Time to access your app. Forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls 8084 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8084 Hello World! $GREETING is: hello from SCONE with encrypted source code and auto-generated certs!!! Clean up To clean up all the resources created in this tutorial, simply delete the namespace: kubectl delete namespace hello-scone Author: Clenimar \u00a9 scontain.com , 2021. Questions or Suggestions?","title":"Hello World Kubernetes"},{"location":"hello_world_kubernetes/#scone-on-kubernetes","text":"This tutorial shows how to deploy confidential applications with the help of SCONE in a Kubernetes cluster. The tutorial is organized as follows: Before you begin Hello World! Hello World! with remote attestation Hello World! with TLS certificates auto-generated by CAS Encrypt your source code An executable version of this code can be cloned as follows: git clone https://github.com/scontain/hello-world-scone-on-kubernetes.git Note that you need to get access to the images Send us an email to get access. You can run this demo by executing: cd hello-world-scone-on-kubernetes ./run_hello_world.sh","title":"SCONE on Kubernetes"},{"location":"hello_world_kubernetes/#before-you-begin","text":"","title":"Before you begin"},{"location":"hello_world_kubernetes/#a-kubernetes-cluster-and-a-separate-namespace","text":"This tutorial assumes that you have access to a Kubernetes cluster (through kubectl ), and that you are deploying everything to a separate namespace, called hello-scone . Namespaces are a good way to separate resources, not to mention how it makes the clean-up process a lot easier at the end: kubectl create namespace hello-scone Don't forget to specify it by adding -n hello-scone to your kubectl commands.","title":"A Kubernetes cluster and a separate namespace"},{"location":"hello_world_kubernetes/#install-sgx-device-plugin","text":"SGX applications need access to the Intel SGX driver, a char device located at the host machine. The device name might differ depending on the driver implementation, but the most common ones are /dev/sgx (DCAP), /dev/sgx/enclave (in-tree) and /dev/isgx (IAS/out-of-tree). If you are in a cluster context, you need the right device mounted into the container. In Kubernetes, mounting host devices requires your applications to run in privileged mode. We provide an SGX device plugin for Kubernetes, which lets you request the SGX driver through the resource.limits field (just like CPU or RAM). The device plugin handles different driver names transparently, and also allows your applications to run unprivileged (which is always good for security). The SGX device plugin can be easily installed with the help of Helm: helm install sgxdevplugin sconeapps/sgxdevplugin Please refer to Kubernetes SGX plugin documentation for more details and Helm setup instructions. This tutorial assumes that you have the SGX device plugin running on your cluster. NOTE : If you can't install the SGX device plugin for some reason, you have to mount the Intel SGX driver device manually into your container. To do so, first figure out which driver is installed in your cluster . Then update all the manifests below by removing the resource.limits field, and adding a hostPath and the respective volumeMount definitions. You also have to run the container in privileged mode by adding privileged: true to securityContext . Check a sample manifest in file example-sgx-manual-mount.yamlr . NOTE : If you don't have Intel SGX in your cluster, you can run SCONE in simulated mode by setting the environment variable SCONE_MODE=sim . The application will run encrypted in memory, but Intel SGX security guarantees do not apply.","title":"Install SGX device plugin"},{"location":"hello_world_kubernetes/#set-yourself-a-workspace","text":"A lot of files are created through this tutorial, and they have to be somewhere. Choose whatever directory you want to run the commands below, but it might be a good idea to create a temporary one: cd $( mktemp -d ) We use the following base image: export BASE_IMAGE = sconecuratedimages/kubernetes:python-3.7.3-alpine3.10-scone4.2 Have fun!","title":"Set yourself a workspace"},{"location":"hello_world_kubernetes/#set-a-configuration-and-attestation-service-cas","text":"When remote attestation enters the scene, you will need a CAS to provide attestation, configuration and secrets. Export now the address of your CAS. If you don't have your own CAS, use our public one at scone.ml: export SCONE_CAS_ADDR = 4 -2-1.scone-cas.cf Now, generate client certificates (needed to submit new sessions to CAS): mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key -days 31 -nodes -sha256 -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" -reqexts SAN -extensions SAN -config < ( cat /etc/ssl/openssl.cnf \\ < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Create a CAS namespace for your sessions. export NS = HelloWorld- $RANDOM - $RANDOM cat > namespace.yaml <<EOF name: $NS access_policy: read: - CREATOR update: - CREATOR EOF curl -v -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @namespace.yaml -X POST https:// $SCONE_CAS_ADDR :8081/v1/sessions","title":"Set a Configuration and Attestation Service (CAS)"},{"location":"hello_world_kubernetes/#hello-world","text":"Let's start by writing a simple Python webserver: it only returns the message Hello World! and the value of the environment variable GREETING . mkdir -p app cat > app/server.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 8080), HTTPHelloWorldHandler) httpd.serve_forever() EOF To build this application with SCONE, simply use our Python 3.7 curated image as base. The Python interpreter will run inside of an enclave! cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 8080 COPY app /app CMD [ \"python3\", \"/app/server.py\" ] EOF Set the name of the image. If you already have an image, skip the building process. export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.1 docker build --pull . -t $IMAGE && docker push $IMAGE Deploying the application with Kubernetes is also simple: we just need a deployment and a service exposing it. Write the Kubernetes manifests: cat > app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: hello-world replicas: 1 template: metadata: labels: run: hello-world spec: containers: - name: hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: GREETING value: howdy! resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: hello-world labels: run: hello-world spec: ports: - port: 8080 protocol: TCP selector: run: hello-world EOF Now, submit the manifests to the Kubernetes API, using kubectl command: kubectl create -f app.yaml -n hello-scone Now that everything is deployed, you can access your Python app running on your cluster. Forward your local 8080 port to the service port: kubectl port-forward svc/hello-world 8080 :8080 -n hello-scone The application will be available at your http://localhost:8080 : $ curl localhost:8080 Hello World! $GREETING is: howdy!","title":"Hello World!"},{"location":"hello_world_kubernetes/#run-with-remote-attestation","text":"SCONE provides a remote attestation feature, so you make sure your application is running unmodified. It's also possible to have secrets and configuration delivered directly to the attested enclave! The remote attestation is provided by two components: LAS (local attestation service, runs on the cluster) and CAS (a trusted service that runs elsewhere. We provide a public one in 4-2-1.scone-cas.cf). You can deploy LAS to your cluster with the help of a DaemonSet, deploying one LAS instance per cluster node. As your application has to contact the LAS container running in the same host, we use the default Docker interface (172.17.0.1) as our LAS address. cat > las.yaml << EOF apiVersion: apps/v1 kind: DaemonSet metadata: name: local-attestation labels: k8s-app: local-attestation spec: selector: matchLabels: k8s-app: local-attestation template: metadata: labels: k8s-app: local-attestation spec: hostNetwork: true containers: - name: local-attestation image: sconecuratedimages/kubernetes:las-scone4.2 resources: limits: sgx.k8s.io/sgx: 1 ports: - containerPort: 18766 hostPort: 18766 EOF kubectl create -f las.yaml -n hello-scone To setup remote attestation, you will need a session file and the MRENCLAVE , which is a unique signature of your application. Extract MRENCLAVE of your application by running its container with the environment variable SCONE_HASH set to 1 : MRENCLAVE = ` docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ` Then create a session file. Please note that we are also defining a secret GREETING . Only the MRENCLAVES registered in this session file will be allowed to see such secret. cat > session.yaml << EOF name: $NS~hello-k8s-scone version: \"0.2\" services: - name: application mrenclaves: [$MRENCLAVE] command: python3 /app/server.py pwd: / environment: GREETING: hello from SCONE!!! EOF Now, post the session file to your CAS of choice: curl -v -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yaml -X POST https:// $SCONE_CAS_ADDR :8081/v1/sessions Once you submit the session file, you just need to inject the CAS address into your Deployment manifest. To showcase that, we're creating a new Deployment: cat > attested-app.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world spec: selector: matchLabels: run: attested-hello-world replicas: 1 template: metadata: labels: run: attested-hello-world spec: containers: - name: attested-hello-world image: $IMAGE imagePullPolicy: Always ports: - containerPort: 8080 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: $NS~hello-k8s-scone/application - name: SCONE_LAS_ADDR value: 172.17.0.1:18766 resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: attested-hello-world labels: run: attested-hello-world spec: ports: - port: 8080 protocol: TCP selector: run: attested-hello-world EOF NOTE : The environment defined in the Kubernetes manifest won't be considered once your app is attested. The environment will be retrieved from your session file. Deploy your attested app: kubectl create -f attested-app.yaml -n hello-scone Once again, forward your local port 8082 to the service port: kubectl port-forward svc/attested-hello-world 8082 :8080 -n hello-scone The attested application will be available at your http://localhost:8082 : $ curl localhost:8082 Hello World! $GREETING is: hello from SCONE!!!","title":"Run with remote attestation"},{"location":"hello_world_kubernetes/#tls-with-certificates-auto-generated-by-cas","text":"The CAS service can also generate secrets and certificates automatically. Combined with access policies, it means that such auto-generated secrets and certificates will be seen only by certain applications. No human (e.g. a system operator) will ever see them: they only exist inside of SGX enclaves. To showcase such feature, let's use the same application as last example. But now, we serve the traffic over TLS, and we let CAS generate the server certificates. Start by rewriting our application to serve with TLS: cat > app/server-tls.py << EOF from http.server import HTTPServer from http.server import BaseHTTPRequestHandler import os import socket import ssl class HTTPHelloWorldHandler(BaseHTTPRequestHandler): def do_GET(self): \"\"\"say \"Hello World!\" and the value of \\`GREETING\\` env. variable.\"\"\" self.send_response(200) self.end_headers() self.wfile.write(b'Hello World!\\n\\$GREETING is: %s\\n' % (os.getenv('GREETING', 'no greeting :(').encode())) httpd = HTTPServer(('0.0.0.0', 4443), HTTPHelloWorldHandler) httpd.socket = ssl.wrap_socket(httpd.socket, keyfile=\"/app/key.pem\", certfile=\"/app/cert.pem\", server_side=True) httpd.serve_forever() EOF Our updated Dockerfile looks like this: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app /app CMD [ \"python3\" ] EOF Build the updated image: export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone0.2 docker build --pull . -t $IMAGE && docker push $IMAGE The magic is done in the session file. First, extract the MRENCLAVE again: MRENCLAVE = $( docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ) Now, create a Session file to be submitted to CAS. The certificates are defined in the secrets field, and are injected into the filesystem through images.injection_files : cat > session-tls-certs.yaml << EOF name: $NS~hello-k8s-scone-tls-certs version: \"0.2\" services: - name: application image_name: application_image mrenclaves: [$MRENCLAVE] command: python3 /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with TLS and auto-generated certs!!! images: - name: application_image injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls-certs.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session The steps to run your app are similar to before: let's create a Kubernetes manifest and submit it. cat > attested-app-tls-certs.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls-certs spec: selector: matchLabels: run: attested-hello-world-tls-certs replicas: 1 template: metadata: labels: run: attested-hello-world-tls-certs spec: containers: - name: attested-hello-world-tls-certs image: $IMAGE imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: $NS~hello-k8s-scone-tls-certs/application - name: SCONE_LAS_ADDR value: \"172.17.0.1\" resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls-certs labels: run: attested-hello-world-tls-certs spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls-certs EOF kubectl create -f attested-app-tls-certs.yaml -n hello-scone Now it's time to access your app. Again, forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls-certs 8083 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8083 Hello World! $GREETING is: hello from SCONE with TLS and auto-generated certs!!!","title":"TLS with certificates auto-generated by CAS"},{"location":"hello_world_kubernetes/#encrypt-your-source-code","text":"Moving further, you can run the exact same application, but now the server source code will be encrypted using SCONE's file protection feature, and only the Python interpreter that you register will be able to read them (after being attested by CAS).","title":"Encrypt your source code"},{"location":"hello_world_kubernetes/#encrypt-the-source-code-and-filesystem","text":"Let's encrypt the source code using SCONE's Fileshield. Run a SCONE CLI container with access to the files: docker run -it --rm -e SCONE_MODE = sim -v $PWD :/tutorial $BASE_IMAGE sh Inside the container, create an encrypted region /app and add all the files in /tutorial/app (the plain files) to it. Lastly, encrypt the key itself. cd /tutorial rm -rf app_image && mkdir -p app_image/app cd app_image scone fspf create fspf.pb scone fspf addr fspf.pb / --not-protected --kernel / scone fspf addr fspf.pb /app --encrypted --kernel /app scone fspf addf fspf.pb /app /tutorial/app /tutorial/app_image/app scone fspf encrypt fspf.pb > /tutorial/app/keytag The contents of /tutorial/app_image/app are now encrypted. Try to cat them from inside the container: $ cd /tutorial/app_image/app && ls server-tls.py $ cat server-tls.py $\ufffd\u0287 ( E@\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffdId\ufffdZ\ufffd\ufffd\ufffdg3uc\ufffd\ufffdm\u046a\ufffdZ.$\ufffd__\ufffd! ) \u04b9S\ufffd\ufffd\u0660\ufffd\ufffd\ufffd ( \ufffd\ufffd \ufffdX\ufffdW\u03d0\ufffd { $\ufffd\ufffd\ufffd | \ufffd\u05d6H\ufffd\ufffd\u01d3\ufffd\ufffd! ; 2 \ufffd\ufffd\ufffd\ufffd>@z4-\ufffdh\ufffd\ufffd3i\u077es\ufffdt\ufffd7\ufffd<>H4:\ufffd\ufffd\ufffd\ufffd ( 9 = \ufffda ) \ufffdj?\ufffd\ufffdp\ufffd\ufffd\ufffd\ufffdq\ufffd\u07e73\ufffd } \ufffd\ufffdH\u0638a\ufffd | \ufffd\ufffd\ufffdw-\ufffdq\ufffd\ufffd96\ufffd\ufffd\ufffdo\ufffd9\u0303\ufffdEv\ufffdv\ufffd\ufffd*$\ufffd\ufffd\ufffd\ufffdTU/\ufffd\ufffd\ufffd\u04d4\ufffd\ufffdv\ufffdG\ufffd\ufffd\ufffdT\ufffd1<\u06b9\ufffd\ufffdC#p | i\ufffd\ufffdA\u01a2\u02c5_!6\ufffd\ufffd\ufffdF\ufffd\ufffd\ufffdw\ufffd@ \ufffdC\ufffd\ufffdJ\ufffd\ufffd\ufffd+81\ufffd8\ufffd","title":"Encrypt the source code and filesystem"},{"location":"hello_world_kubernetes/#build-the-new-image","text":"You can now exit the container. Let's build the server image with our files, now encrypted: cat > Dockerfile << EOF FROM $BASE_IMAGE EXPOSE 4443 COPY app_image / CMD [ \"python3\" ] EOF Choose the new image name and build it: export IMAGE = sconecuratedimages/kubernetes:hello-k8s-scone-0.3 docker build --pull . -t $IMAGE && docker push $IMAGE","title":"Build the new image"},{"location":"hello_world_kubernetes/#run-in-kubernetes","text":"Time to deploy our server to Kubernetes. Let's setup the attestation first, so we make sure that only our application (identified by its MRENCLAVE ) will have access to the secrets to read the encrypted filesystem, as well as our secret greeting. Extract the MRENCLAVE : MRENCLAVE = $( docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE ) Extract SCONE_FSPF_KEY and SCONE_FSPF_TAG : export SCONE_FSPF_KEY = $( cat app/keytag | awk '{print $11}' ) export SCONE_FSPF_TAG = $( cat app/keytag | awk '{print $9}' ) export SCONE_FSPF = /fspf.pb Create a session file: cat > session-tls.yaml << EOF name: $NS~hello-k8s-scone-tls version: \"0.2\" services: - name: application image_name: application_image mrenclaves: [$MRENCLAVE] command: python3 /app/server-tls.py pwd: / environment: GREETING: hello from SCONE with encrypted source code and auto-generated certs!!! fspf_path: $SCONE_FSPF fspf_key: $SCONE_FSPF_KEY fspf_tag: $SCONE_FSPF_TAG images: - name: application_image injection_files: - path: /app/cert.pem content: \\$\\$SCONE::SERVER_CERT.crt\\$\\$ - path: /app/key.pem content: \\$\\$SCONE::SERVER_CERT.key\\$\\$ secrets: - name: SERVER_CERT kind: x509 EOF Post the session file to your CAS of choice: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session-tls.yaml -X POST https:// $SCONE_CAS_ADDR :8081/session Create the manifest and submit it to your Kubernetes cluster: cat > attested-app-tls.yaml << EOF apiVersion: apps/v1 kind: Deployment metadata: name: attested-hello-world-tls spec: selector: matchLabels: run: attested-hello-world-tls replicas: 1 template: metadata: labels: run: attested-hello-world-tls spec: containers: - name: attested-hello-world-tls image: $IMAGE imagePullPolicy: Always ports: - containerPort: 4443 env: - name: SCONE_CAS_ADDR value: $SCONE_CAS_ADDR - name: SCONE_CONFIG_ID value: $NS~hello-k8s-scone-tls/application - name: SCONE_LAS_ADDR value: 172.17.0.1 resources: limits: sgx.k8s.io/sgx: 1 --- apiVersion: v1 kind: Service metadata: name: attested-hello-world-tls labels: run: attested-hello-world-tls spec: ports: - port: 4443 protocol: TCP selector: run: attested-hello-world-tls EOF kubectl create -f attested-app-tls.yaml -n hello-scone Time to access your app. Forward the traffic to its service: kubectl port-forward svc/attested-hello-world-tls 8084 :4443 -n hello-scone And send a request: $ curl -k https://localhost:8084 Hello World! $GREETING is: hello from SCONE with encrypted source code and auto-generated certs!!!","title":"Run in Kubernetes"},{"location":"hello_world_kubernetes/#clean-up","text":"To clean up all the resources created in this tutorial, simply delete the namespace: kubectl delete namespace hello-scone Author: Clenimar \u00a9 scontain.com , 2021. Questions or Suggestions?","title":"Clean up"},{"location":"helm/","text":"SconeApps Helm is the package manager for Kubernetes . It is our favorite way to deploy SCONE-based confidential applications to Kubernetes. Note that we do not need to trust Helm nor Kubernetes : SCONE & SGX can protect the confidentiality and the integrity of applications - even if Helm or Kubernetes would be compromised. Our objective is that one can securely outsource the management of Kubernetes and services, like a database service, to an external entity while still being in control of one's applications and data. One establishes trust with a confidential application via remote attestation . In the case of SCONE, each confidential service is associated with a policy. Attestation ensures that the confidential service conforms to this policy. The attestation is in most cases delegated to TLS. In this way, one can implement mutual attestation of confidential services without the need to modify applications. Prerequisites A Kubernetes cluster At least one of the nodes has an Intel SGX CPU An Intel SGX driver is installed A Helm 3 client Access to our git repository https://github.com/scontain/sconeapps If you need commercial help installing Kubernetes clusters for confidential applications, just drop us an email . sconeapps: Curated Confidential Applications We support a large variety of confidential applications. One of the most convenient ways to deploy confidential applications to Kubernetes is to use Helm . We currently support this for commercial customers only. For the following steps you need to get access to the SCONE standard edition ( email us ). sconeapps is a private Helm repository for which we need to grant you access. After you got access, you need to generate a GitHub token ( Click here to issue a GitHub token ). Use your token to add the sconeapps repository to Helm: export GH_TOKEN = ... helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update SCONE Hub Credentials The sconeapps images are stored in a private Docker Hub repo. Hence, to deploy these images, we need to grant you access and you need to pass your SCONE Hub credentials to Kubernetes. To do so, you need to create a Kubernetes secret sconeapps containing these credentials. You could first define your credentials which includes the generation of an access token to read_registry and set: export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... and then create a Kubernetes secret as follows: kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Our helm charts will refer to this secret by default. In case you already use a different secret name for your SCONE HUB credentials, you can overwrite the name of the secret at the time you install any of the sconeapp charts. Using Helm See all charts available: helm search repo sconeapps Installing a chart: helm install my-database sconeapps/mariadb-scone This repository contains the following charts: Chart Description cas Deploy the SCONE Configuration and Attestation Service (CAS) to Kubernetes . database Umbrella chart to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy las Deploy the SCONE Local Attestation Service (LAS) to all Kubernetes nodes; mariadb-scone Deploy MariaDB SCONE , i.e., MariaDB running inside of SGX enclaves, to Kubernetes maxscale Deploy MaxScale SCONE , i.e., Maxscale running inside of SGX enclaves and optionally, an HAProxy as Ingress memcached Deploy memcached inside of SGX enclaves, generate and inject TLS certificates to secure communication. nginx Deploy nginx inside of SGX enclaves, generate and inject TLS certificates to secure communication. openvino OpenVINO (Open Visual Inference and Neural network Optimization) is a toolkit facilitating the optimization and deployment of Deep Learning models pytorch Deploy pytorch inside of SGX enclaves. sgxdevplug Deploy our SGX Kubernetes plugin to Kubernetes; this is required by all other helm charts; spark Apache Spark is an open-source distributed general-purpose cluster-computing framework. TEEMon Deploy TEEMon, our monitoring framework for confidential applications. It collects and visualizes performance metrics including SGX Metrics; tensorflow Machine Learning framework by Google tensorflowlite Deploy machine learning models Visual Studio Code Deploy VisualStudio Code and the SCONE CrossCompiler to be able to edit and run your confidential applications inside of a Kubernetes cluster. Zookeeper Deploy Zookeeper cluster inside of SGX enclaves. We support a large number of applications running inside of SGX enclave s. Just let us know what applications you need and we will add helm charts for these applications first. Example The database chart will deploy mariadb and maxscale charts together. The example below will deploy 6 instances of MariaDB SCONE : each one will act as a different shard, and also be attested by the provided CAS. MaxScale will generate a maxscale.cnf using the in-cluster DNS entries for each MariaDB instance. The last line enables an HAProxy Ingress exposing the MaxScale replicas to the external world. export SCONE_CAS_ADDR = scone-cas.cf helm install my-database sconeapps/database \\ --set global.mariadb.replicaCount = 6 \\ --set mariadb-scone.scone.attestation.cas = $SCONE_CAS_ADDR \\ --set mariadb-scone.scone.attestation.config_id = database/db \\ --set maxscale.generateConfig = true \\ --set maxscale.haproxy-ingress.enabled = true To check other examples and the complete set of options to further customize these charts, please refer to later sections of this chapter.","title":"SconeApps"},{"location":"helm/#sconeapps","text":"Helm is the package manager for Kubernetes . It is our favorite way to deploy SCONE-based confidential applications to Kubernetes. Note that we do not need to trust Helm nor Kubernetes : SCONE & SGX can protect the confidentiality and the integrity of applications - even if Helm or Kubernetes would be compromised. Our objective is that one can securely outsource the management of Kubernetes and services, like a database service, to an external entity while still being in control of one's applications and data. One establishes trust with a confidential application via remote attestation . In the case of SCONE, each confidential service is associated with a policy. Attestation ensures that the confidential service conforms to this policy. The attestation is in most cases delegated to TLS. In this way, one can implement mutual attestation of confidential services without the need to modify applications. Prerequisites A Kubernetes cluster At least one of the nodes has an Intel SGX CPU An Intel SGX driver is installed A Helm 3 client Access to our git repository https://github.com/scontain/sconeapps If you need commercial help installing Kubernetes clusters for confidential applications, just drop us an email .","title":"SconeApps"},{"location":"helm/#sconeapps-curated-confidential-applications","text":"We support a large variety of confidential applications. One of the most convenient ways to deploy confidential applications to Kubernetes is to use Helm . We currently support this for commercial customers only. For the following steps you need to get access to the SCONE standard edition ( email us ). sconeapps is a private Helm repository for which we need to grant you access. After you got access, you need to generate a GitHub token ( Click here to issue a GitHub token ). Use your token to add the sconeapps repository to Helm: export GH_TOKEN = ... helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update","title":"sconeapps: Curated Confidential Applications"},{"location":"helm/#scone-hub-credentials","text":"The sconeapps images are stored in a private Docker Hub repo. Hence, to deploy these images, we need to grant you access and you need to pass your SCONE Hub credentials to Kubernetes. To do so, you need to create a Kubernetes secret sconeapps containing these credentials. You could first define your credentials which includes the generation of an access token to read_registry and set: export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... and then create a Kubernetes secret as follows: kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Our helm charts will refer to this secret by default. In case you already use a different secret name for your SCONE HUB credentials, you can overwrite the name of the secret at the time you install any of the sconeapp charts.","title":"SCONE Hub Credentials"},{"location":"helm/#using-helm","text":"See all charts available: helm search repo sconeapps Installing a chart: helm install my-database sconeapps/mariadb-scone This repository contains the following charts: Chart Description cas Deploy the SCONE Configuration and Attestation Service (CAS) to Kubernetes . database Umbrella chart to deploy a scalable, confidential database consisting of MariaDB SCONE and MaxScale SCONE and HAProxy las Deploy the SCONE Local Attestation Service (LAS) to all Kubernetes nodes; mariadb-scone Deploy MariaDB SCONE , i.e., MariaDB running inside of SGX enclaves, to Kubernetes maxscale Deploy MaxScale SCONE , i.e., Maxscale running inside of SGX enclaves and optionally, an HAProxy as Ingress memcached Deploy memcached inside of SGX enclaves, generate and inject TLS certificates to secure communication. nginx Deploy nginx inside of SGX enclaves, generate and inject TLS certificates to secure communication. openvino OpenVINO (Open Visual Inference and Neural network Optimization) is a toolkit facilitating the optimization and deployment of Deep Learning models pytorch Deploy pytorch inside of SGX enclaves. sgxdevplug Deploy our SGX Kubernetes plugin to Kubernetes; this is required by all other helm charts; spark Apache Spark is an open-source distributed general-purpose cluster-computing framework. TEEMon Deploy TEEMon, our monitoring framework for confidential applications. It collects and visualizes performance metrics including SGX Metrics; tensorflow Machine Learning framework by Google tensorflowlite Deploy machine learning models Visual Studio Code Deploy VisualStudio Code and the SCONE CrossCompiler to be able to edit and run your confidential applications inside of a Kubernetes cluster. Zookeeper Deploy Zookeeper cluster inside of SGX enclaves. We support a large number of applications running inside of SGX enclave s. Just let us know what applications you need and we will add helm charts for these applications first.","title":"Using Helm"},{"location":"helm/#example","text":"The database chart will deploy mariadb and maxscale charts together. The example below will deploy 6 instances of MariaDB SCONE : each one will act as a different shard, and also be attested by the provided CAS. MaxScale will generate a maxscale.cnf using the in-cluster DNS entries for each MariaDB instance. The last line enables an HAProxy Ingress exposing the MaxScale replicas to the external world. export SCONE_CAS_ADDR = scone-cas.cf helm install my-database sconeapps/database \\ --set global.mariadb.replicaCount = 6 \\ --set mariadb-scone.scone.attestation.cas = $SCONE_CAS_ADDR \\ --set mariadb-scone.scone.attestation.config_id = database/db \\ --set maxscale.generateConfig = true \\ --set maxscale.haproxy-ingress.enabled = true To check other examples and the complete set of options to further customize these charts, please refer to later sections of this chapter.","title":"Example"},{"location":"helm_cas/","text":"SCONE CAS: Deploy and Attest SCONE CAS is the configuration and attestation service that manages policies and attests services. We explain how to start SCONE CAS with the help of helm . Further, we show how to attest the SCONE CAS to ensure that it was properly deployed. Prerequisites A Kubernetes cluster Helm setup was performed The SGX Plugin is installed Deploying SCONE CAS The sconeapps/cas chart will deploy a SCONE CAS . If we do not need to set any special affinity options and we want to deploy the development CAS, you can just execute: helm install cas sconeapps/cas This starts SCONE CAS in the default Kubernetes namespace. As an alternative, you can deploy SCONE CAS with Kubeapps . Configuration Options You can learn about the configuration options by executing: helm install cas sconeapps/cas --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different image by first defining environment variable SCONE_CAS_IMAGE appropriately. You can do this as follows: helm install cas sconeapps/cas --set image = $SCONE_CAS_IMAGE By default, this helm chart uses the SGX Plugin . Hence, it sets the resource limits of cas as follows: resources: limits: sgx.k8s.io/sgx: 1 In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes: - name: dev-isgx hostPath: path: /dev/isgx extraVolumeMounts: - name: dev-isgx path: /dev/isgx You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status cas Attesting SCONE CAS Since we do not know if SCONE CAS was properly started, we need to attest SCONE CAS . Typically, the attestation will be performed by another confidential service. You can also perform the attestation manually. We assume that you trust your local computer that it properly executes the attestation. You can attest your SCONE CAS as follows. First, forward the traffic from your local computer to the release that you just started: kubectl port-forward service/cas 8081 :8081 Second, determine MRENCLAVE of the SCONE CAS image. Note for commercial customers, we provide a secure way to determine MRENCLAVE . A generic way to determine MRENCLAVE is as follows. We set the environment variable IMAGE to the name of the SCONE CAS image. If you changed the SCONE CAS image, we assume that you set the environment variable SCONE_CAS_IMAGE accordingly (see above). We then compute MRENCLAVE of SCONE CAS - assuming that you trust your local computer: export IMAGE = ${ SCONE_CAS_IMAGE :- registry .scontain.com: 5050 /sconecuratedimages/services: cas .trust.group-out-of-date } export CAS_MRENCLAVE = $( docker pull $IMAGE > /dev/null ; docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE cas ) You can view the result like this: $ echo $CAS_MRENCLAVE 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794 You can attest SCONE CAS on your local machine by executing the following command (set CLI_IMAGE to an image that contains the SCONE CLI): docker run -e SCONE_MODE = SIM -it --rm $CLI_IMAGE scone cas attest -G --only_for_testing-debug --only_for_testing-ignore-signer host.docker.internal $CAS_MRENCLAVE It will print the following output if the SCONE CAS runs inside of an enclave: CAS host.docker.internal at https://host.docker.internal:8081/ is trustworthy and it will store the certificate of the SCONE CAS . This certificate can be used later to verify that we connect to the attested SCONE CAS . For attesting a SCONE CAS in production mode, you should use hardware mode. We determine which SGX device to mount with the function determine_sgx_device . determine_sgx_device docker run $MOUNT_SGXDEVICE -e SCONE_MODE = HW -it --rm $CLI_IMAGE scone cas attest host.docker.internal $CAS_MRENCLAVE You can also try to attest with a wrong CAS_MRENCLAVE (e.g. by incrementing by 1): export CAS_MRENCLAVE = 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696795 docker run -e SCONE_MODE = SIM -it --rm $BASE_IMAGE scone cas attest -G --only_for_testing-debug --only_for_testing-ignore-signer host.docker.internal $CAS_MRENCLAVE The output would look like this: Error: Error during attestation of CAS Caused by: Enclave hash does not match expectation: 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794","title":"CAS Deploy & Attest"},{"location":"helm_cas/#scone-cas-deploy-and-attest","text":"SCONE CAS is the configuration and attestation service that manages policies and attests services. We explain how to start SCONE CAS with the help of helm . Further, we show how to attest the SCONE CAS to ensure that it was properly deployed. Prerequisites A Kubernetes cluster Helm setup was performed The SGX Plugin is installed","title":"SCONE CAS: Deploy and Attest"},{"location":"helm_cas/#deploying-scone-cas","text":"The sconeapps/cas chart will deploy a SCONE CAS . If we do not need to set any special affinity options and we want to deploy the development CAS, you can just execute: helm install cas sconeapps/cas This starts SCONE CAS in the default Kubernetes namespace. As an alternative, you can deploy SCONE CAS with Kubeapps .","title":"Deploying SCONE CAS"},{"location":"helm_cas/#configuration-options","text":"You can learn about the configuration options by executing: helm install cas sconeapps/cas --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different image by first defining environment variable SCONE_CAS_IMAGE appropriately. You can do this as follows: helm install cas sconeapps/cas --set image = $SCONE_CAS_IMAGE By default, this helm chart uses the SGX Plugin . Hence, it sets the resource limits of cas as follows: resources: limits: sgx.k8s.io/sgx: 1 In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes: - name: dev-isgx hostPath: path: /dev/isgx extraVolumeMounts: - name: dev-isgx path: /dev/isgx You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status cas","title":"Configuration Options"},{"location":"helm_cas/#attesting-scone-cas","text":"Since we do not know if SCONE CAS was properly started, we need to attest SCONE CAS . Typically, the attestation will be performed by another confidential service. You can also perform the attestation manually. We assume that you trust your local computer that it properly executes the attestation. You can attest your SCONE CAS as follows. First, forward the traffic from your local computer to the release that you just started: kubectl port-forward service/cas 8081 :8081 Second, determine MRENCLAVE of the SCONE CAS image. Note for commercial customers, we provide a secure way to determine MRENCLAVE . A generic way to determine MRENCLAVE is as follows. We set the environment variable IMAGE to the name of the SCONE CAS image. If you changed the SCONE CAS image, we assume that you set the environment variable SCONE_CAS_IMAGE accordingly (see above). We then compute MRENCLAVE of SCONE CAS - assuming that you trust your local computer: export IMAGE = ${ SCONE_CAS_IMAGE :- registry .scontain.com: 5050 /sconecuratedimages/services: cas .trust.group-out-of-date } export CAS_MRENCLAVE = $( docker pull $IMAGE > /dev/null ; docker run -i --rm -e \"SCONE_HASH=1\" $IMAGE cas ) You can view the result like this: $ echo $CAS_MRENCLAVE 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794 You can attest SCONE CAS on your local machine by executing the following command (set CLI_IMAGE to an image that contains the SCONE CLI): docker run -e SCONE_MODE = SIM -it --rm $CLI_IMAGE scone cas attest -G --only_for_testing-debug --only_for_testing-ignore-signer host.docker.internal $CAS_MRENCLAVE It will print the following output if the SCONE CAS runs inside of an enclave: CAS host.docker.internal at https://host.docker.internal:8081/ is trustworthy and it will store the certificate of the SCONE CAS . This certificate can be used later to verify that we connect to the attested SCONE CAS . For attesting a SCONE CAS in production mode, you should use hardware mode. We determine which SGX device to mount with the function determine_sgx_device . determine_sgx_device docker run $MOUNT_SGXDEVICE -e SCONE_MODE = HW -it --rm $CLI_IMAGE scone cas attest host.docker.internal $CAS_MRENCLAVE You can also try to attest with a wrong CAS_MRENCLAVE (e.g. by incrementing by 1): export CAS_MRENCLAVE = 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696795 docker run -e SCONE_MODE = SIM -it --rm $BASE_IMAGE scone cas attest -G --only_for_testing-debug --only_for_testing-ignore-signer host.docker.internal $CAS_MRENCLAVE The output would look like this: Error: Error during attestation of CAS Caused by: Enclave hash does not match expectation: 9a1553cd86fd3358fb4f5ac1c60eb8283185f6ae0e63de38f907dbaab7696794","title":"Attesting SCONE CAS"},{"location":"helm_las/","text":"Deploying SCONE LAS SCONE LAS is the SCONE Local Attestation Service. It has to run on each platform, i.e., on each server, on which confidential applications should run. We explain how to start SCONE LAS with the help of helm . Note that SCONE LAS is implicitly attested by SCONE CAS and hence, we do not need to attest SCONE LAS explicitly. Prerequisites A Kubernetes cluster Helm setup was performed Deploying SCONE LAS The sconeapps/las chart will deploy SCONE LAS : helm install las sconeapps/las This starts SCONE LAS in the default Kubernetes namespace. The application is called las . As an alternative, you can deploy SCONE LAS with Kubeapps . Configuration Options You can learn about the configuration options by executing: helm install las sconeapps/las --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different SCONE LAS image by first defining environment variable SCONE_LAS_IMAGE appropriately. You can do this as follows: helm install las sconeapps/las --set image = $SCONE_LAS_IMAGE You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status las Determining the Platform IDs SCONE policies can limit the execution of an services to certain platforms , i.e., to certain computers. For example, you might need to ensure that your data can only be processed in a certain datacenter. Each platform is identified by a unique public key: This is the public key of its local attestation service. When a las instance is restarted on the same platform, it will get the same public key. The public key might, however, change after a microcode update or an update of the las code base. In these cases, you will have to update the list of permitted platforms in your policies. In a Kubernetes cluster, you can determine the platform ids with the help of the following bash function that queries the public keys of all las instances: # set INSTANCE to the name of your LAS instance # set NAMESPACE in case you do not use the default namespace function get_platform_ids { NAME = ${ INSTANCE :- las } NS = ${ NAMESPACE :- default } kubectl get pod --namespace \" $NS \" --selector = \"app.kubernetes.io/name= $NAME ,app.kubernetes.io/instance= $NAME \" -o json \\ | python3 < ( cat <<EOF import sys, json, subprocess; j=json.load(sys.stdin) print(\"platforms: [\", end=\"\") comma=\"\" for i in j['items']: pubkey=subprocess.check_output(\"kubectl logs pod/%s | grep 'public key' | tail -n 1 | awk '{print \\$8}'\" % i['metadata']['name'], shell=True, encoding='ascii').rstrip(\"\\n\") print(comma, pubkey, end=\"\") comma=\", \" print(\"]\") EOF ) } In case you do not execute las in the default Kubernets namespace, set environment variable NAMESPACE . If you do not name your service las , set environment variable INSTANCE accordingly. Then just execute: get_platform_ids The output might look like this: platforms : [ 126C8098408FEB2002F5EB8B9E6C2AE26197E3617875633C5BD4EB4454278C34 , BCA7B05F55BCA38EA7A0BDEBDC402942BE77BEC7F0D3F37C72F6C1898047B312 ] You can copy and paste this to your policy to ensure that your application(s) can only run in a certain Kubernetes cluster. If you do not even trust your Kubernetes cluster to correctly determine the public keys of the LAS instances, let us know and we can suggest more secure alternatives.","title":"LAS Deploy & Platforms"},{"location":"helm_las/#deploying-scone-las","text":"SCONE LAS is the SCONE Local Attestation Service. It has to run on each platform, i.e., on each server, on which confidential applications should run. We explain how to start SCONE LAS with the help of helm . Note that SCONE LAS is implicitly attested by SCONE CAS and hence, we do not need to attest SCONE LAS explicitly. Prerequisites A Kubernetes cluster Helm setup was performed","title":"Deploying SCONE LAS"},{"location":"helm_las/#deploying-scone-las_1","text":"The sconeapps/las chart will deploy SCONE LAS : helm install las sconeapps/las This starts SCONE LAS in the default Kubernetes namespace. The application is called las . As an alternative, you can deploy SCONE LAS with Kubeapps .","title":"Deploying SCONE LAS"},{"location":"helm_las/#configuration-options","text":"You can learn about the configuration options by executing: helm install las sconeapps/las --dry-run One option to set is the image to be deployed. If you want to run in release mode , you need to specify a different SCONE LAS image by first defining environment variable SCONE_LAS_IMAGE appropriately. You can do this as follows: helm install las sconeapps/las --set image = $SCONE_LAS_IMAGE You can check the status of this release , i.e., the instance that you just deployed, by executing: helm status las","title":"Configuration Options"},{"location":"helm_las/#determining-the-platform-ids","text":"SCONE policies can limit the execution of an services to certain platforms , i.e., to certain computers. For example, you might need to ensure that your data can only be processed in a certain datacenter. Each platform is identified by a unique public key: This is the public key of its local attestation service. When a las instance is restarted on the same platform, it will get the same public key. The public key might, however, change after a microcode update or an update of the las code base. In these cases, you will have to update the list of permitted platforms in your policies. In a Kubernetes cluster, you can determine the platform ids with the help of the following bash function that queries the public keys of all las instances: # set INSTANCE to the name of your LAS instance # set NAMESPACE in case you do not use the default namespace function get_platform_ids { NAME = ${ INSTANCE :- las } NS = ${ NAMESPACE :- default } kubectl get pod --namespace \" $NS \" --selector = \"app.kubernetes.io/name= $NAME ,app.kubernetes.io/instance= $NAME \" -o json \\ | python3 < ( cat <<EOF import sys, json, subprocess; j=json.load(sys.stdin) print(\"platforms: [\", end=\"\") comma=\"\" for i in j['items']: pubkey=subprocess.check_output(\"kubectl logs pod/%s | grep 'public key' | tail -n 1 | awk '{print \\$8}'\" % i['metadata']['name'], shell=True, encoding='ascii').rstrip(\"\\n\") print(comma, pubkey, end=\"\") comma=\", \" print(\"]\") EOF ) } In case you do not execute las in the default Kubernets namespace, set environment variable NAMESPACE . If you do not name your service las , set environment variable INSTANCE accordingly. Then just execute: get_platform_ids The output might look like this: platforms : [ 126C8098408FEB2002F5EB8B9E6C2AE26197E3617875633C5BD4EB4454278C34 , BCA7B05F55BCA38EA7A0BDEBDC402942BE77BEC7F0D3F37C72F6C1898047B312 ] You can copy and paste this to your policy to ensure that your application(s) can only run in a certain Kubernetes cluster. If you do not even trust your Kubernetes cluster to correctly determine the public keys of the LAS instances, let us know and we can suggest more secure alternatives.","title":"Determining the Platform IDs"},{"location":"helm_mariadb/","text":"Deploying and Attesting a Confidential MariaDB We explain how to deploy a confidential MariaDB running inside of an SGX enclave with the help of SCONE and helm . We support two deployment architectures for Confidential MariaDB : a scalable deployment supporting horizontal scaling with the help of MaxScale and HaProxy, and a simple, single-master deployment. Background Privacy regulations define the scope of Personally Identifiable Information (PII), i.e., what is and what is not PII. For example, the date of birth is always considered PII but unique online identifiers might not included in all privacy regulations yet. Each legislation might decide independently on what is considered PII and surely, the definition of PII will change over time. For companies with customers from various geographic regions, conformance with all privacy regulations is a challenge. Non-compliance comes with a variety of risks. Hence, missing to protect some PII and data in general can imply high costs. One approach is to encrypt only those fields or parts of a field of a database that contain PII. One popular way to do so is to use tokenization of these fields. As we explained above, it is not always clear which fields need to be tokenized and this might change over time. Moreover, some fields like customer support notes might contain PII - even if there is a policy against storing such information in the notes. Our approach is as follows. We recommend to encrypt everything all the time, i.e., at rest, on the wire and during runtime. In many cases, this is the best answer for safeguarding against all possible risks. We show how to limit access to the database to authorized clients only and can even prevent operators to log into the database. In this way, one cannot only reduce the scope of security assessments but one can use it as a safety net for other mechanisms like masking and tokenization . Our scalable setup already supports maxscale's data masking out of the box. Architecture In this section, we describe the single-master MariaDB deployment. This setup provides the following security properties: runtime encryption : MariaDB runs inside inside of an enclave and hence, the integrity and confidentiality of the data and the code is protected during processing. encryption at rest : All files - including all databases - are encrypted using MariaDB 's built-in encryption mechanism, i.e., all data written to disk is encrypted before leaving the enclave and all data read from disk is decrypted after entering the enclave . encryption in transit : All data in transit is encrypted with the help of TLS. Data transmitted via the network is encrypted before leaving MariaDB 's enclave and data received is decrypted after entering the enclave . secrets management : MariaDB 's encryption keys and the TLS certificate are provided to MariaDB using a security policy. SCONE CAS enforces this policy, i.e., it ensures that only MariaDB running inside of an enclave will be able to access these keys. protection against insider attacks : MariaDB 's are generated inside of an enclave by SCONE CAS , i.e., the keys cannot be seen or retrieved by any human. authorized platforms only : You can limit the execution of MariaDB to the nodes of your Kubernetes cluster. As such, an adversary cannot, for example, start MariaDB on another computer using a stolen copy of your encrypted databases and try to attack MariaDB after it retrieved the secrets: SCONE CAS will not provide the secrets to this MariaDB instance. authorized clients only : We can limit access to MariaDB to certain clients, e.g., a tokenizer only. This access limitation is based on TLS client certificates. We can enforce that, for example, a tokenizer has to run inside of an enclave to get access to MariaDB . Note that while the tokenizer protects individual data items inside the database, it limits the expressiveness of queries. The combination of a tokenizer with a confidential MariaDB permits better trade-offs to ensure excellent security and expressiveness. data masking : One can use maxscale's data masking to limit the visibility of Personally Identifiable Information. Prerequisites A Kubernetes cluster helm is deployed Kubernetes SGX plugin is installed We granted you access to sconeapps and SCONE CLI container images We granted you access to a MariaDB image and the MariaDB policy templates SCONE CAS and SCONE LAS are deployed Creating a MariaDB Security Policy To run a confidential MariaDB , one needs to create a security policy, a.k.a. session in a SCONE CAS instance. You can deploy CAS with helm . Before we can create a policy, we need to attest CAS , i.e., we determine that SCONE CAS runs inside an enclave and it was not modified. To attest SCONE CAS , we define two environment variables ( see details ): $SCONE_CAS_ADDR and $CAS_MRENCLAVE . We can then attest CAS with the SCONE CLI running on a trusted computer. Alternatively, you can attest with the CLI running inside of an enclave. scone cas attest $SCONE_CAS_ADDR $CAS_MRENCLAVE If the attestation is successful, the SCONE CLI stores the certificate of the attested SCONE CAS in a sealed file on your local computer. Whenever the CLI connects to this SCONE CAS again, it will ensure with TLS that it talks to the same CAS. Next, we need to create a session, for MariaDB and a second policy for the MariaDB clients. We provide some default session templates that can be customized to your needs. Each session has a unique name. We generate unique session names for the database which we store in environment variable $DBSESSION . For the MariaDB client, we store a unique session name in environment variable DBCLIENT . export DBSESSION = \"MariaDBSession- $RANDOM \" export DBCLIENT = \"MariaDBClientSession- $RANDOM \" We limit the execution of our database to the nodes of our Kubernetes cluster (see platforms ): export PLATFORMS = $( get_platform_ids ) We can now create two sessions - with templates that we provide: one for the database, and one for the clients that are permitted to connect to this MariaDB instance: scone session create --name = $DBCLIENT --use-env scone-templates/mariadb/dbclient.yml scone session create --name = $DBSESSION --use-env scone-templates/mariadb/dbsession.yml The MariaDB session template will give session $DBCLIENT access to the MariaDB CA certificate - which is created as part of $DBSESSION - to ensure that the MariDB clients can authenticate this MariaDB instance. Deploying MariaDB The sconeapps/mariadb chart will deploy MariaDB in an enclave . We set the session in which we want to run this MariaDB as follows: helm install mariadb sconeapps/mariadb --set scone.attestation.config_id = $DBSESSION This starts MariaDB inside of an enclave in the default Kubernetes namespace. Its security policy $DBSESSION creates client certificates, exports these to the client policy $DBCLIENT and the client policy $DBCLIENT imports these client certificates. This gives the clients the right to connect to this MariaDB instance. Attestation The attestation of MariaDB is performed implicitly whenever an authorized MariaDB client connects to MariaDB (i.e, a client that has a valid client certificate issued by $DBSESSION ): MariaDB instance only receives its certificate and secrets from SCONE CAS if it satisfies the conditions in its policy $DBSESSIONS a MariaDB client gets authorized , i.e., receives a MariaDB client certificate, if it satisfies the condition stated in its policy $DBCLIENT . It also receives the CA certificate of MaraDB's certificate: This CA certificate only signs certificates if the service - in this case MariaDB - satisfies all its security properties. when an authorized MariaDB client connects to MariaDB , TLS will ensure that both the client as well as the MariaDB instance satisfy their respective security policies.","title":"MariaDB Deploy & Attest"},{"location":"helm_mariadb/#deploying-and-attesting-a-confidential-mariadb","text":"We explain how to deploy a confidential MariaDB running inside of an SGX enclave with the help of SCONE and helm . We support two deployment architectures for Confidential MariaDB : a scalable deployment supporting horizontal scaling with the help of MaxScale and HaProxy, and a simple, single-master deployment.","title":"Deploying and Attesting a Confidential MariaDB"},{"location":"helm_mariadb/#background","text":"Privacy regulations define the scope of Personally Identifiable Information (PII), i.e., what is and what is not PII. For example, the date of birth is always considered PII but unique online identifiers might not included in all privacy regulations yet. Each legislation might decide independently on what is considered PII and surely, the definition of PII will change over time. For companies with customers from various geographic regions, conformance with all privacy regulations is a challenge. Non-compliance comes with a variety of risks. Hence, missing to protect some PII and data in general can imply high costs. One approach is to encrypt only those fields or parts of a field of a database that contain PII. One popular way to do so is to use tokenization of these fields. As we explained above, it is not always clear which fields need to be tokenized and this might change over time. Moreover, some fields like customer support notes might contain PII - even if there is a policy against storing such information in the notes. Our approach is as follows. We recommend to encrypt everything all the time, i.e., at rest, on the wire and during runtime. In many cases, this is the best answer for safeguarding against all possible risks. We show how to limit access to the database to authorized clients only and can even prevent operators to log into the database. In this way, one cannot only reduce the scope of security assessments but one can use it as a safety net for other mechanisms like masking and tokenization . Our scalable setup already supports maxscale's data masking out of the box.","title":"Background"},{"location":"helm_mariadb/#architecture","text":"In this section, we describe the single-master MariaDB deployment. This setup provides the following security properties: runtime encryption : MariaDB runs inside inside of an enclave and hence, the integrity and confidentiality of the data and the code is protected during processing. encryption at rest : All files - including all databases - are encrypted using MariaDB 's built-in encryption mechanism, i.e., all data written to disk is encrypted before leaving the enclave and all data read from disk is decrypted after entering the enclave . encryption in transit : All data in transit is encrypted with the help of TLS. Data transmitted via the network is encrypted before leaving MariaDB 's enclave and data received is decrypted after entering the enclave . secrets management : MariaDB 's encryption keys and the TLS certificate are provided to MariaDB using a security policy. SCONE CAS enforces this policy, i.e., it ensures that only MariaDB running inside of an enclave will be able to access these keys. protection against insider attacks : MariaDB 's are generated inside of an enclave by SCONE CAS , i.e., the keys cannot be seen or retrieved by any human. authorized platforms only : You can limit the execution of MariaDB to the nodes of your Kubernetes cluster. As such, an adversary cannot, for example, start MariaDB on another computer using a stolen copy of your encrypted databases and try to attack MariaDB after it retrieved the secrets: SCONE CAS will not provide the secrets to this MariaDB instance. authorized clients only : We can limit access to MariaDB to certain clients, e.g., a tokenizer only. This access limitation is based on TLS client certificates. We can enforce that, for example, a tokenizer has to run inside of an enclave to get access to MariaDB . Note that while the tokenizer protects individual data items inside the database, it limits the expressiveness of queries. The combination of a tokenizer with a confidential MariaDB permits better trade-offs to ensure excellent security and expressiveness. data masking : One can use maxscale's data masking to limit the visibility of Personally Identifiable Information. Prerequisites A Kubernetes cluster helm is deployed Kubernetes SGX plugin is installed We granted you access to sconeapps and SCONE CLI container images We granted you access to a MariaDB image and the MariaDB policy templates SCONE CAS and SCONE LAS are deployed","title":"Architecture"},{"location":"helm_mariadb/#creating-a-mariadb-security-policy","text":"To run a confidential MariaDB , one needs to create a security policy, a.k.a. session in a SCONE CAS instance. You can deploy CAS with helm . Before we can create a policy, we need to attest CAS , i.e., we determine that SCONE CAS runs inside an enclave and it was not modified. To attest SCONE CAS , we define two environment variables ( see details ): $SCONE_CAS_ADDR and $CAS_MRENCLAVE . We can then attest CAS with the SCONE CLI running on a trusted computer. Alternatively, you can attest with the CLI running inside of an enclave. scone cas attest $SCONE_CAS_ADDR $CAS_MRENCLAVE If the attestation is successful, the SCONE CLI stores the certificate of the attested SCONE CAS in a sealed file on your local computer. Whenever the CLI connects to this SCONE CAS again, it will ensure with TLS that it talks to the same CAS. Next, we need to create a session, for MariaDB and a second policy for the MariaDB clients. We provide some default session templates that can be customized to your needs. Each session has a unique name. We generate unique session names for the database which we store in environment variable $DBSESSION . For the MariaDB client, we store a unique session name in environment variable DBCLIENT . export DBSESSION = \"MariaDBSession- $RANDOM \" export DBCLIENT = \"MariaDBClientSession- $RANDOM \" We limit the execution of our database to the nodes of our Kubernetes cluster (see platforms ): export PLATFORMS = $( get_platform_ids ) We can now create two sessions - with templates that we provide: one for the database, and one for the clients that are permitted to connect to this MariaDB instance: scone session create --name = $DBCLIENT --use-env scone-templates/mariadb/dbclient.yml scone session create --name = $DBSESSION --use-env scone-templates/mariadb/dbsession.yml The MariaDB session template will give session $DBCLIENT access to the MariaDB CA certificate - which is created as part of $DBSESSION - to ensure that the MariDB clients can authenticate this MariaDB instance.","title":"Creating a MariaDB Security Policy"},{"location":"helm_mariadb/#deploying-mariadb","text":"The sconeapps/mariadb chart will deploy MariaDB in an enclave . We set the session in which we want to run this MariaDB as follows: helm install mariadb sconeapps/mariadb --set scone.attestation.config_id = $DBSESSION This starts MariaDB inside of an enclave in the default Kubernetes namespace. Its security policy $DBSESSION creates client certificates, exports these to the client policy $DBCLIENT and the client policy $DBCLIENT imports these client certificates. This gives the clients the right to connect to this MariaDB instance.","title":"Deploying MariaDB"},{"location":"helm_mariadb/#attestation","text":"The attestation of MariaDB is performed implicitly whenever an authorized MariaDB client connects to MariaDB (i.e, a client that has a valid client certificate issued by $DBSESSION ): MariaDB instance only receives its certificate and secrets from SCONE CAS if it satisfies the conditions in its policy $DBSESSIONS a MariaDB client gets authorized , i.e., receives a MariaDB client certificate, if it satisfies the condition stated in its policy $DBCLIENT . It also receives the CA certificate of MaraDB's certificate: This CA certificate only signs certificates if the service - in this case MariaDB - satisfies all its security properties. when an authorized MariaDB client connects to MariaDB , TLS will ensure that both the client as well as the MariaDB instance satisfy their respective security policies.","title":"Attestation"},{"location":"helm_sgxdevplugin/","text":"Kubernetes SGX Plugin (sgxdevplugin) Prerequisites A Kubernetes cluster Helm setup was performed Background Your Kubernetes cluster might be heterogeneous, i.e., it might contain nodes with different capabilities. Hence, some of the nodes might have SGX version 1 support and some might have full SGX version 2 support and some might have partial SGX version 2 support and some servers might have no SGX support at all. With SCONE, you can run the same binary on version 1 or version 2 hosts. Depending on the version of your CPU and your SGX driver, you might see devices /dev/sgx (DCAP) or /dev/isgx (IAS) or ... . You should not - and you do not - need to care about about these details. SCONE will find the right SGX device and will adjust its behavior depending on the kind of CPU you are running on. When scheduling your confidential application on a cluster, you ideally want to just specify that you need SGX support. Of course, in addition to this basic scheduling on a SGX-capable node, you can use all the niceties of Kubernetes , like affinity and tolerations , to optimize the scheduling. To access the SGX device in a Kubernetes cluster, you might need to run the containers in privileged mode. Of course, one wants to avoid running applications in privileged mode. Our Kubernetes SGX plugin allows you to run containers accessing the SGX device without requiring privileged mode. Note, however, that the plugin has to run as a privileged container itself. If you prefer to run without this plugin, you can do so as we describe in this tutorial . Note, however, that our sconeapps helm charts expect this Kubernetes SGX plugin. Usage We provide a plugin for Kubernetes that permits containers to access the SGX device without having privileged access. You will just need to specify that your container requires access to a SGX device. However, you need to give this container access to RAWIO . You can specify that a container ( MariaDB ) of your Pod db requires access to SGX as follows: apiVersion: v1 kind: Pod metadata: name: db spec: containers: - name: MariaDB image: registry.scontain.com:5050/sconecuratedimages/mariadb:production securityContext: capabilities: add: [\"SYS_RAWIO\"] resources: limits: sgx.k8s.io/sgx: 1 # Requires to some sgx device like /dev/isgx or /dev/sgx or successor With sgx you say that any SGX device is fine for your applications. This should be the default with SCONE-based applications. In case you need to run on a machine which has a DCAP-based attestation, you can specify as follows: limits: sgx.k8s.io/dcap: 1 # Requires DCAP-based attestation In case you need to run on a machine which has an IAS-based attestation, you can specify as follows: limits: sgx.k8s.io/ias: 1 # Requires IAS-based attestation Deploying Kubernetes SGX Plugin The sconeapps/sgx_plugin chart will deploy the Kubernetes SGX plugin in your cluster. You can just execute: helm install sgxdevplugin sconeapps/sgxdevplugin This starts the Kubernetes SGX plugin in the default Kubernetes namespace. This will limit the number of SGX devices per node to 20 (default). If you want to run more enclaves per node, please increase the the value. To increase the maximum allocatable SGX devices per node, please increase the value of maxSGXPods . You can set this value by executing: helm install sgxdevplugin sconeapps/sgxdevplugin --set maxSGXPods = 100 If the plugin is already executing, you can change the value as follows: helm upgrade sgxdevplugin sconeapps/sgxdevplugin --set maxSGXPods = 100 Checking Status of SGX devices You can check the number of allocatable SGX devices on all nodes of your Kubernetes cluster as follows: export NODES = ` kubectl get node --output = name ` for N in $NODES ; do echo \" $N :\" ; kubectl get $N -o jsonpath = '{.status.allocatable}' ; echo ; done If the number of SGX devices is too small, increase the number of allocatable SGX devices as explained above.","title":"Kubernetes SGX Plugin"},{"location":"helm_sgxdevplugin/#kubernetes-sgx-plugin-sgxdevplugin","text":"Prerequisites A Kubernetes cluster Helm setup was performed","title":"Kubernetes SGX Plugin (sgxdevplugin)"},{"location":"helm_sgxdevplugin/#background","text":"Your Kubernetes cluster might be heterogeneous, i.e., it might contain nodes with different capabilities. Hence, some of the nodes might have SGX version 1 support and some might have full SGX version 2 support and some might have partial SGX version 2 support and some servers might have no SGX support at all. With SCONE, you can run the same binary on version 1 or version 2 hosts. Depending on the version of your CPU and your SGX driver, you might see devices /dev/sgx (DCAP) or /dev/isgx (IAS) or ... . You should not - and you do not - need to care about about these details. SCONE will find the right SGX device and will adjust its behavior depending on the kind of CPU you are running on. When scheduling your confidential application on a cluster, you ideally want to just specify that you need SGX support. Of course, in addition to this basic scheduling on a SGX-capable node, you can use all the niceties of Kubernetes , like affinity and tolerations , to optimize the scheduling. To access the SGX device in a Kubernetes cluster, you might need to run the containers in privileged mode. Of course, one wants to avoid running applications in privileged mode. Our Kubernetes SGX plugin allows you to run containers accessing the SGX device without requiring privileged mode. Note, however, that the plugin has to run as a privileged container itself. If you prefer to run without this plugin, you can do so as we describe in this tutorial . Note, however, that our sconeapps helm charts expect this Kubernetes SGX plugin.","title":"Background"},{"location":"helm_sgxdevplugin/#usage","text":"We provide a plugin for Kubernetes that permits containers to access the SGX device without having privileged access. You will just need to specify that your container requires access to a SGX device. However, you need to give this container access to RAWIO . You can specify that a container ( MariaDB ) of your Pod db requires access to SGX as follows: apiVersion: v1 kind: Pod metadata: name: db spec: containers: - name: MariaDB image: registry.scontain.com:5050/sconecuratedimages/mariadb:production securityContext: capabilities: add: [\"SYS_RAWIO\"] resources: limits: sgx.k8s.io/sgx: 1 # Requires to some sgx device like /dev/isgx or /dev/sgx or successor With sgx you say that any SGX device is fine for your applications. This should be the default with SCONE-based applications. In case you need to run on a machine which has a DCAP-based attestation, you can specify as follows: limits: sgx.k8s.io/dcap: 1 # Requires DCAP-based attestation In case you need to run on a machine which has an IAS-based attestation, you can specify as follows: limits: sgx.k8s.io/ias: 1 # Requires IAS-based attestation","title":"Usage"},{"location":"helm_sgxdevplugin/#deploying-kubernetes-sgx-plugin","text":"The sconeapps/sgx_plugin chart will deploy the Kubernetes SGX plugin in your cluster. You can just execute: helm install sgxdevplugin sconeapps/sgxdevplugin This starts the Kubernetes SGX plugin in the default Kubernetes namespace. This will limit the number of SGX devices per node to 20 (default). If you want to run more enclaves per node, please increase the the value. To increase the maximum allocatable SGX devices per node, please increase the value of maxSGXPods . You can set this value by executing: helm install sgxdevplugin sconeapps/sgxdevplugin --set maxSGXPods = 100 If the plugin is already executing, you can change the value as follows: helm upgrade sgxdevplugin sconeapps/sgxdevplugin --set maxSGXPods = 100","title":"Deploying Kubernetes SGX Plugin"},{"location":"helm_sgxdevplugin/#checking-status-of-sgx-devices","text":"You can check the number of allocatable SGX devices on all nodes of your Kubernetes cluster as follows: export NODES = ` kubectl get node --output = name ` for N in $NODES ; do echo \" $N :\" ; kubectl get $N -o jsonpath = '{.status.allocatable}' ; echo ; done If the number of SGX devices is too small, increase the number of allocatable SGX devices as explained above.","title":"Checking Status of SGX devices"},{"location":"helm_teemon/","text":"Deploying TEEMon TEEMon is our monitoring infrastructure for Kubernetes clusters. TEEMon provides fine-grained performance metrics during runtime, including SGX metrics like free EPC pages and pages evicted to main memory. It is integrated with several open-source tools: Prometheus, Grafana and Node Exporter. We also added a special exporter for SGX-related metrics. Prerequisites A Kubernetes cluster Helm is deployed Kubernetes SGX plugin is installed We granted you access to sconeapps and TEEMon container images Metrics SGX driver extension is installed TEEMon requires SGX driver metrics extension TEEMon collects SGX-related metrics that are provided by our SGX Driver metrics extension. In Kubernetes clusters without this driver extension (like on AKS), TEEMon will not run correctly. Deployment The monitoring framework runs in native mode. Hence, we delegate the secret management of TEEMon to Kubernetes. Grafana requires an admin password. Hence, we need to set an admin username and password as a Kubernetes secret named teemon . We set MY_PASSWORD to a new random password first and then create a new secret: export MY_PASSWORD = $( openssl rand -base64 32 ) kubectl create secret generic teemon --from-literal = username = admin --from-literal = password = \" $MY_PASSWORD \" The sconeapps/teemon chart will deploy TEEMon . In most cases, just execute: helm install teemon sconeapps/teemon The configuration parameters learn about with the help of KubeApps . You can now connect to the TEEMon dashboard as follows: kubectl port-forward svc/teemon-grafana 8099 :80 The TEEMon dashboard can now be viewed at: http://localhost:8099/ Dashboard TEEMon provides out of the box three dashboards: an SGX dashboard to show resource metrics related to Intel SGX: an Docker dashboard to show the metrics related to containers: an infrastructure dashboard to display resource metrics of the different nodes of a Kubernetes cluster:","title":"TEEMon Deployment"},{"location":"helm_teemon/#deploying-teemon","text":"TEEMon is our monitoring infrastructure for Kubernetes clusters. TEEMon provides fine-grained performance metrics during runtime, including SGX metrics like free EPC pages and pages evicted to main memory. It is integrated with several open-source tools: Prometheus, Grafana and Node Exporter. We also added a special exporter for SGX-related metrics. Prerequisites A Kubernetes cluster Helm is deployed Kubernetes SGX plugin is installed We granted you access to sconeapps and TEEMon container images Metrics SGX driver extension is installed TEEMon requires SGX driver metrics extension TEEMon collects SGX-related metrics that are provided by our SGX Driver metrics extension. In Kubernetes clusters without this driver extension (like on AKS), TEEMon will not run correctly.","title":"Deploying TEEMon"},{"location":"helm_teemon/#deployment","text":"The monitoring framework runs in native mode. Hence, we delegate the secret management of TEEMon to Kubernetes. Grafana requires an admin password. Hence, we need to set an admin username and password as a Kubernetes secret named teemon . We set MY_PASSWORD to a new random password first and then create a new secret: export MY_PASSWORD = $( openssl rand -base64 32 ) kubectl create secret generic teemon --from-literal = username = admin --from-literal = password = \" $MY_PASSWORD \" The sconeapps/teemon chart will deploy TEEMon . In most cases, just execute: helm install teemon sconeapps/teemon The configuration parameters learn about with the help of KubeApps . You can now connect to the TEEMon dashboard as follows: kubectl port-forward svc/teemon-grafana 8099 :80 The TEEMon dashboard can now be viewed at: http://localhost:8099/","title":"Deployment"},{"location":"helm_teemon/#dashboard","text":"TEEMon provides out of the box three dashboards: an SGX dashboard to show resource metrics related to Intel SGX: an Docker dashboard to show the metrics related to containers: an infrastructure dashboard to display resource metrics of the different nodes of a Kubernetes cluster:","title":"Dashboard"},{"location":"helm_vscode/","text":"Visual Studio Code and SCONE CrossCompiler Visual Studio Code is one of our favorite editors. Hence, we combine Visual Studio Code with our SCONE CrossCompiler. We also activate some useful programming language extensions. We also activate a code runner extension; one can compile and run a confidential application with a single click. Deploying Visual Studio Code Prerequisites A Kubernetes cluster Helm is deployed Kubernetes SGX plugin is insatlled We granted you access to the sconeapps repo and to the SCONE IDE images You can easily deploy Visual Studio Code with helm in a Kubernetes cluster. Since Visual Studio Code requires a password to log in, you need to set this password as a Kubernetes secret named ide-secret : kubectl create secret generic ide-secret --from-literal = PASSWORD = New-password Please replace New-password with a new, strong password. While we do not run Visual Code inside of SGX enclaves, we schedule it on a SGX-capable node (using the SGX Plugin ) to ensure that we can run the compiled code inside of SGX enclaves. You can deploy an instance by executing: helm install vscode sconeapps/vscode To display Visual Code Studio in your browser, you need to forward the service port to your local machine. Say, you want to present this on localhost:8081 , then you can forward the service port as follows: kubectl port-forward svc/vscode 8081 :8081 The Visual Studio instance can now be viewed at: http://localhost:8081/ Please use different Kubernetes namespaces (i.e., add -n MyNameSpace in the commands) in case you want to run multiple instances of Visual Code Studio. Usage This Visual Code Studio supports the following SCONE CrossCompiler languages: C C++ Fortran - note that Fortran is not supported by Code Runner) Go Rust If you need support for another programming languages, send us an email. Configuration By default, path /home/scone/workspace is mapped to a persistent volume. The files are not encrypted, e.g., the files are accessible admins of the host on which the container is executing. This chart uses the SCONE SGX Plugin to ensure that binaries generated with the help of vscode can run inside of enclaves. Note that vscode itself is executing in native mode. Note that git is included in the Visual Code image. We recommend the usage of git to ensure the durability of your code.","title":"Visual Studio Code"},{"location":"helm_vscode/#visual-studio-code-and-scone-crosscompiler","text":"Visual Studio Code is one of our favorite editors. Hence, we combine Visual Studio Code with our SCONE CrossCompiler. We also activate some useful programming language extensions. We also activate a code runner extension; one can compile and run a confidential application with a single click.","title":"Visual Studio Code and SCONE CrossCompiler"},{"location":"helm_vscode/#deploying-visual-studio-code","text":"Prerequisites A Kubernetes cluster Helm is deployed Kubernetes SGX plugin is insatlled We granted you access to the sconeapps repo and to the SCONE IDE images You can easily deploy Visual Studio Code with helm in a Kubernetes cluster. Since Visual Studio Code requires a password to log in, you need to set this password as a Kubernetes secret named ide-secret : kubectl create secret generic ide-secret --from-literal = PASSWORD = New-password Please replace New-password with a new, strong password. While we do not run Visual Code inside of SGX enclaves, we schedule it on a SGX-capable node (using the SGX Plugin ) to ensure that we can run the compiled code inside of SGX enclaves. You can deploy an instance by executing: helm install vscode sconeapps/vscode To display Visual Code Studio in your browser, you need to forward the service port to your local machine. Say, you want to present this on localhost:8081 , then you can forward the service port as follows: kubectl port-forward svc/vscode 8081 :8081 The Visual Studio instance can now be viewed at: http://localhost:8081/ Please use different Kubernetes namespaces (i.e., add -n MyNameSpace in the commands) in case you want to run multiple instances of Visual Code Studio.","title":"Deploying Visual Studio Code"},{"location":"helm_vscode/#usage","text":"This Visual Code Studio supports the following SCONE CrossCompiler languages: C C++ Fortran - note that Fortran is not supported by Code Runner) Go Rust If you need support for another programming languages, send us an email.","title":"Usage"},{"location":"helm_vscode/#configuration","text":"By default, path /home/scone/workspace is mapped to a persistent volume. The files are not encrypted, e.g., the files are accessible admins of the host on which the container is executing. This chart uses the SCONE SGX Plugin to ensure that binaries generated with the help of vscode can run inside of enclaves. Note that vscode itself is executing in native mode. Note that git is included in the Visual Code image. We recommend the usage of git to ensure the durability of your code.","title":"Configuration"},{"location":"hostexample/","text":"Host Execution We now show how one can compile a simple hello world program in a container and how to execute the program in the container in simulation mode and on the host in hardware mode . Installation In this example, we assume that you run on a host and have installed the Intel SGX driver and a docker engine . Driver installation is not strictly necessary: without the driver, the program will be automatically be executed in simulation mode on the host. Detailed Description We first need to start a container which includes the SCONE crosscompiler which is based on Ubuntu 1 : docker run -it -v \" $PWD \" :/src registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu We map the local directory of the host into the container (via option -v ) to be able to executed the generated binary on the host. Now execute the following command inside the container to create the hello world program: cd / src cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program with some debug output in the container: SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World The output shows that SCONE is running in simulation mode : \" export SCONE_MODE=sim \" Note Our patched docker engine automatically maps the sgx device inside of containers. In this case, the program would actually be executed in hardware mode. Execution on host Now, exit the container by executing: exit On the host, you can now execute the generated helloworld program 2 . First, we need to ensure that there exists a SCONE configuration file . We store this in the local directory: cat > sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF Now, we start the program with debug messages and using the configuration file in the local directory: SCONE_CONFIG = \" $PWD \" /sgx-musl.conf SCONE_VERSION = 1 ./helloworld In case your host has the Intel sgx driver installed, the output will show that it is executed in hardware mode on the host: ... export SCONE_MODE=hw ... Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World If you do not have the sgx driver installed, the program runs in simulation mode. The Ubuntu crosscompiler creates static binaries that can run on a Linux host. \u21a9 The host must run Linux. \u21a9","title":"Host Execution"},{"location":"hostexample/#host-execution","text":"We now show how one can compile a simple hello world program in a container and how to execute the program in the container in simulation mode and on the host in hardware mode .","title":"Host Execution"},{"location":"hostexample/#installation","text":"In this example, we assume that you run on a host and have installed the Intel SGX driver and a docker engine . Driver installation is not strictly necessary: without the driver, the program will be automatically be executed in simulation mode on the host.","title":"Installation"},{"location":"hostexample/#detailed-description","text":"We first need to start a container which includes the SCONE crosscompiler which is based on Ubuntu 1 : docker run -it -v \" $PWD \" :/src registry.scontain.com:5050/sconecuratedimages/crosscompilers:ubuntu We map the local directory of the host into the container (via option -v ) to be able to executed the generated binary on the host. Now execute the following command inside the container to create the hello world program: cd / src cat > helloworld . c << EOF #include <stdio.h> int main () { printf ( \"Hello World \\n \" ); } EOF Compile the program with: gcc -o helloworld helloworld.c You can run this program with some debug output in the container: SCONE_VERSION = 1 ./helloworld This will print something like: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_KERNEL=0 export SCONE_HEAP=67108864 export SCONE_STACK=81920 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_MODE=sim export SCONE_SGXBOUNDS=no export SCONE_VARYS=no export SCONE_ALLOW_DLOPEN=no export SCONE_MPROTECT=no Revision: 73cd5e415623f0947d635cad861d09bf364ce778 (Fri Jun 1 17:57:15 2018 +0200) Branch: master Configure options: --enable-shared --enable-debug --prefix=/mnt/ssd/franz/subtree-scone2/built/cross-compiler/x86_64-linux-musl Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World The output shows that SCONE is running in simulation mode : \" export SCONE_MODE=sim \" Note Our patched docker engine automatically maps the sgx device inside of containers. In this case, the program would actually be executed in hardware mode.","title":"Detailed Description"},{"location":"hostexample/#execution-on-host","text":"Now, exit the container by executing: exit On the host, you can now execute the generated helloworld program 2 . First, we need to ensure that there exists a SCONE configuration file . We store this in the local directory: cat > sgx-musl.conf << EOF Q 1 e -1 0 0 s -1 0 0 EOF Now, we start the program with debug messages and using the configuration file in the local directory: SCONE_CONFIG = \" $PWD \" /sgx-musl.conf SCONE_VERSION = 1 ./helloworld In case your host has the Intel sgx driver installed, the output will show that it is executed in hardware mode on the host: ... export SCONE_MODE=hw ... Enclave hash: 2805aa551a1019d86f33b6f14774a18792a5a3cc483d002782c9d851d851bf5a Hello World If you do not have the sgx driver installed, the program runs in simulation mode. The Ubuntu crosscompiler creates static binaries that can run on a Linux host. \u21a9 The host must run Linux. \u21a9","title":"Execution on host"},{"location":"installation/","text":"SCONE Installation We recommend to use Alpine Linux for container images using SCONE and and Ubuntu 16.04 LTS or Ubuntu 18.04 LTS for the hosts that run these container images. To ensure that your Ubuntu host has all software installed to run SCONE containers, you can just run: curl -fssl https://raw.githubusercontent.com/scontain/install_dependencies/master/install-host-prerequisites.sh | bash This script will check if the required components are already installed and installs only the components that have not yet been installed. Installation Options You can run SCONE-based application on baremetal servers as well as inside of VMs. Applications need access to a SGX driver and the applications need to be linked either dynamically during load time or during compile time with the SCONE Runtime Encryption Library. In this way, the application will be executed inside of an enclave: Containers are the default way to deploy SCONE-based applications. The application is linked with the SCONE Runtime Encryption Library (dynamically or statically). The Application needs to have access to the SGX driver: We recommend to deploy SCONE-based applications in Kubernetes clusters with the help of helm : Customized Installation You can customize the SCONE installation based on your needs. Depending on how you want to use SCONE, you could instead install software components on a per need basis: Simulation mode: if you want to run SCONE inside a container in simulation mode, you just need to install the docker engine on your host. Hardware mode: If you want to run your applications in hardware mode inside of containers, you need to install the docker engine and install the Intel SGX driver on your host. Running on a host: Running you applications on a host, you need to install the Intel SGX driver on the host. Running on a VM: Running you applications on a virtual machine, you need to install the Intel SGX driver on the VM and you have to ensure that your hypervisor supports SGX: You might want to install a patched version of KVM . Alternatively, you can use commercial hypervisor like HyperV . Ensure that your CPU runs the newest microcode by updating the CPU microcode .","title":"Installation overview"},{"location":"installation/#scone-installation","text":"We recommend to use Alpine Linux for container images using SCONE and and Ubuntu 16.04 LTS or Ubuntu 18.04 LTS for the hosts that run these container images. To ensure that your Ubuntu host has all software installed to run SCONE containers, you can just run: curl -fssl https://raw.githubusercontent.com/scontain/install_dependencies/master/install-host-prerequisites.sh | bash This script will check if the required components are already installed and installs only the components that have not yet been installed.","title":"SCONE Installation"},{"location":"installation/#installation-options","text":"You can run SCONE-based application on baremetal servers as well as inside of VMs. Applications need access to a SGX driver and the applications need to be linked either dynamically during load time or during compile time with the SCONE Runtime Encryption Library. In this way, the application will be executed inside of an enclave: Containers are the default way to deploy SCONE-based applications. The application is linked with the SCONE Runtime Encryption Library (dynamically or statically). The Application needs to have access to the SGX driver: We recommend to deploy SCONE-based applications in Kubernetes clusters with the help of helm :","title":"Installation Options"},{"location":"installation/#customized-installation","text":"You can customize the SCONE installation based on your needs. Depending on how you want to use SCONE, you could instead install software components on a per need basis: Simulation mode: if you want to run SCONE inside a container in simulation mode, you just need to install the docker engine on your host. Hardware mode: If you want to run your applications in hardware mode inside of containers, you need to install the docker engine and install the Intel SGX driver on your host. Running on a host: Running you applications on a host, you need to install the Intel SGX driver on the host. Running on a VM: Running you applications on a virtual machine, you need to install the Intel SGX driver on the VM and you have to ensure that your hypervisor supports SGX: You might want to install a patched version of KVM . Alternatively, you can use commercial hypervisor like HyperV . Ensure that your CPU runs the newest microcode by updating the CPU microcode .","title":"Customized Installation"},{"location":"k8s_concepts/","text":"Kubernetes and SconeApps In this section, we describe how we can build confidential applications with the help of SCONE . We can deploy these applications to a vanilla Kubernetes cluster. Therefore, we do not need to modify Kubernetes in any way. Instead, we maintain an SGX plugin to simplify the scheduling of confidential applications. We assume that you are already familiar with basic Kubernetes concepts. We first introduce some confidential computing concepts , then we show how to install confidential applications. We also introduce an in-depth hello world tutorial . Our recommended way to transform a service into a confidential service is to use a lift-and-shift (a.k.a. sconify ) approach. Additionally, we maintain a set of already sconified services which we call SconeApps . SCONE Confidential Computing Concepts When developing a CC application, we need to consider the following concepts: (native) service : in Kubernetes, a service is an abstract way of exposing an application as a network service at one or more network ports. Multiple processes may actually serve these network ports. In the context of confidential computing, we need to protect all of these processes. We use the term native service to refer to a service that is not protected by SCONE . confidential service : SCONE permits us to run each service's process inside of an SGX enclave . Hence, we refer to such services as confidential services . We refer to all other services native services . If such an enclave is in release mode , the CPU encrypts the enclave's memory. Therefore, this memory cannot be read by any other process, the operating system, the hypervisor, nor any user. We refer to this encryption as runtime encryption. container : a container is a mechanism to deploy services. In most cases, this container will be a Docker container. Note that users can enter a container and inspect all files within it. Further, a root user can inspect any process's memory - unless this process runs in an enclave in release mode . In this case, a root user sees, at most, encrypted pages. containerized CC App : a containerized CC App is an application consisting of confidential services. Users cannot inspect these confidential services. Further, we require that neither any other app nor the system can read the contents of the files of a CC App. Note that enclaves do not protect files. Also, we cannot depend on the operating system to encrypt files since an adversary might already be in control of the OS. (transparently) encrypted volume : an confidential service can get access to transparently encrypted volumes. To Kubernetes, these volumes look like standard volumes. To the CC App, these volumes look like plain text files. The CC App possesses this view as SCONE decrypts/encrypts files inside of the application's enclave. A user that logs into a container will only see encrypted files. pod : a pod is a Kubernetes concept. It is the minimal deployable unit consisting of one or more containers executed on the same node. A pod can consist of confidential as well as native services. CC pod : a CC pod is a SCONE concept. It is a pod that contains only confidential services. Further, all of its volumes are encrypted. Also, all communication between confidential services is encrypted. IP address : a pod has an IP address. The network communication can be inspected unless it is encrypted. We explain below how SCONE helps to provide confidential services with certificates such that we can establish secure channels between confidential services as well as containerized CC Apps. TLS : SCONE can enforce all confidential services of a containerized CC App to communicate with each other via TLS. SCONE can enforce this secure communication for both confidential services running in the same pod as well as confidential services across pods. server certs : SCONE can generate server certificates for services. These certificates are generated by an confidential service (SCONE CAS). Further, the server certificates can be injected into the filesystem of an confidential service. The server certificates are only visible to this confidential service receiving the injection, i.e., they do not exist outside its enclave. client certs : SCONE can also generate client certificates. SCONE CAS can provide a containerized CC app with a policy-controlled CA (as part of the SCONE CAS). Using the CA of the SCONE CAS, we can enforce access control to services. Only services with the appropriate client certificate are permitted to access a service. CA certs : SCONE can inject the CA certificates for the clients as well as the services. Using this injection, clients and services can perform a mutual attestation via TLS. The communication partners may only establish a TLS connection if they both run inside of enclave s, were properly initialized, have encrypted volumes that were not tampered with, and run an untampered binary.","title":"Concepts"},{"location":"k8s_concepts/#kubernetes-and-sconeapps","text":"In this section, we describe how we can build confidential applications with the help of SCONE . We can deploy these applications to a vanilla Kubernetes cluster. Therefore, we do not need to modify Kubernetes in any way. Instead, we maintain an SGX plugin to simplify the scheduling of confidential applications. We assume that you are already familiar with basic Kubernetes concepts. We first introduce some confidential computing concepts , then we show how to install confidential applications. We also introduce an in-depth hello world tutorial . Our recommended way to transform a service into a confidential service is to use a lift-and-shift (a.k.a. sconify ) approach. Additionally, we maintain a set of already sconified services which we call SconeApps .","title":"Kubernetes and SconeApps"},{"location":"k8s_concepts/#scone-confidential-computing-concepts","text":"When developing a CC application, we need to consider the following concepts: (native) service : in Kubernetes, a service is an abstract way of exposing an application as a network service at one or more network ports. Multiple processes may actually serve these network ports. In the context of confidential computing, we need to protect all of these processes. We use the term native service to refer to a service that is not protected by SCONE . confidential service : SCONE permits us to run each service's process inside of an SGX enclave . Hence, we refer to such services as confidential services . We refer to all other services native services . If such an enclave is in release mode , the CPU encrypts the enclave's memory. Therefore, this memory cannot be read by any other process, the operating system, the hypervisor, nor any user. We refer to this encryption as runtime encryption. container : a container is a mechanism to deploy services. In most cases, this container will be a Docker container. Note that users can enter a container and inspect all files within it. Further, a root user can inspect any process's memory - unless this process runs in an enclave in release mode . In this case, a root user sees, at most, encrypted pages. containerized CC App : a containerized CC App is an application consisting of confidential services. Users cannot inspect these confidential services. Further, we require that neither any other app nor the system can read the contents of the files of a CC App. Note that enclaves do not protect files. Also, we cannot depend on the operating system to encrypt files since an adversary might already be in control of the OS. (transparently) encrypted volume : an confidential service can get access to transparently encrypted volumes. To Kubernetes, these volumes look like standard volumes. To the CC App, these volumes look like plain text files. The CC App possesses this view as SCONE decrypts/encrypts files inside of the application's enclave. A user that logs into a container will only see encrypted files. pod : a pod is a Kubernetes concept. It is the minimal deployable unit consisting of one or more containers executed on the same node. A pod can consist of confidential as well as native services. CC pod : a CC pod is a SCONE concept. It is a pod that contains only confidential services. Further, all of its volumes are encrypted. Also, all communication between confidential services is encrypted. IP address : a pod has an IP address. The network communication can be inspected unless it is encrypted. We explain below how SCONE helps to provide confidential services with certificates such that we can establish secure channels between confidential services as well as containerized CC Apps. TLS : SCONE can enforce all confidential services of a containerized CC App to communicate with each other via TLS. SCONE can enforce this secure communication for both confidential services running in the same pod as well as confidential services across pods. server certs : SCONE can generate server certificates for services. These certificates are generated by an confidential service (SCONE CAS). Further, the server certificates can be injected into the filesystem of an confidential service. The server certificates are only visible to this confidential service receiving the injection, i.e., they do not exist outside its enclave. client certs : SCONE can also generate client certificates. SCONE CAS can provide a containerized CC app with a policy-controlled CA (as part of the SCONE CAS). Using the CA of the SCONE CAS, we can enforce access control to services. Only services with the appropriate client certificate are permitted to access a service. CA certs : SCONE can inject the CA certificates for the clients as well as the services. Using this injection, clients and services can perform a mutual attestation via TLS. The communication partners may only establish a TLS connection if they both run inside of enclave s, were properly initialized, have encrypted volumes that were not tampered with, and run an untampered binary.","title":"SCONE Confidential Computing Concepts"},{"location":"k8s_dashboard/","text":"Kubernetes Dashboard You can use the default Kubernetes dashboard to monitor your confidential applications. For an application-orient view, you can use Kubeapps . For a performance-oriented view, you can use TEEMon . Prerequisites A Kubernetes cluster kubectl is properly set up Installation Install the dashboard with kubectl : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml Start the kubectl proxy: kubectl proxy The dashboard can now be viewed at: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ Application Monitoring In the Kubernetes dashboard you can monitor various workload characteristics and identify issues in the workload. Like in this case, some services did not start: By clicking on the failed services, you can see that in this case the issue was a credential issue.","title":"Kubernetes Dashboard"},{"location":"k8s_dashboard/#kubernetes-dashboard","text":"You can use the default Kubernetes dashboard to monitor your confidential applications. For an application-orient view, you can use Kubeapps . For a performance-oriented view, you can use TEEMon . Prerequisites A Kubernetes cluster kubectl is properly set up","title":"Kubernetes Dashboard"},{"location":"k8s_dashboard/#installation","text":"Install the dashboard with kubectl : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml Start the kubectl proxy: kubectl proxy The dashboard can now be viewed at: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/","title":"Installation"},{"location":"k8s_dashboard/#application-monitoring","text":"In the Kubernetes dashboard you can monitor various workload characteristics and identify issues in the workload. Like in this case, some services did not start: By clicking on the failed services, you can see that in this case the issue was a credential issue.","title":"Application Monitoring"},{"location":"kubeapps/","text":"Kubeapps Kubeapps is a dashboard for helm . You can use Kubeapps to deploy and manage your confidential applications. While we expect that most confidential applications will be deployed via the helm CLI , using a dashboard is a convenient way to inspect the running confidential as well as native applications. Moreover, the dashboard informs you when updates of the applications are available to be installed. Prerequisites A Kubernetes cluster Helm setup was performed Deploy and Manage We provide a catalog with sconeapps , i.e., curated, confidential applications that can be installed with the help of helm or via point and click using Kubeapps. Kubeapps provides you with a view of available sconeapps , like this: You can select an application that you want to start and deploy it as described below. Deploy a sconeapp When deploying an application, you can customize its configuration values. For example, for LAS, SCONE's local attestation service, you will be able to configure the parameters of the Helm chart that is used to install this application: Inspecting Applications Kubeapps is a dashboard that can show you all running applications. You can select an application for inspection. Inspecting LAS A view of the LAS application that we started above, will look as follows: Software Updates Kubeapps shows that software updates are available in the dashboard view as well as for the individual applications: center> Note that before you press Upgrade , you need to ensure that your policy permits this upgrade. This requires that your policy is set up to permit software updates. If software updates are not permitted, the services will not be permitted to get access to its secrets. Hence, one would need to downgrade the service again. Deploying Kubeapps sconeapps is a private Helm repository. Hence, we need to grant you access and you need a GitHub token to access the sconeapps repo. Define an environment variable that contains this token: export GH_TOKEN = ... Use the token to give Kubeapps access to the sconeapps repository: if [ -z \" $GH_TOKEN \" ] ; then echo \"You need to set you github token: https://github.com/settings/tokens/new\" else cat > kubeapps_values.yml <<EOF apprepository: initialRepos: - name: sconeapps url: https://${GH_TOKEN}@raw.githubusercontent.com/scontain/sconeapps/master/ - name: bitnami url: https://charts.bitnami.com/bitnami EOF fi You can now start Kubeapps with the help of helm as follows: kubectl create namespace kubeapps || echo \"Does namespace 'kubeapps' already exists?\" helm install -f kubeapps_values.yml kubeapps --namespace kubeapps bitnami/kubeapps --set useHelm3 = true Port Forwarding To display the dashboard on your browser, you need to forward the kubeapp port to your local machine. Say, you want to present this on localhost:8080 , then you can forward the kubeapp port as follows: kubectl port-forward -n kubeapps services/kubeapps 8080 :80 The Kubeapps dashboard can now be viewed at: http://localhost:8080/ Access Control For production mode, you should define a role-based access control. For testing, you might want to create a simple service account: kubectl create serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole = cluster-admin --serviceaccount = default:kubeapps-operator To log into the Kubeapps dashboard, you need to determine the API Token: APITOKEN = $( kubectl get -n default secret $( kubectl get -n default serviceaccount kubeapps-operator -o jsonpath = '{.secrets[].name}' ) -o go-template = '{{.data.token | base64decode}}' && echo ) echo $APITOKEN","title":"Kubeapps"},{"location":"kubeapps/#kubeapps","text":"Kubeapps is a dashboard for helm . You can use Kubeapps to deploy and manage your confidential applications. While we expect that most confidential applications will be deployed via the helm CLI , using a dashboard is a convenient way to inspect the running confidential as well as native applications. Moreover, the dashboard informs you when updates of the applications are available to be installed. Prerequisites A Kubernetes cluster Helm setup was performed","title":"Kubeapps"},{"location":"kubeapps/#deploy-and-manage","text":"We provide a catalog with sconeapps , i.e., curated, confidential applications that can be installed with the help of helm or via point and click using Kubeapps. Kubeapps provides you with a view of available sconeapps , like this: You can select an application that you want to start and deploy it as described below.","title":"Deploy and Manage"},{"location":"kubeapps/#deploy-a-sconeapp","text":"When deploying an application, you can customize its configuration values. For example, for LAS, SCONE's local attestation service, you will be able to configure the parameters of the Helm chart that is used to install this application:","title":"Deploy a sconeapp"},{"location":"kubeapps/#inspecting-applications","text":"Kubeapps is a dashboard that can show you all running applications. You can select an application for inspection.","title":"Inspecting Applications"},{"location":"kubeapps/#inspecting-las","text":"A view of the LAS application that we started above, will look as follows:","title":"Inspecting LAS"},{"location":"kubeapps/#software-updates","text":"Kubeapps shows that software updates are available in the dashboard view as well as for the individual applications: center> Note that before you press Upgrade , you need to ensure that your policy permits this upgrade. This requires that your policy is set up to permit software updates. If software updates are not permitted, the services will not be permitted to get access to its secrets. Hence, one would need to downgrade the service again.","title":"Software Updates"},{"location":"kubeapps/#deploying-kubeapps","text":"sconeapps is a private Helm repository. Hence, we need to grant you access and you need a GitHub token to access the sconeapps repo. Define an environment variable that contains this token: export GH_TOKEN = ... Use the token to give Kubeapps access to the sconeapps repository: if [ -z \" $GH_TOKEN \" ] ; then echo \"You need to set you github token: https://github.com/settings/tokens/new\" else cat > kubeapps_values.yml <<EOF apprepository: initialRepos: - name: sconeapps url: https://${GH_TOKEN}@raw.githubusercontent.com/scontain/sconeapps/master/ - name: bitnami url: https://charts.bitnami.com/bitnami EOF fi You can now start Kubeapps with the help of helm as follows: kubectl create namespace kubeapps || echo \"Does namespace 'kubeapps' already exists?\" helm install -f kubeapps_values.yml kubeapps --namespace kubeapps bitnami/kubeapps --set useHelm3 = true","title":"Deploying Kubeapps"},{"location":"kubeapps/#port-forwarding","text":"To display the dashboard on your browser, you need to forward the kubeapp port to your local machine. Say, you want to present this on localhost:8080 , then you can forward the kubeapp port as follows: kubectl port-forward -n kubeapps services/kubeapps 8080 :80 The Kubeapps dashboard can now be viewed at: http://localhost:8080/","title":"Port Forwarding"},{"location":"kubeapps/#access-control","text":"For production mode, you should define a role-based access control. For testing, you might want to create a simple service account: kubectl create serviceaccount kubeapps-operator kubectl create clusterrolebinding kubeapps-operator --clusterrole = cluster-admin --serviceaccount = default:kubeapps-operator To log into the Kubeapps dashboard, you need to determine the API Token: APITOKEN = $( kubectl get -n default secret $( kubectl get -n default serviceaccount kubeapps-operator -o jsonpath = '{.secrets[].name}' ) -o go-template = '{{.data.token | base64decode}}' && echo ) echo $APITOKEN","title":"Access Control"},{"location":"memory_dump/","text":"Finding Secrets... One of the main advantages of SGX technology is that it cannot only protect against external attackers but also against internal attackers that have root access. We introduce a simple approach to demonstrate that your application is indeed running inside of an enclave and that its secrets are not accessible. Note To ensure that your SCONE application runs inside of an enclave and to distribute secrets, please to use the SCONE configuration and attestation service . To demonstrate how one can check that SGX/SCONE can protect against users with root access 1 , let's consider a simplistic example : we store some secret in a variable secret 2 . By default, binaries are not encrypted. Hence, do not store any secrets in a binary as it is done in this simplistic example. If you really need to to protect your binaries - for example, since you have some legacy code with embedded secrets - SCONE can protect these secrets by permitting to encrypt shared libraries. Example We use the SCONE crosscompiler image and start an container with access to the SGX device.We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers Let's create some program that stores a secret ( MYBIGS ) in a local variable secret : cat > mysecret.c << EOF #include <stdio.h> #include <unistd.h> const char *code_is_not_encrypted=\"THIS_IS_NOT_SECRET\"; int main() { char secret[7]; secret[0] ='M'; secret[1] ='Y'; secret[2] ='B'; secret[3] ='I'; secret[4] ='G'; secret[5] ='S'; secret[6] =0; printf(\"'%s' SECRET at %lx\\n\", secret, secret); printf(\"Kill with Ctrl-C.\\n\"); for(;;) sleep(1); // loop forever } EOF Compile this program with the SCONE crosscompiler (i.e., gcc): gcc -g -o mysecret mysecret.c Simulation Mode You can run this program in SIMULATION MODE , i.e., this program does not protect your secrets: SCONE_VERSION = 1 SCONE_MODE = SIM SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Log into a different terminal on your host . Let us figure out the process ID of the mysecret program: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) Now, we can dump the memory of this process via the /proc filesystem. You can determine the different memory regions of your process via cat /proc/$SPID/maps and the memory is stored in /proc/$SPID/mem . We can use the following Python program to write all pages to stdout : cat > dumpstack.py << EOF import sys, os, string, re pid = sys.argv[1] maps_file = open(\"/proc/%s/maps\" % pid, 'r') mem_file = open(\"/proc/%s/mem\" % pid, 'r') r=0 for line in maps_file.readlines(): # for each mapped region w=line.rsplit(None, 1)[-1] # last word if w != \"[vvar]\" and w != \"[vdso]\" and w != \"[vsyscall]\" and w != \"/dev/isgx\" and w != \"/dev/sgx\": m = re.match(r'([0-9A-Fa-f]+)-([0-9A-Fa-f]+) ([-r])', line) r += 1 p = 0 if m.group(3) == 'r': # if this is a readable region start = int(m.group(1), 16) end = int(m.group(2), 16) while start < end: try: mem_file.seek(start) # seek to region start chunk = mem_file.read(4096) # read region contents sys.stdout.write(chunk) p += 1 if p > 1000: sys.stderr.write(\"region = %02d, index=%x \\r\" % (r,start)) p = 0 start += 4096 except: pass sys.stderr.write(\"\\n\") EOF Enclaves in debug mode do not protect your secrets If your enclave is in debug mode, one can access all secrets of the enclave. This scanner conveniently skips the enclave memory region using condition and w != \"/dev/isgx\" and ... . Drop this condition and you should also find the secret. Run your program in production mode to ensure that one cannot access the secrets. Run this program and grep for the prefix of our secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will take some time but eventually it will print the full secret MYBIGS . Hardware Mode Let us now run the program in hardware mode. First, ensure that you kill the original program by typing control-C . Let's start mysecret in an enclave, i.e., in hardware mode inside the container: SCONE_VERSION = 1 SCONE_MODE = HW SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Update environment variable SPID in a second terminal on your host: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) and then try to find the secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will run for much less time and in particular, it will not print any secrets. Note, however, that secrets stored in the binaries can be found because the binary is not encrypted: a copy of the original binary - which is used to start the enclave - stays in main memory outside the enclave. Let's look for the string THIS_IS_NOT_SECRET in our example application. We can find this secret as follows: sudo python dumpstack.py $SPID | strings -n 5 | grep THIS_IS_NOT_SECRET Note that the enclave runs in this example runs in debug mode, i.e., one can still attach to this enclave with scone-gdb . To prevent access via the debugger, you need to run your enclave in production mode . \u21a9 Note that an adversary could analyse the binary and figure out the secret. The standard way to provide an enclave with secrets is to use SCONE CAS . \u21a9","title":"Finding Secrets"},{"location":"memory_dump/#finding-secrets","text":"One of the main advantages of SGX technology is that it cannot only protect against external attackers but also against internal attackers that have root access. We introduce a simple approach to demonstrate that your application is indeed running inside of an enclave and that its secrets are not accessible. Note To ensure that your SCONE application runs inside of an enclave and to distribute secrets, please to use the SCONE configuration and attestation service . To demonstrate how one can check that SGX/SCONE can protect against users with root access 1 , let's consider a simplistic example : we store some secret in a variable secret 2 . By default, binaries are not encrypted. Hence, do not store any secrets in a binary as it is done in this simplistic example. If you really need to to protect your binaries - for example, since you have some legacy code with embedded secrets - SCONE can protect these secrets by permitting to encrypt shared libraries.","title":"Finding Secrets..."},{"location":"memory_dump/#example","text":"We use the SCONE crosscompiler image and start an container with access to the SGX device.We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/crosscompilers Let's create some program that stores a secret ( MYBIGS ) in a local variable secret : cat > mysecret.c << EOF #include <stdio.h> #include <unistd.h> const char *code_is_not_encrypted=\"THIS_IS_NOT_SECRET\"; int main() { char secret[7]; secret[0] ='M'; secret[1] ='Y'; secret[2] ='B'; secret[3] ='I'; secret[4] ='G'; secret[5] ='S'; secret[6] =0; printf(\"'%s' SECRET at %lx\\n\", secret, secret); printf(\"Kill with Ctrl-C.\\n\"); for(;;) sleep(1); // loop forever } EOF Compile this program with the SCONE crosscompiler (i.e., gcc): gcc -g -o mysecret mysecret.c","title":"Example"},{"location":"memory_dump/#simulation-mode","text":"You can run this program in SIMULATION MODE , i.e., this program does not protect your secrets: SCONE_VERSION = 1 SCONE_MODE = SIM SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Log into a different terminal on your host . Let us figure out the process ID of the mysecret program: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) Now, we can dump the memory of this process via the /proc filesystem. You can determine the different memory regions of your process via cat /proc/$SPID/maps and the memory is stored in /proc/$SPID/mem . We can use the following Python program to write all pages to stdout : cat > dumpstack.py << EOF import sys, os, string, re pid = sys.argv[1] maps_file = open(\"/proc/%s/maps\" % pid, 'r') mem_file = open(\"/proc/%s/mem\" % pid, 'r') r=0 for line in maps_file.readlines(): # for each mapped region w=line.rsplit(None, 1)[-1] # last word if w != \"[vvar]\" and w != \"[vdso]\" and w != \"[vsyscall]\" and w != \"/dev/isgx\" and w != \"/dev/sgx\": m = re.match(r'([0-9A-Fa-f]+)-([0-9A-Fa-f]+) ([-r])', line) r += 1 p = 0 if m.group(3) == 'r': # if this is a readable region start = int(m.group(1), 16) end = int(m.group(2), 16) while start < end: try: mem_file.seek(start) # seek to region start chunk = mem_file.read(4096) # read region contents sys.stdout.write(chunk) p += 1 if p > 1000: sys.stderr.write(\"region = %02d, index=%x \\r\" % (r,start)) p = 0 start += 4096 except: pass sys.stderr.write(\"\\n\") EOF Enclaves in debug mode do not protect your secrets If your enclave is in debug mode, one can access all secrets of the enclave. This scanner conveniently skips the enclave memory region using condition and w != \"/dev/isgx\" and ... . Drop this condition and you should also find the secret. Run your program in production mode to ensure that one cannot access the secrets. Run this program and grep for the prefix of our secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will take some time but eventually it will print the full secret MYBIGS .","title":"Simulation Mode"},{"location":"memory_dump/#hardware-mode","text":"Let us now run the program in hardware mode. First, ensure that you kill the original program by typing control-C . Let's start mysecret in an enclave, i.e., in hardware mode inside the container: SCONE_VERSION = 1 SCONE_MODE = HW SCONE_HEAP = 128K SCONE_STACK = 1K ./mysecret Update environment variable SPID in a second terminal on your host: SPID = $( ps -a | grep -v grep | grep mysecret | awk '{print $1}' ) and then try to find the secret: sudo python dumpstack.py $SPID | strings -n 5 | grep MYBI This will run for much less time and in particular, it will not print any secrets. Note, however, that secrets stored in the binaries can be found because the binary is not encrypted: a copy of the original binary - which is used to start the enclave - stays in main memory outside the enclave. Let's look for the string THIS_IS_NOT_SECRET in our example application. We can find this secret as follows: sudo python dumpstack.py $SPID | strings -n 5 | grep THIS_IS_NOT_SECRET Note that the enclave runs in this example runs in debug mode, i.e., one can still attach to this enclave with scone-gdb . To prevent access via the debugger, you need to run your enclave in production mode . \u21a9 Note that an adversary could analyse the binary and figure out the secret. The standard way to provide an enclave with secrets is to use SCONE CAS . \u21a9","title":"Hardware Mode"},{"location":"microcode/","text":"Microcode Updates As any modern technology, Intel SGX has been affected by security vulnerabilities. Intel addresses these vulnerabilities by updating the microcode of CPUs, changing the hardware of new CPUs and updating the system software. While some of these vulnerabilities are really difficult to exploit and to reproduce, all of them require a swift update of the microcode of the CPU as well as as other system software. Outdated microcode and system software, will result in the attestation of confidential application to fail. The SCONE platform will take care of updating the parts of the system software that affect the attestation. BIOS Upgrade Required Note that dynamic microcode updates offered by the operating system are insufficient: Each microcode update that patches a SGX vulnerability requires a BIOS update. During remote attestation, it is checked that the microcode of the CPU which is deployed by the BIOS is up-to-date. If the BIOS is out-of-date, the microcode is also out-of-date and the attestation will fail. Platform ID Whenever the microcode is updated, the SCONE platform ID will change. This means that whenever there is a microcode update, you will need to update the permitted platform IDs in your security policies. Note that enabling certain CPU features like hyperthreading will also result in a different platform ID. This change of the ID is a security feature since we, for example, strongly recommend to switch off hyperthreading in production. By limiting the execution of your confidential application to platform IDs of, say, a Kubernetes cluster with hyperthreading being switched off, you will prevent your application to run on a platform for which the adversary has switch on hyperthreading.","title":"Updating CPU microcode"},{"location":"microcode/#microcode-updates","text":"As any modern technology, Intel SGX has been affected by security vulnerabilities. Intel addresses these vulnerabilities by updating the microcode of CPUs, changing the hardware of new CPUs and updating the system software. While some of these vulnerabilities are really difficult to exploit and to reproduce, all of them require a swift update of the microcode of the CPU as well as as other system software. Outdated microcode and system software, will result in the attestation of confidential application to fail. The SCONE platform will take care of updating the parts of the system software that affect the attestation.","title":"Microcode Updates"},{"location":"microcode/#bios-upgrade-required","text":"Note that dynamic microcode updates offered by the operating system are insufficient: Each microcode update that patches a SGX vulnerability requires a BIOS update. During remote attestation, it is checked that the microcode of the CPU which is deployed by the BIOS is up-to-date. If the BIOS is out-of-date, the microcode is also out-of-date and the attestation will fail.","title":"BIOS Upgrade Required"},{"location":"microcode/#platform-id","text":"Whenever the microcode is updated, the SCONE platform ID will change. This means that whenever there is a microcode update, you will need to update the permitted platform IDs in your security policies. Note that enabling certain CPU features like hyperthreading will also result in a different platform ID. This change of the ID is a security feature since we, for example, strongly recommend to switch off hyperthreading in production. By limiting the execution of your confidential application to platform IDs of, say, a Kubernetes cluster with hyperthreading being switched off, you will prevent your application to run on a platform for which the adversary has switch on hyperthreading.","title":"Platform ID"},{"location":"mitigation/","text":"Mitigating SGX Vulnerabilities SGX Vulnerabilities As in any complex technology, SGX has had its share of security vulnerabilities. An active research community is looking for vulnerabilities in SGX, and Intel has been fixing these issues. These security vulnerabilities have been mitigated with microcode updates as well as CPU design changes. Hence, we now have a much more hardened SGX. In particular, the new Icelake server CPUs are hardened against most of the vulnerabilities. For any remaining vulnerabilities, we can help to mitigate these with the help of SCONE. Mitigation Mechanisms The most important mechanism to ensure that your application is protected against known security vulnerabilities is the attestation . During application startup, the SCONE runtime and SCONE CAS attests transparently the platform on which your application is executing. Any known security vulnerabilities are detected, reported, and by default, the attestation fails with an error. The following actions might be needed to mitigate security vulnerabilities: firmware updates : most vulnerabilities can be addressed with firmware updates. In case you run your application in a cloud, your cloud provider will take care of these firmware updates. hardware updates : some of the vulnerabilities require a newer CPU. In a cloud setting, a cloud provider will provide you access to newer CPUs. system software updates : some of the attacks require an update of the system software like the SGX drivers and the attestation infrastructure. If you are using a managed Kubernetes, your provider will ensure that the system software is up to date. SCONE LAS updates : some of the vulnerabilities require us to update SCONE LAS (Local Attestation Service). We recommend to install SCONE LAS with the help of helm to keep LAS up-to-date. Recompilation : some vulnerabilities might require recompilations of the applications. SCONE supports the recompilation with the help of its cross-compiler. Since mitigation might need some time, we provide you also with the possibility to accept that your platform has a known vulnerability . We recommend to use this mechanism for short periods only and to update your software/firmware/hardware as quickly as possible. Defense In-Depth To ensure that applications are protected even if there might be some not yet known vulnerabilities in your platform or application, we suggest and support a defense-in-depth approach. Please contact us by email to talk about this option. Firmware Updates Most attack mitigations require an update of the firmware . A firmware update typically updates the microcode of the CPU. Note that updating the microcode of the CPU via the operating system - while being way more convenient - is not sufficient to fix vulnerabilities. Note that the platform ID will change after a security update. Hardware Updates Modern CPUs like the Icelake Server CPUs address most of the known vulnerabilities.","title":"Attack mitigation"},{"location":"mitigation/#mitigating-sgx-vulnerabilities","text":"","title":"Mitigating SGX Vulnerabilities"},{"location":"mitigation/#sgx-vulnerabilities","text":"As in any complex technology, SGX has had its share of security vulnerabilities. An active research community is looking for vulnerabilities in SGX, and Intel has been fixing these issues. These security vulnerabilities have been mitigated with microcode updates as well as CPU design changes. Hence, we now have a much more hardened SGX. In particular, the new Icelake server CPUs are hardened against most of the vulnerabilities. For any remaining vulnerabilities, we can help to mitigate these with the help of SCONE.","title":"SGX Vulnerabilities"},{"location":"mitigation/#mitigation-mechanisms","text":"The most important mechanism to ensure that your application is protected against known security vulnerabilities is the attestation . During application startup, the SCONE runtime and SCONE CAS attests transparently the platform on which your application is executing. Any known security vulnerabilities are detected, reported, and by default, the attestation fails with an error. The following actions might be needed to mitigate security vulnerabilities: firmware updates : most vulnerabilities can be addressed with firmware updates. In case you run your application in a cloud, your cloud provider will take care of these firmware updates. hardware updates : some of the vulnerabilities require a newer CPU. In a cloud setting, a cloud provider will provide you access to newer CPUs. system software updates : some of the attacks require an update of the system software like the SGX drivers and the attestation infrastructure. If you are using a managed Kubernetes, your provider will ensure that the system software is up to date. SCONE LAS updates : some of the vulnerabilities require us to update SCONE LAS (Local Attestation Service). We recommend to install SCONE LAS with the help of helm to keep LAS up-to-date. Recompilation : some vulnerabilities might require recompilations of the applications. SCONE supports the recompilation with the help of its cross-compiler. Since mitigation might need some time, we provide you also with the possibility to accept that your platform has a known vulnerability . We recommend to use this mechanism for short periods only and to update your software/firmware/hardware as quickly as possible.","title":"Mitigation Mechanisms"},{"location":"mitigation/#defense-in-depth","text":"To ensure that applications are protected even if there might be some not yet known vulnerabilities in your platform or application, we suggest and support a defense-in-depth approach. Please contact us by email to talk about this option.","title":"Defense In-Depth"},{"location":"mitigation/#firmware-updates","text":"Most attack mitigations require an update of the firmware . A firmware update typically updates the microcode of the CPU. Note that updating the microcode of the CPU via the operating system - while being way more convenient - is not sufficient to fix vulnerabilities. Note that the platform ID will change after a security update.","title":"Firmware Updates"},{"location":"mitigation/#hardware-updates","text":"Modern CPUs like the Icelake Server CPUs address most of the known vulnerabilities.","title":"Hardware Updates"},{"location":"mono/","text":"Mono and C# Support SCONE supports the mono platform , i.e., the cross platform, open source .NET framework. Hence, SCONE now also supports C#. Image Ensure that you have the newest mono image and determine you SGX device : docker pull registry.scontain.com:5050/sconecuratedimages/mono determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/mono In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode . Example Let's try to compile a simple program: cat > hello.cs <<EOF using System; public class HelloWorld { public static void Main(string[] args) { Console.WriteLine (\"Hello Mono World\"); } } EOF We compile the program with csc inside the container: csc hello.cs Note that his compilation already happens inside of SGX enclaves. Let's execute the binary and switch on debug outputs: mono hello.exe The output will look like: Hello Mono World Debug Output You can show the configuration of SCONE by setting SCONE_VERSION=1 , i.e., if you execute SCONE_VERSION = 1 mono hello.exe the output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_LOG=3 export SCONE_HEAP=1073741824 export SCONE_STACK=2097152 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=sim export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no musl version: 1.1.24 Revision: 6da648645ce29121d24c40c9c18404cc43194f80 (Fri May 29 20:56:17 2020 +0000) Branch: master Enclave hash: fa29f3914117757bbd4e091f90c357ae92b4c4b6b9bfa80b21c32e66fe60ee26 Hello Mono World","title":"Mono"},{"location":"mono/#mono-and-c-support","text":"SCONE supports the mono platform , i.e., the cross platform, open source .NET framework. Hence, SCONE now also supports C#.","title":"Mono and C#  Support"},{"location":"mono/#image","text":"Ensure that you have the newest mono image and determine you SGX device : docker pull registry.scontain.com:5050/sconecuratedimages/mono determine_sgx_device docker run $MOUNT_SGXDEVICE -it registry.scontain.com:5050/sconecuratedimages/mono In case you do not have an SGX driver installed, no device will be mounted and applications will run in SIM mode .","title":"Image"},{"location":"mono/#example","text":"Let's try to compile a simple program: cat > hello.cs <<EOF using System; public class HelloWorld { public static void Main(string[] args) { Console.WriteLine (\"Hello Mono World\"); } } EOF We compile the program with csc inside the container: csc hello.cs Note that his compilation already happens inside of SGX enclaves. Let's execute the binary and switch on debug outputs: mono hello.exe The output will look like: Hello Mono World","title":"Example"},{"location":"mono/#debug-output","text":"You can show the configuration of SCONE by setting SCONE_VERSION=1 , i.e., if you execute SCONE_VERSION = 1 mono hello.exe the output will look like this: export SCONE_QUEUES=4 export SCONE_SLOTS=256 export SCONE_SIGPIPE=0 export SCONE_MMAP32BIT=0 export SCONE_SSPINS=100 export SCONE_SSLEEP=4000 export SCONE_LOG=3 export SCONE_HEAP=1073741824 export SCONE_STACK=2097152 export SCONE_CONFIG=/etc/sgx-musl.conf export SCONE_ESPINS=10000 export SCONE_MODE=sim export SCONE_ALLOW_DLOPEN=yes (unprotected) export SCONE_MPROTECT=no musl version: 1.1.24 Revision: 6da648645ce29121d24c40c9c18404cc43194f80 (Fri May 29 20:56:17 2020 +0000) Branch: master Enclave hash: fa29f3914117757bbd4e091f90c357ae92b4c4b6b9bfa80b21c32e66fe60ee26 Hello Mono World","title":"Debug Output"},{"location":"multistagebuild/","text":"Multi-Stage Build As we mentioned in the context of the dockerfile example , that you should not include the SCONE platform in the images you build - at least if you intent to push you images to public repositories. The easiest way to achieve this, is to use multi-stage builds. The idea is to build you application with the scone cross-compiler image (i.e., registry.scontain.com:5050/sconecuratedimages/crosscompilers ) image and then copy the application to another container with a different base image. You must ensure that you copy all parts of your application are included. If you use static linking , this can be easier than using dynamic linking. We show how to generate a Docker image of a dynamically linked application: we show this for groupcache . Getting access You need access to a private docker hub repository registry.scontain.com:5050/sconecuratedimages/crosscompilers to execute this example. Just register a free account on gitlab.scontain.com . We do want to make sure that the image is as small as possible and in particular, that the image must not contain the SCONE crosscompilers. Hence, we use a multi-stage build during which we copy all dependencies of groupcache : cat > Dockerfile << EOF FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers RUN apk update \\ && apk add git curl go \\ && go get -compiler gccgo -u github.com/golang/groupcache \\ && curl -fsSL --output groupcache.go https://gist.githubusercontent.com/fiorix/816117cfc7573319b72d/raw/797d2ed5b567dcffb8ebd8896a3d7671b1a44b31/groupcache.go \\ && export SCONE_HEAP=1G \\ && go build -compiler gccgo -buildmode=exe groupcache.go FROM alpine:latest COPY --from=0 /groupcache / COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 COPY --from=0 /opt/scone/lib/ld-scone-x86_64.so.1 /opt/scone/lib/ld-scone-x86_64.so.1 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 COPY --from=0 /etc/sgx-musl.conf /etc/sgx-musl.conf CMD sh -c \"SCONE_HEAP=1G /groupcache\" EOF Note that one can figure out the libraries to copy with command ldd groupcache . Let's generate an image groupcache with this Dockerfile: docker build --pull -t groupcache . The size of the groupcache image is about 65MB. You can run this container by executing: docker run --rm --publish 8080 :8080 groupcache You can now query this service from a different terminal on the host this service, e.g.,: curl localhost:8080/color?name = green Warning This service has multiple security issues : we show how to address these with the help of the SCONE shields in a later section. Screencast","title":"Multi-stage build"},{"location":"multistagebuild/#multi-stage-build","text":"As we mentioned in the context of the dockerfile example , that you should not include the SCONE platform in the images you build - at least if you intent to push you images to public repositories. The easiest way to achieve this, is to use multi-stage builds. The idea is to build you application with the scone cross-compiler image (i.e., registry.scontain.com:5050/sconecuratedimages/crosscompilers ) image and then copy the application to another container with a different base image. You must ensure that you copy all parts of your application are included. If you use static linking , this can be easier than using dynamic linking. We show how to generate a Docker image of a dynamically linked application: we show this for groupcache . Getting access You need access to a private docker hub repository registry.scontain.com:5050/sconecuratedimages/crosscompilers to execute this example. Just register a free account on gitlab.scontain.com . We do want to make sure that the image is as small as possible and in particular, that the image must not contain the SCONE crosscompilers. Hence, we use a multi-stage build during which we copy all dependencies of groupcache : cat > Dockerfile << EOF FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers RUN apk update \\ && apk add git curl go \\ && go get -compiler gccgo -u github.com/golang/groupcache \\ && curl -fsSL --output groupcache.go https://gist.githubusercontent.com/fiorix/816117cfc7573319b72d/raw/797d2ed5b567dcffb8ebd8896a3d7671b1a44b31/groupcache.go \\ && export SCONE_HEAP=1G \\ && go build -compiler gccgo -buildmode=exe groupcache.go FROM alpine:latest COPY --from=0 /groupcache / COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgo.so.13 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libgcc_s.so.1 COPY --from=0 /opt/scone/lib/ld-scone-x86_64.so.1 /opt/scone/lib/ld-scone-x86_64.so.1 COPY --from=0 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 /opt/scone/cross-compiler/x86_64-linux-musl/lib/libc.scone-x86_64.so.1 COPY --from=0 /etc/sgx-musl.conf /etc/sgx-musl.conf CMD sh -c \"SCONE_HEAP=1G /groupcache\" EOF Note that one can figure out the libraries to copy with command ldd groupcache . Let's generate an image groupcache with this Dockerfile: docker build --pull -t groupcache . The size of the groupcache image is about 65MB. You can run this container by executing: docker run --rm --publish 8080 :8080 groupcache You can now query this service from a different terminal on the host this service, e.g.,: curl localhost:8080/color?name = green Warning This service has multiple security issues : we show how to address these with the help of the SCONE shields in a later section.","title":"Multi-Stage Build"},{"location":"multistagebuild/#screencast","text":"","title":"Screencast"},{"location":"namespace/","text":"CAS Namespaces Multiple users can share a CAS instance. Since policy names must be unique per CAS instance, there might be conflicts when creating sessions: another user might have already created a session with that name. The standard solution to address such name collisions is to introduce namespaces. Sessions can be organized hierarchically by creating them within the namespace of another session. Organizations and large teams can benefit from namespaces to structure their sessions and restrict the creation and management of nested sessions to a set of authorized users. If one uses SCONE in the context of Kubernetes, one might want to create a corresponding CAS namespace for each Kubernetes namespace in which one executes confidential services. Please note that using CAS namespaces requires SCONE Session Language v0.3 or higher. Namespace Access Policy SCONE CAS supports namespaces , i.e., a scope for session names, since version 5.1. To create a new namespace, one creates a session with the name of the namespace. By creating a namespace, one automatically controls who can create sessions in this namespace. To control access to namespaces, we added a new access policy option: name : my-namespace version : \"0.3\" access_policy : create_sessions : - CREATOR - 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 - ... To create a new sessions in the namespace my-namespace , one has to provide one of the credentials provided of list create_sessions . If omitted, all entities listed under update will be able to create sessions. Creating a namespace To create a new namespace my-namespace , we first create a session description. The session description might be stored in a file policy.yml and look like this name : my-namespace version : \"0.3\" access_policy : read : - CREATOR update : - CREATOR create_sessions : - CREATOR We can upload this session with the SCONE CLI as follows: scone session create policy.yml In order to create session hello-world in the namespace my-namespace , set the new session's name to: name : my-namespace/hello-world version : \"0.3\" You must have permission to create the session in this namespace. Namespaces can be nested. You can use hello-world itself as a namespace. name : my-namespace/hello-world/project-1 version : \"0.3\"","title":"Namespaces"},{"location":"namespace/#cas-namespaces","text":"Multiple users can share a CAS instance. Since policy names must be unique per CAS instance, there might be conflicts when creating sessions: another user might have already created a session with that name. The standard solution to address such name collisions is to introduce namespaces. Sessions can be organized hierarchically by creating them within the namespace of another session. Organizations and large teams can benefit from namespaces to structure their sessions and restrict the creation and management of nested sessions to a set of authorized users. If one uses SCONE in the context of Kubernetes, one might want to create a corresponding CAS namespace for each Kubernetes namespace in which one executes confidential services. Please note that using CAS namespaces requires SCONE Session Language v0.3 or higher.","title":"CAS Namespaces"},{"location":"namespace/#namespace-access-policy","text":"SCONE CAS supports namespaces , i.e., a scope for session names, since version 5.1. To create a new namespace, one creates a session with the name of the namespace. By creating a namespace, one automatically controls who can create sessions in this namespace. To control access to namespaces, we added a new access policy option: name : my-namespace version : \"0.3\" access_policy : create_sessions : - CREATOR - 3s1pm8W6Be6cxvAQRbRP5YXd9YuERAr7KswN97uGtoPkRW87x1 - ... To create a new sessions in the namespace my-namespace , one has to provide one of the credentials provided of list create_sessions . If omitted, all entities listed under update will be able to create sessions.","title":"Namespace Access Policy"},{"location":"namespace/#creating-a-namespace","text":"To create a new namespace my-namespace , we first create a session description. The session description might be stored in a file policy.yml and look like this name : my-namespace version : \"0.3\" access_policy : read : - CREATOR update : - CREATOR create_sessions : - CREATOR We can upload this session with the SCONE CLI as follows: scone session create policy.yml In order to create session hello-world in the namespace my-namespace , set the new session's name to: name : my-namespace/hello-world version : \"0.3\" You must have permission to create the session in this namespace. Namespaces can be nested. You can use hello-world itself as a namespace. name : my-namespace/hello-world/project-1 version : \"0.3\"","title":"Creating a namespace"},{"location":"node-example/","text":"Node Example with Attestation and Secret Provisioning We show how to inject keys ( /tls/key.pem ) and certificate ( '/tls/cert.pem ) in a simple node program with a SCONE policy: var express = require('express'); const https = require(\"https\"), fs = require(\"fs\"); console.log('Example tls app start!'); console.log('read the secret :'+process.env.GREETING) const keyFilePath = '/tls/key.pem'; const certFilePath = '/tls/cert.pem'; var privateKey = fs.readFileSync(keyFilePath, 'utf8'); var certificate = fs.readFileSync(certFilePath, 'utf8'); var credentials = {key: privateKey, cert: certificate}; var app = express(); var httpsServer = https.createServer(credentials, app); app.get('/', function (req, res) { console.log('scone mode is :'+process.env.GREETING) res.send('Hello World!' + process.env.GREETING); }); httpsServer.listen(443, function () { console.log('Example tls app listening on port 443!'); console.log('scone mode is :'+process.env.GREETING) console.log('Ok.'); }); A policy template defines a private key and a certificate that is signed with a CA certificate that is also created in this policy. We set the DNS name of the certifcate to app . We define a secret GREETING: \"Hello NodeJS\" as an environment variable passed to app . In this policy, we assume that our development machine has not seen many firmware updates recently: name: $NODE_SESSION version: \"0.3\" # Access control: # - only the data owner (CREATOR) can read or update the session # - even the data owner cannot read the session secrets (i.e., the volume key and tag) or delete the session access_policy: read: - CREATOR update: - CREATOR services: - name: app image_name: node_image mrenclaves: [$MRENCLAVE] command: node /app/app.js environment: SCONE_MODE: hw SCONE_LOG: \"7\" GREETING: \"Hello NodeJS\" pwd: / images: - name: node_image injection_files: - path: /tls/cert.pem content: $$SCONE::app.crt$$ - path: /tls/key.pem content: $$SCONE::app.key$$ # Import client credentials from DB session. secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: app_key kind: private-key - name: app kind: x509 private_key: app_key issuer: api_ca_cert dns: - app security: attestation: tolerate: [debug-mode, hyperthreading, insecure-igpu, outdated-tcb] ignore_advisories: [\"INTEL-SA-00076\", \"INTEL-SA-00088\", \"INTEL-SA-00106\", \"INTEL-SA-00115\", \"INTEL-SA-00135\", \"INTEL-SA-00203\", \"INTEL-SA-00161\", \"INTEL-SA-00220\", \"INTEL-SA-00270\", \"INTEL-SA-00293\", \"INTEL-SA-00320\", \"INTEL-SA-00329\", \"INTEL-SA-00233\", \"INTEL-SA-00220\", \"INTEL-SA-00270\", \"INTEL-SA-00293\", \"INTEL-SA-00320\", \"INTEL-SA-00329\"] We need to set the MRENCLAVE and the name of the policy ( NODE_SESSION ) before we upload the session to a CAS instance. To simplify this upload and running of the node application, we pushed the code to github. Just clone it and enter the newly cloned directory git clone https://github.com/scontain/node_example.git cd node_example We need to set some environment variables and upload our session: export CAS_ADDR = \"4-2-1.scone-cas.cf\" # we use a public SCONE CAS to store the session policies export IMAGE = \"sconecuratedimages/apps:node-10.14-alpine\" unset NODE_SESSION export NODE_SESSION = $( ./upload_session --template = nodejs-template.yml --session = nodejs-session.yml --image = $IMAGE --cas = $CAS_ADDR ) export DEVICE = $( ./determine_sgx_device ) # determine the SGX device of the local computer and then run locally by executing docker-compose up This will print some log messages and eventually print node_1 | Example tls app start! node_1 | read the secret :Hello NodeJS node_1 | Example tls app listening on port 443! node_1 | scone mode is :Hello NodeJS node_1 | Ok. indicating that the app is ready to process requests. Client Request Execute client request via https: curl -k https://localhost:443 The output will be: Hello World!Hello NodeJS Note that '-k' is the insecure mode and if we drop this option, the output will look like this: curl https://localhost:443 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Let's fix this warning and by doing so we also attest the node service. Get CA Certificate used to sign certificate of node app Retrieve the exported ca certificate with curl from CAS. Note that we told CAS to export the CA certifcate by setting export_public: true in the policy. We store the certificate in file `ca-cert': export ca_cert = $( curl -k https:// ${ CAS_ADDR } :8081/v1/values/session = $NODE_SESSION | jq \".values.api_ca_cert.value\" | tr -d \\\" ) printf \"\\n $ca_cert \" > ca_cert We can now use this ca certificate to verify the certificate from our application: curl --capath \" $( pwd ) \" --cacert ca_cert --verbose https://app:443 --resolve app:443:127.0.0.1 Notes Before restarting the service, please shut it down properly with docker-compose down . Also execute unset NODE_SESSION to ensure that you do not reuse an old session afterwards. You need to ensure that CAS executes inside of an enclave in production mode. To do so, you would need to use our SCONE CLI to attest CAS and to upload a session You need to encrypt the app code and parts of the node image before you go to production. See our flask exmaple on how to do this.","title":"Node"},{"location":"node-example/#node-example-with-attestation-and-secret-provisioning","text":"We show how to inject keys ( /tls/key.pem ) and certificate ( '/tls/cert.pem ) in a simple node program with a SCONE policy: var express = require('express'); const https = require(\"https\"), fs = require(\"fs\"); console.log('Example tls app start!'); console.log('read the secret :'+process.env.GREETING) const keyFilePath = '/tls/key.pem'; const certFilePath = '/tls/cert.pem'; var privateKey = fs.readFileSync(keyFilePath, 'utf8'); var certificate = fs.readFileSync(certFilePath, 'utf8'); var credentials = {key: privateKey, cert: certificate}; var app = express(); var httpsServer = https.createServer(credentials, app); app.get('/', function (req, res) { console.log('scone mode is :'+process.env.GREETING) res.send('Hello World!' + process.env.GREETING); }); httpsServer.listen(443, function () { console.log('Example tls app listening on port 443!'); console.log('scone mode is :'+process.env.GREETING) console.log('Ok.'); }); A policy template defines a private key and a certificate that is signed with a CA certificate that is also created in this policy. We set the DNS name of the certifcate to app . We define a secret GREETING: \"Hello NodeJS\" as an environment variable passed to app . In this policy, we assume that our development machine has not seen many firmware updates recently: name: $NODE_SESSION version: \"0.3\" # Access control: # - only the data owner (CREATOR) can read or update the session # - even the data owner cannot read the session secrets (i.e., the volume key and tag) or delete the session access_policy: read: - CREATOR update: - CREATOR services: - name: app image_name: node_image mrenclaves: [$MRENCLAVE] command: node /app/app.js environment: SCONE_MODE: hw SCONE_LOG: \"7\" GREETING: \"Hello NodeJS\" pwd: / images: - name: node_image injection_files: - path: /tls/cert.pem content: $$SCONE::app.crt$$ - path: /tls/key.pem content: $$SCONE::app.key$$ # Import client credentials from DB session. secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: app_key kind: private-key - name: app kind: x509 private_key: app_key issuer: api_ca_cert dns: - app security: attestation: tolerate: [debug-mode, hyperthreading, insecure-igpu, outdated-tcb] ignore_advisories: [\"INTEL-SA-00076\", \"INTEL-SA-00088\", \"INTEL-SA-00106\", \"INTEL-SA-00115\", \"INTEL-SA-00135\", \"INTEL-SA-00203\", \"INTEL-SA-00161\", \"INTEL-SA-00220\", \"INTEL-SA-00270\", \"INTEL-SA-00293\", \"INTEL-SA-00320\", \"INTEL-SA-00329\", \"INTEL-SA-00233\", \"INTEL-SA-00220\", \"INTEL-SA-00270\", \"INTEL-SA-00293\", \"INTEL-SA-00320\", \"INTEL-SA-00329\"] We need to set the MRENCLAVE and the name of the policy ( NODE_SESSION ) before we upload the session to a CAS instance. To simplify this upload and running of the node application, we pushed the code to github. Just clone it and enter the newly cloned directory git clone https://github.com/scontain/node_example.git cd node_example We need to set some environment variables and upload our session: export CAS_ADDR = \"4-2-1.scone-cas.cf\" # we use a public SCONE CAS to store the session policies export IMAGE = \"sconecuratedimages/apps:node-10.14-alpine\" unset NODE_SESSION export NODE_SESSION = $( ./upload_session --template = nodejs-template.yml --session = nodejs-session.yml --image = $IMAGE --cas = $CAS_ADDR ) export DEVICE = $( ./determine_sgx_device ) # determine the SGX device of the local computer and then run locally by executing docker-compose up This will print some log messages and eventually print node_1 | Example tls app start! node_1 | read the secret :Hello NodeJS node_1 | Example tls app listening on port 443! node_1 | scone mode is :Hello NodeJS node_1 | Ok. indicating that the app is ready to process requests.","title":"Node Example with Attestation and Secret Provisioning"},{"location":"node-example/#client-request","text":"Execute client request via https: curl -k https://localhost:443 The output will be: Hello World!Hello NodeJS Note that '-k' is the insecure mode and if we drop this option, the output will look like this: curl https://localhost:443 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. Let's fix this warning and by doing so we also attest the node service.","title":"Client Request"},{"location":"node-example/#get-ca-certificate-used-to-sign-certificate-of-node-app","text":"Retrieve the exported ca certificate with curl from CAS. Note that we told CAS to export the CA certifcate by setting export_public: true in the policy. We store the certificate in file `ca-cert': export ca_cert = $( curl -k https:// ${ CAS_ADDR } :8081/v1/values/session = $NODE_SESSION | jq \".values.api_ca_cert.value\" | tr -d \\\" ) printf \"\\n $ca_cert \" > ca_cert We can now use this ca certificate to verify the certificate from our application: curl --capath \" $( pwd ) \" --cacert ca_cert --verbose https://app:443 --resolve app:443:127.0.0.1","title":"Get CA Certificate used to sign certificate of node app"},{"location":"node-example/#notes","text":"Before restarting the service, please shut it down properly with docker-compose down . Also execute unset NODE_SESSION to ensure that you do not reuse an old session afterwards. You need to ensure that CAS executes inside of an enclave in production mode. To do so, you would need to use our SCONE CLI to attest CAS and to upload a session You need to encrypt the app code and parts of the node image before you go to production. See our flask exmaple on how to do this.","title":"Notes"},{"location":"openvino/","text":"OpenVino Use Case OpenVino was designed to optimize AI inferencing . This toolkit allows developers to deploy pre-trained deep learning models through its inference engine with high performance and with smaller model sizes. Thus, OpenVino is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We demonstrate how to run OpenVino with SCONE in the following screencast: If you want to evaluate the OpenVino image, send us an email .","title":"OpenVino"},{"location":"openvino/#openvino-use-case","text":"OpenVino was designed to optimize AI inferencing . This toolkit allows developers to deploy pre-trained deep learning models through its inference engine with high performance and with smaller model sizes. Thus, OpenVino is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We demonstrate how to run OpenVino with SCONE in the following screencast: If you want to evaluate the OpenVino image, send us an email .","title":"OpenVino Use Case"},{"location":"operations/","text":"Development and Operations of SCONE-based Services During the lifetime of an application, one has to deal with various development and operational issues: How to transform native applications into confidential applications ? How to monitor a confidential application with Grafana or Kubernetes Dashboard ? How to start an application via a dashboard ? How to mitigate vulnerabilities of SGX ? How to address individual security advisories ? How to deal with firmware updates ? How to deal with application updates ? How to sign a SCONE application for production? How to deal with SCONE runtime updates ? How clients can attest services ? How to deal with SCONE CAS updates? (see enterprise CAS documentation) How to deal with application crash failures? (see enterprise CAS documentation) ... In this chapter, we explain how to address these issues. In case you have a maintenance contract with us, we help you to address these issues.","title":"Workflows"},{"location":"operations/#development-and-operations-of-scone-based-services","text":"During the lifetime of an application, one has to deal with various development and operational issues: How to transform native applications into confidential applications ? How to monitor a confidential application with Grafana or Kubernetes Dashboard ? How to start an application via a dashboard ? How to mitigate vulnerabilities of SGX ? How to address individual security advisories ? How to deal with firmware updates ? How to deal with application updates ? How to sign a SCONE application for production? How to deal with SCONE runtime updates ? How clients can attest services ? How to deal with SCONE CAS updates? (see enterprise CAS documentation) How to deal with application crash failures? (see enterprise CAS documentation) ... In this chapter, we explain how to address these issues. In case you have a maintenance contract with us, we help you to address these issues.","title":"Development and Operations of SCONE-based Services"},{"location":"outline/","text":"Execution Modes To simplify not only getting started with SCONE but also using SCONE, we support multiple ways to develop and run SCONE-based applications. Depending on what execution mode you want to use, you need to install different software components. Execution Modes: simulation mode inside of a container: use this mode to check out SCONE or to develop software on machines without Intel SGX support (e.g., a Mac). hardware mode inside of a container: this mode requires that you install the Intel SGX driver on your host. host mode : compile inside of a container and execute on the host. Dockerfile : build program and container image with the help of a Dockerfile. iExec platform : you can run your SCONE application on the iExec platform. Language Support After gaining access to the SCONE container images 1 , you can compile and run the hello world program as shown in this section . After that, check out how you could automate the compilation with the help of a Dockerfile . If you want to run applications languages not supported by GNU gcc compiler, please read the descriptions in the appropriate section in menu Language Support : in addition to C , C++ , Fortran , GO and Rust , we also support Python , Java , and JavaScript/ Node.js . Please send us an email if you need access to another programming language. Terminology We might use some terms in the SCONE documentation that we do not explicitly introduce. We maintain a Glossary that defines some of the important terms that we use within the SCONE technical documentation. You need to gain permissions to be able to run the examples given in this documentation. Please register a free account at gitlab.scontain.com . \u21a9","title":"Overview"},{"location":"outline/#execution-modes","text":"To simplify not only getting started with SCONE but also using SCONE, we support multiple ways to develop and run SCONE-based applications. Depending on what execution mode you want to use, you need to install different software components. Execution Modes: simulation mode inside of a container: use this mode to check out SCONE or to develop software on machines without Intel SGX support (e.g., a Mac). hardware mode inside of a container: this mode requires that you install the Intel SGX driver on your host. host mode : compile inside of a container and execute on the host. Dockerfile : build program and container image with the help of a Dockerfile. iExec platform : you can run your SCONE application on the iExec platform.","title":"Execution Modes"},{"location":"outline/#language-support","text":"After gaining access to the SCONE container images 1 , you can compile and run the hello world program as shown in this section . After that, check out how you could automate the compilation with the help of a Dockerfile . If you want to run applications languages not supported by GNU gcc compiler, please read the descriptions in the appropriate section in menu Language Support : in addition to C , C++ , Fortran , GO and Rust , we also support Python , Java , and JavaScript/ Node.js . Please send us an email if you need access to another programming language. Terminology We might use some terms in the SCONE documentation that we do not explicitly introduce. We maintain a Glossary that defines some of the important terms that we use within the SCONE technical documentation. You need to gain permissions to be able to run the examples given in this documentation. Please register a free account at gitlab.scontain.com . \u21a9","title":"Language Support"},{"location":"performance/","text":"Performance of SCONE-based Programs SCONE-Based SGX PySpark Performance We designed and implemented a SCONE-based SGX-PySpark 1 - a secure distributed data analytics system which is based on PySpark and relies on SCONE and Intel SGX to provide strong security guarantees. To demonstrate the performance of SGX-PySpark use of a standard data analytics benchmark, i.e.,TPC-H, to demonstrate that SCONE-based SGX-PySpark supports the same range of queries as native PySpark. The figure below presents the latency comparison between SGX-PySpark with native PySpark in processing TPC-H queries. The performance overhead incurred when running Python processes inside enclaves is not significant compared to the native PySpark: the average slowdown is only 22% for TPC-H. Python Performance For many of the standard Python benchmarks, our SCONE PyPy (i.e., the just in time Python engine) inside of an SGX enclave actually runs faster than CPython in native mode. Yes, we get for many of the benchmarks a speedup over native performance: For a detail performance analysis have a look at the normalized PyPy performance shows that overheads are for the standard Python benchmarks are acceptable. Performance Factors The performance of running programs inside of enclaves depends on various factors. The main factors are the following: Locality of memory accesses : if the working set of a process does not fit in the EPC (extended page cache), the process will suffer page faults. During a page fault, a page in the EPC will be selected by the SGX driver, re-encrypted and stored in main memory. The overhead of a process inside of an enclave grows with the page fault rate. If a program has an option to reduce the memory footprint (not include debug symbols in binaries, compiled to reduce size, etc), this will often result in better performance. system calls : each system call requires that memory-based arguments are copied from the enclave to the outside memory and memory-based return values are copied from the main memory into the enclave. Exiting and returning to an enclave takes at least 8000 cycles 2 . SCONE uses an asynchronous system call interface that ensures that threads do not need to exit the enclave to perform a system call. threading : SCONE provides application-level threading. This ensures that in case an application thread waits for some event like the return of a system call, we can switch to a new application thread without the need to exit the enclave. For getting optimal performance in your applications, SCONE provides multiple tuning parameters ( see here ): The default SCONE configuration file often provides reasonable performance but optimal performance - at least for SGXv1 hardware - can often only be achieved by selecting the right number of ethreads , sthreads and heap size . Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. DOI: https://doi.org/10.1145/3308558.3314129 \u21a9 Recent CPU microcode updates have increased this number further. \u21a9","title":"Applications"},{"location":"performance/#performance-of-scone-based-programs","text":"","title":"Performance of SCONE-based Programs"},{"location":"performance/#scone-based-sgx-pyspark-performance","text":"We designed and implemented a SCONE-based SGX-PySpark 1 - a secure distributed data analytics system which is based on PySpark and relies on SCONE and Intel SGX to provide strong security guarantees. To demonstrate the performance of SGX-PySpark use of a standard data analytics benchmark, i.e.,TPC-H, to demonstrate that SCONE-based SGX-PySpark supports the same range of queries as native PySpark. The figure below presents the latency comparison between SGX-PySpark with native PySpark in processing TPC-H queries. The performance overhead incurred when running Python processes inside enclaves is not significant compared to the native PySpark: the average slowdown is only 22% for TPC-H.","title":"SCONE-Based SGX PySpark Performance"},{"location":"performance/#python-performance","text":"For many of the standard Python benchmarks, our SCONE PyPy (i.e., the just in time Python engine) inside of an SGX enclave actually runs faster than CPython in native mode. Yes, we get for many of the benchmarks a speedup over native performance: For a detail performance analysis have a look at the normalized PyPy performance shows that overheads are for the standard Python benchmarks are acceptable.","title":"Python Performance"},{"location":"performance/#performance-factors","text":"The performance of running programs inside of enclaves depends on various factors. The main factors are the following: Locality of memory accesses : if the working set of a process does not fit in the EPC (extended page cache), the process will suffer page faults. During a page fault, a page in the EPC will be selected by the SGX driver, re-encrypted and stored in main memory. The overhead of a process inside of an enclave grows with the page fault rate. If a program has an option to reduce the memory footprint (not include debug symbols in binaries, compiled to reduce size, etc), this will often result in better performance. system calls : each system call requires that memory-based arguments are copied from the enclave to the outside memory and memory-based return values are copied from the main memory into the enclave. Exiting and returning to an enclave takes at least 8000 cycles 2 . SCONE uses an asynchronous system call interface that ensures that threads do not need to exit the enclave to perform a system call. threading : SCONE provides application-level threading. This ensures that in case an application thread waits for some event like the return of a system call, we can switch to a new application thread without the need to exit the enclave. For getting optimal performance in your applications, SCONE provides multiple tuning parameters ( see here ): The default SCONE configuration file often provides reasonable performance but optimal performance - at least for SGXv1 hardware - can often only be achieved by selecting the right number of ethreads , sthreads and heap size . Do Le Quoc, Franz Gregor, Jatinder Singh, and Christof Fetzer. 2019. SGX-PySpark: Secure Distributed Data Analytics. In The World Wide Web Conference (WWW '19), Ling Liu and Ryen White (Eds.). ACM, New York, NY, USA, 3564-3563. DOI: https://doi.org/10.1145/3308558.3314129 \u21a9 Recent CPU microcode updates have increased this number further. \u21a9","title":"Performance Factors"},{"location":"print-arg-env/","text":"Secure Arguments and Environment Variables We show how to pass arguments and environment variables in a secure fashion to an application with the help of CAS. SCONE CAS manages the keys of an application. The keys are unter complete control of the application: only services given explicit control by the application get access to the keys. Prerequisites We assume that you already started a CAS instance on the local host, as well as started a LAS instance on the local host, We also assume that this example is executed in the context of Ubuntu. For other Linux variants, you might need to use a different cross-compiler image. Alternatively, you could execute this example in the crosscompiler container but you need to ensure that the container can establish https connections to CAS and LAS. Simple Example We create a simple C program to print the arguments as well as the environment variables: cat > print-arg-env.c <<EOF #include <stdio.h> extern char **__environ; int main (int argc, char **argv) { printf(\"argv:\"); for (int i = 0; i < argc; i++) { printf(\" %s\", argv[i]); } printf(\"\\n\"); char** envp = __environ; printf(\"environ:\\n\"); while (*envp != NULL) { printf(\"%s\\n\", *envp); envp++; } return 42; } EOF We can compile this natively as follows: gcc print-arg-env.c -g -O3 -o print-arg-env And we can now run this with some some arguments and some environment variables: ENV2 = World ENV1 = ! ./print-arg-env Hello This results in an output similar to this: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Let's cross-compile this program to run inside of enclaves as follows. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker run $MOUNT_SGXDEVICE -it -v ` pwd ` :/work registry.scontain.com:5050/sconecuratedimages/crosscompilers scone-gcc /work/print-arg-env.c -g -O3 -o /work/scone-print-arg-env We can now determine MRENCLAVE as follows: export MRENCLAVE = ` SCONE_HASH = 1 ./scone-print-arg-env ` echo MRENCLAVE of scone-print-arg-env is $MRENCLAVE We can now run the program inside of an enclave but not under the control of CAS as follows: ENV2 = World ENV1 = ! ./scone-print-arg-env Hello This will result in the same output as before: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Without CAS: No Confidentiality and Integrity of Arguments and Environment Variables Note that neither the integrity nor the confidentiality of these arguments are protected. To protect these and to ensure that the program is indeed executed inside an enclave, we need to run the program in the context of CAS.** Secure Arguments and Environment Variables In the next step, we control the environment variables and the arguments with the help of CAS. When does the SCONE runtime use CAS? When environment variable SCONE_CONFIG_ID is not set, the SCONE runtime will not contact CAS. If it is set, we will contact SCONE_CAS_ADDR . If SCONE_CAS_ADDR is not set, we contact by default host \u201ecas\u201c. The SCONE runtime will also contact SCONE_LAS_ADDR for local attestation. If SCONE_LAS_ADDR is not set, we contact by default host \u201elas\u201c. We assume that you already started a CAS in the background via docker-compose up -d cas on the local machine: export SCONE_CAS_ADDR = 127 .0.0.1 Alternatively, you can use a our public CAS instance at domain scone-cas.cf : export SCONE_CAS_ADDR = scone-cas.cf We also assume that you already started a LAS in the background via docker-compose up -d las on the local machine: export SCONE_LAS_ADDR = 127 .0.0.1 We can now create a new session on cas as follows: cat > session.yml <<EOF name: secure-arguments-example digest: somedigest services: - name: scone-print-arg-env mrenclaves: [$MRENCLAVE] tags: [thisissometag1] command: ./scone-print-arg-env arg1 arg2 arg3 environment: SCONE_MODE: hw env1: running env2: in env3: enclave pwd: / EOF Let's make sure that we have a client certificate to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar to this: Created Session[id=ce81e4cd-eed4-4f12-9bfc-c93e3dd4b454, name=secure-arguments, status=Pending]c Note that if you try to upload to a session that already exists, you will get an error code like this: Could not create successor session. Invalid previous session digest: Some(\"5de101e4980f312527e1c308dfe6c7bc465660c0fea6b98983efa5baf94bf362\") != None In other words, if you want to update a session, you need to define a predecessor key in the session that specifies the current session. This is to ensure that a client does not overwrite a just updated session, i.e., it permits to detect write/write conflicts. Executing Print-Arg-Env Let us now execute scone-print-arg-env in the context of session secure-arguments-example . To do so, we need to pass in environment variable SCONE_CONFIG_ID which contains the name of the session and we need to set SCONE_CAS_ADDR as well as SCONE_LAS_ADDR as we have shown above. SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env This will print the following output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave Note that the environment variables - except those expected by the SCONE runtime to locate LAS, CAS and the session - are ignored. This means even if we pass additional arguments and environment variables, the output does not change. The execution of IRGNORED_ENV = 1 SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env IGNORED1 IGNORED2 results in the same output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave The SCONE variables provided by the environment are not protected in any way and we designed SCONE assuming that an attacker could modify these environment variables. This could lead to denial of service attacks but not to the exposure of any secrets. Integrity of SCONE environment variables An adversary could change SCONE_CONFIG_ID , SCONE_CAS_ADDR and SCONE_LAS_ADDR but in this case, the application does not start (attestation fails) up or it does not get access to the secrets and configuration variables to do its job.","title":"Secure Arguments"},{"location":"print-arg-env/#secure-arguments-and-environment-variables","text":"We show how to pass arguments and environment variables in a secure fashion to an application with the help of CAS. SCONE CAS manages the keys of an application. The keys are unter complete control of the application: only services given explicit control by the application get access to the keys.","title":"Secure Arguments and Environment Variables"},{"location":"print-arg-env/#prerequisites","text":"We assume that you already started a CAS instance on the local host, as well as started a LAS instance on the local host, We also assume that this example is executed in the context of Ubuntu. For other Linux variants, you might need to use a different cross-compiler image. Alternatively, you could execute this example in the crosscompiler container but you need to ensure that the container can establish https connections to CAS and LAS.","title":"Prerequisites"},{"location":"print-arg-env/#simple-example","text":"We create a simple C program to print the arguments as well as the environment variables: cat > print-arg-env.c <<EOF #include <stdio.h> extern char **__environ; int main (int argc, char **argv) { printf(\"argv:\"); for (int i = 0; i < argc; i++) { printf(\" %s\", argv[i]); } printf(\"\\n\"); char** envp = __environ; printf(\"environ:\\n\"); while (*envp != NULL) { printf(\"%s\\n\", *envp); envp++; } return 42; } EOF We can compile this natively as follows: gcc print-arg-env.c -g -O3 -o print-arg-env And we can now run this with some some arguments and some environment variables: ENV2 = World ENV1 = ! ./print-arg-env Hello This results in an output similar to this: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Let's cross-compile this program to run inside of enclaves as follows. We determine which SGX device to mount with function determine_sgx_device . determine_sgx_device docker run $MOUNT_SGXDEVICE -it -v ` pwd ` :/work registry.scontain.com:5050/sconecuratedimages/crosscompilers scone-gcc /work/print-arg-env.c -g -O3 -o /work/scone-print-arg-env We can now determine MRENCLAVE as follows: export MRENCLAVE = ` SCONE_HASH = 1 ./scone-print-arg-env ` echo MRENCLAVE of scone-print-arg-env is $MRENCLAVE We can now run the program inside of an enclave but not under the control of CAS as follows: ENV2 = World ENV1 = ! ./scone-print-arg-env Hello This will result in the same output as before: argv: ./print-arg-env Hello environ: ENV2=World ENV1=! ... Without CAS: No Confidentiality and Integrity of Arguments and Environment Variables Note that neither the integrity nor the confidentiality of these arguments are protected. To protect these and to ensure that the program is indeed executed inside an enclave, we need to run the program in the context of CAS.**","title":"Simple Example"},{"location":"print-arg-env/#secure-arguments-and-environment-variables_1","text":"In the next step, we control the environment variables and the arguments with the help of CAS. When does the SCONE runtime use CAS? When environment variable SCONE_CONFIG_ID is not set, the SCONE runtime will not contact CAS. If it is set, we will contact SCONE_CAS_ADDR . If SCONE_CAS_ADDR is not set, we contact by default host \u201ecas\u201c. The SCONE runtime will also contact SCONE_LAS_ADDR for local attestation. If SCONE_LAS_ADDR is not set, we contact by default host \u201elas\u201c. We assume that you already started a CAS in the background via docker-compose up -d cas on the local machine: export SCONE_CAS_ADDR = 127 .0.0.1 Alternatively, you can use a our public CAS instance at domain scone-cas.cf : export SCONE_CAS_ADDR = scone-cas.cf We also assume that you already started a LAS in the background via docker-compose up -d las on the local machine: export SCONE_LAS_ADDR = 127 .0.0.1 We can now create a new session on cas as follows: cat > session.yml <<EOF name: secure-arguments-example digest: somedigest services: - name: scone-print-arg-env mrenclaves: [$MRENCLAVE] tags: [thisissometag1] command: ./scone-print-arg-env arg1 arg2 arg3 environment: SCONE_MODE: hw env1: running env2: in env3: enclave pwd: / EOF Let's make sure that we have a client certificate to identify this client: mkdir -p conf if [[ ! -f conf/client.crt || ! -f conf/client-key.key ]] ; then openssl req -x509 -newkey rsa:4096 -out conf/client.crt -keyout conf/client-key.key \\ -days 31 -nodes -sha256 \\ -subj \"/C=US/ST=Dresden/L=Saxony/O=Scontain/OU=Org/CN=www.scontain.com\" \\ -reqexts SAN -extensions SAN \\ -config < ( cat /etc/ssl/openssl.cnf < ( printf '[SAN]\\nsubjectAltName=DNS:www.scontain.com' )) fi Next, we will upload this session to CAS: curl -k -s --cert conf/client.crt --key conf/client-key.key --data-binary @session.yml -X POST https:// $SCONE_CAS_ADDR :8081/session This results in an output similar to this: Created Session[id=ce81e4cd-eed4-4f12-9bfc-c93e3dd4b454, name=secure-arguments, status=Pending]c Note that if you try to upload to a session that already exists, you will get an error code like this: Could not create successor session. Invalid previous session digest: Some(\"5de101e4980f312527e1c308dfe6c7bc465660c0fea6b98983efa5baf94bf362\") != None In other words, if you want to update a session, you need to define a predecessor key in the session that specifies the current session. This is to ensure that a client does not overwrite a just updated session, i.e., it permits to detect write/write conflicts.","title":"Secure Arguments and Environment Variables"},{"location":"print-arg-env/#executing-print-arg-env","text":"Let us now execute scone-print-arg-env in the context of session secure-arguments-example . To do so, we need to pass in environment variable SCONE_CONFIG_ID which contains the name of the session and we need to set SCONE_CAS_ADDR as well as SCONE_LAS_ADDR as we have shown above. SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env This will print the following output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave Note that the environment variables - except those expected by the SCONE runtime to locate LAS, CAS and the session - are ignored. This means even if we pass additional arguments and environment variables, the output does not change. The execution of IRGNORED_ENV = 1 SCONE_CONFIG_ID = secure-arguments-example/scone-print-arg-env ./scone-print-arg-env IGNORED1 IGNORED2 results in the same output: argv: ./scone-print-arg-env arg1 arg2 arg3 environ: SCONE_MODE=hw env2=in env1=running env3=enclave The SCONE variables provided by the environment are not protected in any way and we designed SCONE assuming that an attacker could modify these environment variables. This could lead to denial of service attacks but not to the exposure of any secrets. Integrity of SCONE environment variables An adversary could change SCONE_CONFIG_ID , SCONE_CAS_ADDR and SCONE_LAS_ADDR but in this case, the application does not start (attestation fails) up or it does not get access to the secrets and configuration variables to do its job.","title":"Executing Print-Arg-Env"},{"location":"public-CAS/","text":"Public CAS We maintain public CAS instances at domain scone-cas.cf . As we support semantic versioning , there is a need to operate multiple CAS versions. Hence, we operate the following CAS instances for testing: 4-0-0.scone-cas.cf : is compatible with version 4.x.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.2) . 4-2-1.scone-cas.cf : is compatible with version 4.3.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.3) . 5-0-0.scone-cas.cf : is compatible with version 5.0.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.3) . 5-1-0.scone-cas.cf : is compatible with version 5.1.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.3) .","title":"Public CAS"},{"location":"public-CAS/#public-cas","text":"We maintain public CAS instances at domain scone-cas.cf . As we support semantic versioning , there is a need to operate multiple CAS versions. Hence, we operate the following CAS instances for testing: 4-0-0.scone-cas.cf : is compatible with version 4.x.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.2) . 4-2-1.scone-cas.cf : is compatible with version 4.3.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.3) . 5-0-0.scone-cas.cf : is compatible with version 5.0.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.3) . 5-1-0.scone-cas.cf : is compatible with version 5.1.y of the SCONE runtime and contains. This CAS instance supports SCONE Session Language (v0.3) .","title":"Public CAS"},{"location":"pypyscone/","text":"PyPy for SCONE PyPy is a Just In Time compiler for Python. We maintain a container image that includes PyPy running inside of an enclave. The speed of pypy inside of an enclave is in many cases faster than running natively (i.e., outside an enclave) using the standard CPython interpreter. To compare the performance we are using the standard python / pypy speed center and add pypy for SCONE. Below you can see a graph that gives an overview of the performance of different Python variants. This depicts normalized performance using pypy as the baseline, i.e., smaller values show better performance. For example, a ratio of 65 means that the program is indeed 65 times slower than pypy running in native mode (i.e., these are not percent values). We can see in this graph that pypy can achieve for some benchmarks dramatic speedups. pypy running inside of an enclave is in almost all benchmarks faster than native CPython: You can compare the performance in more details using the speedcenter website maintained by UFCG .","title":"PyPy"},{"location":"pypyscone/#pypy-for-scone","text":"PyPy is a Just In Time compiler for Python. We maintain a container image that includes PyPy running inside of an enclave. The speed of pypy inside of an enclave is in many cases faster than running natively (i.e., outside an enclave) using the standard CPython interpreter. To compare the performance we are using the standard python / pypy speed center and add pypy for SCONE. Below you can see a graph that gives an overview of the performance of different Python variants. This depicts normalized performance using pypy as the baseline, i.e., smaller values show better performance. For example, a ratio of 65 means that the program is indeed 65 times slower than pypy running in native mode (i.e., these are not percent values). We can see in this graph that pypy can achieve for some benchmarks dramatic speedups. pypy running inside of an enclave is in almost all benchmarks faster than native CPython: You can compare the performance in more details using the speedcenter website maintained by UFCG .","title":"PyPy for SCONE"},{"location":"pyspark/","text":"PySpark Use Case PySpark is the Python API to Spark . We will add some more documentation about the curated PySpark image later. Until then, you can have a look at our PySpark screencast: If you want to evaluate the PySpark image, send us an email .","title":"PySpark"},{"location":"pyspark/#pyspark-use-case","text":"PySpark is the Python API to Spark . We will add some more documentation about the curated PySpark image later. Until then, you can have a look at our PySpark screencast: If you want to evaluate the PySpark image, send us an email .","title":"PySpark Use Case"},{"location":"python_images/","text":"Curated Python Images (Standard Edition) We maintain for all official, Alpine-Linux-based Python images a sconified version, i.e., a version in which the Python engine is executed inside of SGX enclave. These images are only accessible for paid subscribers. Our naming scheme for these Python images is based on the naming scheme as the official Python images. These images have the following naming scheme: python:PYTHONVERSION-alpineALPINEVERSION The PYTHONVERSION is currently between 3.5.10 and 3.9.0rc2 and the ALPINEVERSION is between version 3.7 and 3.12 . We maintain for each of these images a sconified version, in total more than 200 images. The naming scheme of the sconified images is as follows: registry.scontain.com:5050/sconecuratedimages/python:PYTHONVERSION-alpineALPINEVERSION[-sconeMAJOR[.MINOR[.PATCH]]] whereby MAJOR is incremented when some breaking change happens like an underlying protocol changes or we disable some deprecated feature, MINOR is incremented whenever we add a new feature that is backward compatible, and PATCH is incremented whenever we fix some bug or cleanup the code. One can select with which version of SCONE by specifying no SCONE version : use the latest SCONE version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12 contains Python 3.8.5 running on Alpine Linux 3.12 with the latest SCONE version. only major version : use the latest SCONE version of a given major version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12-scone4 contains Python 3.8.5 with the latest minor and patch version for major version 4 of SCONE. major and minor version : use the latest patch level for a given SCONE version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12-scone4.2 contains Python 3.8.5 with the latest patch level for SCONE 4.2. full version : use a given patch level for a given SCONE version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12-scone4.2.1 contains Python 3.8.5 with patch level 1 for SCONE 4.2.","title":"Python Images"},{"location":"python_images/#curated-python-images-standard-edition","text":"We maintain for all official, Alpine-Linux-based Python images a sconified version, i.e., a version in which the Python engine is executed inside of SGX enclave. These images are only accessible for paid subscribers. Our naming scheme for these Python images is based on the naming scheme as the official Python images. These images have the following naming scheme: python:PYTHONVERSION-alpineALPINEVERSION The PYTHONVERSION is currently between 3.5.10 and 3.9.0rc2 and the ALPINEVERSION is between version 3.7 and 3.12 . We maintain for each of these images a sconified version, in total more than 200 images. The naming scheme of the sconified images is as follows: registry.scontain.com:5050/sconecuratedimages/python:PYTHONVERSION-alpineALPINEVERSION[-sconeMAJOR[.MINOR[.PATCH]]] whereby MAJOR is incremented when some breaking change happens like an underlying protocol changes or we disable some deprecated feature, MINOR is incremented whenever we add a new feature that is backward compatible, and PATCH is incremented whenever we fix some bug or cleanup the code. One can select with which version of SCONE by specifying no SCONE version : use the latest SCONE version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12 contains Python 3.8.5 running on Alpine Linux 3.12 with the latest SCONE version. only major version : use the latest SCONE version of a given major version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12-scone4 contains Python 3.8.5 with the latest minor and patch version for major version 4 of SCONE. major and minor version : use the latest patch level for a given SCONE version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12-scone4.2 contains Python 3.8.5 with the latest patch level for SCONE 4.2. full version : use a given patch level for a given SCONE version. Example: registry.scontain.com:5050/sconecuratedimages/python:3.8.5-alpine3.12-scone4.2.1 contains Python 3.8.5 with patch level 1 for SCONE 4.2.","title":"Curated Python Images (Standard Edition)"},{"location":"registry/","text":"Scontain Image Registry Since we ran into rate limitation issues with Docker hub, we needed to move to a new container image registry: registry.scontain.com:5050 . You can register a free account at https://gitlab.scontain.com/users/sign_up (see also below). For new users: Please register an account Please register a free account at https://gitlab.scontain.com/users/sign_up . You need to confirm your email and, we will grant you - typically within a few hours - access to the community edition. Create an access token After you registered an account, you can create an access token for docker login as follows: You can create a https://gitlab.scontain.com/-/profile/personal_access_tokens by selecting a name of the token, e.g., docker_token selecting scope read_registry and then pressing button create personal access token . Copy your new access token and use this for your docker login (see below) For more information, please read this gitlab docu Screenshot docker login You can log into the registry as follows: docker login registry.scontain.com:5050 using your your ID as username , and your access token as password. docker pull When you execute a docker pull , you now need to specify the registry registry.scontain.com:5050 in each pull request. Instead of pulling the image sconecuratedimages/crosscompilers:alpine like this: docker pull sconecuratedimages/crosscompilers:alpine you now need to pull this image like this: docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine Password Reset To reset your password, please visit https://gitlab.scontain.com/users/password/new If you do not remember your email address or your login: You should have gotten an email regarding this and, you need to reset the password because you are given a random password. You should have the same access rights as previously. If not, please let us know.","title":"Scontain Registy"},{"location":"registry/#scontain-image-registry","text":"Since we ran into rate limitation issues with Docker hub, we needed to move to a new container image registry: registry.scontain.com:5050 . You can register a free account at https://gitlab.scontain.com/users/sign_up (see also below).","title":"Scontain Image Registry"},{"location":"registry/#for-new-users-please-register-an-account","text":"Please register a free account at https://gitlab.scontain.com/users/sign_up . You need to confirm your email and, we will grant you - typically within a few hours - access to the community edition.","title":"For new users: Please register an account"},{"location":"registry/#create-an-access-token","text":"After you registered an account, you can create an access token for docker login as follows: You can create a https://gitlab.scontain.com/-/profile/personal_access_tokens by selecting a name of the token, e.g., docker_token selecting scope read_registry and then pressing button create personal access token . Copy your new access token and use this for your docker login (see below) For more information, please read this gitlab docu","title":"Create an access token"},{"location":"registry/#screenshot","text":"","title":"Screenshot"},{"location":"registry/#docker-login","text":"You can log into the registry as follows: docker login registry.scontain.com:5050 using your your ID as username , and your access token as password.","title":"docker login"},{"location":"registry/#docker-pull","text":"When you execute a docker pull , you now need to specify the registry registry.scontain.com:5050 in each pull request. Instead of pulling the image sconecuratedimages/crosscompilers:alpine like this: docker pull sconecuratedimages/crosscompilers:alpine you now need to pull this image like this: docker pull registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine","title":"docker pull"},{"location":"registry/#password-reset","text":"To reset your password, please visit https://gitlab.scontain.com/users/password/new If you do not remember your email address or your login: You should have gotten an email regarding this and, you need to reset the password because you are given a random password. You should have the same access rights as previously. If not, please let us know.","title":"Password Reset"},{"location":"sadvisories/","text":"Security Advisories Intel regularly publishes security advisories at https://www.intel.com/content/www/us/en/security-center/default.html . The attestation of an enclave verifies the hash value of the enclave ( MrEnclave ) and if the CPU, the platform, or the firmware is affected by security advisories. In case they are, attestation fails by default. Each advisory has a unique advisory number. For example, INTEL-SA-00270 : 2019.2 IPU \u2013 TSX Asynchronous Abort Advisory INTEL-SA-00161 : Q3 2018 Speculative Execution Side Channel Update ... In some cases, you need to wait for updated firmware. In case you consider that your application is not affected by the advisory, you can disable this in your security policy. Some of the security advisories are encountered in quite a few platforms, in which case our policy provides shortcuts to ignore them (like hyperthreading ). Others may be more severe or very recent. To ignore these advisories, they and their associated category ( software-hardening-needed / insecure-configuration / outdated-tcb ) need to be specified explicitly. For example: security: attestation: tolerate: [outdated-tcb] ignore_advisories: [INTEL-SA-00161, INTEL-SA-00270] When in debug-mode, all advisories can be ignored by using a wildcard, simplifying the workflow in testing environments: security: attestation: tolerate: [debug-mode, outdated-tcb] ignore_advisories: \"*\" As this silently also ignores newly released platform advisories, wildcards cannot be used in production mode. For more details, please have a look at our policy description .","title":"Security advisories"},{"location":"sadvisories/#security-advisories","text":"Intel regularly publishes security advisories at https://www.intel.com/content/www/us/en/security-center/default.html . The attestation of an enclave verifies the hash value of the enclave ( MrEnclave ) and if the CPU, the platform, or the firmware is affected by security advisories. In case they are, attestation fails by default. Each advisory has a unique advisory number. For example, INTEL-SA-00270 : 2019.2 IPU \u2013 TSX Asynchronous Abort Advisory INTEL-SA-00161 : Q3 2018 Speculative Execution Side Channel Update ... In some cases, you need to wait for updated firmware. In case you consider that your application is not affected by the advisory, you can disable this in your security policy. Some of the security advisories are encountered in quite a few platforms, in which case our policy provides shortcuts to ignore them (like hyperthreading ). Others may be more severe or very recent. To ignore these advisories, they and their associated category ( software-hardening-needed / insecure-configuration / outdated-tcb ) need to be specified explicitly. For example: security: attestation: tolerate: [outdated-tcb] ignore_advisories: [INTEL-SA-00161, INTEL-SA-00270] When in debug-mode, all advisories can be ignored by using a wildcard, simplifying the workflow in testing environments: security: attestation: tolerate: [debug-mode, outdated-tcb] ignore_advisories: \"*\" As this silently also ignores newly released platform advisories, wildcards cannot be used in production mode. For more details, please have a look at our policy description .","title":"Security Advisories"},{"location":"scone-signer/","text":"SCONE SIGNER A SCONE application must be signed by a Signing Identity to run in production mode . MRSIGNER is the public key of the Signing Identity. The signature associates MrEnclave and MRSIGNER with this application. Note that MrEnclave depends on multiple parameters, like the the application code, the heap size, the stack size, the number of threads executing inside of the enclave (see threading ), ... Some of these parameters, like the heap size, are configurable. When signing a SCONE application, you should set all configurable parameters. If a parameter is not specified, a default value is used instead. When starting an application, the SCONE runtime uses by default the parameter values specified during signing. When trying to start an application with a different configuration than it was signed for, MrEnclave will be different. The SCONE runtime will detect this and tries to sign it dynamically during runtime. Note that the private key of MRSIGNER will not be available during runtime, and hence, the startup will fail. CLI The command line interface scone-signer signs SCONE applications: scone-signer sign [ flags ] APPLICATION_PATH To sign a SCONE application for production, you should specify the following flags: --stack=INT : The stack size (default=2MB) [SCONE_STACK]\", --minheap=INT : The minimal heap size (default=20MB) [SCONE_MIN_HEAP]\", --heap=INT : The heap size (default=64MB) [SCONE_HEAP]\", --tcs=INT : The number of TCS (default=8) [SCONE_TCS]\", --mprotect=[0|1] : mprotect: 0 - disable, 1 - enable (default=0) [SCONE_MPROTECT]\", --dlopen=[0|1] : dlopen: 0 - disable, 1 - enable and require loaded libraries to be authenticated/encrypted (default=0) [SCONE_ALLOW_DLOPEN]\", --xfrm=INT : XFRM to set in SIGSTRUCT [SCONE_XFRM]\", --isvsvn=INT : SGX Independent Software Vendor Security Version Number (default=0) [SCONE_ISVSVN]\", --isvprodid=INT SGX Independent Software Vendor Product ID (default=0) [SCONE_ISVPRODID]\" Note that we support the following units (i.e., suffix of INT): G : integer value * 2^30, i.e., GB M : integer value * 2^20, i.e., MB K : integer value * 2^10, i.e., KB The following flags can also be provided: --key=PATH The path to private key file. If not provided, a built-in debug key will be used [SCONE_KEY]\", --production Sign the enclave to run in production mode. [SCONE_PRODUCTION]\", --env Use the SCONE_* environment variables additionally to the provided arguments. Arguments overwrite environment variables\".","title":"Signing of enclaves"},{"location":"scone-signer/#scone-signer","text":"A SCONE application must be signed by a Signing Identity to run in production mode . MRSIGNER is the public key of the Signing Identity. The signature associates MrEnclave and MRSIGNER with this application. Note that MrEnclave depends on multiple parameters, like the the application code, the heap size, the stack size, the number of threads executing inside of the enclave (see threading ), ... Some of these parameters, like the heap size, are configurable. When signing a SCONE application, you should set all configurable parameters. If a parameter is not specified, a default value is used instead. When starting an application, the SCONE runtime uses by default the parameter values specified during signing. When trying to start an application with a different configuration than it was signed for, MrEnclave will be different. The SCONE runtime will detect this and tries to sign it dynamically during runtime. Note that the private key of MRSIGNER will not be available during runtime, and hence, the startup will fail.","title":"SCONE SIGNER"},{"location":"scone-signer/#cli","text":"The command line interface scone-signer signs SCONE applications: scone-signer sign [ flags ] APPLICATION_PATH To sign a SCONE application for production, you should specify the following flags: --stack=INT : The stack size (default=2MB) [SCONE_STACK]\", --minheap=INT : The minimal heap size (default=20MB) [SCONE_MIN_HEAP]\", --heap=INT : The heap size (default=64MB) [SCONE_HEAP]\", --tcs=INT : The number of TCS (default=8) [SCONE_TCS]\", --mprotect=[0|1] : mprotect: 0 - disable, 1 - enable (default=0) [SCONE_MPROTECT]\", --dlopen=[0|1] : dlopen: 0 - disable, 1 - enable and require loaded libraries to be authenticated/encrypted (default=0) [SCONE_ALLOW_DLOPEN]\", --xfrm=INT : XFRM to set in SIGSTRUCT [SCONE_XFRM]\", --isvsvn=INT : SGX Independent Software Vendor Security Version Number (default=0) [SCONE_ISVSVN]\", --isvprodid=INT SGX Independent Software Vendor Product ID (default=0) [SCONE_ISVPRODID]\" Note that we support the following units (i.e., suffix of INT): G : integer value * 2^30, i.e., GB M : integer value * 2^20, i.e., MB K : integer value * 2^10, i.e., KB The following flags can also be provided: --key=PATH The path to private key file. If not provided, a built-in debug key will be used [SCONE_KEY]\", --production Sign the enclave to run in production mode. [SCONE_PRODUCTION]\", --env Use the SCONE_* environment variables additionally to the provided arguments. Arguments overwrite environment variables\".","title":"CLI"},{"location":"scone-use-case-overview/","text":"Overview The SCONE Platform is a generic platform to execute applications inside of trusted execution environments like Intel SGX. It can be used in different settings with multiple stakeholders that might not all trust each other: User , Application Provider , Data Owner , and Auditor . SCONE protects the IP (intellectual property) of multiple stakeholders . In this section, we introduce the different stakeholders and sketch how SCONE can be used to protect the data, code, and secrets not only from attackers but also from other stakeholders. We also give some recommendations - in the form of forward links - on how one could proceed reading about SCONE. Attacker In SCONE, we assume a very powerful attacker that has root access on the hosts on which we execute applications. The attacker can hence dump the memory of the application using standard features of the operating system. In this way, the attacker can gain access to the code, the data and all the keys that are stored in main memory. Note that the attacker can read all files of the application since operating system based access control does not apply to an attacker with root access. Moreover, any operating system based encryption of files does prevent file system accesses by the attacker either. If the application encrypts the files itself, the encryption key has to be stored somewhere in main memory - otherwise, the application would not be able to encrypt and decrypt files. An attacker can dump the main memory to get a hold of this encryption key. User A user wants to protect his computations, data and keys from accesses by other parties and in particular, by accesses by attackers with root access rights: Protection against read accesses by attackers protects the confidentiality of the data, keys, and code. Protection against read accesses by attackers protects the integrity of the data, keys, and code. SCONE provides a simple CLI (command line interface) to execute applications on remote hosts. This CLI supports to encrypt files on the user's computer and push these files to a remote computer. Only the application itself can access the files: SCONE performs a transparent attestation of the application to ensure that neither the application or the file system has been modified . Only then, SCONE transparently encrypts/decrypts the files. In this way, SCONE ensures the confidentiality and integrity of the user provided files. Application Provider An application provider can create container images like, for example, blender , a 3D creation suite. SCONE helps the application provider to run unmodified applications inside of TEEs (trusted execution environments) such that the application is protected from accesses by attackers. SCONE can also transparently encrypt/decrypt the files stored in the file system. The application container image can, for example, be created with the help of a Dockerfile and in some cases, it might need a multistage build . We describe how to build a Go program as part of this tutorial. An application provider can push its container images to a public repository like Docker hub or some private repository. The application provider can encrypt parts of the files to ensure the confidentiality and integrity of the files. For example, some of the application code might be written in Python and hence, the application provider must protect the integrity and confidentiality of that code: Actually, the situation is even more difficult. The user might want to give the application access to some encrypted input and output files (as explained above). The application provider should not have the right to access the files since this is the data that the user must protect. The user should not have access to the files provided by the application provider. In particular, SCONE manages the keys such that the SCONE runtime of the application can get access to the keys but neither the application provider nor the user will see these keys. We describe later how to do this for a simple decentralized copy application . Data Owner A data owner is another stakeholder. The data owner provides data that can be processed by the application on behalf of the user. The data owner wants to protect its data not only accesses by access by attackers but also by direct access by the user and/or the application provider. The data owner therefore encrypts its data and only permits accesses by some explicitly specified application. SCONE supports security policies that ensure that the interests of the different stakeholders are satisfied. This is supported by security policies that are defined by the individual stakeholders. Parts of these policies can be exported to and imported from other policies. In this way, one can compose security policies of mutually distrusting stakeholders. Auditor In very critical application domains, neither the data owner nor the user might trust neither the application provider nor the SCONE platform sufficiently much, to entrust their data & computations to the application. In these domains, an auditor can help to establish trust in the application . The auditor is an independent entity that can access all source code of the application as well as the SCONE platform. The auditor works typically offline, i.e., it is not involved in any of the computations. Instead, she determines which MrEnclaves together with what file system state (i.e., tag) can be trusted by the data owner and the user. The auditor will also check the SCONE Configuration and Attestation Service (CAS): SCONE CAS enforces the security policies and manages the secrets on behalf of the SCONE platform. The auditor publishes the trusted MrEnclave and the file system tag of SCONE CAS. The data owner and the user can specify in their security policies a set of MrEnclaves, each with a specific file system tag, that they trust. Moreover, before submitting a security policy to SCONE CAS, they check that the instance of SCONE CAS has the correct MrEnclave as well as the correct initial file state. This is achieved with the help of the Intel attestation service.","title":"Overview"},{"location":"scone-use-case-overview/#overview","text":"The SCONE Platform is a generic platform to execute applications inside of trusted execution environments like Intel SGX. It can be used in different settings with multiple stakeholders that might not all trust each other: User , Application Provider , Data Owner , and Auditor . SCONE protects the IP (intellectual property) of multiple stakeholders . In this section, we introduce the different stakeholders and sketch how SCONE can be used to protect the data, code, and secrets not only from attackers but also from other stakeholders. We also give some recommendations - in the form of forward links - on how one could proceed reading about SCONE.","title":"Overview"},{"location":"scone-use-case-overview/#attacker","text":"In SCONE, we assume a very powerful attacker that has root access on the hosts on which we execute applications. The attacker can hence dump the memory of the application using standard features of the operating system. In this way, the attacker can gain access to the code, the data and all the keys that are stored in main memory. Note that the attacker can read all files of the application since operating system based access control does not apply to an attacker with root access. Moreover, any operating system based encryption of files does prevent file system accesses by the attacker either. If the application encrypts the files itself, the encryption key has to be stored somewhere in main memory - otherwise, the application would not be able to encrypt and decrypt files. An attacker can dump the main memory to get a hold of this encryption key.","title":"Attacker"},{"location":"scone-use-case-overview/#user","text":"A user wants to protect his computations, data and keys from accesses by other parties and in particular, by accesses by attackers with root access rights: Protection against read accesses by attackers protects the confidentiality of the data, keys, and code. Protection against read accesses by attackers protects the integrity of the data, keys, and code. SCONE provides a simple CLI (command line interface) to execute applications on remote hosts. This CLI supports to encrypt files on the user's computer and push these files to a remote computer. Only the application itself can access the files: SCONE performs a transparent attestation of the application to ensure that neither the application or the file system has been modified . Only then, SCONE transparently encrypts/decrypts the files. In this way, SCONE ensures the confidentiality and integrity of the user provided files.","title":"User"},{"location":"scone-use-case-overview/#application-provider","text":"An application provider can create container images like, for example, blender , a 3D creation suite. SCONE helps the application provider to run unmodified applications inside of TEEs (trusted execution environments) such that the application is protected from accesses by attackers. SCONE can also transparently encrypt/decrypt the files stored in the file system. The application container image can, for example, be created with the help of a Dockerfile and in some cases, it might need a multistage build . We describe how to build a Go program as part of this tutorial. An application provider can push its container images to a public repository like Docker hub or some private repository. The application provider can encrypt parts of the files to ensure the confidentiality and integrity of the files. For example, some of the application code might be written in Python and hence, the application provider must protect the integrity and confidentiality of that code: Actually, the situation is even more difficult. The user might want to give the application access to some encrypted input and output files (as explained above). The application provider should not have the right to access the files since this is the data that the user must protect. The user should not have access to the files provided by the application provider. In particular, SCONE manages the keys such that the SCONE runtime of the application can get access to the keys but neither the application provider nor the user will see these keys. We describe later how to do this for a simple decentralized copy application .","title":"Application Provider"},{"location":"scone-use-case-overview/#data-owner","text":"A data owner is another stakeholder. The data owner provides data that can be processed by the application on behalf of the user. The data owner wants to protect its data not only accesses by access by attackers but also by direct access by the user and/or the application provider. The data owner therefore encrypts its data and only permits accesses by some explicitly specified application. SCONE supports security policies that ensure that the interests of the different stakeholders are satisfied. This is supported by security policies that are defined by the individual stakeholders. Parts of these policies can be exported to and imported from other policies. In this way, one can compose security policies of mutually distrusting stakeholders.","title":"Data Owner"},{"location":"scone-use-case-overview/#auditor","text":"In very critical application domains, neither the data owner nor the user might trust neither the application provider nor the SCONE platform sufficiently much, to entrust their data & computations to the application. In these domains, an auditor can help to establish trust in the application . The auditor is an independent entity that can access all source code of the application as well as the SCONE platform. The auditor works typically offline, i.e., it is not involved in any of the computations. Instead, she determines which MrEnclaves together with what file system state (i.e., tag) can be trusted by the data owner and the user. The auditor will also check the SCONE Configuration and Attestation Service (CAS): SCONE CAS enforces the security policies and manages the secrets on behalf of the SCONE platform. The auditor publishes the trusted MrEnclave and the file system tag of SCONE CAS. The data owner and the user can specify in their security policies a set of MrEnclaves, each with a specific file system tag, that they trust. Moreover, before submitting a security policy to SCONE CAS, they check that the instance of SCONE CAS has the correct MrEnclave as well as the correct initial file state. This is achieved with the help of the Intel attestation service.","title":"Auditor"},{"location":"scone_affinity/","text":"SCONE Affinity CPU Affinity CPU affinity refers to the strong preference a task has for a particular CPU (core). When a task is made to always run on a particular core, the chances of experiencing cache misses reduces. This effectively improves the performance and thus throughput of a program. Affinity in SCONE SCONE has three types of threads: lthreads : these are threads that execute the application generated threads (like pthreads) inside of an enclave. ethreads : these are threads that execute the lthreads inside of enclaves. The number of ethreads has to be defined in the SCONE configuration file . SCONE performs an n:m mapping of lthreads to ethreads. This includes workstealing in the sense that before an ethread goes to sleep, it checks if another ethread has more than one lthread ready to execute. sthreads : run outside the enclave (i.e., in native mode). The sthreads execute the syscalls on behalf of the lthreads. CPU affinity in SCONE is supported by locking ethreads (as well as sthreads) to particular cores/cpus. The affinity of ethreads can be specified in the scone configuration file passed to the SCONE runtime. By default, an ethread has zero cpu preference. This is also the default behavior on Linux. An lthread has no affinity by default, but can be pinned to a particular ethread and a particular core if the ethread is pinned. \u200b \u200b By default, a lthread has zero cpu preference, even if it's parent had affinity for particular ehtread. This is the default behavior on Linux. Number of available CPUs In SCONE, then number of available CPUs is reported as the number of ethreads . This means the number of virtual cores/CPUS is determined by the default SCONE SCONE configuration file ( /etc/sgx-musl.conf ) unless the path is changed via the environment variable SCONE_CONFIG . Using CPU Affinity The standard C library (musl-libc in the case of SCONE), exposes the inquisition and setting of CPU preference by an application through two system calls. #define _GNU_SOURCE #include <sched.h> int sched_setaffinity ( pid_t pid , size_t cpusetsize , const cpu_set_t * mask ); int sched_getaffinity ( pid_t pid , size_t cpusetsize , cpu_set_t * mask ); Check man 2 SCHED_AFFINITY for a complete description. Configuring CPU affinity The CPU affinity for an arbitrary application can be effected by specifying the affinity a particular ethread has. By doing so, all the lthreads that will be locked to the ethreads, will effectively run on the CPUS on which the ethreads run. This will guarantee cpu prefrence is propagated to userspace. Author: Pamenas","title":"affinity"},{"location":"scone_affinity/#scone-affinity","text":"","title":"SCONE Affinity"},{"location":"scone_affinity/#cpu-affinity","text":"CPU affinity refers to the strong preference a task has for a particular CPU (core). When a task is made to always run on a particular core, the chances of experiencing cache misses reduces. This effectively improves the performance and thus throughput of a program.","title":"CPU Affinity"},{"location":"scone_affinity/#affinity-in-scone","text":"SCONE has three types of threads: lthreads : these are threads that execute the application generated threads (like pthreads) inside of an enclave. ethreads : these are threads that execute the lthreads inside of enclaves. The number of ethreads has to be defined in the SCONE configuration file . SCONE performs an n:m mapping of lthreads to ethreads. This includes workstealing in the sense that before an ethread goes to sleep, it checks if another ethread has more than one lthread ready to execute. sthreads : run outside the enclave (i.e., in native mode). The sthreads execute the syscalls on behalf of the lthreads. CPU affinity in SCONE is supported by locking ethreads (as well as sthreads) to particular cores/cpus. The affinity of ethreads can be specified in the scone configuration file passed to the SCONE runtime. By default, an ethread has zero cpu preference. This is also the default behavior on Linux. An lthread has no affinity by default, but can be pinned to a particular ethread and a particular core if the ethread is pinned. \u200b \u200b By default, a lthread has zero cpu preference, even if it's parent had affinity for particular ehtread. This is the default behavior on Linux.","title":"Affinity in SCONE"},{"location":"scone_affinity/#number-of-available-cpus","text":"In SCONE, then number of available CPUs is reported as the number of ethreads . This means the number of virtual cores/CPUS is determined by the default SCONE SCONE configuration file ( /etc/sgx-musl.conf ) unless the path is changed via the environment variable SCONE_CONFIG .","title":"Number of available CPUs"},{"location":"scone_affinity/#using-cpu-affinity","text":"The standard C library (musl-libc in the case of SCONE), exposes the inquisition and setting of CPU preference by an application through two system calls. #define _GNU_SOURCE #include <sched.h> int sched_setaffinity ( pid_t pid , size_t cpusetsize , const cpu_set_t * mask ); int sched_getaffinity ( pid_t pid , size_t cpusetsize , cpu_set_t * mask ); Check man 2 SCHED_AFFINITY for a complete description.","title":"Using CPU Affinity"},{"location":"scone_affinity/#configuring-cpu-affinity","text":"The CPU affinity for an arbitrary application can be effected by specifying the affinity a particular ethread has. By doing so, all the lthreads that will be locked to the ethreads, will effectively run on the CPUS on which the ethreads run. This will guarantee cpu prefrence is propagated to userspace. Author: Pamenas","title":"Configuring CPU affinity"},{"location":"scone_semantic_versioning/","text":"Scone Semantic Versioning Following Semantic Versioning , the SCONE platform is automatically assigned a version number SCONE_VERSION_NUMBER = MAJOR.MINOR.PATCH whereby MAJOR is incremented when some breaking change happens like an underlying protocol changes or we disable some deprecated feature. MINOR is incremented whenever we add a new feature that is backward compatible. PATCH is incremented whenever we fix some bug or cleanup the code. The version is automatically computed by our system, i.e., for each release our build system computes the new version of the platform. Since we started to release periodically (the current plan is every week), you will see a frequent increase in version numbers. This does not mean that clients of the SCONE platform need to update every week as well. Setting your own speed of upgrades We keep the old version of our curated docker images on docker hub and in this way, everybody can set their own speed for upgrading to newer versions of SCONE. One can stay, for example, cutting edge with the newest version of SCONE or one can upgrade only if a new major version is released. Our recommendation is to upgrade all your images at the same time to the same SCONE version. In particular, try to avoid to mix and match versions with two different major versions. Moreover, we recommend to integrate the upgrade process into your CI (Continuous Integration) pipeline: try to upgrade to newer versions automatically to ensure that you stay up-to-date with bug fixes. If you do so, pull in your application a specific image version and even better the image with the specific image digest that you tested in your CI pipeline. The most important point is to always upgrade SCONE CAS (Configuration and Attestation Service) first! This ensures that in case we introduce a breaking change that SCONE CAS can attest new applications as well as old applications (that are not yet updated). While we might have some fallback such that an older CAS can attest a newer version of an application, we strongly recommend to first upgrade CAS before upgrading an application. SCONE CURATED IMAGE VERSIONING Each of the SCONE curated images has a version number. Actually, we push the same image with multiple names to support multiple pull / upgrade strategies. For example, consider image pypy3 . We are releasing different versions of pypy3 for different versions of Alpine Linux and for different versions of the SCONE platform - just to make sure that everybody can find the version they need: Image name = {Application Name and Version}-{Operating System Version}[-SCONE Versions] For the application, we might define various versions like: pypy3 : latest supported pypy3 version - use this if you want to keep cutting edge pypy3-6 : latest supported pypy3 version with major version 6 pypy3-6.0 : latest supported pypy3 version with major version 6 and minor version 0 pypy3-6.0.0 : pypy3 version 6.0.0 - use this if you do not want to upgrade automatically For the operating system, we might define various versions like: alpine3.6 : an old, stable version of Alpine Linux alpine3.7 : a little less old, stable version of Alpine Linux ... alpine : the latest stable version of Alpine Linux that we support - use this to keep up to date with current image. alpine-edge : the newest, not yet stable version of Alpine Linux (i.e, development version of Alpine) - use this for example in development if you need the newest version of an image. For the SCONE platform itself, we will provide various versions: EMPTY : do not specify the SCONE version if you want to keep up to date with the most recent version of the SCONE platform -scone4 : latest supported SCONE version with major version 4 -scone3 : latest supported SCONE version with major version 3 ... -scone4.0 : latest supported SCONE version with major version 4 and minor version 0 -scone3.1 : latest supported SCONE version with major version 3 and minor version 1 ... -scone4.0.0 : SCONE version 4.0.0 - use this if you do not want to upgrade automatically to any other version than 4.0.0 -scone4.2.1 : SCONE version 4.2.1 - use this if you do not want to upgrade automatically to any other version than 4.2.1 ... Note that not all version combinations will be available in the community version of the SCONE platform.","title":"SCONE versioning"},{"location":"scone_semantic_versioning/#scone-semantic-versioning","text":"Following Semantic Versioning , the SCONE platform is automatically assigned a version number SCONE_VERSION_NUMBER = MAJOR.MINOR.PATCH whereby MAJOR is incremented when some breaking change happens like an underlying protocol changes or we disable some deprecated feature. MINOR is incremented whenever we add a new feature that is backward compatible. PATCH is incremented whenever we fix some bug or cleanup the code. The version is automatically computed by our system, i.e., for each release our build system computes the new version of the platform. Since we started to release periodically (the current plan is every week), you will see a frequent increase in version numbers. This does not mean that clients of the SCONE platform need to update every week as well.","title":"Scone Semantic Versioning"},{"location":"scone_semantic_versioning/#setting-your-own-speed-of-upgrades","text":"We keep the old version of our curated docker images on docker hub and in this way, everybody can set their own speed for upgrading to newer versions of SCONE. One can stay, for example, cutting edge with the newest version of SCONE or one can upgrade only if a new major version is released. Our recommendation is to upgrade all your images at the same time to the same SCONE version. In particular, try to avoid to mix and match versions with two different major versions. Moreover, we recommend to integrate the upgrade process into your CI (Continuous Integration) pipeline: try to upgrade to newer versions automatically to ensure that you stay up-to-date with bug fixes. If you do so, pull in your application a specific image version and even better the image with the specific image digest that you tested in your CI pipeline. The most important point is to always upgrade SCONE CAS (Configuration and Attestation Service) first! This ensures that in case we introduce a breaking change that SCONE CAS can attest new applications as well as old applications (that are not yet updated). While we might have some fallback such that an older CAS can attest a newer version of an application, we strongly recommend to first upgrade CAS before upgrading an application.","title":"Setting your own speed of upgrades"},{"location":"scone_semantic_versioning/#scone-curated-image-versioning","text":"Each of the SCONE curated images has a version number. Actually, we push the same image with multiple names to support multiple pull / upgrade strategies. For example, consider image pypy3 . We are releasing different versions of pypy3 for different versions of Alpine Linux and for different versions of the SCONE platform - just to make sure that everybody can find the version they need: Image name = {Application Name and Version}-{Operating System Version}[-SCONE Versions] For the application, we might define various versions like: pypy3 : latest supported pypy3 version - use this if you want to keep cutting edge pypy3-6 : latest supported pypy3 version with major version 6 pypy3-6.0 : latest supported pypy3 version with major version 6 and minor version 0 pypy3-6.0.0 : pypy3 version 6.0.0 - use this if you do not want to upgrade automatically For the operating system, we might define various versions like: alpine3.6 : an old, stable version of Alpine Linux alpine3.7 : a little less old, stable version of Alpine Linux ... alpine : the latest stable version of Alpine Linux that we support - use this to keep up to date with current image. alpine-edge : the newest, not yet stable version of Alpine Linux (i.e, development version of Alpine) - use this for example in development if you need the newest version of an image. For the SCONE platform itself, we will provide various versions: EMPTY : do not specify the SCONE version if you want to keep up to date with the most recent version of the SCONE platform -scone4 : latest supported SCONE version with major version 4 -scone3 : latest supported SCONE version with major version 3 ... -scone4.0 : latest supported SCONE version with major version 4 and minor version 0 -scone3.1 : latest supported SCONE version with major version 3 and minor version 1 ... -scone4.0.0 : SCONE version 4.0.0 - use this if you do not want to upgrade automatically to any other version than 4.0.0 -scone4.2.1 : SCONE version 4.2.1 - use this if you do not want to upgrade automatically to any other version than 4.2.1 ... Note that not all version combinations will be available in the community version of the SCONE platform.","title":"SCONE CURATED IMAGE VERSIONING"},{"location":"sconeapps_database/","text":"database Deploy a complete database topology in your Kubernetes cluster using MariaDB, MaxScale and HAProxy. Prerequisites A Kubernetes cluster; Persistent Volume (PV) provisioner support for persistence. Install the chart Add the repo If you haven't yet, please add this repo to Helm. To deploy the topology with the default parameters to your Kubernetes cluster: helm install my-database sconeapps/database The topology | v +---------+ | HAProxy | +----+----+ | (use any HAProxy load balancing) v-----------v +--------+ +--------+ |MaxScale| |MaxScale| | | | | +---+----+ +---+----+ | | (shard-based routing) +-----+------+------+ v v v v +----+ +----+ +----+ +----+ | DB | | DB | | DB | | DB | | 0 | | 1 | | 2 | | 3 | +----+ +----+ +----+ +----+ Each MariaDB instance deployed act as a shard, supporting also dynamic volumes (PVs) to provide persistence. MaxScale routes the incoming requests to the appropriate shard based on which table the operation (either a read or a write) is performed. The HAProxy (optionally) balances the load accross the MaxScale replicas. Have a look at the Parameters section for a complete list of parameters this chart supports. SGX device By default, MariaDB and MaxScale helm charts use the SGX Plugin . Hence, their respective resource limits are set as follows: resources : limits : sgx.k8s.io/sgx : 1 In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device by setting: mariadb-scone : extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx maxscale : extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into a container requires privileged mode, which will grant the container access to ALL host devices. To enable privileged mode, set securityContext for MariaDB and MaxScale: mariadb-scone : securityContext : privileged : true maxscale : securityContext : privileged : true Parameters This is an umbrella chart composed by subcharts. In fact, the parameters are defined in each subchart (e.g. mariadb, maxscale), but here are a few key parameters. Refer to each subchart in this repo for the complete list of the parameters. Any parameter defined in the umbrella chart will override the subchart value, even those that are not listed below. Parameter Description Default global.mariadb.replicaCount How many MariaDB instances to deploy. This value is used by MaxScale when generating the shard configuration, if auto-generation is enabled 1 mariadb-scone.shardNameTemplate If defined, this will be the prefix of the database created in every MariaDB instance to act as a shard. To the prefix will be added the instance number, generated by the Kubernetes StatefulSet (e.g. db0, db1...) db mariadb-scone.image MariaDB SCONE image registry.scontain.com:5050/sconecuratedimages/apps:mariadb-alpine mariadb-scone.imagePullPolicy MariaDB SCONE pull policy Always mariadb-scone.service.port MariaDB SCONE server port 3306 mariadb-scone.service.type MariaDB SCONE service type. As we are deploying MariaDB as a StatefulSet, we use a headless service. ClusterIP mariadb-scone.extraVolumes Extra volume definitions [] mariadb-scone.extraVolumeMounts Extra volume mounts for MariaDB pod [] mariadb-scone.resources CPU/Memory resource requests/limits for node. Request SGX device through the SGX device plugin. Read more {\"limits\": {\"sgx.k8s.io/sgx\": 1}} mariadb-scone.scone.attestation.enabled Enable SCONE remote attestation true mariadb-scone.scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address 172.17.0.1 mariadb-scone.scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR scone.ml mariadb-scone.scone.attestation.config_id MariaDB SCONE session. To be exported as SCONE_CONFIG_ID database_policy/db mariadb-scone.scone.attestation.env SCONE environment variables to be exported into the container SCONE_HEAP=2G,SCONE_ALLOW_DLOPEN=2,SCONE_MODE=hw,SCONE_LOG=7,SCONE_SYSLIBS=1 maxscale.replicaCount How many MaxScale replicas to deploy 1 maxscale.image MaxScale SCONE image registry.scontain.com:5050/sconecuratedimages/apps:maxscale maxscale.imagePullPolicy MaxScale SCONE pull policy IfNotPresent maxscale.service.port MaxScale listener port 3306 maxscale.service.type MaxScale service type NodePort maxscale.extraVolumes Extra volume definitions [] maxscale.extraVolumeMounts Extra volume mounts for MaxScale pod [] maxscale.resources CPU/Memory resource requests/limits for node. {} maxscale.scone.attestation.enabled Enable SCONE remote attestation false maxscale.scone.attestation.lasUseHostIP Use node host IP as LAS address true maxscale.configuration Define the static configuration the MaxScale replicas will use. The content will be rendered as-is into /etc/maxscale.cnf . Refer to MaxScale chart to read more about configuration options nil maxscale.generateConfig If enabled, let MaxScale generate its config. from provided templates true maxscale.generateConfig.servers List of IP addresses to be added to MaxScale as servers. Leave it undefined or empty and MaxScale will use the DNS entries of the MariaDB instances deployed alongside it nil maxscale.generateConfig.serverTemplate Template used to render server and monitor definitions. Now it defaults to sharded setup Sharded setup, where each instance (e.g. IP address) is considered to be a different shard maxscale.generateConfig.serviceTemplate Template used to render service and listener definitions Considers one service and one listener exposing all shards at the same port maxscale.generateConfig.cliTemplate Template used to render the CLI service for MaxScale admin tools Default one, from MaxScale docs maxscale.haproxy.enabled Deploy HAProxy Ingress Controller alongside MaxScale and expose it through a service false maxscale.haproxy.controller.service.type Type of service used to expose the HAProxy NodePort maxscale.haproxy.controller.tcp Configure TCP services for the Ingress. Please note that MaxScale will be exposed by default (HAProxy's 8000 -> MaxScale's 3306) {} maxscale.haproxy.defaultBackend.enabled Deploy a default backend service alongside the ingress controller true maxscale.haproxy.defaultBackend.image.repository Image repository for the default backend service gcr.io/google_containers/defaultbackend maxscale.haproxy.defaultBackend.image.tag Default backend service tag 1.0 serviceAccount.create Create a serviceAccount to be used by the application false useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"enabled\"","title":"database"},{"location":"sconeapps_database/#database","text":"Deploy a complete database topology in your Kubernetes cluster using MariaDB, MaxScale and HAProxy.","title":"database"},{"location":"sconeapps_database/#prerequisites","text":"A Kubernetes cluster; Persistent Volume (PV) provisioner support for persistence.","title":"Prerequisites"},{"location":"sconeapps_database/#install-the-chart","text":"","title":"Install the chart"},{"location":"sconeapps_database/#add-the-repo","text":"If you haven't yet, please add this repo to Helm. To deploy the topology with the default parameters to your Kubernetes cluster: helm install my-database sconeapps/database","title":"Add the repo"},{"location":"sconeapps_database/#the-topology","text":"| v +---------+ | HAProxy | +----+----+ | (use any HAProxy load balancing) v-----------v +--------+ +--------+ |MaxScale| |MaxScale| | | | | +---+----+ +---+----+ | | (shard-based routing) +-----+------+------+ v v v v +----+ +----+ +----+ +----+ | DB | | DB | | DB | | DB | | 0 | | 1 | | 2 | | 3 | +----+ +----+ +----+ +----+ Each MariaDB instance deployed act as a shard, supporting also dynamic volumes (PVs) to provide persistence. MaxScale routes the incoming requests to the appropriate shard based on which table the operation (either a read or a write) is performed. The HAProxy (optionally) balances the load accross the MaxScale replicas. Have a look at the Parameters section for a complete list of parameters this chart supports.","title":"The topology"},{"location":"sconeapps_database/#sgx-device","text":"By default, MariaDB and MaxScale helm charts use the SGX Plugin . Hence, their respective resource limits are set as follows: resources : limits : sgx.k8s.io/sgx : 1 In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device by setting: mariadb-scone : extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx maxscale : extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into a container requires privileged mode, which will grant the container access to ALL host devices. To enable privileged mode, set securityContext for MariaDB and MaxScale: mariadb-scone : securityContext : privileged : true maxscale : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_database/#parameters","text":"This is an umbrella chart composed by subcharts. In fact, the parameters are defined in each subchart (e.g. mariadb, maxscale), but here are a few key parameters. Refer to each subchart in this repo for the complete list of the parameters. Any parameter defined in the umbrella chart will override the subchart value, even those that are not listed below. Parameter Description Default global.mariadb.replicaCount How many MariaDB instances to deploy. This value is used by MaxScale when generating the shard configuration, if auto-generation is enabled 1 mariadb-scone.shardNameTemplate If defined, this will be the prefix of the database created in every MariaDB instance to act as a shard. To the prefix will be added the instance number, generated by the Kubernetes StatefulSet (e.g. db0, db1...) db mariadb-scone.image MariaDB SCONE image registry.scontain.com:5050/sconecuratedimages/apps:mariadb-alpine mariadb-scone.imagePullPolicy MariaDB SCONE pull policy Always mariadb-scone.service.port MariaDB SCONE server port 3306 mariadb-scone.service.type MariaDB SCONE service type. As we are deploying MariaDB as a StatefulSet, we use a headless service. ClusterIP mariadb-scone.extraVolumes Extra volume definitions [] mariadb-scone.extraVolumeMounts Extra volume mounts for MariaDB pod [] mariadb-scone.resources CPU/Memory resource requests/limits for node. Request SGX device through the SGX device plugin. Read more {\"limits\": {\"sgx.k8s.io/sgx\": 1}} mariadb-scone.scone.attestation.enabled Enable SCONE remote attestation true mariadb-scone.scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address 172.17.0.1 mariadb-scone.scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR scone.ml mariadb-scone.scone.attestation.config_id MariaDB SCONE session. To be exported as SCONE_CONFIG_ID database_policy/db mariadb-scone.scone.attestation.env SCONE environment variables to be exported into the container SCONE_HEAP=2G,SCONE_ALLOW_DLOPEN=2,SCONE_MODE=hw,SCONE_LOG=7,SCONE_SYSLIBS=1 maxscale.replicaCount How many MaxScale replicas to deploy 1 maxscale.image MaxScale SCONE image registry.scontain.com:5050/sconecuratedimages/apps:maxscale maxscale.imagePullPolicy MaxScale SCONE pull policy IfNotPresent maxscale.service.port MaxScale listener port 3306 maxscale.service.type MaxScale service type NodePort maxscale.extraVolumes Extra volume definitions [] maxscale.extraVolumeMounts Extra volume mounts for MaxScale pod [] maxscale.resources CPU/Memory resource requests/limits for node. {} maxscale.scone.attestation.enabled Enable SCONE remote attestation false maxscale.scone.attestation.lasUseHostIP Use node host IP as LAS address true maxscale.configuration Define the static configuration the MaxScale replicas will use. The content will be rendered as-is into /etc/maxscale.cnf . Refer to MaxScale chart to read more about configuration options nil maxscale.generateConfig If enabled, let MaxScale generate its config. from provided templates true maxscale.generateConfig.servers List of IP addresses to be added to MaxScale as servers. Leave it undefined or empty and MaxScale will use the DNS entries of the MariaDB instances deployed alongside it nil maxscale.generateConfig.serverTemplate Template used to render server and monitor definitions. Now it defaults to sharded setup Sharded setup, where each instance (e.g. IP address) is considered to be a different shard maxscale.generateConfig.serviceTemplate Template used to render service and listener definitions Considers one service and one listener exposing all shards at the same port maxscale.generateConfig.cliTemplate Template used to render the CLI service for MaxScale admin tools Default one, from MaxScale docs maxscale.haproxy.enabled Deploy HAProxy Ingress Controller alongside MaxScale and expose it through a service false maxscale.haproxy.controller.service.type Type of service used to expose the HAProxy NodePort maxscale.haproxy.controller.tcp Configure TCP services for the Ingress. Please note that MaxScale will be exposed by default (HAProxy's 8000 -> MaxScale's 3306) {} maxscale.haproxy.defaultBackend.enabled Deploy a default backend service alongside the ingress controller true maxscale.haproxy.defaultBackend.image.repository Image repository for the default backend service gcr.io/google_containers/defaultbackend maxscale.haproxy.defaultBackend.image.tag Default backend service tag 1.0 serviceAccount.create Create a serviceAccount to be used by the application false useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"enabled\"","title":"Parameters"},{"location":"sconeapps_maxscale/","text":"SconeApps: MaxScale Deploy a confidential MaxScale SCONE to your Kubernetes cluster. Prerequisites A Kubernetes cluster with some SGX machines. Install the chart Add the repo If you haven't yet, please add the SconeApps repo to Helm. Create a SCONE CAS policy for MaxScale By default, attestation is switched off for MaxScale. You can enable attestation by setting parameters (see Parameters ): scone.attestation.enabled to true , scone.attestation.cas to a CAS service like 5-0-0.scone-cas.cf , and scone.attestation.config_id to the policy name. Install the chart Deploy MaxScale with the default parameters to your Kubernetes cluster by executing: helm install my-maxscale sconeapps/maxscale MaxScale will be deployed as a Deployment , meaning that each replica will behave exactly the same way. Optionally, an HAProxy Ingress controller can be deployed to balance the incoming requests across all the MaxScale replicas. Have a look at the Parameters section for a complete list of parameters this chart supports. SGX device By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true Parameters A complete list of parameters this chart supports. Parameter Description Default replicaCount How many MaxScale replicas to be deployed 1 image MaxScale SCONE image registry.scontain.com:5050/sconecuratedimages/apps:maxscale-alpine-scone5.1.0 imagePullPolicy MaxScale SCONE pull policy IfNotPresent imagePullSecrets MaxScale SCONE pull secrets [{\"name\": \"sconeapps\"}] nameOverride String to partially override maxscale.fullname template with a string (will prepend the release name) nil fullNameOverride String to fully override maxscale.fullname template with a string nil scone.attestation.enabled Enable SCONE remote attestation false scone.attestation.lasUseHostIP Use node host IP as LAS address true scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address. Note that this value is not considered if lasUseHostIP is set to true . nil scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR nil scone.attestation.config_id MaxScale SCONE session. To be exported as SCONE_CONFIG_ID nil scone.attestation.env SCONE environment variables to be exported into the container nil extraEnvVars Environment variables to be injected into MaxScale container nil podSecurityContext Configure a security context for the pod nil securityContext Configure a security context for the pod {} service.port MaxScale listener port 3306 service.type MaxScale service type NodePort configuration Define the static configuration the MaxScale replicas will use. The content will be rendered as-is into /etc/maxscale.cnf . Refer to Configuration options for more information nil generateConfig.enabled If enabled, let MaxScale generate its config. from provided templates false generateConfig.servers List of IP addresses to be added to MaxScale as servers. Leave it undefined or empty and MaxScale will use the DNS entries of the MariaDB instances deployed alongside it nil generateConfig.serverTemplate Template used to render server and monitor definitions. Now it defaults to sharded setup Sharded setup, where each instance (e.g. IP address) is considered to be a different shard generateConfig.serviceTemplate Template used to render service and listener definitions Considers one service and one listener exposing all shards at the same port generateConfig.cliTemplate Template used to render the CLI service for MaxScale admin tools Default one, from MaxScale docs haproxy.enabled Deploy HAProxy Ingress Controller alongside MaxScale and expose it through a service false haproxy.controller.service.type Type of service used to expose the HAProxy NodePort haproxy.controller.tcp Configure TCP services for the Ingress. Please note that MaxScale will be exposed by default (HAProxy's 8000 -> MaxScale's 3306) {} haproxy.defaultBackend.enabled Deploy a default backend service alongside the ingress controller true haproxy.defaultBackend.image.repository Image repository for the default backend service gcr.io/google_containers/defaultbackend haproxy.defaultBackend.image.tag Default backend service tag 1.0 resources CPU/Memory resource requests/limits for node. {} nodeSelector Node labels for pod assignment (this value is evaluated as a template) {} tolerations List of node taints to tolerate (this value is evaluated as a template) [] affinity Map of node/pod affinities (The value is evaluated as a template) {} initDbScripts Specify dictionary of scripts to be run at first boot nil extraVolumes Extra volume definitions [] extraVolumeMounts Extra volume mounts for MaxScale pod [] serviceAccount.create Create a serviceAccount to be used by the application false useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil Configuration options This chart offers three different ways of configuring MaxScale replicas. They are listed below, and the former takes precedence over the latter: configuration : define an static configuration to be rendered as-is into the MaxScale containers. If defined, the other alternatives will not be considered. Put a maxscale.cnf in files/ directory: its contents will be then injected into the MaxScale containers. Auto-generated configs: If none of the previous alternatives were used and generateConfig=true , MaxScale will try to generate a configuration file for you. Configuration templates must be defined into generateConfig.serverTemplate , generateConfig.serviceTemplate and generateConfig.cliTemplate . The chart will then look at the addresses listed in generateConfig.servers to render these templates. If generateConfig.servers is empty or not defined AND MaxScale is being deployed alongside MariaDB through the umbrella chart, MaxScale will fill the addresses with the DNS entries of the MariaDB instances being deployed in the same Helm release. Of course, this last attempt will panic if MaxScale is being deployed alone. Configuring the HAProxy Ingress The optional HAProxy Ingress is, in fact, a subchart with a customized version of the upstream HAProxy Ingress chart. Few changes were made to the templates, as we need MaxScale exposed as a TCP service. If you want to see the complete set of parameters for the HAProxy Ingress controller, please have a look at the original chart .","title":"maxscale"},{"location":"sconeapps_maxscale/#sconeapps-maxscale","text":"Deploy a confidential MaxScale SCONE to your Kubernetes cluster.","title":"SconeApps: MaxScale"},{"location":"sconeapps_maxscale/#prerequisites","text":"A Kubernetes cluster with some SGX machines.","title":"Prerequisites"},{"location":"sconeapps_maxscale/#install-the-chart","text":"","title":"Install the chart"},{"location":"sconeapps_maxscale/#add-the-repo","text":"If you haven't yet, please add the SconeApps repo to Helm.","title":"Add the repo"},{"location":"sconeapps_maxscale/#create-a-scone-cas-policy-for-maxscale","text":"By default, attestation is switched off for MaxScale. You can enable attestation by setting parameters (see Parameters ): scone.attestation.enabled to true , scone.attestation.cas to a CAS service like 5-0-0.scone-cas.cf , and scone.attestation.config_id to the policy name.","title":"Create a SCONE CAS policy for MaxScale"},{"location":"sconeapps_maxscale/#install-the-chart_1","text":"Deploy MaxScale with the default parameters to your Kubernetes cluster by executing: helm install my-maxscale sconeapps/maxscale MaxScale will be deployed as a Deployment , meaning that each replica will behave exactly the same way. Optionally, an HAProxy Ingress controller can be deployed to balance the incoming requests across all the MaxScale replicas. Have a look at the Parameters section for a complete list of parameters this chart supports.","title":"Install the chart"},{"location":"sconeapps_maxscale/#sgx-device","text":"By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_maxscale/#parameters","text":"A complete list of parameters this chart supports. Parameter Description Default replicaCount How many MaxScale replicas to be deployed 1 image MaxScale SCONE image registry.scontain.com:5050/sconecuratedimages/apps:maxscale-alpine-scone5.1.0 imagePullPolicy MaxScale SCONE pull policy IfNotPresent imagePullSecrets MaxScale SCONE pull secrets [{\"name\": \"sconeapps\"}] nameOverride String to partially override maxscale.fullname template with a string (will prepend the release name) nil fullNameOverride String to fully override maxscale.fullname template with a string nil scone.attestation.enabled Enable SCONE remote attestation false scone.attestation.lasUseHostIP Use node host IP as LAS address true scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address. Note that this value is not considered if lasUseHostIP is set to true . nil scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR nil scone.attestation.config_id MaxScale SCONE session. To be exported as SCONE_CONFIG_ID nil scone.attestation.env SCONE environment variables to be exported into the container nil extraEnvVars Environment variables to be injected into MaxScale container nil podSecurityContext Configure a security context for the pod nil securityContext Configure a security context for the pod {} service.port MaxScale listener port 3306 service.type MaxScale service type NodePort configuration Define the static configuration the MaxScale replicas will use. The content will be rendered as-is into /etc/maxscale.cnf . Refer to Configuration options for more information nil generateConfig.enabled If enabled, let MaxScale generate its config. from provided templates false generateConfig.servers List of IP addresses to be added to MaxScale as servers. Leave it undefined or empty and MaxScale will use the DNS entries of the MariaDB instances deployed alongside it nil generateConfig.serverTemplate Template used to render server and monitor definitions. Now it defaults to sharded setup Sharded setup, where each instance (e.g. IP address) is considered to be a different shard generateConfig.serviceTemplate Template used to render service and listener definitions Considers one service and one listener exposing all shards at the same port generateConfig.cliTemplate Template used to render the CLI service for MaxScale admin tools Default one, from MaxScale docs haproxy.enabled Deploy HAProxy Ingress Controller alongside MaxScale and expose it through a service false haproxy.controller.service.type Type of service used to expose the HAProxy NodePort haproxy.controller.tcp Configure TCP services for the Ingress. Please note that MaxScale will be exposed by default (HAProxy's 8000 -> MaxScale's 3306) {} haproxy.defaultBackend.enabled Deploy a default backend service alongside the ingress controller true haproxy.defaultBackend.image.repository Image repository for the default backend service gcr.io/google_containers/defaultbackend haproxy.defaultBackend.image.tag Default backend service tag 1.0 resources CPU/Memory resource requests/limits for node. {} nodeSelector Node labels for pod assignment (this value is evaluated as a template) {} tolerations List of node taints to tolerate (this value is evaluated as a template) [] affinity Map of node/pod affinities (The value is evaluated as a template) {} initDbScripts Specify dictionary of scripts to be run at first boot nil extraVolumes Extra volume definitions [] extraVolumeMounts Extra volume mounts for MaxScale pod [] serviceAccount.create Create a serviceAccount to be used by the application false useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil","title":"Parameters"},{"location":"sconeapps_maxscale/#configuration-options","text":"This chart offers three different ways of configuring MaxScale replicas. They are listed below, and the former takes precedence over the latter: configuration : define an static configuration to be rendered as-is into the MaxScale containers. If defined, the other alternatives will not be considered. Put a maxscale.cnf in files/ directory: its contents will be then injected into the MaxScale containers. Auto-generated configs: If none of the previous alternatives were used and generateConfig=true , MaxScale will try to generate a configuration file for you. Configuration templates must be defined into generateConfig.serverTemplate , generateConfig.serviceTemplate and generateConfig.cliTemplate . The chart will then look at the addresses listed in generateConfig.servers to render these templates. If generateConfig.servers is empty or not defined AND MaxScale is being deployed alongside MariaDB through the umbrella chart, MaxScale will fill the addresses with the DNS entries of the MariaDB instances being deployed in the same Helm release. Of course, this last attempt will panic if MaxScale is being deployed alone.","title":"Configuration options"},{"location":"sconeapps_maxscale/#configuring-the-haproxy-ingress","text":"The optional HAProxy Ingress is, in fact, a subchart with a customized version of the upstream HAProxy Ingress chart. Few changes were made to the templates, as we need MaxScale exposed as a TCP service. If you want to see the complete set of parameters for the HAProxy Ingress controller, please have a look at the original chart .","title":"Configuring the HAProxy Ingress"},{"location":"sconeapps_memcached/","text":"SconeApps: memcached Deploys a confidential memcached to your Kubernetes cluster. Prerequisites A Kubernetes cluster with access to SGX machines Install the chart Add the repo If you haven't yet, please add the SconeApps repo to Helm. Create a SCONE CAS policy for memcached The default policy name for the memcached chart is set to be memcached_policy/memcached and the default SCONE is defined to be 5-0-0.scone-cas.cf . You can overwrite these defaults by setting parameters scone.attestation.cas and scone.attestation.MEMCACHEDConfigID (see Parameters ). See secure document management for details about how to create a policy that defines TLS certificates for memcached . Install the chart To deploy memcached SCONE with the default parameters to your Kubernetes cluster: helm install my-memcached sconeapps/memcached Have a look at the Parameters section for a complete list of parameters this chart supports. SGX device By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true Testing your chart To test that the memcached is working as expected, simply run: kubectl --namespace default port-forward svc/my-memcached 11211 :11211 & echo stats | nc 127 .0.0.1 11211 You should see something like: STAT pid 1 STAT uptime 28 STAT time 1608215390 STAT version 1 .6.7 ... END Accessing your memcached This chart supports two service types for memcached: ClusterIP and NodePort . ClusterIP If service.type is set to \"ClusterIP\", a headless service will be created for memcached, which means that each memcached instance can only be accessed from within the cluster, through its internal DNS name (e.g., RELEASE-memcached-scone-7dbb5d97c7-ktt78 ). You can access the pod as follows: kubectl --namespace default port-forward svc/my-memcached 11211 :11211 & NodePort You can also set service.type to NodePort , and the memcached instance will be exposed to the outside through the same port in every worker node. By default, the port is randomly assigned by Kubernetes. You can define a custom port by setting service.nodePort , but keep in mind that this port must be available (service creation will fail otherwise). You can then access the memcached as follows: Get the application URL by running these commands: export NODE_PORT = $( kubectl get --namespace default -o jsonpath = \"{.spec.ports[0].nodePort}\" services my-memcached-memcached-scone ) export NODE_IP = $( kubectl get nodes --namespace default -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) echo http:// $NODE_IP : $NODE_PORT You can then manually check that the memcached is working by: curl -o /dev/null -s -w \"%{http_code}\" http:// $NODE_IP : $NODE_PORT It should then return 200 Internal DNS entries (e.g., RELEASE-memcached-scone-7dbb5d97c7-ktt78 ) will still be created, so you can use them when contacting memcached from inside the cluster. Parameters The following tables lists the configurable parameters of the Memcached chart and their default values. Parameter Description Default global.imageRegistry Global Docker image registry nil global.imagePullSecrets Global Docker registry secret names as an array [] (does not add image pull secrets to deployed pods) image.registry Memcached image registry registry.scontain.com:5050 image.repository Memcached Image name sconecuratedimages/apps image.tag Memcached Image tag memcached-1.6.7-alpine-scone5 image.pullPolicy Memcached image pull policy Always image.pullSecrets Specify docker-registry secret names as an array [sconeapps] (does not add image pull secrets to deployed pods) scone.attestation.enabled Enable SCONE remote attestation true scone.attestation.lasUseHostIP Use node host IP as LAS address true scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address. Note that this value is not considered if lasUseHostIP is set to true . nil scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR 5-0-0.scone-cas.cf scone.attestation.MEMCACHEDConfigID memcached SCONE session. To be exported as SCONE_CONFIG_ID memcached_policy/memcached scone.attestation.env SCONE environment variables to be exported into the container SCONE_HEAP=2G,SCONE_MODE=hw useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil extraEnv Additional env vars to pass {} replicaCount Number of containers 1 clusterDomain Kubernetes cluster domain cluster.local nameOverride String to partially override memcached.fullname template with a string nil fullnameOverride String to fully override memcached.fullname template with a string nil arguments Arguments to pass [\"/run.sh\"] service.type Kubernetes service type for Memcached ClusterIP service.port Memcached service port 11211 service.clusterIP Specific cluster IP when service type is cluster IP. Use None for headless service nil service.nodePort Kubernetes Service nodePort nil service.loadBalancerIP loadBalancerIP if service type is LoadBalancer nil service.annotations Additional annotations for Memcached service {} resources.requests CPU/Memory resource requests {memory: \"256Mi\", cpu: \"250m\"} resources.limits CPU/Memory resource limits {} persistence.enabled Enable persistence using PVC (Requires architecture: \"high-availability\") true persistence.storageClass PVC Storage Class for Memcached volume nil (uses alpha storage class annotation) persistence.accessMode PVC Access Mode for Memcached volume ReadWriteOnce persistence.size PVC Storage Request for Memcached volume 8Gi podAnnotations Pod annotations {} podAffinityPreset Pod affinity preset. Ignored if affinity is set. Allowed values: soft or hard \"\" podAntiAffinityPreset Pod anti-affinity preset. Ignored if affinity is set. Allowed values: soft or hard soft nodeAffinityPreset.type Node affinity preset type. Ignored if affinity is set. Allowed values: soft or hard \"\" nodeAffinityPreset.key Node label key to match. Ignored if affinity is set. \"\" nodeAffinityPreset.values Node label values to match. Ignored if affinity is set. [] affinity Affinity for pod assignment {} (evaluated as a template) nodeSelector Node labels for pod assignment {} (evaluated as a template) tolerations Tolerations for pod assignment [] (evaluated as a template) priorityClassName Controller priorityClassName nil Set Parameters Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, helm install my-release --set scone.attestation.MEMCACHEDConfigID=my_name_space/memcached_policy,scone.attestation.cas=cas.example.com sconeapps/memcached The above command sets the policy to my_name_space/memcached_policy and using the CAS cas.example.com . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, helm install my-release -f values.yaml sconeapps/memcached One can use the default values.yaml in the SconeApps repo","title":"memcached"},{"location":"sconeapps_memcached/#sconeapps-memcached","text":"Deploys a confidential memcached to your Kubernetes cluster.","title":"SconeApps: memcached"},{"location":"sconeapps_memcached/#prerequisites","text":"A Kubernetes cluster with access to SGX machines","title":"Prerequisites"},{"location":"sconeapps_memcached/#install-the-chart","text":"","title":"Install the chart"},{"location":"sconeapps_memcached/#add-the-repo","text":"If you haven't yet, please add the SconeApps repo to Helm.","title":"Add the repo"},{"location":"sconeapps_memcached/#create-a-scone-cas-policy-for-memcached","text":"The default policy name for the memcached chart is set to be memcached_policy/memcached and the default SCONE is defined to be 5-0-0.scone-cas.cf . You can overwrite these defaults by setting parameters scone.attestation.cas and scone.attestation.MEMCACHEDConfigID (see Parameters ). See secure document management for details about how to create a policy that defines TLS certificates for memcached .","title":"Create a SCONE CAS policy for memcached"},{"location":"sconeapps_memcached/#install-the-chart_1","text":"To deploy memcached SCONE with the default parameters to your Kubernetes cluster: helm install my-memcached sconeapps/memcached Have a look at the Parameters section for a complete list of parameters this chart supports.","title":"Install the chart"},{"location":"sconeapps_memcached/#sgx-device","text":"By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_memcached/#testing-your-chart","text":"To test that the memcached is working as expected, simply run: kubectl --namespace default port-forward svc/my-memcached 11211 :11211 & echo stats | nc 127 .0.0.1 11211 You should see something like: STAT pid 1 STAT uptime 28 STAT time 1608215390 STAT version 1 .6.7 ... END","title":"Testing your chart"},{"location":"sconeapps_memcached/#accessing-your-memcached","text":"This chart supports two service types for memcached: ClusterIP and NodePort .","title":"Accessing your memcached"},{"location":"sconeapps_memcached/#clusterip","text":"If service.type is set to \"ClusterIP\", a headless service will be created for memcached, which means that each memcached instance can only be accessed from within the cluster, through its internal DNS name (e.g., RELEASE-memcached-scone-7dbb5d97c7-ktt78 ). You can access the pod as follows: kubectl --namespace default port-forward svc/my-memcached 11211 :11211 &","title":"ClusterIP"},{"location":"sconeapps_memcached/#nodeport","text":"You can also set service.type to NodePort , and the memcached instance will be exposed to the outside through the same port in every worker node. By default, the port is randomly assigned by Kubernetes. You can define a custom port by setting service.nodePort , but keep in mind that this port must be available (service creation will fail otherwise). You can then access the memcached as follows: Get the application URL by running these commands: export NODE_PORT = $( kubectl get --namespace default -o jsonpath = \"{.spec.ports[0].nodePort}\" services my-memcached-memcached-scone ) export NODE_IP = $( kubectl get nodes --namespace default -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) echo http:// $NODE_IP : $NODE_PORT You can then manually check that the memcached is working by: curl -o /dev/null -s -w \"%{http_code}\" http:// $NODE_IP : $NODE_PORT It should then return 200 Internal DNS entries (e.g., RELEASE-memcached-scone-7dbb5d97c7-ktt78 ) will still be created, so you can use them when contacting memcached from inside the cluster.","title":"NodePort"},{"location":"sconeapps_memcached/#parameters","text":"The following tables lists the configurable parameters of the Memcached chart and their default values. Parameter Description Default global.imageRegistry Global Docker image registry nil global.imagePullSecrets Global Docker registry secret names as an array [] (does not add image pull secrets to deployed pods) image.registry Memcached image registry registry.scontain.com:5050 image.repository Memcached Image name sconecuratedimages/apps image.tag Memcached Image tag memcached-1.6.7-alpine-scone5 image.pullPolicy Memcached image pull policy Always image.pullSecrets Specify docker-registry secret names as an array [sconeapps] (does not add image pull secrets to deployed pods) scone.attestation.enabled Enable SCONE remote attestation true scone.attestation.lasUseHostIP Use node host IP as LAS address true scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address. Note that this value is not considered if lasUseHostIP is set to true . nil scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR 5-0-0.scone-cas.cf scone.attestation.MEMCACHEDConfigID memcached SCONE session. To be exported as SCONE_CONFIG_ID memcached_policy/memcached scone.attestation.env SCONE environment variables to be exported into the container SCONE_HEAP=2G,SCONE_MODE=hw useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil extraEnv Additional env vars to pass {} replicaCount Number of containers 1 clusterDomain Kubernetes cluster domain cluster.local nameOverride String to partially override memcached.fullname template with a string nil fullnameOverride String to fully override memcached.fullname template with a string nil arguments Arguments to pass [\"/run.sh\"] service.type Kubernetes service type for Memcached ClusterIP service.port Memcached service port 11211 service.clusterIP Specific cluster IP when service type is cluster IP. Use None for headless service nil service.nodePort Kubernetes Service nodePort nil service.loadBalancerIP loadBalancerIP if service type is LoadBalancer nil service.annotations Additional annotations for Memcached service {} resources.requests CPU/Memory resource requests {memory: \"256Mi\", cpu: \"250m\"} resources.limits CPU/Memory resource limits {} persistence.enabled Enable persistence using PVC (Requires architecture: \"high-availability\") true persistence.storageClass PVC Storage Class for Memcached volume nil (uses alpha storage class annotation) persistence.accessMode PVC Access Mode for Memcached volume ReadWriteOnce persistence.size PVC Storage Request for Memcached volume 8Gi podAnnotations Pod annotations {} podAffinityPreset Pod affinity preset. Ignored if affinity is set. Allowed values: soft or hard \"\" podAntiAffinityPreset Pod anti-affinity preset. Ignored if affinity is set. Allowed values: soft or hard soft nodeAffinityPreset.type Node affinity preset type. Ignored if affinity is set. Allowed values: soft or hard \"\" nodeAffinityPreset.key Node label key to match. Ignored if affinity is set. \"\" nodeAffinityPreset.values Node label values to match. Ignored if affinity is set. [] affinity Affinity for pod assignment {} (evaluated as a template) nodeSelector Node labels for pod assignment {} (evaluated as a template) tolerations Tolerations for pod assignment [] (evaluated as a template) priorityClassName Controller priorityClassName nil","title":"Parameters"},{"location":"sconeapps_memcached/#set-parameters","text":"Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, helm install my-release --set scone.attestation.MEMCACHEDConfigID=my_name_space/memcached_policy,scone.attestation.cas=cas.example.com sconeapps/memcached The above command sets the policy to my_name_space/memcached_policy and using the CAS cas.example.com . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, helm install my-release -f values.yaml sconeapps/memcached One can use the default values.yaml in the SconeApps repo","title":"Set Parameters"},{"location":"sconeapps_nginx/","text":"SconeApps: nginx Deploy a secure nginx to your Kubernetes cluster. Prerequisites A Kubernetes cluster; Install the chart Add the repo If you haven't yet, please add the SconeApps repo to Helm. Create a SCONE CAS policy for nginx The default policy name for the memcached chart is set to be nginx_policy/nginx and the default SCONE is defined to be 5-0-0.scone-cas.cf . You can overwrite these defaults by setting parameters scone.attestation.cas and scone.attestation.NGINXConfigID (see Parameters ). See secure document management for details about how to create a policy that defines TLS certificates for nginx . Install the chart To deploy nginx SCONE with the default parameters to your Kubernetes cluster: helm install my-nginx sconeapps/nginx Have a look at the Parameters section for a complete list of parameters this chart supports. SGX device By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true Testing your chart This chart includes tests to make sure your nginx is working as expected. This current version does not yet possess a corresponding policy, and thus scone.attestation is current not enabled. To run the tests, simply run: helm test my-nginx Helm will print the result of the tests: NAME: my-nginx LAST DEPLOYED: Wed Dec 16 11:15:47 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: my-nginx-nginx-scone-test-connection Last Started: Wed Dec 16 11:16:24 2020 Last Completed: Wed Dec 16 11:16:29 2020 Phase: Succeeded If the tests are successful, Helm will delete the deployed tests resources automatically. Accessing your nginx This chart supports two service types for nginx: ClusterIP and NodePort . ClusterIP If service.type is set to \"ClusterIP\", a headless service will be created for nginx, which means that each nginx instance can only be accessed from within the cluster, through its internal DNS name (e.g., RELEASE-nginx-scone-7dbb5d97c7-ktt78 ). You can access the pod as follows: Get the application URL by running these commands: export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/name=nginx-scone,app.kubernetes.io/instance=chart-1608114800\" -o jsonpath = \"{.items[0].metadata.name}\" ) echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace default port-forward $POD_NAME 8080 :80 You can then manually check that the nginx is working by: curl -o /dev/null -s -w \"%{http_code}\" 127 .0.0.1:8080 It should then return 200 NodePort You can also set service.type to NodePort , and the nginx instance will be exposed to the outside through the same port in every worker node. By default, the port is randomly assigned by Kubernetes. You can define a custom port by setting service.nodePort , but keep in mind that this port must be available (service creation will fail otherwise). You can then access the nginx as follows: Get the application URL by running these commands: export NODE_PORT = $( kubectl get --namespace default -o jsonpath = \"{.spec.ports[0].nodePort}\" services my-nginx-nginx-scone ) export NODE_IP = $( kubectl get nodes --namespace default -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) echo http:// $NODE_IP : $NODE_PORT You can then manually check that the nginx is working by: curl -o /dev/null -s -w \"%{http_code}\" http:// $NODE_IP : $NODE_PORT It should then return 200 Internal DNS entries (e.g., RELEASE-nginx-scone-7dbb5d97c7-ktt78 ) will still be created, so you can use them when contacting nginx from inside the cluster. Parameters A complete list of parameters this chart supports. Parameter Description Default replicaCount How many nginx instances to deploy 1 image.repository nginx SCONE repository registry.scontain.com:5050/sconecuratedimages/apps image.pullPolicy nginx SCONE pull policy Always image.tag nginx SCONE tag nginx-1.14.2-alpine-scone5 imagePullSecrets.name nginx SCONE pull secrets [{\"name\": \"sconeapps\"}] scone.attestation.enabled Enable SCONE remote attestation false scone.attestation.lasUseHostIP Use node host IP as LAS address true scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address. Note that this value is not considered if lasUseHostIP is set to true . nil scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR 5-0-0.scone-cas.cf scone.attestation.NGINXConfigID nginx SCONE session. To be exported as SCONE_CONFIG_ID nginx_policy/nginx scone.attestation.env SCONE environment variables to be exported into the container SCONE_HEAP=2G,SCONE_FORK=1,SCONE_MODE=hw useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil volume.enabled Enables a volume in the pod to load custom nginx configuration false volume.name Name of the nginx config volume nginx-config volume.volumeMounts.mountPath The path within the nginx container where the config is to be mounted /etc/nginx volume.hostPath.path The within the host to the nginx config /data/nginx volume.hostPath.type The type of the object accessed by the hostPath Directory serviceAccount.create Create a serviceAccount to be used by the application false serviceAccount.annotations Set annotations for the service account {} serviceAccount.name Set a name for the service account \"\" podAnnotations Define annotations for the pod nil podSecurityContext Configure a security context for the pod nil securityContext Configure a security context for the pod {} service.type nginx SCONE service type. Use ClusterIP for a headless service. Use NodePort to have it exposed to the outside of the cluster. ClusterIP service.port nginx SCONE server port 80 service.nodePort Set a custom port to be used as NodePort. It must be available on all nodes nil ingress.enabled Enable ingress controller resource false ingress.annotations Ingress annotations {} ingress.hosts.host Default host for the ingress resource chart-example.local ingress.hosts.tls Enable TLS configuration for the hostname defined at ingress.hosts.host parameter [] resources CPU/Memory resource requests/limits for node. {} autoscaling.enabled Enable autoscaling for nginx deployment false autoscaling.minReplicas Minimum number of replicas to scale back 1 autoscaling.maxReplicas Maximum number of replicas to scale out 100 autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage 80 nodeSelector Node labels for pod assignment (this value is evaluated as a template) {} tolerations List of node taints to tolerate (this value is evaluated as a template) [] affinity Map of node/pod affinities (The value is evaluated as a template) {} Set Parameters Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, helm install my-nginx-release --set scone.attestation.NGINXConfigID=my_name_space/nginx_policy,scone.attestation.cas=cas.example.com sconeapps/nginx The above command sets the policy to my_name_space/memcached_policy and using the CAS cas.example.com . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, helm install my-nginx-release -f values.yaml sconeapps/nginx One can use the default values.yaml in the SconeApps repo","title":"nginx"},{"location":"sconeapps_nginx/#sconeapps-nginx","text":"Deploy a secure nginx to your Kubernetes cluster.","title":"SconeApps: nginx"},{"location":"sconeapps_nginx/#prerequisites","text":"A Kubernetes cluster;","title":"Prerequisites"},{"location":"sconeapps_nginx/#install-the-chart","text":"","title":"Install the chart"},{"location":"sconeapps_nginx/#add-the-repo","text":"If you haven't yet, please add the SconeApps repo to Helm.","title":"Add the repo"},{"location":"sconeapps_nginx/#create-a-scone-cas-policy-for-nginx","text":"The default policy name for the memcached chart is set to be nginx_policy/nginx and the default SCONE is defined to be 5-0-0.scone-cas.cf . You can overwrite these defaults by setting parameters scone.attestation.cas and scone.attestation.NGINXConfigID (see Parameters ). See secure document management for details about how to create a policy that defines TLS certificates for nginx .","title":"Create a SCONE CAS policy for nginx"},{"location":"sconeapps_nginx/#install-the-chart_1","text":"To deploy nginx SCONE with the default parameters to your Kubernetes cluster: helm install my-nginx sconeapps/nginx Have a look at the Parameters section for a complete list of parameters this chart supports.","title":"Install the chart"},{"location":"sconeapps_nginx/#sgx-device","text":"By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_nginx/#testing-your-chart","text":"This chart includes tests to make sure your nginx is working as expected. This current version does not yet possess a corresponding policy, and thus scone.attestation is current not enabled. To run the tests, simply run: helm test my-nginx Helm will print the result of the tests: NAME: my-nginx LAST DEPLOYED: Wed Dec 16 11:15:47 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: my-nginx-nginx-scone-test-connection Last Started: Wed Dec 16 11:16:24 2020 Last Completed: Wed Dec 16 11:16:29 2020 Phase: Succeeded If the tests are successful, Helm will delete the deployed tests resources automatically.","title":"Testing your chart"},{"location":"sconeapps_nginx/#accessing-your-nginx","text":"This chart supports two service types for nginx: ClusterIP and NodePort .","title":"Accessing your nginx"},{"location":"sconeapps_nginx/#clusterip","text":"If service.type is set to \"ClusterIP\", a headless service will be created for nginx, which means that each nginx instance can only be accessed from within the cluster, through its internal DNS name (e.g., RELEASE-nginx-scone-7dbb5d97c7-ktt78 ). You can access the pod as follows: Get the application URL by running these commands: export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/name=nginx-scone,app.kubernetes.io/instance=chart-1608114800\" -o jsonpath = \"{.items[0].metadata.name}\" ) echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace default port-forward $POD_NAME 8080 :80 You can then manually check that the nginx is working by: curl -o /dev/null -s -w \"%{http_code}\" 127 .0.0.1:8080 It should then return 200","title":"ClusterIP"},{"location":"sconeapps_nginx/#nodeport","text":"You can also set service.type to NodePort , and the nginx instance will be exposed to the outside through the same port in every worker node. By default, the port is randomly assigned by Kubernetes. You can define a custom port by setting service.nodePort , but keep in mind that this port must be available (service creation will fail otherwise). You can then access the nginx as follows: Get the application URL by running these commands: export NODE_PORT = $( kubectl get --namespace default -o jsonpath = \"{.spec.ports[0].nodePort}\" services my-nginx-nginx-scone ) export NODE_IP = $( kubectl get nodes --namespace default -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) echo http:// $NODE_IP : $NODE_PORT You can then manually check that the nginx is working by: curl -o /dev/null -s -w \"%{http_code}\" http:// $NODE_IP : $NODE_PORT It should then return 200 Internal DNS entries (e.g., RELEASE-nginx-scone-7dbb5d97c7-ktt78 ) will still be created, so you can use them when contacting nginx from inside the cluster.","title":"NodePort"},{"location":"sconeapps_nginx/#parameters","text":"A complete list of parameters this chart supports. Parameter Description Default replicaCount How many nginx instances to deploy 1 image.repository nginx SCONE repository registry.scontain.com:5050/sconecuratedimages/apps image.pullPolicy nginx SCONE pull policy Always image.tag nginx SCONE tag nginx-1.14.2-alpine-scone5 imagePullSecrets.name nginx SCONE pull secrets [{\"name\": \"sconeapps\"}] scone.attestation.enabled Enable SCONE remote attestation false scone.attestation.lasUseHostIP Use node host IP as LAS address true scone.attestation.las LAS address, to be exported as SCONE_LAS_ADDR . Defaults to the Docker network interface address. Note that this value is not considered if lasUseHostIP is set to true . nil scone.attestation.cas CAS address, to be exported as SCONE_CAS_ADDR 5-0-0.scone-cas.cf scone.attestation.NGINXConfigID nginx SCONE session. To be exported as SCONE_CONFIG_ID nginx_policy/nginx scone.attestation.env SCONE environment variables to be exported into the container SCONE_HEAP=2G,SCONE_FORK=1,SCONE_MODE=hw useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil volume.enabled Enables a volume in the pod to load custom nginx configuration false volume.name Name of the nginx config volume nginx-config volume.volumeMounts.mountPath The path within the nginx container where the config is to be mounted /etc/nginx volume.hostPath.path The within the host to the nginx config /data/nginx volume.hostPath.type The type of the object accessed by the hostPath Directory serviceAccount.create Create a serviceAccount to be used by the application false serviceAccount.annotations Set annotations for the service account {} serviceAccount.name Set a name for the service account \"\" podAnnotations Define annotations for the pod nil podSecurityContext Configure a security context for the pod nil securityContext Configure a security context for the pod {} service.type nginx SCONE service type. Use ClusterIP for a headless service. Use NodePort to have it exposed to the outside of the cluster. ClusterIP service.port nginx SCONE server port 80 service.nodePort Set a custom port to be used as NodePort. It must be available on all nodes nil ingress.enabled Enable ingress controller resource false ingress.annotations Ingress annotations {} ingress.hosts.host Default host for the ingress resource chart-example.local ingress.hosts.tls Enable TLS configuration for the hostname defined at ingress.hosts.host parameter [] resources CPU/Memory resource requests/limits for node. {} autoscaling.enabled Enable autoscaling for nginx deployment false autoscaling.minReplicas Minimum number of replicas to scale back 1 autoscaling.maxReplicas Maximum number of replicas to scale out 100 autoscaling.targetCPUUtilizationPercentage Target CPU utilization percentage 80 nodeSelector Node labels for pod assignment (this value is evaluated as a template) {} tolerations List of node taints to tolerate (this value is evaluated as a template) [] affinity Map of node/pod affinities (The value is evaluated as a template) {}","title":"Parameters"},{"location":"sconeapps_nginx/#set-parameters","text":"Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, helm install my-nginx-release --set scone.attestation.NGINXConfigID=my_name_space/nginx_policy,scone.attestation.cas=cas.example.com sconeapps/nginx The above command sets the policy to my_name_space/memcached_policy and using the CAS cas.example.com . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, helm install my-nginx-release -f values.yaml sconeapps/nginx One can use the default values.yaml in the SconeApps repo","title":"Set Parameters"},{"location":"sconeapps_openvino/","text":"scone-openvino chart To see the customizations this chart currently supports, have a look at values.yaml . Prerequisites A Kubernetes cluster. Helm 3 installed. Before you begin Submit any SCONE sessions (e.g., the one located in ../../policies ) to a CAS that is accessible from within the cluster. Reference them through extraEnv section. Create the directories you need in the worker nodes, as this application relies on hostPath volumes. They are defined in extraVolumes section. Clone this git repository. Install the chart Add the repo If you haven't yet, please add this repo to Helm. To deploy OpenVINO with the default parameters to your Kubernetes cluster: helm install my-openvino sconeapps/openvino See your deployed pods by running kubectl get pods | grep my-openvino . SGX device By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"openvino"},{"location":"sconeapps_openvino/#scone-openvino-chart","text":"To see the customizations this chart currently supports, have a look at values.yaml .","title":"scone-openvino chart"},{"location":"sconeapps_openvino/#prerequisites","text":"A Kubernetes cluster. Helm 3 installed.","title":"Prerequisites"},{"location":"sconeapps_openvino/#before-you-begin","text":"Submit any SCONE sessions (e.g., the one located in ../../policies ) to a CAS that is accessible from within the cluster. Reference them through extraEnv section. Create the directories you need in the worker nodes, as this application relies on hostPath volumes. They are defined in extraVolumes section. Clone this git repository.","title":"Before you begin"},{"location":"sconeapps_openvino/#install-the-chart","text":"","title":"Install the chart"},{"location":"sconeapps_openvino/#add-the-repo","text":"If you haven't yet, please add this repo to Helm. To deploy OpenVINO with the default parameters to your Kubernetes cluster: helm install my-openvino sconeapps/openvino See your deployed pods by running kubectl get pods | grep my-openvino .","title":"Add the repo"},{"location":"sconeapps_openvino/#sgx-device","text":"By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_pytorch/","text":"scone-pytorch chart To see the customizations this chart currently supports, have a look at values.yaml . Prerequisites A Kubernetes cluster. Helm 3 installed. Before you begin Submit any SCONE sessions (e.g., the one located in ../../policies ) to a CAS that is accessible from within the cluster. Reference them through extraEnv section. Create the directories you need in the worker nodes, as this application relies on hostPath volumes. They are defined in extraVolumes section. Clone this git repository. Install the chart Add the repo If you haven't yet, please add this repo to Helm. To deploy PyTorch with the default parameters to your Kubernetes cluster: helm install my-pytorch sconeapps/pytorch See your deployed pods by running kubectl get pods | grep my-pytorch . SGX device By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"pytorch"},{"location":"sconeapps_pytorch/#scone-pytorch-chart","text":"To see the customizations this chart currently supports, have a look at values.yaml .","title":"scone-pytorch chart"},{"location":"sconeapps_pytorch/#prerequisites","text":"A Kubernetes cluster. Helm 3 installed.","title":"Prerequisites"},{"location":"sconeapps_pytorch/#before-you-begin","text":"Submit any SCONE sessions (e.g., the one located in ../../policies ) to a CAS that is accessible from within the cluster. Reference them through extraEnv section. Create the directories you need in the worker nodes, as this application relies on hostPath volumes. They are defined in extraVolumes section. Clone this git repository.","title":"Before you begin"},{"location":"sconeapps_pytorch/#install-the-chart","text":"","title":"Install the chart"},{"location":"sconeapps_pytorch/#add-the-repo","text":"If you haven't yet, please add this repo to Helm. To deploy PyTorch with the default parameters to your Kubernetes cluster: helm install my-pytorch sconeapps/pytorch See your deployed pods by running kubectl get pods | grep my-pytorch .","title":"Add the repo"},{"location":"sconeapps_pytorch/#sgx-device","text":"By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_spark/","text":"Apache Spark Apache Spark is a high-performance engine for large-scale computing tasks, such as data processing, machine learning and real-time data streaming. It includes APIs for Java, Python, Scala and R. TL;DR; helm install my-spark sconeapps/spark Introduction This chart bootstraps a spark deployment on a Kubernetes cluster using the Helm package manager. Prerequisites Kubernetes 1.12+ Helm 2.12+ or Helm 3.0-beta3+ Before you begin This Chart is a modified version of bitnami/spark using Scone and Intel SGX. Further information og the original chart can be found here . Attestation This chart does not submit any sessions to a CAS, so you have to do it beforehand, from a trusted computer. If you need to pass remote attestation information to your container, such as SCONE_CONFIG_ID and SCONE_CAS_ADDR , use the master.extraEnvVars and worker.extraEnvVars parameter on values.yaml . Installing the Chart To install the chart with the release name my-spark : export GH_TOKEN=... helm repo add sconeapps https://${GH_TOKEN}@raw.githubusercontent.com/scontain/sconeapps/master/ helm install my-spark sconeapps/spark These commands deploy Spark on the Kubernetes cluster in the default configuration. The Parameters section lists the parameters that can be configured during installation. Tip : List all releases using helm list Uninstalling the Chart To uninstall/delete the my-spark statefulset: $ helm delete my-spark The command removes all the Kubernetes components associated with the chart and deletes the release. Use the option --purge to delete all persistent volumes too. Parameters The following tables lists the configurable parameters of the spark chart and their default values. Parameter Description Default global.imageRegistry Global Docker image registry nil global.imagePullSecrets Global Docker registry secret names as an array [] (does not add image pull secrets to deployed pods) image.registry spark image registry docker.io image.repository spark Image name lucasmc/pyspark image.tag spark Image tag {TAG_NAME} image.pullPolicy spark image pull policy IfNotPresent image.pullSecrets Specify docker-registry secret names as an array [] (does not add image pull secrets to deployed pods) nameOverride String to partially override spark.fullname template with a string (will prepend the release name) nil fullnameOverride String to fully override spark.fullname template with a string nil master.debug Specify if debug values should be set on the master false master.webPort Specify the port where the web interface will listen on the master 8080 master.clusterPort Specify the port where the master listens to communicate with workers 7077 master.daemonMemoryLimit Set the memory limit for the master daemon No default master.configOptions Optional configuration if the form -Dx=y No default master.securityContext.enabled Enable security context true master.securityContext.fsGroup Group ID for the container 0 master.securityContext.runAsUser User ID for the container 0 master.podAnnotations Annotations for pods in StatefulSet {} (The value is evaluated as a template) master.nodeSelector Node affinity policy {} (The value is evaluated as a template) master.tolerations Tolerations for pod assignment [] (The value is evaluated as a template) master.affinity Affinity for pod assignment {} (The value is evaluated as a template) master.resources CPU/Memory resource requests/limits {} master.extraEnvVars Extra environment variables to pass to the master container {} master.extraVolumes Array of extra volumes to be added to the Spark master deployment (evaluated as template). Requires setting master.extraVolumeMounts nil master.extraVolumeMounts Array of extra volume mounts to be added to the Spark master deployment (evaluated as template). Normally used with master.extraVolumes . nil master.useSGXDevPlugin Use SGX Device Plugin to access SGX resources. scone master.sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil master.livenessProbe.enabled Turn on and off liveness probe true master.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated 10 master.livenessProbe.periodSeconds How often to perform the probe 10 master.livenessProbe.timeoutSeconds When the probe times out 5 master.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 2 master.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 master.readinessProbe.enabled Turn on and off readiness probe true master.readinessProbe.initialDelaySeconds Delay before liveness probe is initiated 5 master.readinessProbe.periodSeconds How often to perform the probe 10 master.readinessProbe.timeoutSeconds When the probe times out 5 master.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 6 master.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 worker.debug Specify if debug values should be set on workers false worker.webPort Specify the port where the web interface will listen on the worker 8080 worker.clusterPort Specify the port where the worker listens to communicate with the master 7077 worker.daemonMemoryLimit Set the memory limit for the worker daemon No default worker.memoryLimit Set the maximum memory the worker is allowed to use No default worker.coreLimit Se the maximum number of cores that the worker can use No default worker.dir Set a custom working directory for the application No default worker.javaOptions Set options for the JVM in the form -Dx=y No default worker.configOptions Set extra options to configure the worker in the form -Dx=y No default worker.replicaCount Set the number of workers 2 worker.autoscaling.enabled Enable autoscaling depending on CPU false worker.autoscaling.CpuTargetPercentage k8s hpa cpu targetPercentage 50 worker.autoscaling.replicasMax Maximum number of workers when using autoscaling 5 worker.securityContext.enabled Enable security context true worker.securityContext.fsGroup Group ID for the container 1001 worker.securityContext.runAsUser User ID for the container 1001 worker.podAnnotations Annotations for pods in StatefulSet {} worker.nodeSelector Node labels for pod assignment. Used as a template from the values. {} worker.tolerations Toleration labels for pod assignment [] worker.affinity Affinity and AntiAffinity rules for pod assignment {} worker.resources CPU/Memory resource requests/limits Memory: 256Mi , CPU: 250m worker.livenessProbe.enabled Turn on and off liveness probe true worker.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated 10 worker.livenessProbe.periodSeconds How often to perform the probe 10 worker.livenessProbe.timeoutSeconds When the probe times out 5 worker.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 2 worker.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 worker.readinessProbe.enabled Turn on and off readiness probe true worker.readinessProbe.initialDelaySeconds Delay before liveness probe is initiated 5 worker.readinessProbe.periodSeconds How often to perform the probe 10 worker.readinessProbe.timeoutSeconds When the probe times out 5 worker.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 6 worker.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 worker.extraEnvVars Extra environment variables to pass to the worker container {} worker.extraVolumes Array of extra volumes to be added to the Spark worker deployment (evaluated as template). Requires setting worker.extraVolumeMounts nil worker.extraVolumeMounts Array of extra volume mounts to be added to the Spark worker deployment (evaluated as template). Normally used with worker.extraVolumes . nil worker.useSGXDevPlugin Use SGX Device Plugin to access SGX resources. scone worker.sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil security.passwordsSecretName Secret to use when using security configuration to set custom passwords No default security.rpc.authenticationEnabled Enable the RPC authentication false security.rpc.encryptionEnabled Enable the encryption for RPC false security.storageEncryptionEnabled Enable the encryption of the storage false security.ssl.enabled Enable the SSL configuration false security.ssl.needClientAuth Enable the client authentication false security.ssl.protocol Set the SSL protocol TLSv1.2 security.certificatesSecretName Set the name of the secret that contains the certificates No default service.type Kubernetes Service type ClusterIP service.webPort Spark client port 80 service.clusterPort Spark cluster port 7077 service.nodePort Port to bind to for NodePort service type (client port) nil service.nodePorts.cluster Kubernetes cluster node port \"\" service.nodePorts.web Kubernetes web node port \"\" service.annotations Annotations for spark service {} service.loadBalancerIP loadBalancerIP if spark service type is LoadBalancer nil ingress.enabled Enable the use of the ingress controller to access the web UI false ingress.certManager Add annotations for cert-manager false ingress.annotations Ingress annotations {} ingress.hosts[0].name Hostname to your Spark installation spark.local ingress.hosts[0].path Path within the url structure / ingress.hosts[0].tls Utilize TLS backend in ingress false ingress.hosts[0].tlsHosts Array of TLS hosts for ingress record (defaults to ingress.hosts[0].name if nil ) nil ingress.hosts[0].tlsSecret TLS Secret (certificates) spark.local-tls Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, helm install my-spark \\ --set master.webPort=8081 sconeapps/spark The above command sets the spark master web port to 8081 . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, $ helm install my-spark -f values.yaml sconeapps/spark One can use the default values.yaml in the SconeApps repo Configuration and installation details Rolling VS Immutable tags It is strongly recommended to use immutable tags in a production environment. This ensures your deployment does not change automatically if the same tag is updated with a different image. Submit an application To submit an application to the cluster use the spark-submit script. You can obtain the script here . For example, to deploy one of the example applications: $ ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://<master-IP>:<master-cluster-port> --deploy-mode cluster ./examples/jars/spark-examples_2.11-2.4.3.jar 1000 Where the master IP and port must be changed by you master IP address and port. Be aware that currently is not possible to submit an application to a standalone cluster if RPC authentication is configured. More info about the issue here .","title":"spark"},{"location":"sconeapps_spark/#apache-spark","text":"Apache Spark is a high-performance engine for large-scale computing tasks, such as data processing, machine learning and real-time data streaming. It includes APIs for Java, Python, Scala and R.","title":"Apache Spark"},{"location":"sconeapps_spark/#tldr","text":"helm install my-spark sconeapps/spark","title":"TL;DR;"},{"location":"sconeapps_spark/#introduction","text":"This chart bootstraps a spark deployment on a Kubernetes cluster using the Helm package manager.","title":"Introduction"},{"location":"sconeapps_spark/#prerequisites","text":"Kubernetes 1.12+ Helm 2.12+ or Helm 3.0-beta3+","title":"Prerequisites"},{"location":"sconeapps_spark/#before-you-begin","text":"This Chart is a modified version of bitnami/spark using Scone and Intel SGX. Further information og the original chart can be found here .","title":"Before you begin"},{"location":"sconeapps_spark/#attestation","text":"This chart does not submit any sessions to a CAS, so you have to do it beforehand, from a trusted computer. If you need to pass remote attestation information to your container, such as SCONE_CONFIG_ID and SCONE_CAS_ADDR , use the master.extraEnvVars and worker.extraEnvVars parameter on values.yaml .","title":"Attestation"},{"location":"sconeapps_spark/#installing-the-chart","text":"To install the chart with the release name my-spark : export GH_TOKEN=... helm repo add sconeapps https://${GH_TOKEN}@raw.githubusercontent.com/scontain/sconeapps/master/ helm install my-spark sconeapps/spark These commands deploy Spark on the Kubernetes cluster in the default configuration. The Parameters section lists the parameters that can be configured during installation. Tip : List all releases using helm list","title":"Installing the Chart"},{"location":"sconeapps_spark/#uninstalling-the-chart","text":"To uninstall/delete the my-spark statefulset: $ helm delete my-spark The command removes all the Kubernetes components associated with the chart and deletes the release. Use the option --purge to delete all persistent volumes too.","title":"Uninstalling the Chart"},{"location":"sconeapps_spark/#parameters","text":"The following tables lists the configurable parameters of the spark chart and their default values. Parameter Description Default global.imageRegistry Global Docker image registry nil global.imagePullSecrets Global Docker registry secret names as an array [] (does not add image pull secrets to deployed pods) image.registry spark image registry docker.io image.repository spark Image name lucasmc/pyspark image.tag spark Image tag {TAG_NAME} image.pullPolicy spark image pull policy IfNotPresent image.pullSecrets Specify docker-registry secret names as an array [] (does not add image pull secrets to deployed pods) nameOverride String to partially override spark.fullname template with a string (will prepend the release name) nil fullnameOverride String to fully override spark.fullname template with a string nil master.debug Specify if debug values should be set on the master false master.webPort Specify the port where the web interface will listen on the master 8080 master.clusterPort Specify the port where the master listens to communicate with workers 7077 master.daemonMemoryLimit Set the memory limit for the master daemon No default master.configOptions Optional configuration if the form -Dx=y No default master.securityContext.enabled Enable security context true master.securityContext.fsGroup Group ID for the container 0 master.securityContext.runAsUser User ID for the container 0 master.podAnnotations Annotations for pods in StatefulSet {} (The value is evaluated as a template) master.nodeSelector Node affinity policy {} (The value is evaluated as a template) master.tolerations Tolerations for pod assignment [] (The value is evaluated as a template) master.affinity Affinity for pod assignment {} (The value is evaluated as a template) master.resources CPU/Memory resource requests/limits {} master.extraEnvVars Extra environment variables to pass to the master container {} master.extraVolumes Array of extra volumes to be added to the Spark master deployment (evaluated as template). Requires setting master.extraVolumeMounts nil master.extraVolumeMounts Array of extra volume mounts to be added to the Spark master deployment (evaluated as template). Normally used with master.extraVolumes . nil master.useSGXDevPlugin Use SGX Device Plugin to access SGX resources. scone master.sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil master.livenessProbe.enabled Turn on and off liveness probe true master.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated 10 master.livenessProbe.periodSeconds How often to perform the probe 10 master.livenessProbe.timeoutSeconds When the probe times out 5 master.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 2 master.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 master.readinessProbe.enabled Turn on and off readiness probe true master.readinessProbe.initialDelaySeconds Delay before liveness probe is initiated 5 master.readinessProbe.periodSeconds How often to perform the probe 10 master.readinessProbe.timeoutSeconds When the probe times out 5 master.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 6 master.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 worker.debug Specify if debug values should be set on workers false worker.webPort Specify the port where the web interface will listen on the worker 8080 worker.clusterPort Specify the port where the worker listens to communicate with the master 7077 worker.daemonMemoryLimit Set the memory limit for the worker daemon No default worker.memoryLimit Set the maximum memory the worker is allowed to use No default worker.coreLimit Se the maximum number of cores that the worker can use No default worker.dir Set a custom working directory for the application No default worker.javaOptions Set options for the JVM in the form -Dx=y No default worker.configOptions Set extra options to configure the worker in the form -Dx=y No default worker.replicaCount Set the number of workers 2 worker.autoscaling.enabled Enable autoscaling depending on CPU false worker.autoscaling.CpuTargetPercentage k8s hpa cpu targetPercentage 50 worker.autoscaling.replicasMax Maximum number of workers when using autoscaling 5 worker.securityContext.enabled Enable security context true worker.securityContext.fsGroup Group ID for the container 1001 worker.securityContext.runAsUser User ID for the container 1001 worker.podAnnotations Annotations for pods in StatefulSet {} worker.nodeSelector Node labels for pod assignment. Used as a template from the values. {} worker.tolerations Toleration labels for pod assignment [] worker.affinity Affinity and AntiAffinity rules for pod assignment {} worker.resources CPU/Memory resource requests/limits Memory: 256Mi , CPU: 250m worker.livenessProbe.enabled Turn on and off liveness probe true worker.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated 10 worker.livenessProbe.periodSeconds How often to perform the probe 10 worker.livenessProbe.timeoutSeconds When the probe times out 5 worker.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 2 worker.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 worker.readinessProbe.enabled Turn on and off readiness probe true worker.readinessProbe.initialDelaySeconds Delay before liveness probe is initiated 5 worker.readinessProbe.periodSeconds How often to perform the probe 10 worker.readinessProbe.timeoutSeconds When the probe times out 5 worker.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded. 6 worker.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed 1 worker.extraEnvVars Extra environment variables to pass to the worker container {} worker.extraVolumes Array of extra volumes to be added to the Spark worker deployment (evaluated as template). Requires setting worker.extraVolumeMounts nil worker.extraVolumeMounts Array of extra volume mounts to be added to the Spark worker deployment (evaluated as template). Normally used with worker.extraVolumes . nil worker.useSGXDevPlugin Use SGX Device Plugin to access SGX resources. scone worker.sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil security.passwordsSecretName Secret to use when using security configuration to set custom passwords No default security.rpc.authenticationEnabled Enable the RPC authentication false security.rpc.encryptionEnabled Enable the encryption for RPC false security.storageEncryptionEnabled Enable the encryption of the storage false security.ssl.enabled Enable the SSL configuration false security.ssl.needClientAuth Enable the client authentication false security.ssl.protocol Set the SSL protocol TLSv1.2 security.certificatesSecretName Set the name of the secret that contains the certificates No default service.type Kubernetes Service type ClusterIP service.webPort Spark client port 80 service.clusterPort Spark cluster port 7077 service.nodePort Port to bind to for NodePort service type (client port) nil service.nodePorts.cluster Kubernetes cluster node port \"\" service.nodePorts.web Kubernetes web node port \"\" service.annotations Annotations for spark service {} service.loadBalancerIP loadBalancerIP if spark service type is LoadBalancer nil ingress.enabled Enable the use of the ingress controller to access the web UI false ingress.certManager Add annotations for cert-manager false ingress.annotations Ingress annotations {} ingress.hosts[0].name Hostname to your Spark installation spark.local ingress.hosts[0].path Path within the url structure / ingress.hosts[0].tls Utilize TLS backend in ingress false ingress.hosts[0].tlsHosts Array of TLS hosts for ingress record (defaults to ingress.hosts[0].name if nil ) nil ingress.hosts[0].tlsSecret TLS Secret (certificates) spark.local-tls Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, helm install my-spark \\ --set master.webPort=8081 sconeapps/spark The above command sets the spark master web port to 8081 . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, $ helm install my-spark -f values.yaml sconeapps/spark One can use the default values.yaml in the SconeApps repo","title":"Parameters"},{"location":"sconeapps_spark/#configuration-and-installation-details","text":"","title":"Configuration and installation details"},{"location":"sconeapps_spark/#rolling-vs-immutable-tags","text":"It is strongly recommended to use immutable tags in a production environment. This ensures your deployment does not change automatically if the same tag is updated with a different image.","title":"Rolling VS Immutable tags"},{"location":"sconeapps_spark/#submit-an-application","text":"To submit an application to the cluster use the spark-submit script. You can obtain the script here . For example, to deploy one of the example applications: $ ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://<master-IP>:<master-cluster-port> --deploy-mode cluster ./examples/jars/spark-examples_2.11-2.4.3.jar 1000 Where the master IP and port must be changed by you master IP address and port. Be aware that currently is not possible to submit an application to a standalone cluster if RPC authentication is configured. More info about the issue here .","title":"Submit an application"},{"location":"sconeapps_tensorflow/","text":"Tensorflow Run a Tensorflow workload on a Kubernetes cluster. Prerequisites A Kubernetes cluster; Helm 3 client. Please refer to the official setup guide . Install this chart Add the repo If you haven't yet, please add this repo to Helm. Install the chart Use Helm to run Tensorflow on your cluster. We are deploying a Helm release called my-tensorflow , with default parameters (i.e., a simple model training): helm install my-tensorflow sconeapps/tensorflow Have a look at the Parameters section for a complete list of parameters this chart supports. SGX device By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true Before you begin Attestation This chart does not submit any sessions to a CAS, so you have to do it beforehand, from a trusted computer. If you need to pass remote attestation information to your container, such as SCONE_CONFIG_ID and SCONE_CAS_ADDR , use the extraEnv parameter on values.yaml . Data output and external volumes If your workload produces any output artifacts that need to be saved before the container is gone, consider having an auxiliary task uploading it to somewhere after the main task is finished. For now, any output volumes are considered to be either hostPath (meaning that their respective directories have to exist on the worker node) or emptyDir , which maps everything to random-generated directory under /tmp . Parameters Parameter Description Default image Tensorflow image registry.scontain.com:5050/sconecuratedimages/datasystems:tensorflow-1.15 imagePullPolicy Tensorflow pull policy IfNotPresent imagePullSecrets Tensorflow pull secrets, in case of private repositories [{\"name\": \"sconeapps\"}] nameOverride String to partially override tensorflow.fullname template with a string (will prepend the release name) nil fullNameOverride String to fully override tensorflow.fullname template with a string nil podAnnotations Additional pod annotations {} securityContext Security context for Tensorflow container {} extraVolumes Extra volume definitions [] extraVolumeMounts Extra volume mounts for Tensorflow pod [] extraEnv Additional environment variables for Tensorflow container [{\"name\": \"SCONE_LAS_ADDR\", \"valueFrom\": {\"fieldRef\": {\"fieldPath\": \"status.hostIP\"}}}] resources CPU/Memory resource requests/limits for node. {} nodeSelector Node labels for pod assignment (this value is evaluated as a template) {} tolerations List of node taints to tolerate (this value is evaluated as a template) [] affinity Map of node/pod affinities (The value is evaluated as a template) {} useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil","title":"tensorflow"},{"location":"sconeapps_tensorflow/#tensorflow","text":"Run a Tensorflow workload on a Kubernetes cluster.","title":"Tensorflow"},{"location":"sconeapps_tensorflow/#prerequisites","text":"A Kubernetes cluster; Helm 3 client. Please refer to the official setup guide .","title":"Prerequisites"},{"location":"sconeapps_tensorflow/#install-this-chart","text":"","title":"Install this chart"},{"location":"sconeapps_tensorflow/#add-the-repo","text":"If you haven't yet, please add this repo to Helm.","title":"Add the repo"},{"location":"sconeapps_tensorflow/#install-the-chart","text":"Use Helm to run Tensorflow on your cluster. We are deploying a Helm release called my-tensorflow , with default parameters (i.e., a simple model training): helm install my-tensorflow sconeapps/tensorflow Have a look at the Parameters section for a complete list of parameters this chart supports.","title":"Install the chart"},{"location":"sconeapps_tensorflow/#sgx-device","text":"By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_tensorflow/#before-you-begin","text":"","title":"Before you begin"},{"location":"sconeapps_tensorflow/#attestation","text":"This chart does not submit any sessions to a CAS, so you have to do it beforehand, from a trusted computer. If you need to pass remote attestation information to your container, such as SCONE_CONFIG_ID and SCONE_CAS_ADDR , use the extraEnv parameter on values.yaml .","title":"Attestation"},{"location":"sconeapps_tensorflow/#data-output-and-external-volumes","text":"If your workload produces any output artifacts that need to be saved before the container is gone, consider having an auxiliary task uploading it to somewhere after the main task is finished. For now, any output volumes are considered to be either hostPath (meaning that their respective directories have to exist on the worker node) or emptyDir , which maps everything to random-generated directory under /tmp .","title":"Data output and external volumes"},{"location":"sconeapps_tensorflow/#parameters","text":"Parameter Description Default image Tensorflow image registry.scontain.com:5050/sconecuratedimages/datasystems:tensorflow-1.15 imagePullPolicy Tensorflow pull policy IfNotPresent imagePullSecrets Tensorflow pull secrets, in case of private repositories [{\"name\": \"sconeapps\"}] nameOverride String to partially override tensorflow.fullname template with a string (will prepend the release name) nil fullNameOverride String to fully override tensorflow.fullname template with a string nil podAnnotations Additional pod annotations {} securityContext Security context for Tensorflow container {} extraVolumes Extra volume definitions [] extraVolumeMounts Extra volume mounts for Tensorflow pod [] extraEnv Additional environment variables for Tensorflow container [{\"name\": \"SCONE_LAS_ADDR\", \"valueFrom\": {\"fieldRef\": {\"fieldPath\": \"status.hostIP\"}}}] resources CPU/Memory resource requests/limits for node. {} nodeSelector Node labels for pod assignment (this value is evaluated as a template) {} tolerations List of node taints to tolerate (this value is evaluated as a template) [] affinity Map of node/pod affinities (The value is evaluated as a template) {} useSGXDevPlugin Use SGX Device Plugin to access SGX resources. \"scone\" sgxEpcMem Required to Azure SGX Device Plugin. Protected EPC memory in MiB nil","title":"Parameters"},{"location":"sconeapps_tensorflowlite/","text":"scone-tensorflowlite chart To see the customizations this chart currently supports, have a look at values.yaml . Prerequisites A Kubernetes cluster. Helm 3 installed. Before you begin Submit any SCONE sessions (e.g., the one located in ../../policies ) to a CAS that is accessible from within the cluster. Reference them through extraEnv section. Create the directories you need in the worker nodes, as this application relies on hostPath volumes. They are defined in extraVolumes section. Clone this git repository. Install the chart Add the repo If you haven't yet, please add this repo to Helm. To deploy TensorFlow Lite with the default parameters to your Kubernetes cluster: helm install my-tensorflowlite sconeapps/tensorflowlite See your deployed pods by running kubectl get pods | grep my-tensorflowlite . SGX device By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"tensorflowlite"},{"location":"sconeapps_tensorflowlite/#scone-tensorflowlite-chart","text":"To see the customizations this chart currently supports, have a look at values.yaml .","title":"scone-tensorflowlite chart"},{"location":"sconeapps_tensorflowlite/#prerequisites","text":"A Kubernetes cluster. Helm 3 installed.","title":"Prerequisites"},{"location":"sconeapps_tensorflowlite/#before-you-begin","text":"Submit any SCONE sessions (e.g., the one located in ../../policies ) to a CAS that is accessible from within the cluster. Reference them through extraEnv section. Create the directories you need in the worker nodes, as this application relies on hostPath volumes. They are defined in extraVolumes section. Clone this git repository.","title":"Before you begin"},{"location":"sconeapps_tensorflowlite/#install-the-chart","text":"","title":"Install the chart"},{"location":"sconeapps_tensorflowlite/#add-the-repo","text":"If you haven't yet, please add this repo to Helm. To deploy TensorFlow Lite with the default parameters to your Kubernetes cluster: helm install my-tensorflowlite sconeapps/tensorflowlite See your deployed pods by running kubectl get pods | grep my-tensorflowlite .","title":"Add the repo"},{"location":"sconeapps_tensorflowlite/#sgx-device","text":"By default, this helm chart uses the SCONE SGX Plugin . Hence, it sets the resource limits of CAS as follows: resources : limits : sgx.k8s.io/sgx : 1 Alternatively, set useSGXDevPlugin to azure (e.g., --useSGXDevPlugin=azure ) to support Azure's SGX Device Plugin. Since Azure requires the amount of EPC memory allocated to your application to be specified, the parameter sgxEpcMem (SGX EPC memory in MiB) becomes required too (e.g., --set useSGXDevPlugin=azure --set sgxEpcMem=16 ). In case you do not want to use the SGX plugin, you can remove the resource limit and explicitly mount the local SGX device into your container by setting: extraVolumes : - name : dev-isgx hostPath : path : /dev/isgx extraVolumeMounts : - name : dev-isgx path : /dev/isgx Please note that mounting the local SGX device into your container requires privileged mode, which will grant your container access to ALL host devices. To enable privileged mode, set securityContext : securityContext : privileged : true","title":"SGX device"},{"location":"sconeapps_zookeeper/","text":"ZooKeeper ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or other by distributed applications. Introduction This chart bootstraps a ZooKeeper deployment on a Kubernetes cluster using the Helm package manager. Prerequisites Kubernetes 1.12+ Helm 3.0-beta3+ PV provisioner support in the underlying infrastructure Installing the Chart To install the chart with the release name my-zoo : $ helm install my-zoo sconeapps/zookeeper These commands deploy ZooKeeper on the Kubernetes cluster in the default configuration. The Parameters section lists the parameters that can be configured during installation. List all releases using helm list Uninstalling the Chart To uninstall/delete the my-zoo deployment: $ helm delete my-release The command removes all the Kubernetes components associated with the chart and deletes the release. Parameters The following tables lists the configurable parameters of the ZooKeeper chart and their default values per section/component: Parameter Description Default global.imageRegistry Global Docker image registry nil global.imagePullSecrets Global Docker registry secret names as an array [] (does not add image pull secrets to deployed pods) global.storageClass Global storage class for dynamic provisioning nil Common parameters Parameter Description Default nameOverride String to partially override zookeeper.fullname nil fullnameOverride String to fully override zookeeper.fullname nil clusterDomain Default Kubernetes cluster domain cluster.local commonLabels Labels to add to all deployed objects {} commonAnnotations Annotations to add to all deployed objects {} schedulerName Kubernetes pod scheduler registry nil (use the default-scheduler) Zookeeper chart parameters Parameter Description Default image.registry ZooKeeper image registry registry.scontain.com:5050 image.repository ZooKeeper Image name sconecuratedimages/apps image.tag ZooKeeper Image tag {TAG_NAME} image.pullPolicy ZooKeeper image pull policy IfNotPresent image.pullSecrets Specify docker-registry secret names as an array [] (does not add image pull secrets to deployed pods) image.debug Specify if debug values should be set false tickTime Basic time unit in milliseconds used by ZooKeeper for heartbeats 2000 initLimit Time the ZooKeeper servers in quorum have to connect to a leader 10 syncLimit How far out of date a server can be from a leader 5 maxClientCnxns Number of concurrent connections that a single client may make to a single member 60 maxSessionTimeout Maximum session timeout in milliseconds that the server will allow the client to negotiate. 40000 autopurge.snapRetainCount Number of retains snapshots for autopurge 3 autopurge.purgeInterval The time interval in hours for which the purge task has to be triggered 0 fourlwCommandsWhitelist A list of comma separated Four Letter Words commands to use srvr, mntr listenOnAllIPs Allow Zookeeper to listen for connections from its peers on all available IP addresses. false allowAnonymousLogin Allow to accept connections from unauthenticated users yes auth.existingSecret Use existing secret (ignores previous password) nil auth.enabled Enable ZooKeeper auth false auth.clientUser User that will use ZooKeeper clients to auth nil auth.clientPassword Password that will use ZooKeeper clients to auth nil auth.serverUsers List of user to be created nil auth.serverPasswords List of passwords to assign to users when created nil heapSize Size in MB for the Java Heap options (Xmx and XMs) [] logLevel Log level of ZooKeeper server ERROR jvmFlags Default JVMFLAGS for the ZooKeeper process nil config Configure ZooKeeper with a custom zoo.conf file nil dataLogDir Data log directory \"\" namespaceOverride Namespace for ZooKeeper resources. Overrides the release namespace. The Release Namespace Statefulset parameters Parameter Description Default replicaCount Number of ZooKeeper nodes 1 updateStrategy Update strategy for the statefulset RollingUpdate rollingUpdatePartition Partition update strategy nil podManagementPolicy Pod management policy Parallel podLabels ZooKeeper pod labels {} (evaluated as a template) podAnnotations ZooKeeper Pod annotations {} (evaluated as a template) affinity Affinity for pod assignment {} (evaluated as a template) nodeSelector Node labels for pod assignment {} (evaluated as a template) tolerations Tolerations for pod assignment [] (evaluated as a template) priorityClassName Name of the existing priority class to be used by ZooKeeper pods \"\" securityContext.enabled Enable security context (ZooKeeper master pod) true securityContext.fsGroup Group ID for the container (ZooKeeper master pod) 1001 securityContext.runAsUser User ID for the container (ZooKeeper master pod) 1001 resources CPU/Memory resource requests/limits Memory: 256Mi , CPU: 250m livenessProbe Liveness probe configuration for ZooKeeper Check values.yaml file readinessProbe Readiness probe configuration for ZooKeeper Check values.yaml file extraVolumes Extra volumes nil extraVolumeMounts Mount extra volume(s) nil podDisruptionBudget.maxUnavailable Max number of pods down simultaneously 1 Exposure parameters Parameter Description Default service.type Kubernetes Service type ClusterIP service.loadBalancerIP Use with service.type LoadBalancer to assign static IP to Load Balancer instance \"\" service.port ZooKeeper port 2181 service.followerPort ZooKeeper follower port 2888 service.electionPort ZooKeeper election port 3888 service.publishNotReadyAddresses If the ZooKeeper headless service should publish DNS records for not ready pods true serviceAccount.create Enable creation of ServiceAccount for zookeeper pod false serviceAccount.name The name of the service account to use. If not set and create is true , a name is generated Generated using the zookeeper.fullname template service.tls.client_enable Enable tls for client connections false service.tls.quorum_enable Enable tls for quorum protocol false service.tls.disable_base_client_port Remove client port from service definitions. false service.tls.client_port Service port for tls client connections 3181 service.tls.client_keystore_path KeyStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_key_store/key_store_file service.tls.client_keystore_password KeyStore password. You can use environment variables. nil service.tls.client_truststore_path TrustStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_trust_store/trust_store_file service.tls.client_truststore_password TrustStore password. You can use environment variables. nil service.tls.quorum_keystore_path KeyStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_key_store/key_store_file service.tls.quorum_keystore_password KeyStore password. You can use environment variables. nil service.tls.quorum_truststore_path TrustStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_trust_store/trust_store_file service.tls.quorum_truststore_password TrustStore password. You can use environment variables. nil service.annotations Annotations for the Service {} service.headless.annotations Annotations for the Headless Service {} networkPolicy.enabled Enable NetworkPolicy false networkPolicy.allowExternal Don't require client label for connections true Persistence parameters Parameter Description Default persistence.enabled Enable Zookeeper data persistence using PVC true persistence.existingClaim Provide an existing PersistentVolumeClaim nil (evaluated as a template) persistence.storageClass PVC Storage Class for ZooKeeper data volume nil persistence.accessMode PVC Access Mode for ZooKeeper data volume ReadWriteOnce persistence.size PVC Storage Request for ZooKeeper data volume 8Gi persistence.annotations Annotations for the PVC {} (evaluated as a template) persistence.dataLogDir.size PVC Storage Request for ZooKeeper's Data log directory 8Gi persistence.dataLogDir.existingClaim Provide an existing PersistentVolumeClaim for Zookeeper's Data log directory nil (evaluated as a template) Volume Permissions parameters Parameter Description Default volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to runAsUser:fsGroup false volumePermissions.image.registry Init container volume-permissions image registry docker.io volumePermissions.image.repository Init container volume-permissions image name minideb volumePermissions.image.tag Init container volume-permissions image tag buster volumePermissions.image.pullPolicy Init container volume-permissions image pull policy Always volumePermissions.resources Init container resource requests/limit nil Metrics parameters Parameter Description Default metrics.enabled Enable prometheus to access zookeeper metrics endpoint false metrics.containerPort Port where a Jetty server will expose Prometheus metrics 9141 metrics.service.type Kubernetes service type ( ClusterIP , NodePort or LoadBalancer ) for Jetty server exposing Prometheus metrics ClusterIP metrics.service.port Prometheus metrics service port 9141 metrics.service.annotations Service annotations for Prometheus to auto-discover the metrics endpoint {prometheus.io/scrape: \"true\", prometheus.io/port: \"9141\"} metrics.serviceMonitor.enabled if true , creates a Prometheus Operator ServiceMonitor (also requires metrics.enabled to be true ) false metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource The Release Namespace metrics.serviceMonitor.interval Interval at which metrics should be scraped. nil (Prometheus Operator default value) metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended nil (Prometheus Operator default value) metrics.serviceMonitor.selector Prometheus instance selector labels nil metrics.prometheusRule.enabled if true , creates a Prometheus Operator PrometheusRule (also requires metrics.enabled to be true and metrics.prometheusRule.rules ) false metrics.prometheusRule.namespace Namespace for the PrometheusRule Resource The Release Namespace metrics.prometheusRule.selector Prometheus instance selector labels nil metrics.prometheusRule.rules Prometheus Rule definitions (see values.yaml for examples) [] Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, $ helm install my-release \\ --set auth.clientUser = newUser \\ sconeapps/zookeeper The above command sets the ZooKeeper user to newUser . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, $ helm install my-release -f values.yaml sconeapps/zookeeper One can use the default values.yaml in the SconeApps repo Configuration and installation details Rolling VS Immutable tags It is strongly recommended to use immutable tags in a production environment. This ensures your deployment does not change automatically if the same tag is updated with a different image. Production configuration This chart includes a values-production.yaml file where you can find some parameters oriented to production configuration in comparison to the regular values.yaml . You can use this file instead of the default one. Number of ZooKeeper nodes: - replicaCount: 1 + replicaCount: 3 Enable prometheus metrics: - metrics.enabled: false + metrics.enabled: true Log level You can configure the ZooKeeper log level using the ZOO_LOG_LEVEL environment variable. By default, it is set to ERROR because of each readiness probe produce an INFO message on connection and a WARN message on disconnection.","title":"zookeeper"},{"location":"sconeapps_zookeeper/#zookeeper","text":"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or other by distributed applications.","title":"ZooKeeper"},{"location":"sconeapps_zookeeper/#introduction","text":"This chart bootstraps a ZooKeeper deployment on a Kubernetes cluster using the Helm package manager.","title":"Introduction"},{"location":"sconeapps_zookeeper/#prerequisites","text":"Kubernetes 1.12+ Helm 3.0-beta3+ PV provisioner support in the underlying infrastructure","title":"Prerequisites"},{"location":"sconeapps_zookeeper/#installing-the-chart","text":"To install the chart with the release name my-zoo : $ helm install my-zoo sconeapps/zookeeper These commands deploy ZooKeeper on the Kubernetes cluster in the default configuration. The Parameters section lists the parameters that can be configured during installation. List all releases using helm list","title":"Installing the Chart"},{"location":"sconeapps_zookeeper/#uninstalling-the-chart","text":"To uninstall/delete the my-zoo deployment: $ helm delete my-release The command removes all the Kubernetes components associated with the chart and deletes the release.","title":"Uninstalling the Chart"},{"location":"sconeapps_zookeeper/#parameters","text":"The following tables lists the configurable parameters of the ZooKeeper chart and their default values per section/component: Parameter Description Default global.imageRegistry Global Docker image registry nil global.imagePullSecrets Global Docker registry secret names as an array [] (does not add image pull secrets to deployed pods) global.storageClass Global storage class for dynamic provisioning nil","title":"Parameters"},{"location":"sconeapps_zookeeper/#common-parameters","text":"Parameter Description Default nameOverride String to partially override zookeeper.fullname nil fullnameOverride String to fully override zookeeper.fullname nil clusterDomain Default Kubernetes cluster domain cluster.local commonLabels Labels to add to all deployed objects {} commonAnnotations Annotations to add to all deployed objects {} schedulerName Kubernetes pod scheduler registry nil (use the default-scheduler)","title":"Common parameters"},{"location":"sconeapps_zookeeper/#zookeeper-chart-parameters","text":"Parameter Description Default image.registry ZooKeeper image registry registry.scontain.com:5050 image.repository ZooKeeper Image name sconecuratedimages/apps image.tag ZooKeeper Image tag {TAG_NAME} image.pullPolicy ZooKeeper image pull policy IfNotPresent image.pullSecrets Specify docker-registry secret names as an array [] (does not add image pull secrets to deployed pods) image.debug Specify if debug values should be set false tickTime Basic time unit in milliseconds used by ZooKeeper for heartbeats 2000 initLimit Time the ZooKeeper servers in quorum have to connect to a leader 10 syncLimit How far out of date a server can be from a leader 5 maxClientCnxns Number of concurrent connections that a single client may make to a single member 60 maxSessionTimeout Maximum session timeout in milliseconds that the server will allow the client to negotiate. 40000 autopurge.snapRetainCount Number of retains snapshots for autopurge 3 autopurge.purgeInterval The time interval in hours for which the purge task has to be triggered 0 fourlwCommandsWhitelist A list of comma separated Four Letter Words commands to use srvr, mntr listenOnAllIPs Allow Zookeeper to listen for connections from its peers on all available IP addresses. false allowAnonymousLogin Allow to accept connections from unauthenticated users yes auth.existingSecret Use existing secret (ignores previous password) nil auth.enabled Enable ZooKeeper auth false auth.clientUser User that will use ZooKeeper clients to auth nil auth.clientPassword Password that will use ZooKeeper clients to auth nil auth.serverUsers List of user to be created nil auth.serverPasswords List of passwords to assign to users when created nil heapSize Size in MB for the Java Heap options (Xmx and XMs) [] logLevel Log level of ZooKeeper server ERROR jvmFlags Default JVMFLAGS for the ZooKeeper process nil config Configure ZooKeeper with a custom zoo.conf file nil dataLogDir Data log directory \"\" namespaceOverride Namespace for ZooKeeper resources. Overrides the release namespace. The Release Namespace","title":"Zookeeper chart parameters"},{"location":"sconeapps_zookeeper/#statefulset-parameters","text":"Parameter Description Default replicaCount Number of ZooKeeper nodes 1 updateStrategy Update strategy for the statefulset RollingUpdate rollingUpdatePartition Partition update strategy nil podManagementPolicy Pod management policy Parallel podLabels ZooKeeper pod labels {} (evaluated as a template) podAnnotations ZooKeeper Pod annotations {} (evaluated as a template) affinity Affinity for pod assignment {} (evaluated as a template) nodeSelector Node labels for pod assignment {} (evaluated as a template) tolerations Tolerations for pod assignment [] (evaluated as a template) priorityClassName Name of the existing priority class to be used by ZooKeeper pods \"\" securityContext.enabled Enable security context (ZooKeeper master pod) true securityContext.fsGroup Group ID for the container (ZooKeeper master pod) 1001 securityContext.runAsUser User ID for the container (ZooKeeper master pod) 1001 resources CPU/Memory resource requests/limits Memory: 256Mi , CPU: 250m livenessProbe Liveness probe configuration for ZooKeeper Check values.yaml file readinessProbe Readiness probe configuration for ZooKeeper Check values.yaml file extraVolumes Extra volumes nil extraVolumeMounts Mount extra volume(s) nil podDisruptionBudget.maxUnavailable Max number of pods down simultaneously 1","title":"Statefulset parameters"},{"location":"sconeapps_zookeeper/#exposure-parameters","text":"Parameter Description Default service.type Kubernetes Service type ClusterIP service.loadBalancerIP Use with service.type LoadBalancer to assign static IP to Load Balancer instance \"\" service.port ZooKeeper port 2181 service.followerPort ZooKeeper follower port 2888 service.electionPort ZooKeeper election port 3888 service.publishNotReadyAddresses If the ZooKeeper headless service should publish DNS records for not ready pods true serviceAccount.create Enable creation of ServiceAccount for zookeeper pod false serviceAccount.name The name of the service account to use. If not set and create is true , a name is generated Generated using the zookeeper.fullname template service.tls.client_enable Enable tls for client connections false service.tls.quorum_enable Enable tls for quorum protocol false service.tls.disable_base_client_port Remove client port from service definitions. false service.tls.client_port Service port for tls client connections 3181 service.tls.client_keystore_path KeyStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_key_store/key_store_file service.tls.client_keystore_password KeyStore password. You can use environment variables. nil service.tls.client_truststore_path TrustStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_trust_store/trust_store_file service.tls.client_truststore_password TrustStore password. You can use environment variables. nil service.tls.quorum_keystore_path KeyStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_key_store/key_store_file service.tls.quorum_keystore_password KeyStore password. You can use environment variables. nil service.tls.quorum_truststore_path TrustStore file path. Refer to extraVolumes amd extraVolumeMounts for mounting files into the pods /tls_trust_store/trust_store_file service.tls.quorum_truststore_password TrustStore password. You can use environment variables. nil service.annotations Annotations for the Service {} service.headless.annotations Annotations for the Headless Service {} networkPolicy.enabled Enable NetworkPolicy false networkPolicy.allowExternal Don't require client label for connections true","title":"Exposure parameters"},{"location":"sconeapps_zookeeper/#persistence-parameters","text":"Parameter Description Default persistence.enabled Enable Zookeeper data persistence using PVC true persistence.existingClaim Provide an existing PersistentVolumeClaim nil (evaluated as a template) persistence.storageClass PVC Storage Class for ZooKeeper data volume nil persistence.accessMode PVC Access Mode for ZooKeeper data volume ReadWriteOnce persistence.size PVC Storage Request for ZooKeeper data volume 8Gi persistence.annotations Annotations for the PVC {} (evaluated as a template) persistence.dataLogDir.size PVC Storage Request for ZooKeeper's Data log directory 8Gi persistence.dataLogDir.existingClaim Provide an existing PersistentVolumeClaim for Zookeeper's Data log directory nil (evaluated as a template)","title":"Persistence parameters"},{"location":"sconeapps_zookeeper/#volume-permissions-parameters","text":"Parameter Description Default volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to runAsUser:fsGroup false volumePermissions.image.registry Init container volume-permissions image registry docker.io volumePermissions.image.repository Init container volume-permissions image name minideb volumePermissions.image.tag Init container volume-permissions image tag buster volumePermissions.image.pullPolicy Init container volume-permissions image pull policy Always volumePermissions.resources Init container resource requests/limit nil","title":"Volume Permissions parameters"},{"location":"sconeapps_zookeeper/#metrics-parameters","text":"Parameter Description Default metrics.enabled Enable prometheus to access zookeeper metrics endpoint false metrics.containerPort Port where a Jetty server will expose Prometheus metrics 9141 metrics.service.type Kubernetes service type ( ClusterIP , NodePort or LoadBalancer ) for Jetty server exposing Prometheus metrics ClusterIP metrics.service.port Prometheus metrics service port 9141 metrics.service.annotations Service annotations for Prometheus to auto-discover the metrics endpoint {prometheus.io/scrape: \"true\", prometheus.io/port: \"9141\"} metrics.serviceMonitor.enabled if true , creates a Prometheus Operator ServiceMonitor (also requires metrics.enabled to be true ) false metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource The Release Namespace metrics.serviceMonitor.interval Interval at which metrics should be scraped. nil (Prometheus Operator default value) metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended nil (Prometheus Operator default value) metrics.serviceMonitor.selector Prometheus instance selector labels nil metrics.prometheusRule.enabled if true , creates a Prometheus Operator PrometheusRule (also requires metrics.enabled to be true and metrics.prometheusRule.rules ) false metrics.prometheusRule.namespace Namespace for the PrometheusRule Resource The Release Namespace metrics.prometheusRule.selector Prometheus instance selector labels nil metrics.prometheusRule.rules Prometheus Rule definitions (see values.yaml for examples) [] Specify each parameter using the --set key=value[,key=value] argument to helm install . For example, $ helm install my-release \\ --set auth.clientUser = newUser \\ sconeapps/zookeeper The above command sets the ZooKeeper user to newUser . Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example, $ helm install my-release -f values.yaml sconeapps/zookeeper One can use the default values.yaml in the SconeApps repo","title":"Metrics parameters"},{"location":"sconeapps_zookeeper/#configuration-and-installation-details","text":"","title":"Configuration and installation details"},{"location":"sconeapps_zookeeper/#rolling-vs-immutable-tags","text":"It is strongly recommended to use immutable tags in a production environment. This ensures your deployment does not change automatically if the same tag is updated with a different image.","title":"Rolling VS Immutable tags"},{"location":"sconeapps_zookeeper/#production-configuration","text":"This chart includes a values-production.yaml file where you can find some parameters oriented to production configuration in comparison to the regular values.yaml . You can use this file instead of the default one. Number of ZooKeeper nodes: - replicaCount: 1 + replicaCount: 3 Enable prometheus metrics: - metrics.enabled: false + metrics.enabled: true","title":"Production configuration"},{"location":"sconeapps_zookeeper/#log-level","text":"You can configure the ZooKeeper log level using the ZOO_LOG_LEVEL environment variable. By default, it is set to ERROR because of each readiness probe produce an INFO message on connection and a WARN message on disconnection.","title":"Log level"},{"location":"sconeinstall/","text":"Installing SCONE Typically, you will use the SCONE crosscompilers and CLI inside of a container. Hence, just start a container that contains SCONE in simulation mode or in hardware mode . In case you want to executed your programs on a host , you can compile in a container and move the generated binary to the host and execute on this host. In case you want to distribute your programs in container images and execute them in containers, you probably want to use a Dockerfile to compile your program and generate your container image .","title":"Installing SCONE"},{"location":"sconeinstall/#installing-scone","text":"Typically, you will use the SCONE crosscompilers and CLI inside of a container. Hence, just start a container that contains SCONE in simulation mode or in hardware mode . In case you want to executed your programs on a host , you can compile in a container and move the generated binary to the host and execute on this host. In case you want to distribute your programs in container images and execute them in containers, you probably want to use a Dockerfile to compile your program and generate your container image .","title":"Installing SCONE"},{"location":"sconify/","text":"Workflow: Lift-And-Shift TL;DR We show how to build a container image such that the service deployed by this image runs automagically inside of SGX enclaves. This workflow uses a \"native\" container image as input (typically, generated by an existing CI pipeline) and translates this into an image such that all files are protected and the service runs inside of an enclave. Sconify Container Images The general workflow is as follows. We add in an existing CI pipeline an extra stage that sconifies (i.e., converts) a native image into a confidential image. This sconification is typically performed as a stage of a CI pipeline. For a node -based application/service, this might look as follows: The actual transform is controlled via some arguments. Typically, one would select the appropriate arguments when setting up the pipeline. The image transform would be completely automated and executed as part of the CI pipeline. We provide a simple variant of sconifiy as part of the community edition. A full variant provides all feature required for production use. File Types A container image typically contains files with different lifetimes . Some of these files are ephemeral , i.e., are lost if either the service crashes or the container terminates. Other files must be persistent , i.e., they need to survive even after the service crashes or the container is terminated. Persistent files must be stored in volumes . Also, the might exist different protection requirements for the files of a container image. Some of the files might only need integrity protection , e.g., files containing library functions. Other files might need confidentiality and integrity protection , i.e., files containing novel AI algorithms written in Python. To integrity protect files, like the Python libraries, we recommend to use the SCONE binary file system . For files that need to be confidentiality and integrity protected, please use the SCONE file shield . We recommend, however, that instead of using the SCONE file shield manually to encrypt file regions, use sconify to automate this process for encrypting images, and encrypted volumes (see example ) that are automagically encrypted. Sconify Image The enterprise version of the sconify tool, uses the binary file system to embed all standard libraries of Python or Node services into the binary itself: Note that MrEnclave , i.e., the hash of the enclave, will include the measurement of all files in the binary file system. Note that files protected by the SCONE File Shield are also measured during attestation by SCONE.","title":"Sconify"},{"location":"sconify/#workflow-lift-and-shift","text":"","title":"Workflow: Lift-And-Shift"},{"location":"sconify/#tldr","text":"We show how to build a container image such that the service deployed by this image runs automagically inside of SGX enclaves. This workflow uses a \"native\" container image as input (typically, generated by an existing CI pipeline) and translates this into an image such that all files are protected and the service runs inside of an enclave.","title":"TL;DR"},{"location":"sconify/#sconify-container-images","text":"The general workflow is as follows. We add in an existing CI pipeline an extra stage that sconifies (i.e., converts) a native image into a confidential image. This sconification is typically performed as a stage of a CI pipeline. For a node -based application/service, this might look as follows: The actual transform is controlled via some arguments. Typically, one would select the appropriate arguments when setting up the pipeline. The image transform would be completely automated and executed as part of the CI pipeline. We provide a simple variant of sconifiy as part of the community edition. A full variant provides all feature required for production use.","title":"Sconify Container Images"},{"location":"sconify/#file-types","text":"A container image typically contains files with different lifetimes . Some of these files are ephemeral , i.e., are lost if either the service crashes or the container terminates. Other files must be persistent , i.e., they need to survive even after the service crashes or the container is terminated. Persistent files must be stored in volumes . Also, the might exist different protection requirements for the files of a container image. Some of the files might only need integrity protection , e.g., files containing library functions. Other files might need confidentiality and integrity protection , i.e., files containing novel AI algorithms written in Python. To integrity protect files, like the Python libraries, we recommend to use the SCONE binary file system . For files that need to be confidentiality and integrity protected, please use the SCONE file shield . We recommend, however, that instead of using the SCONE file shield manually to encrypt file regions, use sconify to automate this process for encrypting images, and encrypted volumes (see example ) that are automagically encrypted.","title":"File Types"},{"location":"sconify/#sconify-image","text":"The enterprise version of the sconify tool, uses the binary file system to embed all standard libraries of Python or Node services into the binary itself: Note that MrEnclave , i.e., the hash of the enclave, will include the measurement of all files in the binary file system. Note that files protected by the SCONE File Shield are also measured during attestation by SCONE.","title":"Sconify Image"},{"location":"sconify_image/","text":"Sconify Container Images (Community Version) To integrate with existing container image pipelines, we support the encryption of existing images. In this example, we first, generate a native image that contains a Flask-based application: this image is the result of an existing image generation pipeline second, we sconify this native image, i.e., we generate an encrypted image in which all Python code and dependencies are encrypted. Note that is required for both integrity as well as confidentiality of the Python code. generate a SCONE security policy that ensures that only our application can read the code in clear text sconify_image supports only Alpine Linux-based images This 1-step conversion supports native images based on Alpine Linux only. We plan to support Ubuntu-based images in the near future too. This second step uses a SCONE community version of Python instead of actually sconifying the native Python binary. For the commercial version, we support to convert a binary suh that it runs inside of an SGX enclave. This community version is only intended for development To run the sconified binary in production, the binary must be signed using our scone-signer utility: only signed binaries can be considered sufficiently secure! Moreover, this code does not attest the SCONE CAS, i.e., the SCONE Configuration and Attestation service. You can clone the code of this example as follows: git clone https://github.com/scontain/sconify_image.git cd sconify_image Stage One: Native Image Generation We generate our native image with the help of a Dockerfile by executing: ./create_native_image.sh You could try out this image by running docker-compose --file native-docker-compose.yml up You can now submit requests as follows: export URL = https://api:4996 curl -k -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 curl -k -X GET ${ URL } /patient/patient_3 --resolve api:4996:127.0.0.1 Stage Two: Encrypted Image We now transform the native image that we generated in stage one, into a new encrypted image in which the application runs inside of an SGX enclave. Determine directories that we need to encrypt One can use a dynamic analyis approach to determine which directories are accessed by the application: docker run --init --cap-add = SYS_PTRACE -it --rm native_flask_restapi_image timeout 30 sh -c \"apk add --no-cache strace ; strace python3 /app/rest_api.py\" > strace.log One can see that the application access the following directories: /usr/local/lib /app /usr/lib/python3.7 Security Policy We provide a default security policy template (see file session-template.yml ). In this example, we need to provide the application, i.e., flask, with a certificate and a private key. We added two hooks in the default policy to 1) define secrets in the policy, and 2) inject these files in the filesystem of the application, i.e., they are only visible to the application after successfully attesting the application. We define for our application a TLS certificate ( flask ) and a private key ( flask_key ). Note that we specify a DNS name api , i.e., clients need to access this service as https://api . To generate a certificate, we require a CA (Certification Authority). Hence, we generate a CA ( api_ca_cert ) and the private key of the CA ( api_ca_key ). export SECRETS = $( cat <<EOF secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api EOF ) The certificate and the private key need to be made available to the program - which expects these at paths /tls/flask.crt and /tls/flask.key , respectively. Hence, we inject these secrets into files: export INJECTED_FILES = $( cat <<EOF injection_files: - path: \"/tls/flask.crt\" content: \\$\\$SCONE::flask.crt\\$\\$ - path: \"/tls/flask.key\" content: \"\\$\\$SCONE::flask.key\\$\\$\" EOF ) Generate the encrypted image We define some environment variables to sconify the image The name of the container image that was created by ./create_native_image.sh : export NATIVE_IMAGE = \"native_flask_restapi_image\" The name of the generated encrypted container image: export IMAGE = \"flask_restapi_image\" We use a public SCONE CAS to store the session policies export SCONE_CAS_ADDR = \"4-2-1.scone-cas.cf\" we define a random namespace for this policy: export NAMESPACE = \"my_namespace- $RANDOM \" we define the binary that needs to be sconified: export BINARY = \"/usr/bin/python3\" Now, we can create the encrypted image, instantiate policy template and upload the policy in one step: export SESSION = $( ./sconify_image --namespace = $NAMESPACE --name = flask --from = $NATIVE_IMAGE --to = $IMAGE --cas = $SCONE_CAS_ADDR --dir = \"/home\" --dir = \"/usr/local/lib\" --dir = \"/app\" --dir = \"/usr/lib/python3.7\" --binary = $BINARY ) Environment variable SESSION will contain the name of the session, which in this case would be \"$NAMESPACE-flask\" because we set the name of the session to be flask by passing argument --name=flask . Sconification of binary The community version of sconify_image requires the specification of a --base image that contains the sconified binary. The commercial version can also sconify the binary, i.e., convert it such that it can run automagically inside of SGX enclaves and it can sign the binary to run this in production. STEP 3: Execute Encrypted Image We show how to run this locally by executing. We first determine the SGX device name of the local computer: export DEVICE = $( ./determine_sgx_device ) and then we run the encrypted image using docker-compose : docker-compose up We show next how to run a little more elaborated version on AKS.","title":"Sconify Image"},{"location":"sconify_image/#sconify-container-images-community-version","text":"To integrate with existing container image pipelines, we support the encryption of existing images. In this example, we first, generate a native image that contains a Flask-based application: this image is the result of an existing image generation pipeline second, we sconify this native image, i.e., we generate an encrypted image in which all Python code and dependencies are encrypted. Note that is required for both integrity as well as confidentiality of the Python code. generate a SCONE security policy that ensures that only our application can read the code in clear text sconify_image supports only Alpine Linux-based images This 1-step conversion supports native images based on Alpine Linux only. We plan to support Ubuntu-based images in the near future too. This second step uses a SCONE community version of Python instead of actually sconifying the native Python binary. For the commercial version, we support to convert a binary suh that it runs inside of an SGX enclave. This community version is only intended for development To run the sconified binary in production, the binary must be signed using our scone-signer utility: only signed binaries can be considered sufficiently secure! Moreover, this code does not attest the SCONE CAS, i.e., the SCONE Configuration and Attestation service. You can clone the code of this example as follows: git clone https://github.com/scontain/sconify_image.git cd sconify_image","title":"Sconify Container Images (Community Version)"},{"location":"sconify_image/#stage-one-native-image-generation","text":"We generate our native image with the help of a Dockerfile by executing: ./create_native_image.sh You could try out this image by running docker-compose --file native-docker-compose.yml up You can now submit requests as follows: export URL = https://api:4996 curl -k -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 curl -k -X GET ${ URL } /patient/patient_3 --resolve api:4996:127.0.0.1","title":"Stage One: Native Image Generation"},{"location":"sconify_image/#stage-two-encrypted-image","text":"We now transform the native image that we generated in stage one, into a new encrypted image in which the application runs inside of an SGX enclave.","title":"Stage Two: Encrypted Image"},{"location":"sconify_image/#determine-directories-that-we-need-to-encrypt","text":"One can use a dynamic analyis approach to determine which directories are accessed by the application: docker run --init --cap-add = SYS_PTRACE -it --rm native_flask_restapi_image timeout 30 sh -c \"apk add --no-cache strace ; strace python3 /app/rest_api.py\" > strace.log One can see that the application access the following directories: /usr/local/lib /app /usr/lib/python3.7","title":"Determine directories that we need to encrypt"},{"location":"sconify_image/#security-policy","text":"We provide a default security policy template (see file session-template.yml ). In this example, we need to provide the application, i.e., flask, with a certificate and a private key. We added two hooks in the default policy to 1) define secrets in the policy, and 2) inject these files in the filesystem of the application, i.e., they are only visible to the application after successfully attesting the application. We define for our application a TLS certificate ( flask ) and a private key ( flask_key ). Note that we specify a DNS name api , i.e., clients need to access this service as https://api . To generate a certificate, we require a CA (Certification Authority). Hence, we generate a CA ( api_ca_cert ) and the private key of the CA ( api_ca_key ). export SECRETS = $( cat <<EOF secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api EOF ) The certificate and the private key need to be made available to the program - which expects these at paths /tls/flask.crt and /tls/flask.key , respectively. Hence, we inject these secrets into files: export INJECTED_FILES = $( cat <<EOF injection_files: - path: \"/tls/flask.crt\" content: \\$\\$SCONE::flask.crt\\$\\$ - path: \"/tls/flask.key\" content: \"\\$\\$SCONE::flask.key\\$\\$\" EOF )","title":"Security Policy"},{"location":"sconify_image/#generate-the-encrypted-image","text":"We define some environment variables to sconify the image The name of the container image that was created by ./create_native_image.sh : export NATIVE_IMAGE = \"native_flask_restapi_image\" The name of the generated encrypted container image: export IMAGE = \"flask_restapi_image\" We use a public SCONE CAS to store the session policies export SCONE_CAS_ADDR = \"4-2-1.scone-cas.cf\" we define a random namespace for this policy: export NAMESPACE = \"my_namespace- $RANDOM \" we define the binary that needs to be sconified: export BINARY = \"/usr/bin/python3\" Now, we can create the encrypted image, instantiate policy template and upload the policy in one step: export SESSION = $( ./sconify_image --namespace = $NAMESPACE --name = flask --from = $NATIVE_IMAGE --to = $IMAGE --cas = $SCONE_CAS_ADDR --dir = \"/home\" --dir = \"/usr/local/lib\" --dir = \"/app\" --dir = \"/usr/lib/python3.7\" --binary = $BINARY ) Environment variable SESSION will contain the name of the session, which in this case would be \"$NAMESPACE-flask\" because we set the name of the session to be flask by passing argument --name=flask . Sconification of binary The community version of sconify_image requires the specification of a --base image that contains the sconified binary. The commercial version can also sconify the binary, i.e., convert it such that it can run automagically inside of SGX enclaves and it can sign the binary to run this in production.","title":"Generate the encrypted image"},{"location":"sconify_image/#step-3-execute-encrypted-image","text":"We show how to run this locally by executing. We first determine the SGX device name of the local computer: export DEVICE = $( ./determine_sgx_device ) and then we run the encrypted image using docker-compose : docker-compose up We show next how to run a little more elaborated version on AKS.","title":"STEP 3: Execute Encrypted Image"},{"location":"secure_doc_management/","text":"Secure Document Manager Application This application demo is a simple secure document web application. This service enables users to securely upload and download documents. Users can thereby authenticate themselves by creating accounts and logging in. The application is made up of multiple components: - a Python FastAPI service serving as application's REST API - a MariaDB which stores the documents and user authentication data - a Memcached service, serving as a rate limiter for the application - an NGINX, which serves as a proxy server for the application, ensuring termination and forwarding with TLS All of these components run securely inside of enclaves using the SCONE framework. These services are further integrity protected, and attest each other transparently using TLS in conjunction with a SCONE Configuration and Attestation Service (CAS). Further, the application protects the confidentiality and integrity of all data it receives. We deploy this application using helm . An overview of how these different components interact with one another is as follows: Prerequisites A Kubernetes cluster Helm setup was performed The SGX Plugin is installed TL;DR ask us for the corresponding permissions (registry secret, repository access) pull this repository create the corresponding kubernetes secret for the registry and then $ export IMAGE_PULL_SECRET=<your_secret> set target images that you can push to for MEMCACHED_TARGET_IMAGE , NGINX_TARGET_IMAGE and MARIADB_TARGET_IMAGE (again with $ export ... ) execute $ ./setup-secure-doc-management.sh to use application: kubectl exec --stdin --tty $(kubectl get pods --selector=app.kubernetes.io/name=client-scone -o jsonpath='{.items[*].metadata.name}') -- /bin/bash available commands for the REST API are documented in ./secure-doc-management/templates/NOTES.txt (displayed after script is finished terminating) Secure FastAPI service We implement a simple FastAPI based service. The python code implements a REST API: we create an account by sending a request to /users/create_account with the flag -u<username>:<password> all subsequent requests containing the -u<username>:<password> flag authenticate this user we upload documents by sending a POST to /documents/create_document with a json containing the names record_id with the value of the ID of the record, and content , with the value of the documents contents. we get a document by sending a GET request to /documents/<record_id> , retrieving the document with the corresponding record ID We execute the Python service within an enclave, thus ensuring that even privileged adversaries with root access cannot read the contents of the documents from the Python process. Binary File system This FastAPI service employs the new SCONE features binary-fs . We thereby bind the file system of the service to the binary. Among other security advantages, the MRENCLAVE of the service relfects this inclusion of the file system. Thus, we can verify that the Python service is indeed running the code we expect it to. For more information, please refer to the binary fs docs . Secure MariaDB We store the documents and user authentication information in a secure MariaDB, which we also run inside of an enclave. Further, we ensure its memory remains encrypted. As such, the documents and login data users store inside the database remain secure. We further elaborate on the database in this section . Secure Memcached Rate Limiter To ensure that our MariaDB is not overloaded by excess requests, we limit requests per user using a Memcached service. We thereby ensure that a user can make at most 10 requests per minute. If the user's requests exceed that limit, we do not serve requests to their corresponding IP address for the remainder of the minute. We achieve this limiter by storing the user's IP and their number of requests in a key/value pair in the Memcached service in our Python program : mc_client.set(key=client_ip, value=1, expire=60) . Upon a request, we increment the key/value pair using Memcached's incr command: mc_client.incr(key=client_ip, value=1) . If this value exceeds 10, we do not serve the requests. The key value pair expires after 60 seconds. As we attribute the IP of a user to Personally Identifiable Information (PII), we have to protect this data. We ensure this protection by running the Memcached inside of an enclave. Secure NGINX Proxy Server We employ an NGINX to be the main API of the service, and to ensure secure communication over the wire with TLS. The NGINX requires users to send their requests over TLS, as we specify in its configuration . The NGINX then forwards such requests to the FastAPI service, also using TLS. It thereby specifies the correct FastAPI port, thus enabling the user to send the service requests without having to specify the port themselves. As the NGINX thereby receives the sensitive information of the documents, we run the NGINX proxy server in an enclave as well. TLS Certificates We must secure the communication between 1) the application's services and between 2) the user and the application. Therefore, we need to issue and provision multiple certificates: MariaDB: requires server certificates Memcached: requires server certificates FastAPI: requires server certificates and client certificates for the Memcached and MariaDB for client verification NGINX: requires server certificates, whereby the certification authority (CA) certificate must be accessible to users, and client certificates for the FastAPI for client verification An overview of how these certificates secure the communication is as follows: We provision these certificates using SCONE policies . We now illustrate the use of the policies for certificate provisioning by inspecting the NGINX policy . secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true In the first section, # nginx - fastapi tls , the NGINX session imports the client and the server CA certificates from the FastAPI session. It thereby uses the CA certificate to identify the FastAPI service, and identifies itself to the service using the client certificates. In the second section, # (...) nginx - client tls , we issue server certificates for the NGINX. For the NGINX Server CA certificate, we set export_public: true . This settings allows users to extract the CA certificate corresponding to this policy, thus enabling them to verify the NGINX server. To make the certificates available to the NGINX service, we inject the certificates into the NGINX file system. We specify this injection in the NGINX policy using the previous SCONE policy secrets as follows: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ # specific for nginx - client tls - path : /etc/nginx/server.crt content : $$SCONE::server.crt$$ - path : /etc/nginx/server.key content : $$SCONE::server.key$$ The NGINX can then access these injected files as normal files, e.g., in its configuration: events {} http { server { listen 443 ssl; server_name secure-doc-management-nginx-scone; ssl_certificate /etc/nginx/server.crt; ssl_certificate_key /etc/nginx/server.key; location / { proxy_pass https://secure-doc-management-fastapi-scone:8000; proxy_ssl_certificate /etc/nginx/fastapi-client.crt; proxy_ssl_certificate_key /etc/nginx/fastapi-client.key; proxy_ssl_trusted_certificate /etc/nginx/fastapi-ca.crt; proxy_ssl_verify on; proxy_ssl_session_reuse on; } } } TLS-Based Mutual Attestation As we observed in the previous section, we encrypt the communication between the NGINX and the FastAPI using TLS, enabled by SCONE policies. The NGINX and FastAPI policies thereby export the necessary certificates to each other by referencing each other in their policies. They thereby attest each other. They achieve this attestation by verifying the policies that they reference in their own policy. Furthermore, they check that the corresponding service satisfies all requirements specified in the service's policy. We can easily enforce mutual attestation using TLS client authentication. We illustrate this attestation with the FastAPI's policy , which ensures TLS-based attestation between itself and the NGINX service. This FastAPI policy generates a FastAPI CA certificate ( FASTAPI_CA_CERT ) and a FastAPI server certificate ( fastapi ) as well as a corresponding FastAPI client certificate ( FASTAPI_CLIENT_CERT ) and client key ( FASTAPI_CLIENT_KEY ). The policy exports this certificate and private key to the NGINX policy. secrets : ... # specific for fastapi - nginx tls - name : fastapi-key # automatically generate FASTAPI server certificate kind : private-key - name : fastapi # automatically generate FASTAPI server certificate private_key : fastapi-key issuer : FASTAPI_CA_CERT kind : x509 dns : - $FASTAPI_HOST - name : FASTAPI_CLIENT_KEY kind : private-key export : - session : $NGINX_SESSION - name : FASTAPI_CLIENT_CERT # automatically generate client certificate private_key : FASTAPI_CLIENT_KEY issuer : FASTAPI_CA_CERT common_name : FASTAPI_CLIENT_CERT kind : x509 export : - session : $NGINX_SESSION # export client cert/key to upload session - name : FASTAPI_CA_KEY # export session CA certificate as FASTAPI CA certificate kind : private-key - name : FASTAPI_CA_CERT # export session CA certificate as FASTAPI CA certificate kind : x509-ca common_name : FASTAPI_CA private_key : FASTAPI_CA_KEY export : - session : $NGINX_SESSION # export the session CA certificate to upload session We thereby replace $NGINX_SESSION with the policy name of the NGINX. For increased security, we can also specify the corresponding session hash . In this scenario, the policies are on the same SCONE CAS . In more complex cases, the policies can be stored on different SCONE CAS . The NGINX can then import the FastAPI CA certificate, client certificate and private key, as we saw before: secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true $FASTAPI_SESSION is the exporting policy of the FastAPI. The NGINX can then use these exported secrets, as we also saw before: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ To ensure that the SCONE CAS supplying these policies is authentic, we typically attest the SCONE CAS before uploading any policies using the SCONE CAS CLI . Binary FS for FastAPI As we mentioned before, we secure our FastAPI server using the binary-fs . We use the tool while building the initial Docker image. We seperate this process in three stages. In the first stage, we apply the binaryfs command to create a file system .c file. In the second stage, we compile the .c file into an .so file using the scone gcc . In the third stage, we link the generated .so file into the binary using patchelf . We apply these stages in the Dockerfile as follows: # First stage: apply the binary-fs FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5.1.0-binaryFS AS binary-fs COPY rest_api.py /. COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt RUN rm /usr/lib/python3.7/config-3.7m-x86_64-linux-gnu/libpython3.7m.a && \\ SCONE_MODE = auto scone binaryfs / /binary-fs.c -v \\ --include '/usr/lib/python3.7/*' \\ --include /lib/libssl.so.1.1 \\ --include /lib/libcrypto.so.1.1 \\ --include '/lib/libz.so.1*' \\ --include '/usr/lib/libbz2.so.1*' \\ --include '/usr/lib/libsqlite3.so.0*' \\ --include '/usr/lib/libev.so.4*' \\ --include '/usr/lib/libffi.so.6*' \\ --include '/usr/lib/libexpat.so.1*' \\ --include /rest_api.py # Second stage: compile the binary fs FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine-scone5.1.0-binaryFS as crosscompiler COPY --from = binary-fs /binary-fs.c /. RUN scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so # Third stage: patch the binary-fs into the enclave executable FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5.1.0-binaryFS COPY --from = crosscompiler /libbinary-fs.so /. RUN apk add --no-cache patchelf && \\ patchelf --add-needed libbinary-fs.so ` which python3 ` && \\ apk del patchelf ENV SCONE_HEAP = 512M ENV SCONE_LOG = debug ENV LD_LIBRARY_PATH = \"/\" CMD sh -c \"python3 /rest_api.py\" The binary also requires certain /etc/ files for networking within the cluster, e.g., /etc/resolv.conf . Currently, we simply inject these files into the confidential service using the FastAPI's policy . This workaround is only temporary- we are working on making these files accessible without file injection while running the service, as the Kubernetes cluster should be able to alter these files during execution. Code The source code of this example is available on our repository- ask us for permission, and then continue with the following commands: git clone https://gitlab.scontain.com/enterJazz/secure-doc-management.git cd secure-doc-management Execution on a Kubernetes Cluster We intend this example to be run on a Kubernetes cluster. Install SCONE Services Get access to SconeApps (see helm.md): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update Give SconeApps access to the private docker images (see the helm docs ): export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Label your Kubernetes nodes that are sgx -enabled: kubectl label nodes <my-node> sgx = enabled Start Local Attestation Service (LAS) with Azure (we use a remote CAS): helm install las sconeapps/las \\ --set useSGXDevPlugin = azure \\ --set sgxEpcMem = 8 \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 \\ --set nodeSelector.sgx = \"enabled\" To setup without Azure: helm install las sconeapps/las \\ --set service.hostPort = true \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 helm install sgxdevplugin sconeapps/sgxdevplugin Run the Application Start by by specifying Docker image repositories to which you may push: export MARIADB_TARGET_IMAGE = your/repo:mariadb-protected export MEMCACHED_TARGET_IMAGE = your/repo:memcached-tls-protected export NGINX_TARGET_IMAGE = your/repo:nginx-proxy-server-protected Then use the Helm chart in ./secure-doc-management to deploy the application to a Kubernetes cluster. Note that by default this script deploys the service on Azure Kubernetes Cluster . We strongly recommend using the script: export NAMESPACE = <your_kubernetes_namespace> # e.g. default ./setup-secure-doc-management.sh # without Azure: # export USE_AZURE=false; ./setup-secure-doc-management.sh Test the Application After all resources are Running , you can test the API via Helm: helm test secure-doc-management Helm will run a pod with a couple of pre-set queries to check if the API is working properly. Access the Application For ease of use, we access the application within the cluster through a client pod, which the helm charts also deploys. kubectl exec --stdin --tty $( kubectl get pods --selector = app.kubernetes.io/name = client-scone -o jsonpath = '{.items[*].metadata.name}' ) -- /bin/bash The IP of the host is stored under $NGINX_HOST in the pod. The pod has also retrieved the certificate of the application beforehand, in /tmp/nginx-ca.crt . The exact commands to establish the secure communication with the application are: echo \"Attest SCONE CAS\" scone cas attest --accept-configuration-needed --accept-group-out-of-date --only_for_testing-debug --only_for_testing-ignore-signer $SCONE_CAS_ADDR $CAS_MRENCLAVE echo \"Get SCONE CAS CACERT\" scone cas show-certificate $SCONE_CAS_ADDR > /tmp/scone-ca.crt echo \"Get nginx public CACERT from CAS REST API\" echo -e $( curl --cacert /tmp/scone-ca.crt https:// $SCONE_CAS_ADDR :8081/v1/values/session = $NGINX_CONFIG_ID ,secret = SERVER_CA_CERT | jq '.value' ) | head -c -2 | tail -c +2 > /tmp/nginx-ca.crt Now, from within the pod, you can perform queries such as: curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https:// $NGINX_HOST /users/create_account curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X POST -H \"Content-Type: application/json\" -d '{\"record_id\":\"31\",\"content\":\"Ever make mistakes in life? Lets make them birds. Yeah, theyre birds now.\"}' https:// $NGINX_HOST /documents/create_document curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https://secure-doc-management-nginx-scone/documents/31 Clean Up To uninstall the charts we installed during this demo, execute: helm uninstall las helm uninstall secure-doc-management # if you are not using Azure: # helm uninstall sgxdevplugin","title":"Secure Document Manager Application"},{"location":"secure_doc_management/#secure-document-manager-application","text":"This application demo is a simple secure document web application. This service enables users to securely upload and download documents. Users can thereby authenticate themselves by creating accounts and logging in. The application is made up of multiple components: - a Python FastAPI service serving as application's REST API - a MariaDB which stores the documents and user authentication data - a Memcached service, serving as a rate limiter for the application - an NGINX, which serves as a proxy server for the application, ensuring termination and forwarding with TLS All of these components run securely inside of enclaves using the SCONE framework. These services are further integrity protected, and attest each other transparently using TLS in conjunction with a SCONE Configuration and Attestation Service (CAS). Further, the application protects the confidentiality and integrity of all data it receives. We deploy this application using helm . An overview of how these different components interact with one another is as follows: Prerequisites A Kubernetes cluster Helm setup was performed The SGX Plugin is installed","title":"Secure Document Manager Application"},{"location":"secure_doc_management/#tldr","text":"ask us for the corresponding permissions (registry secret, repository access) pull this repository create the corresponding kubernetes secret for the registry and then $ export IMAGE_PULL_SECRET=<your_secret> set target images that you can push to for MEMCACHED_TARGET_IMAGE , NGINX_TARGET_IMAGE and MARIADB_TARGET_IMAGE (again with $ export ... ) execute $ ./setup-secure-doc-management.sh to use application: kubectl exec --stdin --tty $(kubectl get pods --selector=app.kubernetes.io/name=client-scone -o jsonpath='{.items[*].metadata.name}') -- /bin/bash available commands for the REST API are documented in ./secure-doc-management/templates/NOTES.txt (displayed after script is finished terminating)","title":"TL;DR"},{"location":"secure_doc_management/#secure-fastapi-service","text":"We implement a simple FastAPI based service. The python code implements a REST API: we create an account by sending a request to /users/create_account with the flag -u<username>:<password> all subsequent requests containing the -u<username>:<password> flag authenticate this user we upload documents by sending a POST to /documents/create_document with a json containing the names record_id with the value of the ID of the record, and content , with the value of the documents contents. we get a document by sending a GET request to /documents/<record_id> , retrieving the document with the corresponding record ID We execute the Python service within an enclave, thus ensuring that even privileged adversaries with root access cannot read the contents of the documents from the Python process.","title":"Secure FastAPI service"},{"location":"secure_doc_management/#binary-file-system","text":"This FastAPI service employs the new SCONE features binary-fs . We thereby bind the file system of the service to the binary. Among other security advantages, the MRENCLAVE of the service relfects this inclusion of the file system. Thus, we can verify that the Python service is indeed running the code we expect it to. For more information, please refer to the binary fs docs .","title":"Binary File system"},{"location":"secure_doc_management/#secure-mariadb","text":"We store the documents and user authentication information in a secure MariaDB, which we also run inside of an enclave. Further, we ensure its memory remains encrypted. As such, the documents and login data users store inside the database remain secure. We further elaborate on the database in this section .","title":"Secure MariaDB"},{"location":"secure_doc_management/#secure-memcached-rate-limiter","text":"To ensure that our MariaDB is not overloaded by excess requests, we limit requests per user using a Memcached service. We thereby ensure that a user can make at most 10 requests per minute. If the user's requests exceed that limit, we do not serve requests to their corresponding IP address for the remainder of the minute. We achieve this limiter by storing the user's IP and their number of requests in a key/value pair in the Memcached service in our Python program : mc_client.set(key=client_ip, value=1, expire=60) . Upon a request, we increment the key/value pair using Memcached's incr command: mc_client.incr(key=client_ip, value=1) . If this value exceeds 10, we do not serve the requests. The key value pair expires after 60 seconds. As we attribute the IP of a user to Personally Identifiable Information (PII), we have to protect this data. We ensure this protection by running the Memcached inside of an enclave.","title":"Secure Memcached Rate Limiter"},{"location":"secure_doc_management/#secure-nginx-proxy-server","text":"We employ an NGINX to be the main API of the service, and to ensure secure communication over the wire with TLS. The NGINX requires users to send their requests over TLS, as we specify in its configuration . The NGINX then forwards such requests to the FastAPI service, also using TLS. It thereby specifies the correct FastAPI port, thus enabling the user to send the service requests without having to specify the port themselves. As the NGINX thereby receives the sensitive information of the documents, we run the NGINX proxy server in an enclave as well.","title":"Secure NGINX Proxy Server"},{"location":"secure_doc_management/#tls-certificates","text":"We must secure the communication between 1) the application's services and between 2) the user and the application. Therefore, we need to issue and provision multiple certificates: MariaDB: requires server certificates Memcached: requires server certificates FastAPI: requires server certificates and client certificates for the Memcached and MariaDB for client verification NGINX: requires server certificates, whereby the certification authority (CA) certificate must be accessible to users, and client certificates for the FastAPI for client verification An overview of how these certificates secure the communication is as follows: We provision these certificates using SCONE policies . We now illustrate the use of the policies for certificate provisioning by inspecting the NGINX policy . secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true In the first section, # nginx - fastapi tls , the NGINX session imports the client and the server CA certificates from the FastAPI session. It thereby uses the CA certificate to identify the FastAPI service, and identifies itself to the service using the client certificates. In the second section, # (...) nginx - client tls , we issue server certificates for the NGINX. For the NGINX Server CA certificate, we set export_public: true . This settings allows users to extract the CA certificate corresponding to this policy, thus enabling them to verify the NGINX server. To make the certificates available to the NGINX service, we inject the certificates into the NGINX file system. We specify this injection in the NGINX policy using the previous SCONE policy secrets as follows: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ # specific for nginx - client tls - path : /etc/nginx/server.crt content : $$SCONE::server.crt$$ - path : /etc/nginx/server.key content : $$SCONE::server.key$$ The NGINX can then access these injected files as normal files, e.g., in its configuration: events {} http { server { listen 443 ssl; server_name secure-doc-management-nginx-scone; ssl_certificate /etc/nginx/server.crt; ssl_certificate_key /etc/nginx/server.key; location / { proxy_pass https://secure-doc-management-fastapi-scone:8000; proxy_ssl_certificate /etc/nginx/fastapi-client.crt; proxy_ssl_certificate_key /etc/nginx/fastapi-client.key; proxy_ssl_trusted_certificate /etc/nginx/fastapi-ca.crt; proxy_ssl_verify on; proxy_ssl_session_reuse on; } } }","title":"TLS Certificates"},{"location":"secure_doc_management/#tls-based-mutual-attestation","text":"As we observed in the previous section, we encrypt the communication between the NGINX and the FastAPI using TLS, enabled by SCONE policies. The NGINX and FastAPI policies thereby export the necessary certificates to each other by referencing each other in their policies. They thereby attest each other. They achieve this attestation by verifying the policies that they reference in their own policy. Furthermore, they check that the corresponding service satisfies all requirements specified in the service's policy. We can easily enforce mutual attestation using TLS client authentication. We illustrate this attestation with the FastAPI's policy , which ensures TLS-based attestation between itself and the NGINX service. This FastAPI policy generates a FastAPI CA certificate ( FASTAPI_CA_CERT ) and a FastAPI server certificate ( fastapi ) as well as a corresponding FastAPI client certificate ( FASTAPI_CLIENT_CERT ) and client key ( FASTAPI_CLIENT_KEY ). The policy exports this certificate and private key to the NGINX policy. secrets : ... # specific for fastapi - nginx tls - name : fastapi-key # automatically generate FASTAPI server certificate kind : private-key - name : fastapi # automatically generate FASTAPI server certificate private_key : fastapi-key issuer : FASTAPI_CA_CERT kind : x509 dns : - $FASTAPI_HOST - name : FASTAPI_CLIENT_KEY kind : private-key export : - session : $NGINX_SESSION - name : FASTAPI_CLIENT_CERT # automatically generate client certificate private_key : FASTAPI_CLIENT_KEY issuer : FASTAPI_CA_CERT common_name : FASTAPI_CLIENT_CERT kind : x509 export : - session : $NGINX_SESSION # export client cert/key to upload session - name : FASTAPI_CA_KEY # export session CA certificate as FASTAPI CA certificate kind : private-key - name : FASTAPI_CA_CERT # export session CA certificate as FASTAPI CA certificate kind : x509-ca common_name : FASTAPI_CA private_key : FASTAPI_CA_KEY export : - session : $NGINX_SESSION # export the session CA certificate to upload session We thereby replace $NGINX_SESSION with the policy name of the NGINX. For increased security, we can also specify the corresponding session hash . In this scenario, the policies are on the same SCONE CAS . In more complex cases, the policies can be stored on different SCONE CAS . The NGINX can then import the FastAPI CA certificate, client certificate and private key, as we saw before: secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true $FASTAPI_SESSION is the exporting policy of the FastAPI. The NGINX can then use these exported secrets, as we also saw before: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ To ensure that the SCONE CAS supplying these policies is authentic, we typically attest the SCONE CAS before uploading any policies using the SCONE CAS CLI .","title":"TLS-Based Mutual Attestation"},{"location":"secure_doc_management/#binary-fs-for-fastapi","text":"As we mentioned before, we secure our FastAPI server using the binary-fs . We use the tool while building the initial Docker image. We seperate this process in three stages. In the first stage, we apply the binaryfs command to create a file system .c file. In the second stage, we compile the .c file into an .so file using the scone gcc . In the third stage, we link the generated .so file into the binary using patchelf . We apply these stages in the Dockerfile as follows: # First stage: apply the binary-fs FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5.1.0-binaryFS AS binary-fs COPY rest_api.py /. COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt RUN rm /usr/lib/python3.7/config-3.7m-x86_64-linux-gnu/libpython3.7m.a && \\ SCONE_MODE = auto scone binaryfs / /binary-fs.c -v \\ --include '/usr/lib/python3.7/*' \\ --include /lib/libssl.so.1.1 \\ --include /lib/libcrypto.so.1.1 \\ --include '/lib/libz.so.1*' \\ --include '/usr/lib/libbz2.so.1*' \\ --include '/usr/lib/libsqlite3.so.0*' \\ --include '/usr/lib/libev.so.4*' \\ --include '/usr/lib/libffi.so.6*' \\ --include '/usr/lib/libexpat.so.1*' \\ --include /rest_api.py # Second stage: compile the binary fs FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine-scone5.1.0-binaryFS as crosscompiler COPY --from = binary-fs /binary-fs.c /. RUN scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so # Third stage: patch the binary-fs into the enclave executable FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5.1.0-binaryFS COPY --from = crosscompiler /libbinary-fs.so /. RUN apk add --no-cache patchelf && \\ patchelf --add-needed libbinary-fs.so ` which python3 ` && \\ apk del patchelf ENV SCONE_HEAP = 512M ENV SCONE_LOG = debug ENV LD_LIBRARY_PATH = \"/\" CMD sh -c \"python3 /rest_api.py\" The binary also requires certain /etc/ files for networking within the cluster, e.g., /etc/resolv.conf . Currently, we simply inject these files into the confidential service using the FastAPI's policy . This workaround is only temporary- we are working on making these files accessible without file injection while running the service, as the Kubernetes cluster should be able to alter these files during execution.","title":"Binary FS for FastAPI"},{"location":"secure_doc_management/#code","text":"The source code of this example is available on our repository- ask us for permission, and then continue with the following commands: git clone https://gitlab.scontain.com/enterJazz/secure-doc-management.git cd secure-doc-management","title":"Code"},{"location":"secure_doc_management/#execution-on-a-kubernetes-cluster","text":"We intend this example to be run on a Kubernetes cluster.","title":"Execution on a Kubernetes Cluster"},{"location":"secure_doc_management/#install-scone-services","text":"Get access to SconeApps (see helm.md): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update Give SconeApps access to the private docker images (see the helm docs ): export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Label your Kubernetes nodes that are sgx -enabled: kubectl label nodes <my-node> sgx = enabled Start Local Attestation Service (LAS) with Azure (we use a remote CAS): helm install las sconeapps/las \\ --set useSGXDevPlugin = azure \\ --set sgxEpcMem = 8 \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 \\ --set nodeSelector.sgx = \"enabled\" To setup without Azure: helm install las sconeapps/las \\ --set service.hostPort = true \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 helm install sgxdevplugin sconeapps/sgxdevplugin","title":"Install SCONE Services"},{"location":"secure_doc_management/#run-the-application","text":"Start by by specifying Docker image repositories to which you may push: export MARIADB_TARGET_IMAGE = your/repo:mariadb-protected export MEMCACHED_TARGET_IMAGE = your/repo:memcached-tls-protected export NGINX_TARGET_IMAGE = your/repo:nginx-proxy-server-protected Then use the Helm chart in ./secure-doc-management to deploy the application to a Kubernetes cluster. Note that by default this script deploys the service on Azure Kubernetes Cluster . We strongly recommend using the script: export NAMESPACE = <your_kubernetes_namespace> # e.g. default ./setup-secure-doc-management.sh # without Azure: # export USE_AZURE=false; ./setup-secure-doc-management.sh","title":"Run the Application"},{"location":"secure_doc_management/#test-the-application","text":"After all resources are Running , you can test the API via Helm: helm test secure-doc-management Helm will run a pod with a couple of pre-set queries to check if the API is working properly.","title":"Test the Application"},{"location":"secure_doc_management/#access-the-application","text":"For ease of use, we access the application within the cluster through a client pod, which the helm charts also deploys. kubectl exec --stdin --tty $( kubectl get pods --selector = app.kubernetes.io/name = client-scone -o jsonpath = '{.items[*].metadata.name}' ) -- /bin/bash The IP of the host is stored under $NGINX_HOST in the pod. The pod has also retrieved the certificate of the application beforehand, in /tmp/nginx-ca.crt . The exact commands to establish the secure communication with the application are: echo \"Attest SCONE CAS\" scone cas attest --accept-configuration-needed --accept-group-out-of-date --only_for_testing-debug --only_for_testing-ignore-signer $SCONE_CAS_ADDR $CAS_MRENCLAVE echo \"Get SCONE CAS CACERT\" scone cas show-certificate $SCONE_CAS_ADDR > /tmp/scone-ca.crt echo \"Get nginx public CACERT from CAS REST API\" echo -e $( curl --cacert /tmp/scone-ca.crt https:// $SCONE_CAS_ADDR :8081/v1/values/session = $NGINX_CONFIG_ID ,secret = SERVER_CA_CERT | jq '.value' ) | head -c -2 | tail -c +2 > /tmp/nginx-ca.crt Now, from within the pod, you can perform queries such as: curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https:// $NGINX_HOST /users/create_account curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X POST -H \"Content-Type: application/json\" -d '{\"record_id\":\"31\",\"content\":\"Ever make mistakes in life? Lets make them birds. Yeah, theyre birds now.\"}' https:// $NGINX_HOST /documents/create_document curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https://secure-doc-management-nginx-scone/documents/31","title":"Access the Application"},{"location":"secure_doc_management/#clean-up","text":"To uninstall the charts we installed during this demo, execute: helm uninstall las helm uninstall secure-doc-management # if you are not using Azure: # helm uninstall sgxdevplugin","title":"Clean Up"},{"location":"secure_document_management/","text":"Confidential Document Manager Application This application demo is a confidential document web application. This service enables users to upload and download documents and ensures that the documents are always encrypted. Users can create accounts. We use a simple password-based authentication. For production, one should add a two-factor authentication. The application consists of the following components: a Python FastAPI service serving as the application's REST API, a MariaDB service stores the documents and user authentication data, a memcached service, serving as a rate limiter for the application, and an nginx instance serves as a proxy server for the application, ensuring termination and forwarding with TLS. All of these components run securely inside of enclaves using the SCONE framework. These services are also integrity protected, and attest each other transparently using TLS in conjunction with a SCONE Configuration and Attestation Service (CAS). Furthermore, the application protects the confidentiality and integrity of all data it receives. We deploy this application using helm . An overview of how these different components interact with one another is as follows: Prerequisites A Kubernetes cluster Helm setup was performed Either the SCONE SGX Plugin or the Azure SGX Plugin is installed. Azure Kubernetes Services (AKS) You can run this demo works on AKS. To set up the necessary infrastructure on AKS, you need to start a local attestations service: (you may have to change the tags according to your AKS Nodes) helm install las sconeapps/las \\ --set useSGXDevPlugin = azure \\ --set sgxEpcMem = 8 \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 \\ --set nodeSelector.agentpool = \"confcompool1\" TL;DR please, register an account with the SCONE registry ask us for access to SconeApps via email pull this repository create the corresponding Kubernetes secret for the registry and then $ export IMAGE_PULL_SECRET=<your_secret> set target images that you can push to for MEMCACHED_TARGET_IMAGE , NGINX_TARGET_IMAGE and MARIADB_TARGET_IMAGE (again with $ export ... ) execute $ ./setup-secure-doc-management.sh to upload the policies to SCONE CAS and issue client requests: kubectl exec --stdin --tty $(kubectl get pods --selector=app.kubernetes.io/name=client-scone -o jsonpath='{.items[*].metadata.name}') -- /bin/bash available commands for the REST API are documented in ./secure-doc-management/templates/NOTES.txt (displayed after script has terminated) Secure FastAPI service We implement a simple FastAPI-based service. The Python code implements a REST API: we create a new account by sending a request to /users/create_account with the flag -u<username>:<password> all subsequent requests containing the -u<username>:<password> flag authenticate this user we upload documents by sending a POST to /documents/create_document with a json containing the names record_id with the value of the ID of the record, and content , with the value of the documents contents. we get a document by sending a GET request to /documents/<record_id> , retrieving the document with the corresponding record ID We execute the Python service within an enclave, thus ensuring that even privileged adversaries with root access cannot read the contents of the documents from the Python process. Binary File system This FastAPI service employs the new SCONE features binary-fs . We thereby bind the file system of the service to the binary. Among other security advantages, the MRENCLAVE of the service reflects this inclusion of the file system. Thus, we can verify that the Python service is indeed running the code we expect it to run. For more information, please refer to https://sconedocs.github.io/binary_fs/ . Secure MariaDB We store the documents and user authentication information in a secure MariaDB, which we also run inside of an enclave. Further, we ensure its memory remains encrypted. As such, the documents and login data users store inside the database remain secure. We further elaborate on the database in this section . Secure Memcached Rate Limiter To ensure that our MariaDB is not overloaded by excess requests, we limit requests per user using a Memcached service. We thereby ensure that a user can make at most 10 requests per minute. If the user's requests exceed that limit, we do not serve requests to their corresponding IP address for the remainder of the minute. We implement this limiter by storing the user's IP and their number of requests in a key/value pair in the Memcached service in our Python program : mc_client.set(key=client_ip, value=1, expire=60) . Upon a request, we increment the key/value pair using Memcached's incr command: mc_client.incr(key=client_ip, value=1) . If this value exceeds 10, we do not serve the requests. The key value pair expires after 60 seconds. As we attribute the IP of a user to Personally Identifiable Information (PII), we have to protect this data. We ensure this protection by running the Memcached inside of an enclave. Secure NGINX Proxy Server We employ an NGINX to be the main API of the service, and to ensure secure communication over the wire with TLS. The NGINX requires users to send their requests over TLS, as we specify in its configuration . The NGINX then forwards such requests to the FastAPI service, also using TLS. It thereby specifies the correct FastAPI port, thus enabling the user to send the service requests without having to specify the port themselves. As the NGINX thereby receives the sensitive information of the documents, we run the NGINX proxy server in an enclave as well. TLS Certificates We must secure the communication between 1) the application's services and between 2) the user and the application. Therefore, we need to issue and provision multiple certificates: MariaDB : requires server certificates memcached : requires server certificates FastAPI : requires server certificates and client certificates for the Memcached and MariaDB for client verification nginx : requires server certificates, whereby the certification authority (CA) certificate must be accessible to users, and client certificates for the FastAPI for client verification. An overview of how these certificates secure the communication is as follows: We provision these certificates using SCONE policies . We now illustrate the use of the policies for certificate provisioning by inspecting the NGINX policy . secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true In the first section, # nginx - fastapi tls , the NGINX session imports the client and the server CA certificates from the FastAPI session. It thereby uses the CA certificate to identify the FastAPI service, and identifies itself to the service using the client certificates. In the second section, # (...) nginx - client tls , we issue server certificates for the NGINX. For the NGINX Server CA certificate, we set export_public: true . This settings allows users to extract the CA certificate corresponding to this policy, thus enabling them to verify the NGINX server. To make the certificates available to the NGINX service, we inject the certificates into the NGINX file system. We specify this injection in the NGINX policy using the previous SCONE policy secrets as follows: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ # specific for nginx - client tls - path : /etc/nginx/server.crt content : $$SCONE::server.crt$$ - path : /etc/nginx/server.key content : $$SCONE::server.key$$ The NGINX can then access these injected files as normal files, e.g., in its configuration: events {} http { server { listen 443 ssl; server_name secure-doc-management-nginx-scone; ssl_certificate /etc/nginx/server.crt; ssl_certificate_key /etc/nginx/server.key; location / { proxy_pass https://secure-doc-management-fastapi-scone:8000; proxy_ssl_certificate /etc/nginx/fastapi-client.crt; proxy_ssl_certificate_key /etc/nginx/fastapi-client.key; proxy_ssl_trusted_certificate /etc/nginx/fastapi-ca.crt; proxy_ssl_verify on; proxy_ssl_session_reuse on; } } } TLS-Based Mutual Attestation As we observed in the previous section, we encrypt the communication between the NGINX and the FastAPI using TLS, enabled by SCONE policies. The NGINX and FastAPI policies thereby export the necessary certificates to each other by referencing each other in their policies. They thereby attest each other. They achieve this attestation by verifying the policies that they reference in their own policy. Furthermore, they check that the corresponding service satisfies all requirements specified in the service's policy. We can easily enforce mutual attestation using TLS client authentication. We illustrate this attestation with the FastAPI's policy , which ensures TLS-based attestation between itself and the NGINX service. This FastAPI policy generates a FastAPI CA certificate ( FASTAPI_CA_CERT ) and a FastAPI server certificate ( fastapi ) as well as a corresponding FastAPI client certificate ( FASTAPI_CLIENT_CERT ) and client key ( FASTAPI_CLIENT_KEY ). The policy exports this certificate and private key to the NGINX policy. secrets : ... # specific for fastapi - nginx tls - name : fastapi-key # automatically generate FASTAPI server certificate kind : private-key - name : fastapi # automatically generate FASTAPI server certificate private_key : fastapi-key issuer : FASTAPI_CA_CERT kind : x509 dns : - $FASTAPI_HOST - name : FASTAPI_CLIENT_KEY kind : private-key export : - session : $NGINX_SESSION - name : FASTAPI_CLIENT_CERT # automatically generate client certificate private_key : FASTAPI_CLIENT_KEY issuer : FASTAPI_CA_CERT common_name : FASTAPI_CLIENT_CERT kind : x509 export : - session : $NGINX_SESSION # export client cert/key to upload session - name : FASTAPI_CA_KEY # export session CA certificate as FASTAPI CA certificate kind : private-key - name : FASTAPI_CA_CERT # export session CA certificate as FASTAPI CA certificate kind : x509-ca common_name : FASTAPI_CA private_key : FASTAPI_CA_KEY export : - session : $NGINX_SESSION # export the session CA certificate to upload session We thereby replace $NGINX_SESSION with the policy name of the NGINX. For increased security, we can also specify the corresponding session hash . In this scenario, the policies are on the same SCONE CAS . In more complex cases, the policies can be stored on different SCONE CAS . The NGINX can then import the FastAPI CA certificate, client certificate and private key, as we saw before: secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true $FASTAPI_SESSION is the exporting policy of the FastAPI. The NGINX can then use these exported secrets, as we also saw before: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ To ensure that the SCONE CAS supplying these policies is authentic, we typically attest the SCONE CAS before uploading any policies using the SCONE CAS CLI . Binary FS for FastAPI As we mentioned before, we secure our FastAPI server using the binary-fs . We use the tool while building the initial Docker image. We seperate this process in three stages. In the first stage, we apply the binaryfs command to create a file system .c file. In the second stage, we compile the .c file into an .so file using the scone gcc . In the third stage, we link the generated .so file into the binary using patchelf . We apply these stages in the Dockerfile as follows: # First stage: apply the binary-fs FROM sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5.0.0 AS binary-fs COPY rest_api.py /. COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt # here we apply the binary-fs command to create the file system .c file RUN rm /usr/lib/python3.7/config-3.7m-x86_64-linux-gnu/libpython3.7m.a && \\ SCONE_MODE = auto scone binaryfs / /binary-fs.c -v \\ --include '/usr/lib/python3.7/*' \\ --include /lib/libssl.so.1.1 \\ --include /lib/libcrypto.so.1.1 \\ --include '/lib/libz.so.1*' \\ --include '/usr/lib/libbz2.so.1*' \\ --include '/usr/lib/libsqlite3.so.0*' \\ --include '/usr/lib/libev.so.4*' \\ --include '/usr/lib/libffi.so.6*' \\ --include '/usr/lib/libexpat.so.1*' \\ --include /rest_api.py # Second stage: compile the binary fs FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine-scone5.0.0 as crosscompiler COPY --from = binary-fs /binary-fs.c /. RUN scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so # Third stage: patch the binary-fs into the enclave executable FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 COPY --from = crosscompiler /libbinary-fs.so /. RUN apk add --no-cache patchelf && \\ patchelf --add-needed libbinary-fs.so ` which python3 ` && \\ apk del patchelf ENV SCONE_HEAP = 512M ENV SCONE_LOG = debug ENV LD_LIBRARY_PATH = \"/\" CMD sh -c \"python3 /rest_api.py\" The binary also requires certain /etc/ files for networking within the cluster, e.g., /etc/resolv.conf . Currently, we simply inject these files into the confidential service using the FastAPI's policy . The source code of this example is available on our repository- ask us for permission, and then continue with the following commands: git clone https://gitlab.scontain.com/enterJazz/secure-doc-management.git cd secure-doc-management Execution on a Kubernetes Cluster We intend this example to be run on a Kubernetes cluster. Install SCONE Services Get access to SconeApps (see https://sconedocs.github.io/helm/ ): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/SconeApps/master/ helm repo update Give SconeApps access to the private docker images (see SconeApps ): export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Start Local Attestation Service (LAS) on Azure (we use a remote CAS in this case): # nodeSelector may have to be adjusted according to your nodes helm install las sconeapps/las \\ --set useSGXDevPlugin = azure \\ --set sgxEpcMem = 8 \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 \\ --set nodeSelector.agentpool = \"confcompool1\" To setup on a vanilla Kubernetes cluster: helm install las sconeapps/las --set service.hostPort = true helm install sgxdevplugin sconeapps/sgxdevplugin Run the Application Start by by specifying Docker image repositories to which you may push: export MARIADB_TARGET_IMAGE = your/repo:mariadb-protected export MEMCACHED_TARGET_IMAGE = your/repo:memcached-tls-protected export NGINX_TARGET_IMAGE = your/repo:nginx-proxy-server-protected Then use the Helm chart in ./secure-doc-management to deploy the application to a Kubernetes cluster. We strongly recommend using the script: export NAMESPACE = <your_kubernetes_namespace> # e.g. default ./setup-secure-doc-management.sh # without Azure: # export USE_AZURE=false; ./setup-secure-doc-management.sh Test the Application After all resources are Running , you can test the API via Helm: helm test secure-doc-management Helm will run a pod with a couple of pre-set queries to check if the API is working properly. Access the Application For ease of use, we access the application within the cluster through a client pod, which the helm charts also deploys. kubectl exec --stdin --tty $( kubectl get pods --selector = app.kubernetes.io/name = client-scone -o jsonpath = '{.items[*].metadata.name}' ) -- /bin/bash The IP of the host is stored under $NGINX_HOST in the pod. The pod has also retrieved the certificate of the application beforehand, in /tmp/nginx-ca.crt . The exact commands to establish the secure communication with the application are: echo \"Attest SCONE CAS\" scone cas attest --accept-configuration-needed --accept-group-out-of-date --only_for_testing-debug --only_for_testing-ignore-signer $SCONE_CAS_ADDR $CAS_MRENCLAVE echo \"Get SCONE CAS CACERT\" scone cas show-certificate $SCONE_CAS_ADDR > /tmp/scone-ca.crt echo \"Get nginx public CACERT from CAS REST API\" echo -e $( curl --cacert /tmp/scone-ca.crt https:// $SCONE_CAS_ADDR :8081/v1/values/session = $NGINX_CONFIG_ID ,secret = SERVER_CA_CERT | jq '.value' ) | head -c -2 | tail -c +2 > /tmp/nginx-ca.crt Now, from within the pod, you can perform queries such as: curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https:// $NGINX_HOST /users/create_account curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X POST -H \"Content-Type: application/json\" -d '{\"record_id\":\"31\",\"content\":\"Ever make mistakes in life? Lets make them birds. Yeah, theyre birds now.\"}' https:// $NGINX_HOST /documents/create_document curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https://secure-doc-management-nginx-scone/documents/31 Clean Up To uninstall the charts we installed during this demo, execute: helm uninstall las helm uninstall secure-doc-management # if you are not using Azure: # helm uninstall sgxdevplugin","title":"Confidential Document Management"},{"location":"secure_document_management/#confidential-document-manager-application","text":"This application demo is a confidential document web application. This service enables users to upload and download documents and ensures that the documents are always encrypted. Users can create accounts. We use a simple password-based authentication. For production, one should add a two-factor authentication. The application consists of the following components: a Python FastAPI service serving as the application's REST API, a MariaDB service stores the documents and user authentication data, a memcached service, serving as a rate limiter for the application, and an nginx instance serves as a proxy server for the application, ensuring termination and forwarding with TLS. All of these components run securely inside of enclaves using the SCONE framework. These services are also integrity protected, and attest each other transparently using TLS in conjunction with a SCONE Configuration and Attestation Service (CAS). Furthermore, the application protects the confidentiality and integrity of all data it receives. We deploy this application using helm . An overview of how these different components interact with one another is as follows: Prerequisites A Kubernetes cluster Helm setup was performed Either the SCONE SGX Plugin or the Azure SGX Plugin is installed.","title":"Confidential Document Manager Application"},{"location":"secure_document_management/#azure-kubernetes-services-aks","text":"You can run this demo works on AKS. To set up the necessary infrastructure on AKS, you need to start a local attestations service: (you may have to change the tags according to your AKS Nodes) helm install las sconeapps/las \\ --set useSGXDevPlugin = azure \\ --set sgxEpcMem = 8 \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 \\ --set nodeSelector.agentpool = \"confcompool1\"","title":"Azure Kubernetes Services (AKS)"},{"location":"secure_document_management/#tldr","text":"please, register an account with the SCONE registry ask us for access to SconeApps via email pull this repository create the corresponding Kubernetes secret for the registry and then $ export IMAGE_PULL_SECRET=<your_secret> set target images that you can push to for MEMCACHED_TARGET_IMAGE , NGINX_TARGET_IMAGE and MARIADB_TARGET_IMAGE (again with $ export ... ) execute $ ./setup-secure-doc-management.sh to upload the policies to SCONE CAS and issue client requests: kubectl exec --stdin --tty $(kubectl get pods --selector=app.kubernetes.io/name=client-scone -o jsonpath='{.items[*].metadata.name}') -- /bin/bash available commands for the REST API are documented in ./secure-doc-management/templates/NOTES.txt (displayed after script has terminated)","title":"TL;DR"},{"location":"secure_document_management/#secure-fastapi-service","text":"We implement a simple FastAPI-based service. The Python code implements a REST API: we create a new account by sending a request to /users/create_account with the flag -u<username>:<password> all subsequent requests containing the -u<username>:<password> flag authenticate this user we upload documents by sending a POST to /documents/create_document with a json containing the names record_id with the value of the ID of the record, and content , with the value of the documents contents. we get a document by sending a GET request to /documents/<record_id> , retrieving the document with the corresponding record ID We execute the Python service within an enclave, thus ensuring that even privileged adversaries with root access cannot read the contents of the documents from the Python process.","title":"Secure FastAPI service"},{"location":"secure_document_management/#binary-file-system","text":"This FastAPI service employs the new SCONE features binary-fs . We thereby bind the file system of the service to the binary. Among other security advantages, the MRENCLAVE of the service reflects this inclusion of the file system. Thus, we can verify that the Python service is indeed running the code we expect it to run. For more information, please refer to https://sconedocs.github.io/binary_fs/ .","title":"Binary File system"},{"location":"secure_document_management/#secure-mariadb","text":"We store the documents and user authentication information in a secure MariaDB, which we also run inside of an enclave. Further, we ensure its memory remains encrypted. As such, the documents and login data users store inside the database remain secure. We further elaborate on the database in this section .","title":"Secure MariaDB"},{"location":"secure_document_management/#secure-memcached-rate-limiter","text":"To ensure that our MariaDB is not overloaded by excess requests, we limit requests per user using a Memcached service. We thereby ensure that a user can make at most 10 requests per minute. If the user's requests exceed that limit, we do not serve requests to their corresponding IP address for the remainder of the minute. We implement this limiter by storing the user's IP and their number of requests in a key/value pair in the Memcached service in our Python program : mc_client.set(key=client_ip, value=1, expire=60) . Upon a request, we increment the key/value pair using Memcached's incr command: mc_client.incr(key=client_ip, value=1) . If this value exceeds 10, we do not serve the requests. The key value pair expires after 60 seconds. As we attribute the IP of a user to Personally Identifiable Information (PII), we have to protect this data. We ensure this protection by running the Memcached inside of an enclave.","title":"Secure Memcached Rate Limiter"},{"location":"secure_document_management/#secure-nginx-proxy-server","text":"We employ an NGINX to be the main API of the service, and to ensure secure communication over the wire with TLS. The NGINX requires users to send their requests over TLS, as we specify in its configuration . The NGINX then forwards such requests to the FastAPI service, also using TLS. It thereby specifies the correct FastAPI port, thus enabling the user to send the service requests without having to specify the port themselves. As the NGINX thereby receives the sensitive information of the documents, we run the NGINX proxy server in an enclave as well.","title":"Secure NGINX Proxy Server"},{"location":"secure_document_management/#tls-certificates","text":"We must secure the communication between 1) the application's services and between 2) the user and the application. Therefore, we need to issue and provision multiple certificates: MariaDB : requires server certificates memcached : requires server certificates FastAPI : requires server certificates and client certificates for the Memcached and MariaDB for client verification nginx : requires server certificates, whereby the certification authority (CA) certificate must be accessible to users, and client certificates for the FastAPI for client verification. An overview of how these certificates secure the communication is as follows: We provision these certificates using SCONE policies . We now illustrate the use of the policies for certificate provisioning by inspecting the NGINX policy . secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true In the first section, # nginx - fastapi tls , the NGINX session imports the client and the server CA certificates from the FastAPI session. It thereby uses the CA certificate to identify the FastAPI service, and identifies itself to the service using the client certificates. In the second section, # (...) nginx - client tls , we issue server certificates for the NGINX. For the NGINX Server CA certificate, we set export_public: true . This settings allows users to extract the CA certificate corresponding to this policy, thus enabling them to verify the NGINX server. To make the certificates available to the NGINX service, we inject the certificates into the NGINX file system. We specify this injection in the NGINX policy using the previous SCONE policy secrets as follows: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ # specific for nginx - client tls - path : /etc/nginx/server.crt content : $$SCONE::server.crt$$ - path : /etc/nginx/server.key content : $$SCONE::server.key$$ The NGINX can then access these injected files as normal files, e.g., in its configuration: events {} http { server { listen 443 ssl; server_name secure-doc-management-nginx-scone; ssl_certificate /etc/nginx/server.crt; ssl_certificate_key /etc/nginx/server.key; location / { proxy_pass https://secure-doc-management-fastapi-scone:8000; proxy_ssl_certificate /etc/nginx/fastapi-client.crt; proxy_ssl_certificate_key /etc/nginx/fastapi-client.key; proxy_ssl_trusted_certificate /etc/nginx/fastapi-ca.crt; proxy_ssl_verify on; proxy_ssl_session_reuse on; } } }","title":"TLS Certificates"},{"location":"secure_document_management/#tls-based-mutual-attestation","text":"As we observed in the previous section, we encrypt the communication between the NGINX and the FastAPI using TLS, enabled by SCONE policies. The NGINX and FastAPI policies thereby export the necessary certificates to each other by referencing each other in their policies. They thereby attest each other. They achieve this attestation by verifying the policies that they reference in their own policy. Furthermore, they check that the corresponding service satisfies all requirements specified in the service's policy. We can easily enforce mutual attestation using TLS client authentication. We illustrate this attestation with the FastAPI's policy , which ensures TLS-based attestation between itself and the NGINX service. This FastAPI policy generates a FastAPI CA certificate ( FASTAPI_CA_CERT ) and a FastAPI server certificate ( fastapi ) as well as a corresponding FastAPI client certificate ( FASTAPI_CLIENT_CERT ) and client key ( FASTAPI_CLIENT_KEY ). The policy exports this certificate and private key to the NGINX policy. secrets : ... # specific for fastapi - nginx tls - name : fastapi-key # automatically generate FASTAPI server certificate kind : private-key - name : fastapi # automatically generate FASTAPI server certificate private_key : fastapi-key issuer : FASTAPI_CA_CERT kind : x509 dns : - $FASTAPI_HOST - name : FASTAPI_CLIENT_KEY kind : private-key export : - session : $NGINX_SESSION - name : FASTAPI_CLIENT_CERT # automatically generate client certificate private_key : FASTAPI_CLIENT_KEY issuer : FASTAPI_CA_CERT common_name : FASTAPI_CLIENT_CERT kind : x509 export : - session : $NGINX_SESSION # export client cert/key to upload session - name : FASTAPI_CA_KEY # export session CA certificate as FASTAPI CA certificate kind : private-key - name : FASTAPI_CA_CERT # export session CA certificate as FASTAPI CA certificate kind : x509-ca common_name : FASTAPI_CA private_key : FASTAPI_CA_KEY export : - session : $NGINX_SESSION # export the session CA certificate to upload session We thereby replace $NGINX_SESSION with the policy name of the NGINX. For increased security, we can also specify the corresponding session hash . In this scenario, the policies are on the same SCONE CAS . In more complex cases, the policies can be stored on different SCONE CAS . The NGINX can then import the FastAPI CA certificate, client certificate and private key, as we saw before: secrets : # nginx - fastapi tls - name : FASTAPI_CLIENT_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CLIENT_CERT - name : FASTAPI_CA_CERT import : session : $FASTAPI_SESSION secret : FASTAPI_CA_CERT # specific for nginx - client tls - name : server-key # automatically generate SERVER server certificate kind : private-key - name : server # automatically generate SERVER server certificate private_key : server-key issuer : SERVER_CA_CERT kind : x509 dns : - $NGINX_HOST - secure-doc-management - name : SERVER_CA_KEY # export session CA certificate as SERVER CA certificate kind : private-key - name : SERVER_CA_CERT # export session CA certificate as SERVER CA certificate kind : x509-ca common_name : SERVER_CA private_key : SERVER_CA_KEY export_public : true $FASTAPI_SESSION is the exporting policy of the FastAPI. The NGINX can then use these exported secrets, as we also saw before: images : - name : nginx_image injection_files : # nginx - fastapi tls - path : /etc/nginx/fastapi-ca.crt content : $$SCONE::FASTAPI_CA_CERT.chain$$ - path : /etc/nginx/fastapi-client.crt content : $$SCONE::FASTAPI_CLIENT_CERT.crt$$ - path : /etc/nginx/fastapi-client.key content : $$SCONE::FASTAPI_CLIENT_CERT.key$$ To ensure that the SCONE CAS supplying these policies is authentic, we typically attest the SCONE CAS before uploading any policies using the SCONE CAS CLI .","title":"TLS-Based Mutual Attestation"},{"location":"secure_document_management/#binary-fs-for-fastapi","text":"As we mentioned before, we secure our FastAPI server using the binary-fs . We use the tool while building the initial Docker image. We seperate this process in three stages. In the first stage, we apply the binaryfs command to create a file system .c file. In the second stage, we compile the .c file into an .so file using the scone gcc . In the third stage, we link the generated .so file into the binary using patchelf . We apply these stages in the Dockerfile as follows: # First stage: apply the binary-fs FROM sconecuratedimages/apps:python-3.7.3-alpine3.10-scone5.0.0 AS binary-fs COPY rest_api.py /. COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt # here we apply the binary-fs command to create the file system .c file RUN rm /usr/lib/python3.7/config-3.7m-x86_64-linux-gnu/libpython3.7m.a && \\ SCONE_MODE = auto scone binaryfs / /binary-fs.c -v \\ --include '/usr/lib/python3.7/*' \\ --include /lib/libssl.so.1.1 \\ --include /lib/libcrypto.so.1.1 \\ --include '/lib/libz.so.1*' \\ --include '/usr/lib/libbz2.so.1*' \\ --include '/usr/lib/libsqlite3.so.0*' \\ --include '/usr/lib/libev.so.4*' \\ --include '/usr/lib/libffi.so.6*' \\ --include '/usr/lib/libexpat.so.1*' \\ --include /rest_api.py # Second stage: compile the binary fs FROM registry.scontain.com:5050/sconecuratedimages/crosscompilers:alpine-scone5.0.0 as crosscompiler COPY --from = binary-fs /binary-fs.c /. RUN scone gcc /binary-fs.c -O0 -shared -o /libbinary-fs.so # Third stage: patch the binary-fs into the enclave executable FROM registry.scontain.com:5050/sconecuratedimages/apps:python-3.7.3-alpine3.10 COPY --from = crosscompiler /libbinary-fs.so /. RUN apk add --no-cache patchelf && \\ patchelf --add-needed libbinary-fs.so ` which python3 ` && \\ apk del patchelf ENV SCONE_HEAP = 512M ENV SCONE_LOG = debug ENV LD_LIBRARY_PATH = \"/\" CMD sh -c \"python3 /rest_api.py\" The binary also requires certain /etc/ files for networking within the cluster, e.g., /etc/resolv.conf . Currently, we simply inject these files into the confidential service using the FastAPI's policy . The source code of this example is available on our repository- ask us for permission, and then continue with the following commands: git clone https://gitlab.scontain.com/enterJazz/secure-doc-management.git cd secure-doc-management","title":"Binary FS for FastAPI"},{"location":"secure_document_management/#execution-on-a-kubernetes-cluster","text":"We intend this example to be run on a Kubernetes cluster.","title":"Execution on a Kubernetes Cluster"},{"location":"secure_document_management/#install-scone-services","text":"Get access to SconeApps (see https://sconedocs.github.io/helm/ ): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/SconeApps/master/ helm repo update Give SconeApps access to the private docker images (see SconeApps ): export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Start Local Attestation Service (LAS) on Azure (we use a remote CAS in this case): # nodeSelector may have to be adjusted according to your nodes helm install las sconeapps/las \\ --set useSGXDevPlugin = azure \\ --set sgxEpcMem = 8 \\ --set image = registry.scontain.com:5050/sconecuratedimages/kubernetes:las-scone5.0.0 \\ --set nodeSelector.agentpool = \"confcompool1\" To setup on a vanilla Kubernetes cluster: helm install las sconeapps/las --set service.hostPort = true helm install sgxdevplugin sconeapps/sgxdevplugin","title":"Install SCONE Services"},{"location":"secure_document_management/#run-the-application","text":"Start by by specifying Docker image repositories to which you may push: export MARIADB_TARGET_IMAGE = your/repo:mariadb-protected export MEMCACHED_TARGET_IMAGE = your/repo:memcached-tls-protected export NGINX_TARGET_IMAGE = your/repo:nginx-proxy-server-protected Then use the Helm chart in ./secure-doc-management to deploy the application to a Kubernetes cluster. We strongly recommend using the script: export NAMESPACE = <your_kubernetes_namespace> # e.g. default ./setup-secure-doc-management.sh # without Azure: # export USE_AZURE=false; ./setup-secure-doc-management.sh","title":"Run the Application"},{"location":"secure_document_management/#test-the-application","text":"After all resources are Running , you can test the API via Helm: helm test secure-doc-management Helm will run a pod with a couple of pre-set queries to check if the API is working properly.","title":"Test the Application"},{"location":"secure_document_management/#access-the-application","text":"For ease of use, we access the application within the cluster through a client pod, which the helm charts also deploys. kubectl exec --stdin --tty $( kubectl get pods --selector = app.kubernetes.io/name = client-scone -o jsonpath = '{.items[*].metadata.name}' ) -- /bin/bash The IP of the host is stored under $NGINX_HOST in the pod. The pod has also retrieved the certificate of the application beforehand, in /tmp/nginx-ca.crt . The exact commands to establish the secure communication with the application are: echo \"Attest SCONE CAS\" scone cas attest --accept-configuration-needed --accept-group-out-of-date --only_for_testing-debug --only_for_testing-ignore-signer $SCONE_CAS_ADDR $CAS_MRENCLAVE echo \"Get SCONE CAS CACERT\" scone cas show-certificate $SCONE_CAS_ADDR > /tmp/scone-ca.crt echo \"Get nginx public CACERT from CAS REST API\" echo -e $( curl --cacert /tmp/scone-ca.crt https:// $SCONE_CAS_ADDR :8081/v1/values/session = $NGINX_CONFIG_ID ,secret = SERVER_CA_CERT | jq '.value' ) | head -c -2 | tail -c +2 > /tmp/nginx-ca.crt Now, from within the pod, you can perform queries such as: curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https:// $NGINX_HOST /users/create_account curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X POST -H \"Content-Type: application/json\" -d '{\"record_id\":\"31\",\"content\":\"Ever make mistakes in life? Lets make them birds. Yeah, theyre birds now.\"}' https:// $NGINX_HOST /documents/create_document curl --cacert /tmp/nginx-ca.crt -ubobross:password123 -X GET https://secure-doc-management-nginx-scone/documents/31","title":"Access the Application"},{"location":"secure_document_management/#clean-up","text":"To uninstall the charts we installed during this demo, execute: helm uninstall las helm uninstall secure-doc-management # if you are not using Azure: # helm uninstall sgxdevplugin","title":"Clean Up"},{"location":"secure_remote_execution/","text":"Secure Remote Execution We show how to execute applications in an always encrypted fashion on a remote, untrusted machine via a simple CLI (Command Line Interface). The CLI runs inside a container on a trusted client computer: Prerequisites Remote host installation You need to install the SGX driver and Docker engine . So far, we support the execution on remote hosts / VMs via ssh and via the iExec platform. To start a container on a remote host via this CLI , you need to give the CLI the credentials of this hosts. These credentials are stored in a volume stored on the client's computer. In this example, we use directory $PWD/conf/ to store the credentials. Ensure that only you have access to this directory to protect the credentials. This directory must be mapped in the container at location /conf : eval docker run -it --rm -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli add-host --alias caroline --hostname 1 .2.3.4 This adds an alias ( caroline ) for the host identified by option --hostname . The argument --hostname can either be a hostname or an IP address. The CLI generates a key pair inside the container and stores it in /conf . The public key is used to grant access to the container on the remote host. The output of add-host will look something like this: ssh ubuntu@1.2.3.4 \"echo \\\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDXVqr0diJjgMPx7WNTQWJVhV3ea8L1I/8mDZTsQUH5gazB+laIiM7tEAKbddbessItmA9bLOZEq4CIrYmtXpjG2tBSES2YpqNhQ8+3r4U3ozSBD1XAc6OzRmnHP+wmuVpCQw0QxBKj0Qq5MCehmLIXLYephOtjVWwsA3EprVHIq0+/wYZp4mU3evCgcvE46nxOrmHzu5X4iDUMSY59XHdavIVkkS87qp4RlyFQa0gBRHOfcGEVJ3oS/pmKHxasdLwjiVmCovLG4RPS88RoKCf8zWis8vpPUKt/04xjlj4gqsV/U7VR2S/kFcvq1yuvOno+BhcsGme2U2CKTV4Y16ZJ scone-CLI\\\" >> .ssh/authorized_keys\" eval executes the output and gives your local client to give the container access to the remote host. Execution To execute an application on a remote host, you first have to add this host via add-host as described above. We show how to execute the simple copy application that we introduce in the next section of this tutorial . This copy application takes some input files in a given directory INPUTS and copies these files to directory OUTPUTS in a very elaborate way: encrypt the files in directory INPUTS inside the container running on the client machine push the encrypted files to a remote file service - which is operated by an entity that we do not know start the copy application container on a remote host - which is given via option --host HOST a script running inside the container pulls the encrypted files from the remote file service the script starts the copy application inside of an enclave the SCONE runtime transparently decrypts the input files and transparently encrypts the output files the SCONE runtime ensures both the confidentiality as well as the integrity of the files the copy applications copies the input files to the output files and then terminates the shell script pushes the encrypted output files to the remote file service the CLI pulls the output files from the remote file service and decrypts the files and stores the files inside the OUTPUT directory. The copy application is stored in image registry.scontain.com:5050/sconecuratedimages/iexecsgx:copy_demo on registry.scontain.com . You can execute this image as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline Example: Copy Demo You could perform the following steps to see the copy_demo in action. Create some directory and a file to copy. mkdir -p INPUTS OUTPUTS echo \"Hello World\" > INPUTS/f1.txt Now execute the copy application (as shown above): docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline The file f1.txt was copied to directory OUTPUTS . You can check the output, by executing: cat OUTPUTS/f1.txt which results in the output: Hello World Example: Blender We show to run blender with a blender file. To do so, show to render some blender file from our collaborator iExec as well as some other blender files accessible via github. Setup Let's set up an EXAMPLE directory and create some sub-directories for input and output files. We also need to specify a remote file service like transfer.sh or filepush.co/upload , and an instance of the SCONE CAS configuration and attestation service. mkdir -p EXAMPLE TRANSFER = \"transfer.sh\" CAS = scone-cas.cf cd EXAMPLE mkdir -p conf INPUTS OUTPUTS To reduce the typing, let's define an alias for the secure remote execution of blender: alias aeblender = \"docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender --host caroline -r $TRANSFER -s $CAS \" We assume that you added the host (in this case caroline ) already via add-host (see above). Render Image Let's first download a blender file: curl https://raw.githubusercontent.com/iExecBlockchainComputing/iexec-dapps-registry/master/iExecBlockchainComputing/Blender/iexec-rlc.blend -o INPUTS/iexec-rlc.blend and we render this blender file remotely ensuring all artifacts are always encrypted : aeblender We can now look at the files in OUTPUT directory: open OUTPUTS/0001.png Which shows the following picture which was computed using always encrypted approach: Cube Let's download another blender file from github: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/cube.blend and render this blender file on the remote and untrusted host caroline : aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted : Helicopter Example Another example from github : curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/scene-Helicopter-27-cycles.blend We are execution with the same command as above: aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted : Car Example We show one more example: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/bmw27_cpu.blend and render this (as above): aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted : Advanced Example You can also run blender with Python scripts that describe your images and movies - see blender use case for details.","title":"Secure Remote Execution"},{"location":"secure_remote_execution/#secure-remote-execution","text":"We show how to execute applications in an always encrypted fashion on a remote, untrusted machine via a simple CLI (Command Line Interface). The CLI runs inside a container on a trusted client computer:","title":"Secure Remote Execution"},{"location":"secure_remote_execution/#prerequisites","text":"Remote host installation You need to install the SGX driver and Docker engine . So far, we support the execution on remote hosts / VMs via ssh and via the iExec platform. To start a container on a remote host via this CLI , you need to give the CLI the credentials of this hosts. These credentials are stored in a volume stored on the client's computer. In this example, we use directory $PWD/conf/ to store the credentials. Ensure that only you have access to this directory to protect the credentials. This directory must be mapped in the container at location /conf : eval docker run -it --rm -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli add-host --alias caroline --hostname 1 .2.3.4 This adds an alias ( caroline ) for the host identified by option --hostname . The argument --hostname can either be a hostname or an IP address. The CLI generates a key pair inside the container and stores it in /conf . The public key is used to grant access to the container on the remote host. The output of add-host will look something like this: ssh ubuntu@1.2.3.4 \"echo \\\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDXVqr0diJjgMPx7WNTQWJVhV3ea8L1I/8mDZTsQUH5gazB+laIiM7tEAKbddbessItmA9bLOZEq4CIrYmtXpjG2tBSES2YpqNhQ8+3r4U3ozSBD1XAc6OzRmnHP+wmuVpCQw0QxBKj0Qq5MCehmLIXLYephOtjVWwsA3EprVHIq0+/wYZp4mU3evCgcvE46nxOrmHzu5X4iDUMSY59XHdavIVkkS87qp4RlyFQa0gBRHOfcGEVJ3oS/pmKHxasdLwjiVmCovLG4RPS88RoKCf8zWis8vpPUKt/04xjlj4gqsV/U7VR2S/kFcvq1yuvOno+BhcsGme2U2CKTV4Y16ZJ scone-CLI\\\" >> .ssh/authorized_keys\" eval executes the output and gives your local client to give the container access to the remote host.","title":"Prerequisites"},{"location":"secure_remote_execution/#execution","text":"To execute an application on a remote host, you first have to add this host via add-host as described above. We show how to execute the simple copy application that we introduce in the next section of this tutorial . This copy application takes some input files in a given directory INPUTS and copies these files to directory OUTPUTS in a very elaborate way: encrypt the files in directory INPUTS inside the container running on the client machine push the encrypted files to a remote file service - which is operated by an entity that we do not know start the copy application container on a remote host - which is given via option --host HOST a script running inside the container pulls the encrypted files from the remote file service the script starts the copy application inside of an enclave the SCONE runtime transparently decrypts the input files and transparently encrypts the output files the SCONE runtime ensures both the confidentiality as well as the integrity of the files the copy applications copies the input files to the output files and then terminates the shell script pushes the encrypted output files to the remote file service the CLI pulls the output files from the remote file service and decrypts the files and stores the files inside the OUTPUT directory. The copy application is stored in image registry.scontain.com:5050/sconecuratedimages/iexecsgx:copy_demo on registry.scontain.com . You can execute this image as follows: docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline","title":"Execution"},{"location":"secure_remote_execution/#example-copy-demo","text":"You could perform the following steps to see the copy_demo in action. Create some directory and a file to copy. mkdir -p INPUTS OUTPUTS echo \"Hello World\" > INPUTS/f1.txt Now execute the copy application (as shown above): docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:copy_demo --host caroline The file f1.txt was copied to directory OUTPUTS . You can check the output, by executing: cat OUTPUTS/f1.txt which results in the output: Hello World","title":"Example: Copy Demo"},{"location":"secure_remote_execution/#example-blender","text":"We show to run blender with a blender file. To do so, show to render some blender file from our collaborator iExec as well as some other blender files accessible via github.","title":"Example: Blender"},{"location":"secure_remote_execution/#setup","text":"Let's set up an EXAMPLE directory and create some sub-directories for input and output files. We also need to specify a remote file service like transfer.sh or filepush.co/upload , and an instance of the SCONE CAS configuration and attestation service. mkdir -p EXAMPLE TRANSFER = \"transfer.sh\" CAS = scone-cas.cf cd EXAMPLE mkdir -p conf INPUTS OUTPUTS To reduce the typing, let's define an alias for the secure remote execution of blender: alias aeblender = \"docker run -it --rm -v $PWD /INPUTS:/inputs -v $PWD /OUTPUTS:/decryptedOutputs -v $PWD /conf/:/conf registry.scontain.com:5050/sconecuratedimages/iexecsgx:cli execute --application sconecuratedimages/iexecsgx:blender --host caroline -r $TRANSFER -s $CAS \" We assume that you added the host (in this case caroline ) already via add-host (see above).","title":"Setup"},{"location":"secure_remote_execution/#render-image","text":"Let's first download a blender file: curl https://raw.githubusercontent.com/iExecBlockchainComputing/iexec-dapps-registry/master/iExecBlockchainComputing/Blender/iexec-rlc.blend -o INPUTS/iexec-rlc.blend and we render this blender file remotely ensuring all artifacts are always encrypted : aeblender We can now look at the files in OUTPUT directory: open OUTPUTS/0001.png Which shows the following picture which was computed using always encrypted approach:","title":"Render Image"},{"location":"secure_remote_execution/#cube","text":"Let's download another blender file from github: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/cube.blend and render this blender file on the remote and untrusted host caroline : aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted :","title":"Cube"},{"location":"secure_remote_execution/#helicopter-example","text":"Another example from github : curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/scene-Helicopter-27-cycles.blend We are execution with the same command as above: aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted :","title":"Helicopter Example"},{"location":"secure_remote_execution/#car-example","text":"We show one more example: curl --output INPUTS/iexec-rlc.blend https://raw.githubusercontent.com/golemfactory/golem/develop/apps/blender/benchmark/test_task/bmw27_cpu.blend and render this (as above): aeblender The OUTPUT directory contains the following picture - which was computed remotely with all data and processing being always encrypted :","title":"Car Example"},{"location":"secure_remote_execution/#advanced-example","text":"You can also run blender with Python scripts that describe your images and movies - see blender use case for details.","title":"Advanced Example"},{"location":"session_secret/","text":"NOTE Sorry, this is not yet part of the online documentation.","title":"NOTE"},{"location":"session_secret/#note","text":"Sorry, this is not yet part of the online documentation.","title":"NOTE"},{"location":"sgxinstall/","text":"Installation of SGX driver Starting with Linux kernel 5.11, you do not need to install an SGX drivers anymore. To install an SGX driver on Linux distributions, follow the description . On server CPUs, you might need to install the DCAP SGX driver . Alternatively, on a modern Ubuntu system on which you have sudo access, you could execute the following: curl -fsSL https://raw.githubusercontent.com/scontain/SH/master/install_sgx_driver.sh \\ | bash -s - install --auto --dkms -p metrics -p page0 -p version The advantage of this installation option is that: auto selects the right SGX driver for your CPU dkms (Dynamic Kernel Module Support) ensures that the SGX driver is automatically updated after each kernel update metrics provides SGX metrics that can be visualized by TEEMon Driver Extensions We maintain several extensions of the Intel SGX driver. The public extensions add the following features: metrics : adds variables to monitor SGX performance metrics. This is, for example, being used by our TEEMon performance monitoring framework. page0 : permits an enclave to start at address 0 to protect against certain attacks. version : permits to check what driver version and what extensions are installed. fsgsbase : installs the fsgsbase patch which is required to run glibc-based applications. For production, do not use option fsgsbase ! Please use a Linux kernel 5.11 (or, newer). You can install only one extension, for example, the version extension as follows: curl -fssl https://raw.githubusercontent.com/scontain/SH/master/install_sgx_driver.sh \\ | bash -s - install --auto --dkms -p version Alternatively, you can download the script as follows: curl -fssl https://raw.githubusercontent.com/scontain/SH/master/install_sgx_driver.sh --output install_sgx_driver.sh chmod u+x install_sgx_driver.sh The script supports the install commands: install the current Intel out of branch driver if no SGX driver is installed -d, --dcap installs the DCAP driver instead -a, --auto select the driver according to the machine capabilities ( DCAP or OOT ) -p, --patch =[ PATCH ] apply patches to the SGX driver. The valid values for PATCH are: 'version' , 'metrics' , 'page0' . -p version installs the version patch ( recommended ) -p metrics installs the metrics patch -p page0 installs the page0 patch ( not available for DCAP ) -k, --dkms installs the driver with DKMS ( default for DCAP ) -l, --latest installs the latest upstream driver ( not recommended ) -f, --force replaces existing SGX driver, if installed and the check command: check checks if a certain driver is installed -p metrics check the status of 'metrics' extension -p page0 check the status of 'page0' extension In case of absence or outdated driver, or absence or outdated extension, check will return error. Here are some common examples on how to use this installer. Our recommended way to install the driver is as follows: # installs OOT SGX driver with metrics, page0, and version extension if no SGX driver is yet installed ./install_sgx_driver.sh install --auto --dkms -p metrics -p page0 -p version This installs the correct driver for the host, ensures that the driver is automatically updated when the kernel is updated and adds all extensions that are required for monitoring and security. If you need to replace an existing service, add the option --force : # install SGX driver with metrics extension; replaces any existing SGX driver ./install_sgx_driver.sh install --force -p metrics # install DCAP SGX driver with metrics extension; replaces any existing SGX driver ./install_sgx_driver.sh install --dcap --force -p metrics -p version Option --help prints all commands and all options. Check which SGX driver and extensions are installed The check command tests if an up-to-date SGX driver and some required extension are installed. Typically, you would execute as follows: ./install_sgx_driver.sh check The output will look as follows: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 Detected patches: version metrics page0 Use DKMS: Yes Driver commit: 602374c738ca58f83a1c17574d08e5d5e6341953 Driver status: Up to date Patch 'metrics' version: 2 (Up to date) Patch 'page0' version : 1 (Up to date) OK To check if an SGX driver and the metrics extension are installed and if they are up-to-date, just execute: ./install_sgx_driver.sh check -p metrics If no SGX driver is installed the output would look like this FAIL: No SGX driver detected! ERROR: installation of SGX driver failed (script=install_sgx_driver.sh, Line: 1, 1166) When an OOT driver without version patch is installed: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 FAIL: Unable to detect 'version' patch! This patch is required for running this command. To install the driver with 'version' patch, run: ./install_sgx_driver.sh install -p version --force When executing ./install_sgx_driver.sh check the output for an OOT outdated driver without patches would look like this: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 Detected patches: version Driver commit: 95eaa6f6693cd86c35e10a22b4f8e483373c987c Driver status: Outdated - 1 new commit(s) available WARNING: Update is needed! One can also check that certain extensions are installed. For example, one might want to verify that the metrics and the page0 extensions are installed: ./install_sgx_driver.sh check -p metrics -p page0 The output might look like this: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 Detected patches: version Driver commit: 95eaa6f6693cd86c35e10a22b4f8e483373c987c Driver status: Outdated - 1 new commit(s) available Patch 'metrics' not found! Patch 'page0' not found! WARNING: Update is needed! Note that on some platforms that support SGX but the disables SGX in the BIOS, the driver might be successfully installed and even load but using the SGX driver fails. Check on the host as well as inside your containers that the SGX device is visible. Determine SGX Device Depending if you have DCAP or a non-DCAP driver installed, you need to use a different SGX device. We can try to determine this with a little bash function: function determine_sgx_device { export SGXDEVICE = \"/dev/sgx\" export MOUNT_SGXDEVICE = \"-v /dev/sgx/:/dev/sgx\" if [[ ! -e \" $SGXDEVICE \" ]] ; then export SGXDEVICE = \"/dev/isgx\" export MOUNT_SGXDEVICE = \"--device=/dev/isgx\" if [[ ! -c \" $SGXDEVICE \" ]] ; then echo \"Warning: No SGX device found! Will run in SIM mode.\" > /dev/stderr export MOUNT_SGXDEVICE = \"\" export SGXDEVICE = \"\" fi fi } Storing determine_sgx_device in a separate file Note that if define and execute determine_sgx_device in a separate file, say, dsd.sh , the environment variables SGXDEVICE and MOUNT_SGXDEVICE are only visible in the shell that executes this file. Use command source dsd.sh to make these variables visible in your current bash shell. We can now determine if and which SGX driver is installed: determine_sgx_device echo $SGXDEVICE Checking availability of SGX device inside of containers The SGX device is not automatically mapped into a container: you can to map the device as follows into the container: # alternative: use --device option without --privileged flag docker run $MOUNT_SGXDEVICE --rm registry.scontain.com:5050/sconecuratedimages/checksgx || echo \"failed to open SGX device $SGXDEVICE inside of container\" In case that the device is not mapped in the container, you can try to see if the container must be privileged or if we might need to remap the device ids: docker run $MOUNT_SGXDEVICE --privileged --rm registry.scontain.com:5050/sconecuratedimages/checksgx || echo \"failed to open SGX device $SGXDEVICE inside of container\"","title":"Installing Intel SGX driver"},{"location":"sgxinstall/#installation-of-sgx-driver","text":"Starting with Linux kernel 5.11, you do not need to install an SGX drivers anymore. To install an SGX driver on Linux distributions, follow the description . On server CPUs, you might need to install the DCAP SGX driver . Alternatively, on a modern Ubuntu system on which you have sudo access, you could execute the following: curl -fsSL https://raw.githubusercontent.com/scontain/SH/master/install_sgx_driver.sh \\ | bash -s - install --auto --dkms -p metrics -p page0 -p version The advantage of this installation option is that: auto selects the right SGX driver for your CPU dkms (Dynamic Kernel Module Support) ensures that the SGX driver is automatically updated after each kernel update metrics provides SGX metrics that can be visualized by TEEMon","title":"Installation of SGX driver"},{"location":"sgxinstall/#driver-extensions","text":"We maintain several extensions of the Intel SGX driver. The public extensions add the following features: metrics : adds variables to monitor SGX performance metrics. This is, for example, being used by our TEEMon performance monitoring framework. page0 : permits an enclave to start at address 0 to protect against certain attacks. version : permits to check what driver version and what extensions are installed. fsgsbase : installs the fsgsbase patch which is required to run glibc-based applications. For production, do not use option fsgsbase ! Please use a Linux kernel 5.11 (or, newer). You can install only one extension, for example, the version extension as follows: curl -fssl https://raw.githubusercontent.com/scontain/SH/master/install_sgx_driver.sh \\ | bash -s - install --auto --dkms -p version Alternatively, you can download the script as follows: curl -fssl https://raw.githubusercontent.com/scontain/SH/master/install_sgx_driver.sh --output install_sgx_driver.sh chmod u+x install_sgx_driver.sh The script supports the install commands: install the current Intel out of branch driver if no SGX driver is installed -d, --dcap installs the DCAP driver instead -a, --auto select the driver according to the machine capabilities ( DCAP or OOT ) -p, --patch =[ PATCH ] apply patches to the SGX driver. The valid values for PATCH are: 'version' , 'metrics' , 'page0' . -p version installs the version patch ( recommended ) -p metrics installs the metrics patch -p page0 installs the page0 patch ( not available for DCAP ) -k, --dkms installs the driver with DKMS ( default for DCAP ) -l, --latest installs the latest upstream driver ( not recommended ) -f, --force replaces existing SGX driver, if installed and the check command: check checks if a certain driver is installed -p metrics check the status of 'metrics' extension -p page0 check the status of 'page0' extension In case of absence or outdated driver, or absence or outdated extension, check will return error. Here are some common examples on how to use this installer. Our recommended way to install the driver is as follows: # installs OOT SGX driver with metrics, page0, and version extension if no SGX driver is yet installed ./install_sgx_driver.sh install --auto --dkms -p metrics -p page0 -p version This installs the correct driver for the host, ensures that the driver is automatically updated when the kernel is updated and adds all extensions that are required for monitoring and security. If you need to replace an existing service, add the option --force : # install SGX driver with metrics extension; replaces any existing SGX driver ./install_sgx_driver.sh install --force -p metrics # install DCAP SGX driver with metrics extension; replaces any existing SGX driver ./install_sgx_driver.sh install --dcap --force -p metrics -p version Option --help prints all commands and all options.","title":"Driver Extensions"},{"location":"sgxinstall/#check-which-sgx-driver-and-extensions-are-installed","text":"The check command tests if an up-to-date SGX driver and some required extension are installed. Typically, you would execute as follows: ./install_sgx_driver.sh check The output will look as follows: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 Detected patches: version metrics page0 Use DKMS: Yes Driver commit: 602374c738ca58f83a1c17574d08e5d5e6341953 Driver status: Up to date Patch 'metrics' version: 2 (Up to date) Patch 'page0' version : 1 (Up to date) OK To check if an SGX driver and the metrics extension are installed and if they are up-to-date, just execute: ./install_sgx_driver.sh check -p metrics If no SGX driver is installed the output would look like this FAIL: No SGX driver detected! ERROR: installation of SGX driver failed (script=install_sgx_driver.sh, Line: 1, 1166) When an OOT driver without version patch is installed: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 FAIL: Unable to detect 'version' patch! This patch is required for running this command. To install the driver with 'version' patch, run: ./install_sgx_driver.sh install -p version --force When executing ./install_sgx_driver.sh check the output for an OOT outdated driver without patches would look like this: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 Detected patches: version Driver commit: 95eaa6f6693cd86c35e10a22b4f8e483373c987c Driver status: Outdated - 1 new commit(s) available WARNING: Update is needed! One can also check that certain extensions are installed. For example, one might want to verify that the metrics and the page0 extensions are installed: ./install_sgx_driver.sh check -p metrics -p page0 The output might look like this: Getting SGX Driver information: OOT driver detected. Version: 2.6.0 Detected patches: version Driver commit: 95eaa6f6693cd86c35e10a22b4f8e483373c987c Driver status: Outdated - 1 new commit(s) available Patch 'metrics' not found! Patch 'page0' not found! WARNING: Update is needed! Note that on some platforms that support SGX but the disables SGX in the BIOS, the driver might be successfully installed and even load but using the SGX driver fails. Check on the host as well as inside your containers that the SGX device is visible.","title":"Check which SGX driver and extensions are installed"},{"location":"sgxinstall/#determine-sgx-device","text":"Depending if you have DCAP or a non-DCAP driver installed, you need to use a different SGX device. We can try to determine this with a little bash function: function determine_sgx_device { export SGXDEVICE = \"/dev/sgx\" export MOUNT_SGXDEVICE = \"-v /dev/sgx/:/dev/sgx\" if [[ ! -e \" $SGXDEVICE \" ]] ; then export SGXDEVICE = \"/dev/isgx\" export MOUNT_SGXDEVICE = \"--device=/dev/isgx\" if [[ ! -c \" $SGXDEVICE \" ]] ; then echo \"Warning: No SGX device found! Will run in SIM mode.\" > /dev/stderr export MOUNT_SGXDEVICE = \"\" export SGXDEVICE = \"\" fi fi } Storing determine_sgx_device in a separate file Note that if define and execute determine_sgx_device in a separate file, say, dsd.sh , the environment variables SGXDEVICE and MOUNT_SGXDEVICE are only visible in the shell that executes this file. Use command source dsd.sh to make these variables visible in your current bash shell. We can now determine if and which SGX driver is installed: determine_sgx_device echo $SGXDEVICE","title":"Determine SGX Device"},{"location":"sgxinstall/#checking-availability-of-sgx-device-inside-of-containers","text":"The SGX device is not automatically mapped into a container: you can to map the device as follows into the container: # alternative: use --device option without --privileged flag docker run $MOUNT_SGXDEVICE --rm registry.scontain.com:5050/sconecuratedimages/checksgx || echo \"failed to open SGX device $SGXDEVICE inside of container\" In case that the device is not mapped in the container, you can try to see if the container must be privileged or if we might need to remap the device ids: docker run $MOUNT_SGXDEVICE --privileged --rm registry.scontain.com:5050/sconecuratedimages/checksgx || echo \"failed to open SGX device $SGXDEVICE inside of container\"","title":"Checking availability of SGX device inside of containers"},{"location":"shared_source_code/","text":"SCONE Confidential Code Deployment We have witnessed in several contexts the following problem: Multiple stakeholders want to collaborate in some joint project. However, they have limited trust in each other since they are also competitors in different projects. Since some money might be at stake, some stakeholders might not play by the rules to get some advantage over the other stakeholders. For example, the stakeholders might want to pool their data to provide a joint service. They want to use the data to build models using federated machine learning. A stakeholder might not want to provide all its data to the other stakeholders to get some competitive advantage (a freeloader ). Or, it might give access to corrupted data only (a Byzantine stakeholder ). The stakeholders might want to ensure that all participants can use the same training data, and participants can only access their training data in the clear. To solve this problem, the stakeholders could agree on a joint codebase: they agree that they use a joint, agreed-upon codebase. This codebase is hosted in some git repo, each participant has access to the git repo and can inspect the code, and the code will need to be updated periodically. All stakeholders need to agree on the code base. To do so, one can use the SCONE CAS policy board to agree on a common policy: This policy permits each stakeholder to create a confidential image of the source code using a confidential application that pulls the code and creates a confidential container image which runs the agreed codebase, i.e., a confidential service. The stakeholders can deploy the confidential services and also the policy in clouds or datacenters of their choice. The confidential services can attest each other automatically via TLS. This is the case even if the policies of the individual confidential services would use different CAS instances to store the policies. For more details on how to set this up, please send us an email.","title":"Confidential Code Deployment"},{"location":"shared_source_code/#scone-confidential-code-deployment","text":"We have witnessed in several contexts the following problem: Multiple stakeholders want to collaborate in some joint project. However, they have limited trust in each other since they are also competitors in different projects. Since some money might be at stake, some stakeholders might not play by the rules to get some advantage over the other stakeholders. For example, the stakeholders might want to pool their data to provide a joint service. They want to use the data to build models using federated machine learning. A stakeholder might not want to provide all its data to the other stakeholders to get some competitive advantage (a freeloader ). Or, it might give access to corrupted data only (a Byzantine stakeholder ). The stakeholders might want to ensure that all participants can use the same training data, and participants can only access their training data in the clear. To solve this problem, the stakeholders could agree on a joint codebase: they agree that they use a joint, agreed-upon codebase. This codebase is hosted in some git repo, each participant has access to the git repo and can inspect the code, and the code will need to be updated periodically. All stakeholders need to agree on the code base. To do so, one can use the SCONE CAS policy board to agree on a common policy: This policy permits each stakeholder to create a confidential image of the source code using a confidential application that pulls the code and creates a confidential container image which runs the agreed codebase, i.e., a confidential service. The stakeholders can deploy the confidential services and also the policy in clouds or datacenters of their choice. The confidential services can attest each other automatically via TLS. This is the case even if the policies of the individual confidential services would use different CAS instances to store the policies. For more details on how to set this up, please send us an email.","title":"SCONE Confidential Code Deployment"},{"location":"software_updates/","text":"Rolling Software Updates TL;DR Rolling updates of confidential services \u2013 what needs to be touched? Software Updates SCONE supports the rolling update of services and applications. Updates are a three-step process: Update the security policy of the application: add the new MrEnclave for the updated service / application. trigger a rolling update with Kubernetes / helm Update the security policy of the application: remove the old MrEnclave s of the service / application For each service, one can specify a sequence of MrEnclave s. This might look as follows: services: - name: application mrenclaves: [0239...] and we want to update to a new version that has mrenclave of 0239... . If the service support horizontal scaling, we might want a rolling update, e.g., we gradually replace old service instances by new instances. This means there is some time interval in which both old instances as well as new instances a running concurrently. First Step Consider that you want to update application to a new version with a new MrEnclave of, say, 4759... . In this case, you would update your policy as follows: services: - name: application mrenclaves: [0239..., 4759...] Second Step Now you can upgrade your application with the help of helm . You might trigger a rolling update as follows: helm upgrade application . You need to check that all your service instances have been upgraded. Third Step We now ensure that the old version of the service / application cannot run anymore by removing the old MrEnclave from the policy: services: - name: application mrenclaves: [4759...]","title":"Software updates"},{"location":"software_updates/#rolling-software-updates","text":"","title":"Rolling Software Updates"},{"location":"software_updates/#tldr","text":"Rolling updates of confidential services \u2013 what needs to be touched?","title":"TL;DR"},{"location":"software_updates/#software-updates","text":"SCONE supports the rolling update of services and applications. Updates are a three-step process: Update the security policy of the application: add the new MrEnclave for the updated service / application. trigger a rolling update with Kubernetes / helm Update the security policy of the application: remove the old MrEnclave s of the service / application For each service, one can specify a sequence of MrEnclave s. This might look as follows: services: - name: application mrenclaves: [0239...] and we want to update to a new version that has mrenclave of 0239... . If the service support horizontal scaling, we might want a rolling update, e.g., we gradually replace old service instances by new instances. This means there is some time interval in which both old instances as well as new instances a running concurrently.","title":"Software Updates"},{"location":"software_updates/#first-step","text":"Consider that you want to update application to a new version with a new MrEnclave of, say, 4759... . In this case, you would update your policy as follows: services: - name: application mrenclaves: [0239..., 4759...]","title":"First Step"},{"location":"software_updates/#second-step","text":"Now you can upgrade your application with the help of helm . You might trigger a rolling update as follows: helm upgrade application . You need to check that all your service instances have been upgraded.","title":"Second Step"},{"location":"software_updates/#third-step","text":"We now ensure that the old version of the service / application cannot run anymore by removing the old MrEnclave from the policy: services: - name: application mrenclaves: [4759...]","title":"Third Step"},{"location":"technical_summary/","text":"SCONE Technical Summary SCONE is a platform to build and run secure applications with the help of Intel SGX (Software Guard eXtensions) 1 . In a nutshell, our objective is to run applications such that data is always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against attackers with root access . Our aim is it to make it as easy as possible to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran. Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in - even into SCONE. SCONE can be used on top of Kubernetes and Docker. So, what problems can SCONE help to solve? Secure application configuration SCONE provides applications with secrets in a secure fashion. Why is that a problem? Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways: secure configuration files . SCONE can transparently decrypt encrypted configuration files, i.e., without the need to modify the application. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work. secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. Why would I need this? Consider the MySQL example from above. You can pass user passwords via environment variables like MYSQL_ROOT_PASSWORD and MYSQL_PASSWORD to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database. secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments. Transparent attestation SCONE verifies that the correct code is running before passing any configuration info to the application. To ensure this, SCONE provides a local attestation and configuration service : this service provides only the code with the correct signature ( MrEnclave ) with its secrets: certificates, arguments, environment variables and keys. It also provides the application with a certificate that shows that the application runs inside an enclave. Note that this can be done completely transparent to the application, i.e., no application source code changes are required: the encrypted certificate can be stored in the file system where the application expects its certificates. For debugging and development, you can run code inside of enclaves without attestation. Two applications can ensure that they run inside enclaves via TLS authentication. In this way we can ensure that the client certificate and the server certificate was issued by the SCONE CAS, i.e., both communication partners run inside of enclaves and have the expected MrEnclave . Secure main memory An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to protect the main memory : no access by adversaries - even those who have root access, no access by the operating system - even if compromised, no access by the hypervisor - even if compromised, and no access by the cloud provider, and no access by evil maids - despite having physical access to the host. Integration with secure key store Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one can generate keys and stores in SCONE CAS. SCONE also supports the integration with a keystore like Vault. SCONE can run Vault inside of an enclave to protect Vaults secrets in main memory. Transparent TLS encryption Some popular applications like memcached or Zookeeper 2 do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections: the connections are terminated inside of the enclave. In this way, the plain text is never seen by the operating system or any adversary. Note that one should not use an external process for TLS termination 3 . Transparent file protection SCONE protects the integrity and confidentiality of files via transparent file protection . This protection does not require any source code changes. A file can either be integrity-protected only (i.e., the file is stored in plain text but modifications are detected) or confidentiality- and integrity-protected (i.e., the file is encrypted and modifications are detected). Ease of use We provide an easy to use CLI supporting always encrypted execution as well as simplify the construction of remote applications that support always encryption. We plan to support alternative trusted execution environments in future releases of SCONE. \u21a9 Zookeeper replicates its state amongst a group of servers. Zookeeper does not support protecting the communication between these servers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected. \u21a9 Memcached could be protected, for example, with the help of a stunnel . The communication between memcached and stunnel is not encrypted and hence, adversaries with root access would see the unencrypted traffic. \u21a9","title":"Technical summary of SCONE"},{"location":"technical_summary/#scone-technical-summary","text":"SCONE is a platform to build and run secure applications with the help of Intel SGX (Software Guard eXtensions) 1 . In a nutshell, our objective is to run applications such that data is always encrypted , i.e., all data at rest, all data on the wire as well as all data in main memory is encrypted. Even the program code can be encrypted. SCONE helps to protect data, computations and code against attackers with root access . Our aim is it to make it as easy as possible to secure existing application. Switching to SCONE is simple since applications do not need to be modified. SCONE supports the most popular programming languages like JavaScript, Python - including PyPy, Java, Rust, Go, C, and C++ but also some ancient languages like Fortran. Avoiding source code changes helps to ensure that applications can later run on different trusted execution environments. Moreover, there is no risk for hardware lock-in nor software lock-in - even into SCONE. SCONE can be used on top of Kubernetes and Docker.","title":"SCONE Technical Summary"},{"location":"technical_summary/#so-what-problems-can-scone-help-to-solve","text":"","title":"So, what problems can SCONE help to solve?"},{"location":"technical_summary/#secure-application-configuration","text":"SCONE provides applications with secrets in a secure fashion. Why is that a problem? Say, you want to run MySQL and you configure MySQL to encrypt its data at rest. To do so, MySQL requires a key to decrypt and encrypt its files. One can store this key in the MySQL configuration file but this configuration file cannot be encrypted since MySQL would need a key to decrypt the file. SCONE helps developers to solve such configuration issues in the following ways: secure configuration files . SCONE can transparently decrypt encrypted configuration files, i.e., without the need to modify the application. It will give access to the plain text only to a given program, like, MySQL. No source code changes are needed for this to work. secure environment variables . SCONE gives applications access to environment variables that are not visible to anybody else - even users with root access or the operating system. Why would I need this? Consider the MySQL example from above. You can pass user passwords via environment variables like MYSQL_ROOT_PASSWORD and MYSQL_PASSWORD to MySQL. We need to protect these environment variables to prevent unauthorized accesses to the MySQL database. secure command line arguments . Some applications might not use environment variables but command line arguments to pass secrets to the application. SCONE provides a secure way to pass arguments to your application without other privileged parties, like the operating system, being able to see the arguments.","title":"Secure application configuration"},{"location":"technical_summary/#transparent-attestation","text":"SCONE verifies that the correct code is running before passing any configuration info to the application. To ensure this, SCONE provides a local attestation and configuration service : this service provides only the code with the correct signature ( MrEnclave ) with its secrets: certificates, arguments, environment variables and keys. It also provides the application with a certificate that shows that the application runs inside an enclave. Note that this can be done completely transparent to the application, i.e., no application source code changes are required: the encrypted certificate can be stored in the file system where the application expects its certificates. For debugging and development, you can run code inside of enclaves without attestation. Two applications can ensure that they run inside enclaves via TLS authentication. In this way we can ensure that the client certificate and the server certificate was issued by the SCONE CAS, i.e., both communication partners run inside of enclaves and have the expected MrEnclave .","title":"Transparent attestation"},{"location":"technical_summary/#secure-main-memory","text":"An adversary with root access can read the memory content of any process. In this way, an adversary can gain access to keys that an application is using, for example, the keys to protect its data at rest. SCONE helps to protect the main memory : no access by adversaries - even those who have root access, no access by the operating system - even if compromised, no access by the hypervisor - even if compromised, and no access by the cloud provider, and no access by evil maids - despite having physical access to the host.","title":"Secure main memory"},{"location":"technical_summary/#integration-with-secure-key-store","text":"Encryption keys must be protected. In many installations, one does not want humans to be able to see encryption keys. Hence, one can generate keys and stores in SCONE CAS. SCONE also supports the integration with a keystore like Vault. SCONE can run Vault inside of an enclave to protect Vaults secrets in main memory.","title":"Integration with secure key store"},{"location":"technical_summary/#transparent-tls-encryption","text":"Some popular applications like memcached or Zookeeper 2 do not support TLS out of the box. SCONE can transparently add TLS encryption to TCP connections: the connections are terminated inside of the enclave. In this way, the plain text is never seen by the operating system or any adversary. Note that one should not use an external process for TLS termination 3 .","title":"Transparent TLS encryption"},{"location":"technical_summary/#transparent-file-protection","text":"SCONE protects the integrity and confidentiality of files via transparent file protection . This protection does not require any source code changes. A file can either be integrity-protected only (i.e., the file is stored in plain text but modifications are detected) or confidentiality- and integrity-protected (i.e., the file is encrypted and modifications are detected).","title":"Transparent file protection"},{"location":"technical_summary/#ease-of-use","text":"We provide an easy to use CLI supporting always encrypted execution as well as simplify the construction of remote applications that support always encryption. We plan to support alternative trusted execution environments in future releases of SCONE. \u21a9 Zookeeper replicates its state amongst a group of servers. Zookeeper does not support protecting the communication between these servers by TLS. SCONE can add transparent support TLS for Zookeeper to ensure that the integrity and confidentiality of the data exchanged between the Zookeeper server is protected. \u21a9 Memcached could be protected, for example, with the help of a stunnel . The communication between memcached and stunnel is not encrypted and hence, adversaries with root access would see the unencrypted traffic. \u21a9","title":"Ease of use"},{"location":"teemon/","text":"TEEMon Introduction We designed and implemented TEEMon \u2014 a real-time performance monitoring and analysis tool for Intel SGX based applications. TEEMon provides fine-grained performance metrics during runtime, including SGX metrics e.g., total EPC pages, free EPC pages, pages marked as old, pages evicted to main memory, pages added to enclaves, pages reclaimed from main memory, etc. It also performs the analysis to identify the causes of performance bottlenecks. It is integrated with open-source tools like Prometheus and Grafana to offer a comprehensive monitoring solution running inside Docker containers and providing a wide-ranging set of SGX metrics such as and visualizations with a low performance overhead. TEEMon requires the driver metrics extension TEEMon collects SGX-related metrics that are provided by our SGX Driver metrics extension. Without this driver extension, TEEMon will not run correctly. TEEMon is integrated with Kubernetes to monitor the performance of applications running inside SGX enclaves We integrated TEEMon with Kubernetes to monitor the performance of an application running inside more than 6000 distributed SGX enclaves using SCONE (See the following screencast) TEEMon Deployment One can install TEEMon with the help of helm in a Kubernetes cluster. For details, please follow our deployment instructions .","title":"Monitoring tool"},{"location":"teemon/#teemon","text":"","title":"TEEMon"},{"location":"teemon/#introduction","text":"We designed and implemented TEEMon \u2014 a real-time performance monitoring and analysis tool for Intel SGX based applications. TEEMon provides fine-grained performance metrics during runtime, including SGX metrics e.g., total EPC pages, free EPC pages, pages marked as old, pages evicted to main memory, pages added to enclaves, pages reclaimed from main memory, etc. It also performs the analysis to identify the causes of performance bottlenecks. It is integrated with open-source tools like Prometheus and Grafana to offer a comprehensive monitoring solution running inside Docker containers and providing a wide-ranging set of SGX metrics such as and visualizations with a low performance overhead. TEEMon requires the driver metrics extension TEEMon collects SGX-related metrics that are provided by our SGX Driver metrics extension. Without this driver extension, TEEMon will not run correctly.","title":"Introduction"},{"location":"teemon/#teemon-is-integrated-with-kubernetes-to-monitor-the-performance-of-applications-running-inside-sgx-enclaves","text":"We integrated TEEMon with Kubernetes to monitor the performance of an application running inside more than 6000 distributed SGX enclaves using SCONE (See the following screencast)","title":"TEEMon is integrated with Kubernetes to monitor the performance of applications running inside SGX enclaves"},{"location":"teemon/#teemon-deployment","text":"One can install TEEMon with the help of helm in a Kubernetes cluster. For details, please follow our deployment instructions .","title":"TEEMon Deployment"},{"location":"tensorflow/","text":"Palaemon Tensorflow Classification Demo We created a Tensorflow demo to simplify your evaluating TensorFlow running inside of Intel SGX. To get access to the TensorFlow image, send us an email . Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device . Try it out the demo by executing: docker pull registry.scontain.com:5050/sconecuratedimages/datasystems:tensorflow-full determine_sgx_device docker run -it registry.scontain.com:5050/sconecuratedimages/datasystems:tensorflow-full sh Test Tensorflow with SCONE by performing image classification cd /demo/ && ./run.sh","title":"TensorFlow"},{"location":"tensorflow/#palaemon-tensorflow-classification-demo","text":"We created a Tensorflow demo to simplify your evaluating TensorFlow running inside of Intel SGX. To get access to the TensorFlow image, send us an email . Ensure that you have the newest SCONE cross compiler image and determine which SGX device to mount with function determine_sgx_device . Try it out the demo by executing: docker pull registry.scontain.com:5050/sconecuratedimages/datasystems:tensorflow-full determine_sgx_device docker run -it registry.scontain.com:5050/sconecuratedimages/datasystems:tensorflow-full sh Test Tensorflow with SCONE by performing image classification cd /demo/ && ./run.sh","title":"Palaemon Tensorflow Classification Demo"},{"location":"tensorflowlite/","text":"TensorFlow Lite Use Case TensorFlow Lite was designed for on-device machine learning inference with low latency and a small binary size . Hence, TensorFlow Lite is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We will add some more documentation about the curated TensorFlow Lite image later. Until then, you can have a look at our TensorFlow Lite screencast: If you want to evaluate the TensorFlow Lite image, send us an email .","title":"TensorFlow Lite"},{"location":"tensorflowlite/#tensorflow-lite-use-case","text":"TensorFlow Lite was designed for on-device machine learning inference with low latency and a small binary size . Hence, TensorFlow Lite is ideally suited for running inside of Intel SGX enclaves with the help of SCONE. We will add some more documentation about the curated TensorFlow Lite image later. Until then, you can have a look at our TensorFlow Lite screencast: If you want to evaluate the TensorFlow Lite image, send us an email .","title":"TensorFlow Lite Use Case"},{"location":"usecases/","text":"SCONE Use Cases Overview In this chapter, we introduce some use cases of how SCONE can be used. Most of these use cases will be based on curated images 1 that users can subscribe to. We introduce some advanced use cases: Confidential Document Management to show how to convert a cloud-native application into a confidential cloud-native application with the help of SCONE. Federated Machine Learning we shows how to implement federated machine learning with the help of use SCONE. Confidential Code Deployment shows how to establish trust between entities that share a common source code. We also have some more basic use cases: blender : Blender 3D creation suite. We show how to protect the blender image while permitting clients that deploy this image to customize the image. They can customize without having access to the key used to protect that image. PySpark : Apache Spark with Python inside of Intel SGX enclaves with the help of SCONE. TensorFlow : Run TensorFlow inside of Intel SGX enclaves with the help of SCONE. TensorFlow Lite : TensorFlow Lite inside of Intel SGX enclaves with the help of SCONE. OpenVino : Run OpenVino based applications inside of Intel SGX enclaves with the help of SCONE. We will add more use cases over time. A curated image is a container image of a popular service maintained by scontain.com. \u21a9","title":"Introduction"},{"location":"usecases/#scone-use-cases-overview","text":"In this chapter, we introduce some use cases of how SCONE can be used. Most of these use cases will be based on curated images 1 that users can subscribe to. We introduce some advanced use cases: Confidential Document Management to show how to convert a cloud-native application into a confidential cloud-native application with the help of SCONE. Federated Machine Learning we shows how to implement federated machine learning with the help of use SCONE. Confidential Code Deployment shows how to establish trust between entities that share a common source code. We also have some more basic use cases: blender : Blender 3D creation suite. We show how to protect the blender image while permitting clients that deploy this image to customize the image. They can customize without having access to the key used to protect that image. PySpark : Apache Spark with Python inside of Intel SGX enclaves with the help of SCONE. TensorFlow : Run TensorFlow inside of Intel SGX enclaves with the help of SCONE. TensorFlow Lite : TensorFlow Lite inside of Intel SGX enclaves with the help of SCONE. OpenVino : Run OpenVino based applications inside of Intel SGX enclaves with the help of SCONE. We will add more use cases over time. A curated image is a container image of a popular service maintained by scontain.com. \u21a9","title":"SCONE Use Cases Overview"},{"location":"vault/","text":"Scone Vault Vault is a popular and nicely designed secret management system. We maintain a hardened version of Vault that runs with Scone. It addresses the following issues: Vault keeps secrets in the clear in the main memory. Even if a secret is encrypted when stored externally, they are stored in the clear in main memory, e.g., just before they are sent to a client. An attacker with root access could hence dump the memory of Vault to retrieve the secrets. Vault's encryption key to read from and to write to the external storage is stored in the main memory. Again, an attacker could dump the memory to gain access to Vault's encryption key. Vault Image We maintain a container image registry.scontain.com:5050/sconecuratedimages/apps:scone-vault-latest which contains the latest version of Vault (1.5.3), runs inside of an SGX enclaves (default is a pre-release enclave), on top of Alpine Linux This protects against dumping of the main memory (- when run in release mode). Demo This image also contains a demo that you can try out. This demo shows how to set up the configuration of an nginx instance. To run this demo, you first need to checkout the demo repostiory: git clone https://github.com/scontain/scone-vault cd scone-vault you need to ensure to have a docker compose file with content as follows: cat > docker-compose.yml <<EOF version: '3.2' services: vault: image: sconecuratedimages/apps:vault-1.5.3-alpine-scone5 command: sh -c \"cd build_dir && ./start_vault.sh\" environment: - VAULT_DEV_ROOT_TOKEN_ID=RootToken volumes: - ./:/build_dir cap_add: - IPC_LOCK scone-vault-nginx: image: sconecuratedimages/apps:nginx-1.14.2-alpine-scone5 environment: - URL=\"http://vault:8200\" - INDEX=nginx - VAULT_ADDR=\"http://vault:8200\" - TOKEN=RootToken command: sh -c \"cd build_dir && ./install-deps.sh && ./bench.sh\" volumes: - ./:/build_dir depends_on: - vault EOF Next, try it out by executing: docker-compose up Please ensure to execute docker-compose down before starting it with up again. Note that the script start_vault.sh is used to start Vault server and inject some secrets used for Nginx. Details You can perform the individual steps manually as described below. Run the demo container using docker-compose: docker-compose run scone-vault-nginx sh Go to the deployment directory: cd /build_dir/ Install dependencies: ./install-deps.sh Now, run the benchmark to test if SCONE Vault is setting up the configuration for Nginx. ./bench.sh Integration with SCONE CAS For added security, we recommend the following to run Vault in a release enclave to prevent attackers from dumping the content of the Vault enclave and to integrate the execution of Vault with SCONE CAS such that the command line arguments and environment variables are passed to Vault in an encrypted fashion stdin/stdout/stderr of Vault are encrypted CAS can pass a TLS private / public key to Vault the Vault encryption key can be stored by SCONE CAS Vault's HSM PIN pin can be specified by the VAULT_HSM_PIN environment variable. We can securely set the environment variable with the help of SCONE CAS since SCONE can pass encrypted environment variables to applications. Please contact us, if you are interested in a setup together with SCONE CAS.","title":"Vault"},{"location":"vault/#scone-vault","text":"Vault is a popular and nicely designed secret management system. We maintain a hardened version of Vault that runs with Scone. It addresses the following issues: Vault keeps secrets in the clear in the main memory. Even if a secret is encrypted when stored externally, they are stored in the clear in main memory, e.g., just before they are sent to a client. An attacker with root access could hence dump the memory of Vault to retrieve the secrets. Vault's encryption key to read from and to write to the external storage is stored in the main memory. Again, an attacker could dump the memory to gain access to Vault's encryption key.","title":"Scone Vault"},{"location":"vault/#vault-image","text":"We maintain a container image registry.scontain.com:5050/sconecuratedimages/apps:scone-vault-latest which contains the latest version of Vault (1.5.3), runs inside of an SGX enclaves (default is a pre-release enclave), on top of Alpine Linux This protects against dumping of the main memory (- when run in release mode).","title":"Vault Image"},{"location":"vault/#demo","text":"This image also contains a demo that you can try out. This demo shows how to set up the configuration of an nginx instance. To run this demo, you first need to checkout the demo repostiory: git clone https://github.com/scontain/scone-vault cd scone-vault you need to ensure to have a docker compose file with content as follows: cat > docker-compose.yml <<EOF version: '3.2' services: vault: image: sconecuratedimages/apps:vault-1.5.3-alpine-scone5 command: sh -c \"cd build_dir && ./start_vault.sh\" environment: - VAULT_DEV_ROOT_TOKEN_ID=RootToken volumes: - ./:/build_dir cap_add: - IPC_LOCK scone-vault-nginx: image: sconecuratedimages/apps:nginx-1.14.2-alpine-scone5 environment: - URL=\"http://vault:8200\" - INDEX=nginx - VAULT_ADDR=\"http://vault:8200\" - TOKEN=RootToken command: sh -c \"cd build_dir && ./install-deps.sh && ./bench.sh\" volumes: - ./:/build_dir depends_on: - vault EOF Next, try it out by executing: docker-compose up Please ensure to execute docker-compose down before starting it with up again. Note that the script start_vault.sh is used to start Vault server and inject some secrets used for Nginx.","title":"Demo"},{"location":"vault/#details","text":"You can perform the individual steps manually as described below. Run the demo container using docker-compose: docker-compose run scone-vault-nginx sh Go to the deployment directory: cd /build_dir/ Install dependencies: ./install-deps.sh Now, run the benchmark to test if SCONE Vault is setting up the configuration for Nginx. ./bench.sh","title":"Details"},{"location":"vault/#integration-with-scone-cas","text":"For added security, we recommend the following to run Vault in a release enclave to prevent attackers from dumping the content of the Vault enclave and to integrate the execution of Vault with SCONE CAS such that the command line arguments and environment variables are passed to Vault in an encrypted fashion stdin/stdout/stderr of Vault are encrypted CAS can pass a TLS private / public key to Vault the Vault encryption key can be stored by SCONE CAS Vault's HSM PIN pin can be specified by the VAULT_HSM_PIN environment variable. We can securely set the environment variable with the help of SCONE CAS since SCONE can pass encrypted environment variables to applications. Please contact us, if you are interested in a setup together with SCONE CAS.","title":"Integration with SCONE CAS"},{"location":"vm_vs_enclave/","text":"Confidential Computing with SCONE Confidential computing focuses on the protection of applications. We require that confidential computing protects an application's confidentiality , i.e., no other entity, like a root user, can read the data, the code, or the secrets of the application in memory , on disk or on the network , integrity , i.e., no other entity, like the hypervisor, can modify the data, or at least any modifications are detected in the memory, on disk or on the network, and consistency , i.e., the application reads always the value that was written last - both in memory as well as on disk and on the network. Modern cloud-native applications consist of multiple services that communicate via the network with each other and with external clients. These services are deployed with the help of containers. In this section, we focus on confidential, cloud-native applications. Hardware Support Currently, there are two classes of hardware support for confidential computing available. The current mechanisms can be classified as follows: one can encrypt the VM (Virtual Machine) in which the application is executing, and one can encrypt each individual service inside of an enclave - with the help of SCONE. One has to be careful regarding the security guarantees because the use of an encrypted VMs does not necessarily mean that a root user of the host does not have access to the VM. In current AMD CPUs without Secure Nested Pages (SNP), the hypervisor could break the confidentiality and integrity of an application. In our discussion we assume that we already be able to use SNPs. Note that the consistency of memory pages is not protected by AMD CPUs, i.e., one could replace a memory page by an older version: this would be properly encrypted but would break the consistency. In Intel SGX enclaves, it is guaranteed that an application always reads the most recent data that was written. Encrypted VMs When running cloud-native applications, multiple stakeholders are involved. This includes: a host admin that maintains the host OS (operating system) a Kubernetes admin that maintains Kubernetes, say, we have a managed Kubernetes service, and a container/service admin that takes care of the services and the containers. In the context of cloud-native applications, an encrypted VM would be used to represent a Kubernetes node. The trusted computing base would not only include the application but we would also need to trust: the operating system and the OS admin, Kubernetes and the Kubernetes admin, the container/service admin. One could try to execute each container in a separate encrypted VM. This would reduce the size of the trusted computing base (TCB) since the Kubernetes admin could - if one does this correctly - be not part of the trusted computing base anymore. Still the container/service admin, the operating system in the VM and its OS admin would still be part of the TCB. Enclaves Enclaves permit to reduce the size of trusted computing base to the application itself: we can remove all admins and all code outside of the application from the trusted computing base. Note that SCONE will help to protect the files of an application and also the network if needed, i.e., a service admin will only see encrypted files and will not know any application secrets. Isolation and Cooperation The advantage of enclaves together with SCONE is that one can protect services from each other. A service has only access to its own enclave and to its files but not to the data/files of other enclaves. Using encrypted VMs, one would need to run each service in a separate VM with its own operating system - which increases the TCB as we explained above. This also increases the resource usage since each container would come with its own operating system. With the help of SCONE, services of an application can cooperate and implicitly attest each other via TLS: a service can only establish a TLS connection with another service of the application if that service executes inside of an enclave, its code was not modified and the filesystem is in the correct state. Simplicity One of the arguments for using encrypted VMs (instead of enclaves) is that it simplifies the protection of existing applications. In reality, this is of course not that simple since one has to encrypt files of the VM, one has to ensure that the VM learns the filesystem encryption key but only if neither the operating system nor the application was not modified and it is executed in an encrypted VM, one has to ensure that the service in the different VMs do attest each other, and one has to provision secrets etc With the help of SCONE, one can transform existing container image into confidential container image in a single step (see sconify images ). SCONE provides a policy language that permits to define on how to provision secrets and how to attest applications, i.e., address all these issues using a simple, Yaml-based policy language. We argue that the use of enclaves is with the help of SCONE not only more secure than encrypted VMs but also more convenient since the transformation is automated and the policy-support allows both cooperation between services as well as isolation without the need to change the services.","title":"Confidential Computing"},{"location":"vm_vs_enclave/#confidential-computing-with-scone","text":"Confidential computing focuses on the protection of applications. We require that confidential computing protects an application's confidentiality , i.e., no other entity, like a root user, can read the data, the code, or the secrets of the application in memory , on disk or on the network , integrity , i.e., no other entity, like the hypervisor, can modify the data, or at least any modifications are detected in the memory, on disk or on the network, and consistency , i.e., the application reads always the value that was written last - both in memory as well as on disk and on the network. Modern cloud-native applications consist of multiple services that communicate via the network with each other and with external clients. These services are deployed with the help of containers. In this section, we focus on confidential, cloud-native applications.","title":"Confidential Computing with SCONE"},{"location":"vm_vs_enclave/#hardware-support","text":"Currently, there are two classes of hardware support for confidential computing available. The current mechanisms can be classified as follows: one can encrypt the VM (Virtual Machine) in which the application is executing, and one can encrypt each individual service inside of an enclave - with the help of SCONE. One has to be careful regarding the security guarantees because the use of an encrypted VMs does not necessarily mean that a root user of the host does not have access to the VM. In current AMD CPUs without Secure Nested Pages (SNP), the hypervisor could break the confidentiality and integrity of an application. In our discussion we assume that we already be able to use SNPs. Note that the consistency of memory pages is not protected by AMD CPUs, i.e., one could replace a memory page by an older version: this would be properly encrypted but would break the consistency. In Intel SGX enclaves, it is guaranteed that an application always reads the most recent data that was written.","title":"Hardware Support"},{"location":"vm_vs_enclave/#encrypted-vms","text":"When running cloud-native applications, multiple stakeholders are involved. This includes: a host admin that maintains the host OS (operating system) a Kubernetes admin that maintains Kubernetes, say, we have a managed Kubernetes service, and a container/service admin that takes care of the services and the containers. In the context of cloud-native applications, an encrypted VM would be used to represent a Kubernetes node. The trusted computing base would not only include the application but we would also need to trust: the operating system and the OS admin, Kubernetes and the Kubernetes admin, the container/service admin. One could try to execute each container in a separate encrypted VM. This would reduce the size of the trusted computing base (TCB) since the Kubernetes admin could - if one does this correctly - be not part of the trusted computing base anymore. Still the container/service admin, the operating system in the VM and its OS admin would still be part of the TCB.","title":"Encrypted VMs"},{"location":"vm_vs_enclave/#enclaves","text":"Enclaves permit to reduce the size of trusted computing base to the application itself: we can remove all admins and all code outside of the application from the trusted computing base. Note that SCONE will help to protect the files of an application and also the network if needed, i.e., a service admin will only see encrypted files and will not know any application secrets.","title":"Enclaves"},{"location":"vm_vs_enclave/#isolation-and-cooperation","text":"The advantage of enclaves together with SCONE is that one can protect services from each other. A service has only access to its own enclave and to its files but not to the data/files of other enclaves. Using encrypted VMs, one would need to run each service in a separate VM with its own operating system - which increases the TCB as we explained above. This also increases the resource usage since each container would come with its own operating system. With the help of SCONE, services of an application can cooperate and implicitly attest each other via TLS: a service can only establish a TLS connection with another service of the application if that service executes inside of an enclave, its code was not modified and the filesystem is in the correct state.","title":"Isolation and Cooperation"},{"location":"vm_vs_enclave/#simplicity","text":"One of the arguments for using encrypted VMs (instead of enclaves) is that it simplifies the protection of existing applications. In reality, this is of course not that simple since one has to encrypt files of the VM, one has to ensure that the VM learns the filesystem encryption key but only if neither the operating system nor the application was not modified and it is executed in an encrypted VM, one has to ensure that the service in the different VMs do attest each other, and one has to provision secrets etc With the help of SCONE, one can transform existing container image into confidential container image in a single step (see sconify images ). SCONE provides a policy language that permits to define on how to provision secrets and how to attest applications, i.e., address all these issues using a simple, Yaml-based policy language. We argue that the use of enclaves is with the help of SCONE not only more secure than encrypted VMs but also more convenient since the transformation is automated and the policy-support allows both cooperation between services as well as isolation without the need to change the services.","title":"Simplicity"},{"location":"whyscone/","text":"SCONE vs Intel SGX SDK In this section, we present the advantages of SCONE compared with Intel SGX SDK. The table below shows why should we use SCONE for Confidential Computing instead of Intel SGX SDK. Features Intel SGX SDK SCONE Platform SLA: Startup times Slow Efficient startup/attestation SLA: Scheduling - SLA-based scheduling SLA: Efficiency Many enclave exits Reduced enclave exits Security: CVEs CVE handling by application CVEs addressed by platform Security: policy No policy support Advanced-policy support Security: platform - Integrated OS and Application Sec. Security: Side-channel No protection Side-channel protection Monitoring: SLA - SLA-based monitoring Monitoring: SGX - SGX-resources & scheduling Encryption at rest / in transit Source code changes required No source code changes Encryption at use Source code changes required No source code changes Attestation Explicit code required Automatic by SCONE Key Provisioning Explicit code required Automatic by SCONE CI/CD Integration - Modern IDE Languages C/C++ Most modern languages (C/C++, Python, Rust, Java, Nodejs, R, ...) Portability Intel SGX-specific (eventually other CPUs) TCO Higher Lower","title":"SCONE vs Intel SGX SDK"},{"location":"whyscone/#scone-vs-intel-sgx-sdk","text":"In this section, we present the advantages of SCONE compared with Intel SGX SDK. The table below shows why should we use SCONE for Confidential Computing instead of Intel SGX SDK. Features Intel SGX SDK SCONE Platform SLA: Startup times Slow Efficient startup/attestation SLA: Scheduling - SLA-based scheduling SLA: Efficiency Many enclave exits Reduced enclave exits Security: CVEs CVE handling by application CVEs addressed by platform Security: policy No policy support Advanced-policy support Security: platform - Integrated OS and Application Sec. Security: Side-channel No protection Side-channel protection Monitoring: SLA - SLA-based monitoring Monitoring: SGX - SGX-resources & scheduling Encryption at rest / in transit Source code changes required No source code changes Encryption at use Source code changes required No source code changes Attestation Explicit code required Automatic by SCONE Key Provisioning Explicit code required Automatic by SCONE CI/CD Integration - Modern IDE Languages C/C++ Most modern languages (C/C++, Python, Rust, Java, Nodejs, R, ...) Portability Intel SGX-specific (eventually other CPUs) TCO Higher Lower","title":"SCONE vs Intel SGX SDK"},{"location":"windows10/","text":"Windows 10 First, SCONE-based applications run on top of Linux. We actually support multiple flavors like Alpine Linux, Ubuntu, Fedora and RHEL/Centos. Our recommended distribution for SCONE-based application is Alpine Linux since it is ideally suited to run in contains. Second, you can run SCONE-based applications on top of Windows 10/HyperV hosts by running a Linux VM. You can test this by running SCONE-based applications in simulation mode in a Docker engine - which runs on Windows 10 systems with HyperV support. Of course, you would like to run your applications in hardware mode. To do so, you need to give your Linux VM access to SGX. You can follow our Windows tutorial to learn how to install Alpine Linux and to run SCONE-based applications on top of Windows 10. Note that these SCONE-based applications run with paging on, i.e., the memory size of these applications can be as large as 32GB on current CPUs and much larger on future CPUs.","title":"Windows 10"},{"location":"windows10/#windows-10","text":"First, SCONE-based applications run on top of Linux. We actually support multiple flavors like Alpine Linux, Ubuntu, Fedora and RHEL/Centos. Our recommended distribution for SCONE-based application is Alpine Linux since it is ideally suited to run in contains. Second, you can run SCONE-based applications on top of Windows 10/HyperV hosts by running a Linux VM. You can test this by running SCONE-based applications in simulation mode in a Docker engine - which runs on Windows 10 systems with HyperV support. Of course, you would like to run your applications in hardware mode. To do so, you need to give your Linux VM access to SGX. You can follow our Windows tutorial to learn how to install Alpine Linux and to run SCONE-based applications on top of Windows 10. Note that these SCONE-based applications run with paging on, i.e., the memory size of these applications can be as large as 32GB on current CPUs and much larger on future CPUs.","title":"Windows 10"},{"location":"aks/flask_demo/","text":"A Confidential Flask-Based Application We demonstrate with the help of a simple Flask-based Service multiple features of the SCONE platform: we show that we can execute unmodified Python programs inside of SGX enclaves we show how to encrypt the Python program to protect the confidentiality and integrity of the Python code how to implicitly attest other services with the help of TLS, i.e., to ensure that one communicates with a service that satisfy its security policy. we demonstrate how Redis, an in-memory data structure store, and the Python flask attest each other via TLS without needing to change the code of neither Redis nor the Flask-based service. we show how to generate TLS certificates with the help of a policy: a SCONE security policy describes how to attest applications and services (i.e., describe the code, the filesystem state, the environment, the node on which to execute, and secrets). a SCONE policy can generate secrets and in particular, key-pairs and TLS certificates. we show how to execute this example on a local computer with the help of docker-compose on a generic Kubernetes cluster, and on Azure Kubernetes Service (AKS). Next Step In the second version of this example, we simplify the workflow in the sense that we use a generic script to transform an existing native container image into an encrypted, confidential container image. Flask-Based Confidential Service We implement a simple Flask-based service. The Python code implements a REST API: to store patient records (i.e., POST to resource /patient/<string:patient_id> ) to retrieve patient records (i.e., GET of resource /patient/<string:patient_id> ) to retrieve some score for a patient (i.e., GET of ressource '/score/<string:patient_id> ) The Python code is executed inside of an enclave to ensure that even users with root access cannot read the patient data. TLS Certificates The service uses a Redis instance to store the resources. The communication between 1) the Flask-based service and its clients and 2) Redis and the application is encrypted with the help of TLS. To do so, we need to provision the application and Redis with multiple keys and certificates: Redis client certificate Redis server certificate Flask server certificate Redis and the Flask-based service, require that the private keys and certificates are stored in the filesystem. We generate and provision these TLS-related files with the help of a SCONE policy . To do so, we generate secrets related to the Flask-based service. We specify in the flask policy that a private key ( api_ca_key ) for a new certificate authority (CA) is generated a certificate ( api_ca_cert ) for a certification authority is generated using the private key (i.e., api_ca_key ), and making this certificate available to everybody (see export_public: true ) we generate a private key for the certificate used by the REST API (i.e., flask_key ) we generate a certificate ( flask ) with the help of CA api_ca_cert and assign it a dns name api . The SCONE policy is based on Yaml and the flask policy contains the following section to define these secrets: secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api The private keys and certificates are expected at certain locations in the file system. SCONE permits to map these secrets into the filesystem of the Flask-based service: these files are only visible to the service inside of an SGX enclave after a successful attestation (see below) and in particular, not visible on the outside i.e., in the filesystem of the container. To map the private keys and certificates into the filesystem of a service, we specify in the policy which secrets are visible to a service at which path. In the flask policy this is done as follows: images: - name: flask_restapi_image injection_files: - path: /tls/flask.crt content: $$SCONE::flask.crt$$ - path: /tls/flask.key content: $$SCONE::flask.key$$ And in the Python program, one can just access these files as normal files. One can create a SSL context (see code ): app . run ( host = '0.0.0.0' , port = 4996 , threaded = True , ssl_context = (( \"/tls/flask.crt\" , \"/tls/flask.key\" ))) While we do not show how to enforce client authentication of the REST API, we show how to do this for Redis in the next section. TLS-based Mutual Attestation The communication between the Flask-based service, say, S S and Redis instance, say, R R is encrypted via TLS. Actually, we make sure that the service S S and instance R R attest each other. Attestation means that S S ensures that R R satisfies all requirements specified in R R 's security policy and R R ensures that S S satisfies all the requirements of S S 's policy. Of course, this should be done without changing the code of neither S S nor R R . In case that S S and R R are using TLS with client authentication, this is straightforward to enforce. (If this is not the case, please contact us for an alternative.) To ensure mutual attestation, the operator of Redis defines a policy in which it defines a certification authority ( redis_ca_cert ) and defines both a Redis certificate ( redis_ca_cert ) as well as a Redis client certificate ( redis_client_cert ). The client certificate and the private key ( redis_client_key ) are exported to the policy of the Flask service S S . The policy for this looks like this: secrets: - name: redis_key kind: private-key - name: redis # automatically generate Redis server certificate kind: x509 private_key: redis_key issuer: redis_ca_cert dns: - redis - name: redis_client_key kind: private-key export: - session: $FLASK_SESSION - name: redis_client_cert # automatically generate client certificate kind: x509 issuer: redis_ca_cert private_key: redis_client_key export: - session: $FLASK_SESSION # export client cert/key to client session - name: redis_ca_key kind: private-key - name: redis_ca_cert # export session CA certificate as Redis CA certificate kind: x509-ca private_key: redis_ca_key export: - session: $FLASK_SESSION # export the session CA certificate to client session Note that $FLASK_SESSION is replaced by the unique name of the policy of S S . The security policies are in this example on the same SCONE CAS (Configuration and Attestation Service) . In more complex scenarios, the policies could also be stored on separate SCONE CAS instances operated by different entities. The flask service can import the Redis CA certificate, client certificate and private key as follows: secrets: - name: redis_client_key import: session: $REDIS_SESSION secret: redis_client_key - name: redis_client_cert import: session: $REDIS_SESSION secret: redis_client_cert - name: redis_ca_cert import: session: $REDIS_SESSION secret: redis_ca_cert These secrets are made available to the Flask-based service in the filesystem (i.e., files /tls/redis-ca.crt , /tls/client.crt and /tls/client.key ) via the following entries in its security policy: images: - name: flask_restapi_image injection_files: - path: /tls/redis-ca.crt content: $$SCONE::redis_ca_cert.chain$$ - path: /tls/client.crt content: $$SCONE::redis_client_cert.crt$$ - path: /tls/client.key content: $$SCONE::redis_client_cert.key$$ Note that before uploading a policy to SCONE CAS, one first attests that one indeed communicates with a genuine SCONE CAS running inside of a production enclave. This is done with the help of a SCONE CAS CLI . Code The source code of this example is open source and available on github: git clone https://github.com/scontain/flask_example.git cd flask_example Run Service On Local Computer You can use docker-compose to run this example on your local SGX-enabled computer as follows. You first generate an encrypted image using script create_image.sh . This generates some environment variables that stored in file myenv and are loaded via source myenv . The service and Redis are started with docker-compose up . ./create_image.sh source myenv docker-compose up We use a public instance of SCONE CAS in this example. Testing the service Retrieve the API certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api:4996 curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /patient/patient_3 --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /score/patient_3 --resolve api:4996:127.0.0.1 The output might look as follows: $ curl --cacert cacert.pem -X POST https://localhost:4996/patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/patient/patient_3 --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/score/patient_3 --resolve api:4996:127.0.0.1 {\"id\":\"patient_3\",\"score\":0.2781606437899131} Execution on a Kubernetes Cluster and AKS You can run this example on a Kubernetes cluster or Azure Kubernetes Service (AKS). Install SCONE services Get access to SconeApps (see https://sconedocs.github.io/helm/ ): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update Give SconeApps access to the private docker images (see the helm docs ): export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Start LAS and CAS service: helm install las sconeapps/las --set service.hostPort = true helm install cas sconeapps/cas Install the SGX device plugin for Kubernetes: helm install sgxdevplugin sconeapps/sgxdevplugin Run the application Start by creating a Docker image and setting its name. Remember to specify a repository to which you are allowed to push: export IMAGE = registry.scontain.com:5050/sconecuratedimages/application:v0.4 # please change to an image that you can push ./create_image.sh source myenv docker push $IMAGE Use the Helm chart in deploy/helm to deploy the application to a Kubernetes cluster. helm install api-v1 deploy/helm \\ --set image = $IMAGE \\ --set scone.cas = $SCONE_CAS_ADDR \\ --set scone.flask_session = $FLASK_SESSION /flask_restapi \\ --set scone.redis_session = $REDIS_SESSION /redis \\ --set service.type = LoadBalancer NOTE : Setting service.type=LoadBalancer will allow the application to get traffic from the internet (through a managed LoadBalancer). Test the application After all resources are Running , you can test the API via Helm: helm test api-v1 Helm will run a pod with a couple of pre-set queries to check if the API is working properly. Access the application If the application is exposed to the world through a service of type LoadBalancer, you can retrieve its CA certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Retrieve the service public IP address: export SERVICE_IP = $( kubectl get svc --namespace default api-v1-example --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api Now you can perform queries such as: curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:443: ${ SERVICE_IP } Clean up helm delete cas helm delete las helm delete sgxdevplugin helm delete api-v1 kubectl delete pod api-v1-example-test-api Next, we introduce the different sconified Python versions that we support.","title":"Flask Demo"},{"location":"aks/flask_demo/#a-confidential-flask-based-application","text":"We demonstrate with the help of a simple Flask-based Service multiple features of the SCONE platform: we show that we can execute unmodified Python programs inside of SGX enclaves we show how to encrypt the Python program to protect the confidentiality and integrity of the Python code how to implicitly attest other services with the help of TLS, i.e., to ensure that one communicates with a service that satisfy its security policy. we demonstrate how Redis, an in-memory data structure store, and the Python flask attest each other via TLS without needing to change the code of neither Redis nor the Flask-based service. we show how to generate TLS certificates with the help of a policy: a SCONE security policy describes how to attest applications and services (i.e., describe the code, the filesystem state, the environment, the node on which to execute, and secrets). a SCONE policy can generate secrets and in particular, key-pairs and TLS certificates. we show how to execute this example on a local computer with the help of docker-compose on a generic Kubernetes cluster, and on Azure Kubernetes Service (AKS). Next Step In the second version of this example, we simplify the workflow in the sense that we use a generic script to transform an existing native container image into an encrypted, confidential container image.","title":"A Confidential Flask-Based Application"},{"location":"aks/flask_demo/#flask-based-confidential-service","text":"We implement a simple Flask-based service. The Python code implements a REST API: to store patient records (i.e., POST to resource /patient/<string:patient_id> ) to retrieve patient records (i.e., GET of resource /patient/<string:patient_id> ) to retrieve some score for a patient (i.e., GET of ressource '/score/<string:patient_id> ) The Python code is executed inside of an enclave to ensure that even users with root access cannot read the patient data.","title":"Flask-Based Confidential Service"},{"location":"aks/flask_demo/#tls-certificates","text":"The service uses a Redis instance to store the resources. The communication between 1) the Flask-based service and its clients and 2) Redis and the application is encrypted with the help of TLS. To do so, we need to provision the application and Redis with multiple keys and certificates: Redis client certificate Redis server certificate Flask server certificate Redis and the Flask-based service, require that the private keys and certificates are stored in the filesystem. We generate and provision these TLS-related files with the help of a SCONE policy . To do so, we generate secrets related to the Flask-based service. We specify in the flask policy that a private key ( api_ca_key ) for a new certificate authority (CA) is generated a certificate ( api_ca_cert ) for a certification authority is generated using the private key (i.e., api_ca_key ), and making this certificate available to everybody (see export_public: true ) we generate a private key for the certificate used by the REST API (i.e., flask_key ) we generate a certificate ( flask ) with the help of CA api_ca_cert and assign it a dns name api . The SCONE policy is based on Yaml and the flask policy contains the following section to define these secrets: secrets: - name: api_ca_key kind: private-key - name: api_ca_cert kind: x509-ca export_public: true private_key: api_ca_key - name: flask_key kind: private-key - name: flask kind: x509 private_key: flask_key issuer: api_ca_cert dns: - api The private keys and certificates are expected at certain locations in the file system. SCONE permits to map these secrets into the filesystem of the Flask-based service: these files are only visible to the service inside of an SGX enclave after a successful attestation (see below) and in particular, not visible on the outside i.e., in the filesystem of the container. To map the private keys and certificates into the filesystem of a service, we specify in the policy which secrets are visible to a service at which path. In the flask policy this is done as follows: images: - name: flask_restapi_image injection_files: - path: /tls/flask.crt content: $$SCONE::flask.crt$$ - path: /tls/flask.key content: $$SCONE::flask.key$$ And in the Python program, one can just access these files as normal files. One can create a SSL context (see code ): app . run ( host = '0.0.0.0' , port = 4996 , threaded = True , ssl_context = (( \"/tls/flask.crt\" , \"/tls/flask.key\" ))) While we do not show how to enforce client authentication of the REST API, we show how to do this for Redis in the next section.","title":"TLS Certificates"},{"location":"aks/flask_demo/#tls-based-mutual-attestation","text":"The communication between the Flask-based service, say, S S and Redis instance, say, R R is encrypted via TLS. Actually, we make sure that the service S S and instance R R attest each other. Attestation means that S S ensures that R R satisfies all requirements specified in R R 's security policy and R R ensures that S S satisfies all the requirements of S S 's policy. Of course, this should be done without changing the code of neither S S nor R R . In case that S S and R R are using TLS with client authentication, this is straightforward to enforce. (If this is not the case, please contact us for an alternative.) To ensure mutual attestation, the operator of Redis defines a policy in which it defines a certification authority ( redis_ca_cert ) and defines both a Redis certificate ( redis_ca_cert ) as well as a Redis client certificate ( redis_client_cert ). The client certificate and the private key ( redis_client_key ) are exported to the policy of the Flask service S S . The policy for this looks like this: secrets: - name: redis_key kind: private-key - name: redis # automatically generate Redis server certificate kind: x509 private_key: redis_key issuer: redis_ca_cert dns: - redis - name: redis_client_key kind: private-key export: - session: $FLASK_SESSION - name: redis_client_cert # automatically generate client certificate kind: x509 issuer: redis_ca_cert private_key: redis_client_key export: - session: $FLASK_SESSION # export client cert/key to client session - name: redis_ca_key kind: private-key - name: redis_ca_cert # export session CA certificate as Redis CA certificate kind: x509-ca private_key: redis_ca_key export: - session: $FLASK_SESSION # export the session CA certificate to client session Note that $FLASK_SESSION is replaced by the unique name of the policy of S S . The security policies are in this example on the same SCONE CAS (Configuration and Attestation Service) . In more complex scenarios, the policies could also be stored on separate SCONE CAS instances operated by different entities. The flask service can import the Redis CA certificate, client certificate and private key as follows: secrets: - name: redis_client_key import: session: $REDIS_SESSION secret: redis_client_key - name: redis_client_cert import: session: $REDIS_SESSION secret: redis_client_cert - name: redis_ca_cert import: session: $REDIS_SESSION secret: redis_ca_cert These secrets are made available to the Flask-based service in the filesystem (i.e., files /tls/redis-ca.crt , /tls/client.crt and /tls/client.key ) via the following entries in its security policy: images: - name: flask_restapi_image injection_files: - path: /tls/redis-ca.crt content: $$SCONE::redis_ca_cert.chain$$ - path: /tls/client.crt content: $$SCONE::redis_client_cert.crt$$ - path: /tls/client.key content: $$SCONE::redis_client_cert.key$$ Note that before uploading a policy to SCONE CAS, one first attests that one indeed communicates with a genuine SCONE CAS running inside of a production enclave. This is done with the help of a SCONE CAS CLI .","title":"TLS-based Mutual Attestation"},{"location":"aks/flask_demo/#code","text":"The source code of this example is open source and available on github: git clone https://github.com/scontain/flask_example.git cd flask_example","title":"Code"},{"location":"aks/flask_demo/#run-service-on-local-computer","text":"You can use docker-compose to run this example on your local SGX-enabled computer as follows. You first generate an encrypted image using script create_image.sh . This generates some environment variables that stored in file myenv and are loaded via source myenv . The service and Redis are started with docker-compose up . ./create_image.sh source myenv docker-compose up We use a public instance of SCONE CAS in this example.","title":"Run Service On Local Computer"},{"location":"aks/flask_demo/#testing-the-service","text":"Retrieve the API certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api:4996 curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /patient/patient_3 --resolve api:4996:127.0.0.1 curl --cacert cacert.pem -X GET ${ URL } /score/patient_3 --resolve api:4996:127.0.0.1 The output might look as follows: $ curl --cacert cacert.pem -X POST https://localhost:4996/patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/patient/patient_3 --resolve api:4996:127.0.0.1 {\"address\":\"'123 Main Street'\",\"city\":\"Richmond\",\"contactphone\":\"123-234-3456\",\"dateadmitted\":\"01/05/2010\",\"dob\":\"01/01/2010\",\"drugallergies\":\"'Sulpha, Penicillin, Tree Nut'\",\"email\":\"nr@aaa.com\",\"fname\":\"Jane\",\"id\":\"patient_3\",\"insurancedetails\":\"'Primera Blue Cross'\",\"lname\":\"Doe\",\"preexistingconditions\":\"'diabetes, hypertension, asthma'\",\"score\":0.1168424489618366,\"ssn\":\"123-223-2345\",\"state\":\"Washington\"} $ curl --cacert cacert.pem -X GET localhost:4996/score/patient_3 --resolve api:4996:127.0.0.1 {\"id\":\"patient_3\",\"score\":0.2781606437899131}","title":"Testing the service"},{"location":"aks/flask_demo/#execution-on-a-kubernetes-cluster-and-aks","text":"You can run this example on a Kubernetes cluster or Azure Kubernetes Service (AKS).","title":"Execution on a Kubernetes Cluster and AKS"},{"location":"aks/flask_demo/#install-scone-services","text":"Get access to SconeApps (see https://sconedocs.github.io/helm/ ): helm repo add sconeapps https:// ${ GH_TOKEN } @raw.githubusercontent.com/scontain/sconeapps/master/ helm repo update Give SconeApps access to the private docker images (see the helm docs ): export SCONE_HUB_USERNAME = ... export SCONE_HUB_ACCESS_TOKEN = ... export SCONE_HUB_EMAIL = ... kubectl create secret docker-registry sconeapps --docker-server = registry.scontain.com:5050 --docker-username = $SCONE_HUB_USERNAME --docker-password = $SCONE_HUB_ACCESS_TOKEN --docker-email = $SCONE_HUB_EMAIL Start LAS and CAS service: helm install las sconeapps/las --set service.hostPort = true helm install cas sconeapps/cas Install the SGX device plugin for Kubernetes: helm install sgxdevplugin sconeapps/sgxdevplugin","title":"Install SCONE services"},{"location":"aks/flask_demo/#run-the-application","text":"Start by creating a Docker image and setting its name. Remember to specify a repository to which you are allowed to push: export IMAGE = registry.scontain.com:5050/sconecuratedimages/application:v0.4 # please change to an image that you can push ./create_image.sh source myenv docker push $IMAGE Use the Helm chart in deploy/helm to deploy the application to a Kubernetes cluster. helm install api-v1 deploy/helm \\ --set image = $IMAGE \\ --set scone.cas = $SCONE_CAS_ADDR \\ --set scone.flask_session = $FLASK_SESSION /flask_restapi \\ --set scone.redis_session = $REDIS_SESSION /redis \\ --set service.type = LoadBalancer NOTE : Setting service.type=LoadBalancer will allow the application to get traffic from the internet (through a managed LoadBalancer).","title":"Run the application"},{"location":"aks/flask_demo/#test-the-application","text":"After all resources are Running , you can test the API via Helm: helm test api-v1 Helm will run a pod with a couple of pre-set queries to check if the API is working properly.","title":"Test the application"},{"location":"aks/flask_demo/#access-the-application","text":"If the application is exposed to the world through a service of type LoadBalancer, you can retrieve its CA certificate from CAS: source myenv curl -k -X GET \"https:// ${ SCONE_CAS_ADDR -cas } :8081/v1/values/session= $FLASK_SESSION \" | jq -r .values.api_ca_cert.value > cacert.pem Retrieve the service public IP address: export SERVICE_IP = $( kubectl get svc --namespace default api-v1-example --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) Since the API certificates are issued to the host name \"api\", we have to use it. You can rely on cURL's --resolve option to point to the actual address (you can also edit your /etc/hosts file). export URL = https://api Now you can perform queries such as: curl --cacert cacert.pem -X POST ${ URL } /patient/patient_3 -d \"fname=Jane&lname=Doe&address='123 Main Street'&city=Richmond&state=Washington&ssn=123-223-2345&email=nr@aaa.com&dob=01/01/2010&contactphone=123-234-3456&drugallergies='Sulpha, Penicillin, Tree Nut'&preexistingconditions='diabetes, hypertension, asthma'&dateadmitted=01/05/2010&insurancedetails='Primera Blue Cross'\" --resolve api:443: ${ SERVICE_IP }","title":"Access the application"},{"location":"aks/flask_demo/#clean-up","text":"helm delete cas helm delete las helm delete sgxdevplugin helm delete api-v1 kubectl delete pod api-v1-example-test-api Next, we introduce the different sconified Python versions that we support.","title":"Clean up"},{"location":"cc-telenovela/","text":"Telenovela: Confidential Computing with SCONE Confidential Computing not only provides better security , i.e., enforces the confidentiality and integrity of data, code and secrets but it also facilitates the use of new solutions and services . The main advantage of confidential computing is that it permits to outsource services to external entities while one still keeps control of one's data, code and secrets. We explain what problems one can solve and also what new services one can offer with the help of a telenovela 1 on Confidential computing with SCONE . The main actors of our telenovela are Alice and Bob and their friend Mallory . Actually, Mallory is mainly focused on her own advantage. Hence, Alice and Bob need to be careful to ensure that they are not take advantage of by Mallory. The beginning Our first episode shows that reaching an agreement between entities that do not (yet) trust each other is difficult. Our protagonists solve this problem with the help of SCONE confidential computing: one can establish trust between parties with the help of SCONE's security policies. Your browser does not support the video tag. The first issue Like in every real relationship, Alice and Bob experience their first issue that they need to deal with. Luckily - both being technically skilled - they find a good solution for their first issue. With the help of SCONE one can achieve confidential caching and confidential edge computing in general: Your browser does not support the video tag. Their friends Alice and Bob have lots of friends. As we learn, this can cause some issues and not all of them can be trusted. Luckily, with SCONE Confidential Computing one can delegate the management of services without giving up control: Your browser does not support the video tag. Mallory In the latest episode is centered around Mallory who spreads some lies about poor Bob. Luckily, SCONE helps to disproves these lies. Your browser does not support the video tag. Home Office In the newest episode, we show how SCONE helps Alice and Bob to protect their teleconferences. Some modern teleconference systems record, by default, all conversations on disk, i.e., without pressing the record button. SCONE helps to protect the teleconferences from nosy cloud admins, like Mallory, who has access to the servers that run the teleconference software. Your browser does not support the video tag. A soap opera consisting of self-contained episodes. \u21a9","title":"CC Soap Opera"},{"location":"cc-telenovela/#telenovela-confidential-computing-with-scone","text":"Confidential Computing not only provides better security , i.e., enforces the confidentiality and integrity of data, code and secrets but it also facilitates the use of new solutions and services . The main advantage of confidential computing is that it permits to outsource services to external entities while one still keeps control of one's data, code and secrets. We explain what problems one can solve and also what new services one can offer with the help of a telenovela 1 on Confidential computing with SCONE . The main actors of our telenovela are Alice and Bob and their friend Mallory . Actually, Mallory is mainly focused on her own advantage. Hence, Alice and Bob need to be careful to ensure that they are not take advantage of by Mallory.","title":"Telenovela: Confidential Computing with SCONE"},{"location":"cc-telenovela/#the-beginning","text":"Our first episode shows that reaching an agreement between entities that do not (yet) trust each other is difficult. Our protagonists solve this problem with the help of SCONE confidential computing: one can establish trust between parties with the help of SCONE's security policies. Your browser does not support the video tag.","title":"The beginning"},{"location":"cc-telenovela/#the-first-issue","text":"Like in every real relationship, Alice and Bob experience their first issue that they need to deal with. Luckily - both being technically skilled - they find a good solution for their first issue. With the help of SCONE one can achieve confidential caching and confidential edge computing in general: Your browser does not support the video tag.","title":"The first issue"},{"location":"cc-telenovela/#their-friends","text":"Alice and Bob have lots of friends. As we learn, this can cause some issues and not all of them can be trusted. Luckily, with SCONE Confidential Computing one can delegate the management of services without giving up control: Your browser does not support the video tag.","title":"Their friends"},{"location":"cc-telenovela/#mallory","text":"In the latest episode is centered around Mallory who spreads some lies about poor Bob. Luckily, SCONE helps to disproves these lies. Your browser does not support the video tag.","title":"Mallory"},{"location":"cc-telenovela/#home-office","text":"In the newest episode, we show how SCONE helps Alice and Bob to protect their teleconferences. Some modern teleconference systems record, by default, all conversations on disk, i.e., without pressing the record button. SCONE helps to protect the teleconferences from nosy cloud admins, like Mallory, who has access to the servers that run the teleconference software. Your browser does not support the video tag. A soap opera consisting of self-contained episodes. \u21a9","title":"Home Office"},{"location":"cc-university/","text":"Confidential Computing University As part of our CC University* , we will present a sequence of screencasts introducing confidential computing with the help of SCONE. The target audience of these screencasts are all that want to understand the concepts of confidential computing . These will be taught as part of the lectures. We will also include screencasts that focus on how to implement these concepts with the help of SCONE. This is an alpha release This is a very first version of an upcoming university course on confidential computing. The hope is that this is already useful despite not being very polished yet. The content and presentation will be iterated within the next months. We plan to use some nice animations and a pleasant AI speaker instead. Ideally, eventually this will look as our CC Telenovela but until then, we would be happy to get constructive feedback on how to improve the content. Motivation The first episode focuses on the protection objectives , i.e., protecting confidentiality and integrity of data , code and secrets . We need to defend against a very strong adversary who has control of the infrastructure, e.g., the adversary has root access on the servers. For example, if we outsource the management of our computing infrastructure to external providers, we might need to expect such an adversary. Even if we use our own computing infrastructure to host some valuable data, we might also facing some powerful adversary. Your browser does not support the video tag. The next question that we address is on why do we need confidential computing at all? Don't we already know how to build secure systems and should we not focus on system security anyhow? If we need to outsource the management of resources to external providers, we need a new approach as confidential computing . Your browser does not support the video tag. While we use the term confidential computing , we actually prefer the term application-oriented security to emphasize that we protect applications - even against powerful adversaries that have hardware and root access on the machines that we run our applications on.","title":"CC University"},{"location":"cc-university/#confidential-computing-university","text":"As part of our CC University* , we will present a sequence of screencasts introducing confidential computing with the help of SCONE. The target audience of these screencasts are all that want to understand the concepts of confidential computing . These will be taught as part of the lectures. We will also include screencasts that focus on how to implement these concepts with the help of SCONE. This is an alpha release This is a very first version of an upcoming university course on confidential computing. The hope is that this is already useful despite not being very polished yet. The content and presentation will be iterated within the next months. We plan to use some nice animations and a pleasant AI speaker instead. Ideally, eventually this will look as our CC Telenovela but until then, we would be happy to get constructive feedback on how to improve the content.","title":"Confidential Computing University"},{"location":"cc-university/#motivation","text":"The first episode focuses on the protection objectives , i.e., protecting confidentiality and integrity of data , code and secrets . We need to defend against a very strong adversary who has control of the infrastructure, e.g., the adversary has root access on the servers. For example, if we outsource the management of our computing infrastructure to external providers, we might need to expect such an adversary. Even if we use our own computing infrastructure to host some valuable data, we might also facing some powerful adversary. Your browser does not support the video tag. The next question that we address is on why do we need confidential computing at all? Don't we already know how to build secure systems and should we not focus on system security anyhow? If we need to outsource the management of resources to external providers, we need a new approach as confidential computing . Your browser does not support the video tag. While we use the term confidential computing , we actually prefer the term application-oriented security to emphasize that we protect applications - even against powerful adversaries that have hardware and root access on the machines that we run our applications on.","title":"Motivation"}]}